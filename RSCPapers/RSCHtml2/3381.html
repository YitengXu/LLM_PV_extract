<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE html><html xmlns="http://www.w3.org/1999/xhtml" xmlns:art="http://www.rsc.org/schema/rscart38" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:rsc="urn:rsc.org" xml:lang="en" lang="en"><head><!--Google Tag Manager--><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&amp;l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-56FZ7G');</script><!--End Google Tag Manager--><title>Quantum machine learning for chemistry and physics  - Chemical Society Reviews (RSC Publishing) DOI:10.1039/D2CS00203E</title><link rel="canonical" href="https://pubs.rsc.org/en/content/articlehtml/2022/cs/d2cs00203e"/><meta http-equiv="content-type" content="application/xhtml+xml; charset=utf-8"/><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info = {"beacon":"bam.nr-data.net","errorBeacon":"bam.nr-data.net","licenseKey":"NRJS-aaa897feae8feeca979","applicationID":"1313546638","transactionName":"M1wANxQFCEcDVU0PWgoWLzUlSyVbDEJcCEEnVgwXFAsKWAdEFgdHEFABDwMMElkO","queueTime":0,"applicationTime":92,"agent":"","atts":""}</script><script type="text/javascript">(window.NREUM||(NREUM={})).init={privacy:{cookies_enabled:true},ajax:{deny_list:["bam.nr-data.net"]},distributed_tracing:{enabled:true}};(window.NREUM||(NREUM={})).loader_config={agentID:"1386013924",accountID:"2851366",trustKey:"1029994",xpid:"Vg4CUFVVDhABV1BRAgUBUFcJ",licenseKey:"NRJS-aaa897feae8feeca979",applicationID:"1313546638"};;/*! For license information please see nr-loader-spa-1.236.0.min.js.LICENSE.txt */
(()=>{"use strict";var e,t,r={5763:(e,t,r)=>{r.d(t,{P_:()=>l,Mt:()=>g,C5:()=>s,DL:()=>v,OP:()=>T,lF:()=>D,Yu:()=>y,Dg:()=>h,CX:()=>c,GE:()=>b,sU:()=>_});var n=r(8632),i=r(9567);const o={beacon:n.ce.beacon,errorBeacon:n.ce.errorBeacon,licenseKey:void 0,applicationID:void 0,sa:void 0,queueTime:void 0,applicationTime:void 0,ttGuid:void 0,user:void 0,account:void 0,product:void 0,extra:void 0,jsAttributes:{},userAttributes:void 0,atts:void 0,transactionName:void 0,tNamePlain:void 0},a={};function s(e){if(!e)throw new Error("All info objects require an agent identifier!");if(!a[e])throw new Error("Info for ".concat(e," was never set"));return a[e]}function c(e,t){if(!e)throw new Error("All info objects require an agent identifier!");a[e]=(0,i.D)(t,o),(0,n.Qy)(e,a[e],"info")}var u=r(7056);const d=()=>{const e={blockSelector:"[data-nr-block]",maskInputOptions:{password:!0}};return{allow_bfcache:!0,privacy:{cookies_enabled:!0},ajax:{deny_list:void 0,enabled:!0,harvestTimeSeconds:10},distributed_tracing:{enabled:void 0,exclude_newrelic_header:void 0,cors_use_newrelic_header:void 0,cors_use_tracecontext_headers:void 0,allowed_origins:void 0},session:{domain:void 0,expiresMs:u.oD,inactiveMs:u.Hb},ssl:void 0,obfuscate:void 0,jserrors:{enabled:!0,harvestTimeSeconds:10},metrics:{enabled:!0},page_action:{enabled:!0,harvestTimeSeconds:30},page_view_event:{enabled:!0},page_view_timing:{enabled:!0,harvestTimeSeconds:30,long_task:!1},session_trace:{enabled:!0,harvestTimeSeconds:10},harvest:{tooManyRequestsDelay:60},session_replay:{enabled:!1,harvestTimeSeconds:60,sampleRate:.1,errorSampleRate:.1,maskTextSelector:"*",maskAllInputs:!0,get blockClass(){return"nr-block"},get ignoreClass(){return"nr-ignore"},get maskTextClass(){return"nr-mask"},get blockSelector(){return e.blockSelector},set blockSelector(t){e.blockSelector+=",".concat(t)},get maskInputOptions(){return e.maskInputOptions},set maskInputOptions(t){e.maskInputOptions={...t,password:!0}}},spa:{enabled:!0,harvestTimeSeconds:10}}},f={};function l(e){if(!e)throw new Error("All configuration objects require an agent identifier!");if(!f[e])throw new Error("Configuration for ".concat(e," was never set"));return f[e]}function h(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");f[e]=(0,i.D)(t,d()),(0,n.Qy)(e,f[e],"config")}function g(e,t){if(!e)throw new Error("All configuration objects require an agent identifier!");var r=l(e);if(r){for(var n=t.split("."),i=0;i<n.length-1;i++)if("object"!=typeof(r=r[n[i]]))return;r=r[n[n.length-1]]}return r}const p={accountID:void 0,trustKey:void 0,agentID:void 0,licenseKey:void 0,applicationID:void 0,xpid:void 0},m={};function v(e){if(!e)throw new Error("All loader-config objects require an agent identifier!");if(!m[e])throw new Error("LoaderConfig for ".concat(e," was never set"));return m[e]}function b(e,t){if(!e)throw new Error("All loader-config objects require an agent identifier!");m[e]=(0,i.D)(t,p),(0,n.Qy)(e,m[e],"loader_config")}const y=(0,n.mF)().o;var w=r(385),x=r(6818);const A={buildEnv:x.Re,bytesSent:{},queryBytesSent:{},customTransaction:void 0,disabled:!1,distMethod:x.gF,isolatedBacklog:!1,loaderType:void 0,maxBytes:3e4,offset:Math.floor(w._A?.performance?.timeOrigin||w._A?.performance?.timing?.navigationStart||Date.now()),onerror:void 0,origin:""+w._A.location,ptid:void 0,releaseIds:{},session:void 0,xhrWrappable:"function"==typeof w._A.XMLHttpRequest?.prototype?.addEventListener,version:x.q4},E={};function T(e){if(!e)throw new Error("All runtime objects require an agent identifier!");if(!E[e])throw new Error("Runtime for ".concat(e," was never set"));return E[e]}function _(e,t){if(!e)throw new Error("All runtime objects require an agent identifier!");E[e]=(0,i.D)(t,A),(0,n.Qy)(e,E[e],"runtime")}function D(e){return function(e){try{const t=s(e);return!!t.licenseKey&&!!t.errorBeacon&&!!t.applicationID}catch(e){return!1}}(e)}},9567:(e,t,r)=>{r.d(t,{D:()=>i});var n=r(50);function i(e,t){try{if(!e||"object"!=typeof e)return(0,n.Z)("Setting a Configurable requires an object as input");if(!t||"object"!=typeof t)return(0,n.Z)("Setting a Configurable requires a model to set its initial properties");const r=Object.create(Object.getPrototypeOf(t),Object.getOwnPropertyDescriptors(t)),o=0===Object.keys(r).length?e:r;for(let a in o)if(void 0!==e[a])try{"object"==typeof e[a]&&"object"==typeof t[a]?r[a]=i(e[a],t[a]):r[a]=e[a]}catch(e){(0,n.Z)("An error occurred while setting a property of a Configurable",e)}return r}catch(e){(0,n.Z)("An error occured while setting a Configurable",e)}}},6818:(e,t,r)=>{r.d(t,{Re:()=>i,gF:()=>o,q4:()=>n});const n="1.236.0",i="PROD",o="CDN"},385:(e,t,r)=>{r.d(t,{FN:()=>a,IF:()=>u,Nk:()=>f,Tt:()=>s,_A:()=>o,il:()=>n,pL:()=>c,v6:()=>i,w1:()=>d});const n="undefined"!=typeof window&&!!window.document,i="undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self.navigator instanceof WorkerNavigator||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis.navigator instanceof WorkerNavigator),o=n?window:"undefined"!=typeof WorkerGlobalScope&&("undefined"!=typeof self&&self instanceof WorkerGlobalScope&&self||"undefined"!=typeof globalThis&&globalThis instanceof WorkerGlobalScope&&globalThis),a=""+o?.location,s=/iPad|iPhone|iPod/.test(navigator.userAgent),c=s&&"undefined"==typeof SharedWorker,u=(()=>{const e=navigator.userAgent.match(/Firefox[/\s](\d+\.\d+)/);return Array.isArray(e)&&e.length>=2?+e[1]:0})(),d=Boolean(n&&window.document.documentMode),f=!!navigator.sendBeacon},1117:(e,t,r)=>{r.d(t,{w:()=>o});var n=r(50);const i={agentIdentifier:"",ee:void 0};class o{constructor(e){try{if("object"!=typeof e)return(0,n.Z)("shared context requires an object as input");this.sharedContext={},Object.assign(this.sharedContext,i),Object.entries(e).forEach((e=>{let[t,r]=e;Object.keys(i).includes(t)&&(this.sharedContext[t]=r)}))}catch(e){(0,n.Z)("An error occured while setting SharedContext",e)}}}},8e3:(e,t,r)=>{r.d(t,{L:()=>d,R:()=>c});var n=r(2177),i=r(1284),o=r(4322),a=r(3325);const s={};function c(e,t){const r={staged:!1,priority:a.p[t]||0};u(e),s[e].get(t)||s[e].set(t,r)}function u(e){e&&(s[e]||(s[e]=new Map))}function d(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:"",t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:"feature";if(u(e),!e||!s[e].get(t))return a(t);s[e].get(t).staged=!0;const r=[...s[e]];function a(t){const r=e?n.ee.get(e):n.ee,a=o.X.handlers;if(r.backlog&&a){var s=r.backlog[t],c=a[t];if(c){for(var u=0;s&&u<s.length;++u)f(s[u],c);(0,i.D)(c,(function(e,t){(0,i.D)(t,(function(t,r){r[0].on(e,r[1])}))}))}delete a[t],r.backlog[t]=null,r.emit("drain-"+t,[])}}r.every((e=>{let[t,r]=e;return r.staged}))&&(r.sort(((e,t)=>e[1].priority-t[1].priority)),r.forEach((e=>{let[t]=e;a(t)})))}function f(e,t){var r=e[1];(0,i.D)(t[r],(function(t,r){var n=e[0];if(r[0]===n){var i=r[1],o=e[3],a=e[2];i.apply(o,a)}}))}},2177:(e,t,r)=>{r.d(t,{c:()=>f,ee:()=>u});var n=r(8632),i=r(2210),o=r(1284),a=r(5763),s="nr@context";let c=(0,n.fP)();var u;function d(){}function f(e){return(0,i.X)(e,s,l)}function l(){return new d}function h(){u.aborted=!0,u.backlog={}}c.ee?u=c.ee:(u=function e(t,r){var n={},c={},f={},g=!1;try{g=16===r.length&&(0,a.OP)(r).isolatedBacklog}catch(e){}var p={on:b,addEventListener:b,removeEventListener:y,emit:v,get:x,listeners:w,context:m,buffer:A,abort:h,aborted:!1,isBuffering:E,debugId:r,backlog:g?{}:t&&"object"==typeof t.backlog?t.backlog:{}};return p;function m(e){return e&&e instanceof d?e:e?(0,i.X)(e,s,l):l()}function v(e,r,n,i,o){if(!1!==o&&(o=!0),!u.aborted||i){t&&o&&t.emit(e,r,n);for(var a=m(n),s=w(e),d=s.length,f=0;f<d;f++)s[f].apply(a,r);var l=T()[c[e]];return l&&l.push([p,e,r,a]),a}}function b(e,t){n[e]=w(e).concat(t)}function y(e,t){var r=n[e];if(r)for(var i=0;i<r.length;i++)r[i]===t&&r.splice(i,1)}function w(e){return n[e]||[]}function x(t){return f[t]=f[t]||e(p,t)}function A(e,t){var r=T();p.aborted||(0,o.D)(e,(function(e,n){t=t||"feature",c[n]=t,t in r||(r[t]=[])}))}function E(e){return!!T()[c[e]]}function T(){return p.backlog}}(void 0,"globalEE"),c.ee=u)},5546:(e,t,r)=>{r.d(t,{E:()=>n,p:()=>i});var n=r(2177).ee.get("handle");function i(e,t,r,i,o){o?(o.buffer([e],i),o.emit(e,t,r)):(n.buffer([e],i),n.emit(e,t,r))}},4322:(e,t,r)=>{r.d(t,{X:()=>o});var n=r(5546);o.on=a;var i=o.handlers={};function o(e,t,r,o){a(o||n.E,i,e,t,r)}function a(e,t,r,i,o){o||(o="feature"),e||(e=n.E);var a=t[o]=t[o]||{};(a[r]=a[r]||[]).push([e,i])}},3239:(e,t,r)=>{r.d(t,{bP:()=>s,iz:()=>c,m$:()=>a});var n=r(385);let i=!1,o=!1;try{const e={get passive(){return i=!0,!1},get signal(){return o=!0,!1}};n._A.addEventListener("test",null,e),n._A.removeEventListener("test",null,e)}catch(e){}function a(e,t){return i||o?{capture:!!e,passive:i,signal:t}:!!e}function s(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;window.addEventListener(e,t,a(r,n))}function c(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2],n=arguments.length>3?arguments[3]:void 0;document.addEventListener(e,t,a(r,n))}},4402:(e,t,r)=>{r.d(t,{Ht:()=>u,M:()=>c,Rl:()=>a,ky:()=>s});var n=r(385);const i="xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx";function o(e,t){return e?15&e[t]:16*Math.random()|0}function a(){const e=n._A?.crypto||n._A?.msCrypto;let t,r=0;return e&&e.getRandomValues&&(t=e.getRandomValues(new Uint8Array(31))),i.split("").map((e=>"x"===e?o(t,++r).toString(16):"y"===e?(3&o()|8).toString(16):e)).join("")}function s(e){const t=n._A?.crypto||n._A?.msCrypto;let r,i=0;t&&t.getRandomValues&&(r=t.getRandomValues(new Uint8Array(31)));const a=[];for(var s=0;s<e;s++)a.push(o(r,++i).toString(16));return a.join("")}function c(){return s(16)}function u(){return s(32)}},7056:(e,t,r)=>{r.d(t,{Bq:()=>n,Hb:()=>o,oD:()=>i});const n="NRBA",i=144e5,o=18e5},7894:(e,t,r)=>{function n(){return Math.round(performance.now())}r.d(t,{z:()=>n})},7243:(e,t,r)=>{r.d(t,{e:()=>o});var n=r(385),i={};function o(e){if(e in i)return i[e];if(0===(e||"").indexOf("data:"))return{protocol:"data"};let t;var r=n._A?.location,o={};if(n.il)t=document.createElement("a"),t.href=e;else try{t=new URL(e,r.href)}catch(e){return o}o.port=t.port;var a=t.href.split("://");!o.port&&a[1]&&(o.port=a[1].split("/")[0].split("@").pop().split(":")[1]),o.port&&"0"!==o.port||(o.port="https"===a[0]?"443":"80"),o.hostname=t.hostname||r.hostname,o.pathname=t.pathname,o.protocol=a[0],"/"!==o.pathname.charAt(0)&&(o.pathname="/"+o.pathname);var s=!t.protocol||":"===t.protocol||t.protocol===r.protocol,c=t.hostname===r.hostname&&t.port===r.port;return o.sameOrigin=s&&(!t.hostname||c),"/"===o.pathname&&(i[e]=o),o}},50:(e,t,r)=>{function n(e,t){"function"==typeof console.warn&&(console.warn("New Relic: ".concat(e)),t&&console.warn(t))}r.d(t,{Z:()=>n})},2587:(e,t,r)=>{r.d(t,{N:()=>c,T:()=>u});var n=r(2177),i=r(5546),o=r(8e3),a=r(3325);const s={stn:[a.D.sessionTrace],err:[a.D.jserrors,a.D.metrics],ins:[a.D.pageAction],spa:[a.D.spa],sr:[a.D.sessionReplay,a.D.sessionTrace]};function c(e,t){const r=n.ee.get(t);e&&"object"==typeof e&&(Object.entries(e).forEach((e=>{let[t,n]=e;void 0===u[t]&&(s[t]?s[t].forEach((e=>{n?(0,i.p)("feat-"+t,[],void 0,e,r):(0,i.p)("block-"+t,[],void 0,e,r),(0,i.p)("rumresp-"+t,[Boolean(n)],void 0,e,r)})):n&&(0,i.p)("feat-"+t,[],void 0,void 0,r),u[t]=Boolean(n))})),Object.keys(s).forEach((e=>{void 0===u[e]&&(s[e]?.forEach((t=>(0,i.p)("rumresp-"+e,[!1],void 0,t,r))),u[e]=!1)})),(0,o.L)(t,a.D.pageViewEvent))}const u={}},2210:(e,t,r)=>{r.d(t,{X:()=>i});var n=Object.prototype.hasOwnProperty;function i(e,t,r){if(n.call(e,t))return e[t];var i=r();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,t,{value:i,writable:!0,enumerable:!1}),i}catch(e){}return e[t]=i,i}},1284:(e,t,r)=>{r.d(t,{D:()=>n});const n=(e,t)=>Object.entries(e||{}).map((e=>{let[r,n]=e;return t(r,n)}))},4351:(e,t,r)=>{r.d(t,{P:()=>o});var n=r(2177);const i=()=>{const e=new WeakSet;return(t,r)=>{if("object"==typeof r&&null!==r){if(e.has(r))return;e.add(r)}return r}};function o(e){try{return JSON.stringify(e,i())}catch(e){try{n.ee.emit("internal-error",[e])}catch(e){}}}},3960:(e,t,r)=>{r.d(t,{K:()=>a,b:()=>o});var n=r(3239);function i(){return"undefined"==typeof document||"complete"===document.readyState}function o(e,t){if(i())return e();(0,n.bP)("load",e,t)}function a(e){if(i())return e();(0,n.iz)("DOMContentLoaded",e)}},8632:(e,t,r)=>{r.d(t,{EZ:()=>u,Qy:()=>c,ce:()=>o,fP:()=>a,gG:()=>d,mF:()=>s});var n=r(7894),i=r(385);const o={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net"};function a(){return i._A.NREUM||(i._A.NREUM={}),void 0===i._A.newrelic&&(i._A.newrelic=i._A.NREUM),i._A.NREUM}function s(){let e=a();return e.o||(e.o={ST:i._A.setTimeout,SI:i._A.setImmediate,CT:i._A.clearTimeout,XHR:i._A.XMLHttpRequest,REQ:i._A.Request,EV:i._A.Event,PR:i._A.Promise,MO:i._A.MutationObserver,FETCH:i._A.fetch}),e}function c(e,t,r){let i=a();const o=i.initializedAgents||{},s=o[e]||{};return Object.keys(s).length||(s.initializedAt={ms:(0,n.z)(),date:new Date}),i.initializedAgents={...o,[e]:{...s,[r]:t}},i}function u(e,t){a()[e]=t}function d(){return function(){let e=a();const t=e.info||{};e.info={beacon:o.beacon,errorBeacon:o.errorBeacon,...t}}(),function(){let e=a();const t=e.init||{};e.init={...t}}(),s(),function(){let e=a();const t=e.loader_config||{};e.loader_config={...t}}(),a()}},7956:(e,t,r)=>{r.d(t,{N:()=>i});var n=r(3239);function i(e){let t=arguments.length>1&&void 0!==arguments[1]&&arguments[1],r=arguments.length>2?arguments[2]:void 0,i=arguments.length>3?arguments[3]:void 0;return void(0,n.iz)("visibilitychange",(function(){if(t)return void("hidden"==document.visibilityState&&e());e(document.visibilityState)}),r,i)}},1214:(e,t,r)=>{r.d(t,{em:()=>v,u5:()=>N,QU:()=>S,_L:()=>I,Gm:()=>L,Lg:()=>M,gy:()=>U,BV:()=>Q,Kf:()=>ee});var n=r(2177);const i="nr@original";var o=Object.prototype.hasOwnProperty,a=!1;function s(e,t){return e||(e=n.ee),r.inPlace=function(e,t,n,i,o){n||(n="");var a,s,c,u="-"===n.charAt(0);for(c=0;c<t.length;c++)d(a=e[s=t[c]])||(e[s]=r(a,u?s+n:n,i,s,o))},r.flag=i,r;function r(t,r,n,a,s){return d(t)?t:(r||(r=""),nrWrapper[i]=t,u(t,nrWrapper,e),nrWrapper);function nrWrapper(){var i,u,d,f;try{u=this,i=[...arguments],d="function"==typeof n?n(i,u):n||{}}catch(t){c([t,"",[i,u,a],d],e)}o(r+"start",[i,u,a],d,s);try{return f=t.apply(u,i)}catch(e){throw o(r+"err",[i,u,e],d,s),e}finally{o(r+"end",[i,u,f],d,s)}}}function o(r,n,i,o){if(!a||t){var s=a;a=!0;try{e.emit(r,n,i,t,o)}catch(t){c([t,r,n,i],e)}a=s}}}function c(e,t){t||(t=n.ee);try{t.emit("internal-error",e)}catch(e){}}function u(e,t,r){if(Object.defineProperty&&Object.keys)try{return Object.keys(e).forEach((function(r){Object.defineProperty(t,r,{get:function(){return e[r]},set:function(t){return e[r]=t,t}})})),t}catch(e){c([e],r)}for(var n in e)o.call(e,n)&&(t[n]=e[n]);return t}function d(e){return!(e&&e instanceof Function&&e.apply&&!e[i])}var f=r(2210),l=r(385);const h={},g=XMLHttpRequest,p="addEventListener",m="removeEventListener";function v(e){var t=function(e){return(e||n.ee).get("events")}(e);if(h[t.debugId]++)return t;h[t.debugId]=1;var r=s(t,!0);function i(e){r.inPlace(e,[p,m],"-",o)}function o(e,t){return e[1]}return"getPrototypeOf"in Object&&(l.il&&b(document,i),b(l._A,i),b(g.prototype,i)),t.on(p+"-start",(function(e,t){var n=e[1];if(null!==n&&("function"==typeof n||"object"==typeof n)){var i=(0,f.X)(n,"nr@wrapped",(function(){var e={object:function(){if("function"!=typeof n.handleEvent)return;return n.handleEvent.apply(n,arguments)},function:n}[typeof n];return e?r(e,"fn-",null,e.name||"anonymous"):n}));this.wrapped=e[1]=i}})),t.on(m+"-start",(function(e){e[1]=this.wrapped||e[1]})),t}function b(e,t){let r=e;for(;"object"==typeof r&&!Object.prototype.hasOwnProperty.call(r,p);)r=Object.getPrototypeOf(r);for(var n=arguments.length,i=new Array(n>2?n-2:0),o=2;o<n;o++)i[o-2]=arguments[o];r&&t(r,...i)}var y="fetch-",w=y+"body-",x=["arrayBuffer","blob","json","text","formData"],A=l._A.Request,E=l._A.Response,T="prototype",_="nr@context";const D={};function N(e){const t=function(e){return(e||n.ee).get("fetch")}(e);if(!(A&&E&&l._A.fetch))return t;if(D[t.debugId]++)return t;function r(e,r,n){var i=e[r];"function"==typeof i&&(e[r]=function(){var e,r=[...arguments],o={};t.emit(n+"before-start",[r],o),o[_]&&o[_].dt&&(e=o[_].dt);var a=i.apply(this,r);return t.emit(n+"start",[r,e],a),a.then((function(e){return t.emit(n+"end",[null,e],a),e}),(function(e){throw t.emit(n+"end",[e],a),e}))})}return D[t.debugId]=1,x.forEach((e=>{r(A[T],e,w),r(E[T],e,w)})),r(l._A,"fetch",y),t.on(y+"end",(function(e,r){var n=this;if(r){var i=r.headers.get("content-length");null!==i&&(n.rxSize=i),t.emit(y+"done",[null,r],n)}else t.emit(y+"done",[e],n)})),t}const O={},j=["pushState","replaceState"];function S(e){const t=function(e){return(e||n.ee).get("history")}(e);return!l.il||O[t.debugId]++||(O[t.debugId]=1,s(t).inPlace(window.history,j,"-")),t}var P=r(3239);const C={},R=["appendChild","insertBefore","replaceChild"];function I(e){const t=function(e){return(e||n.ee).get("jsonp")}(e);if(!l.il||C[t.debugId])return t;C[t.debugId]=!0;var r=s(t),i=/[?&](?:callback|cb)=([^&#]+)/,o=/(.*)\.([^.]+)/,a=/^(\w+)(\.|$)(.*)$/;function c(e,t){var r=e.match(a),n=r[1],i=r[3];return i?c(i,t[n]):t[n]}return r.inPlace(Node.prototype,R,"dom-"),t.on("dom-start",(function(e){!function(e){if(!e||"string"!=typeof e.nodeName||"script"!==e.nodeName.toLowerCase())return;if("function"!=typeof e.addEventListener)return;var n=(a=e.src,s=a.match(i),s?s[1]:null);var a,s;if(!n)return;var u=function(e){var t=e.match(o);if(t&&t.length>=3)return{key:t[2],parent:c(t[1],window)};return{key:e,parent:window}}(n);if("function"!=typeof u.parent[u.key])return;var d={};function f(){t.emit("jsonp-end",[],d),e.removeEventListener("load",f,(0,P.m$)(!1)),e.removeEventListener("error",l,(0,P.m$)(!1))}function l(){t.emit("jsonp-error",[],d),t.emit("jsonp-end",[],d),e.removeEventListener("load",f,(0,P.m$)(!1)),e.removeEventListener("error",l,(0,P.m$)(!1))}r.inPlace(u.parent,[u.key],"cb-",d),e.addEventListener("load",f,(0,P.m$)(!1)),e.addEventListener("error",l,(0,P.m$)(!1)),t.emit("new-jsonp",[e.src],d)}(e[0])})),t}var k=r(5763);const H={};function L(e){const t=function(e){return(e||n.ee).get("mutation")}(e);if(!l.il||H[t.debugId])return t;H[t.debugId]=!0;var r=s(t),i=k.Yu.MO;return i&&(window.MutationObserver=function(e){return this instanceof i?new i(r(e,"fn-")):i.apply(this,arguments)},MutationObserver.prototype=i.prototype),t}const z={};function M(e){const t=function(e){return(e||n.ee).get("promise")}(e);if(z[t.debugId])return t;z[t.debugId]=!0;var r=n.c,o=s(t),a=k.Yu.PR;return a&&function(){function e(r){var n=t.context(),i=o(r,"executor-",n,null,!1);const s=Reflect.construct(a,[i],e);return t.context(s).getCtx=function(){return n},s}l._A.Promise=e,Object.defineProperty(e,"name",{value:"Promise"}),e.toString=function(){return a.toString()},Object.setPrototypeOf(e,a),["all","race"].forEach((function(r){const n=a[r];e[r]=function(e){let i=!1;[...e||[]].forEach((e=>{this.resolve(e).then(a("all"===r),a(!1))}));const o=n.apply(this,arguments);return o;function a(e){return function(){t.emit("propagate",[null,!i],o,!1,!1),i=i||!e}}}})),["resolve","reject"].forEach((function(r){const n=a[r];e[r]=function(e){const r=n.apply(this,arguments);return e!==r&&t.emit("propagate",[e,!0],r,!1,!1),r}})),e.prototype=a.prototype;const n=a.prototype.then;a.prototype.then=function(){var e=this,i=r(e);i.promise=e;for(var a=arguments.length,s=new Array(a),c=0;c<a;c++)s[c]=arguments[c];s[0]=o(s[0],"cb-",i,null,!1),s[1]=o(s[1],"cb-",i,null,!1);const u=n.apply(this,s);return i.nextPromise=u,t.emit("propagate",[e,!0],u,!1,!1),u},a.prototype.then[i]=n,t.on("executor-start",(function(e){e[0]=o(e[0],"resolve-",this,null,!1),e[1]=o(e[1],"resolve-",this,null,!1)})),t.on("executor-err",(function(e,t,r){e[1](r)})),t.on("cb-end",(function(e,r,n){t.emit("propagate",[n,!0],this.nextPromise,!1,!1)})),t.on("propagate",(function(e,r,n){this.getCtx&&!r||(this.getCtx=function(){if(e instanceof Promise)var r=t.context(e);return r&&r.getCtx?r.getCtx():this})}))}(),t}const B={},F="requestAnimationFrame";function U(e){const t=function(e){return(e||n.ee).get("raf")}(e);if(!l.il||B[t.debugId]++)return t;B[t.debugId]=1;var r=s(t);return r.inPlace(window,[F],"raf-"),t.on("raf-start",(function(e){e[0]=r(e[0],"fn-")})),t}const q={},G="setTimeout",V="setInterval",X="clearTimeout",W="-start",Z="-",$=[G,"setImmediate",V,X,"clearImmediate"];function Q(e){const t=function(e){return(e||n.ee).get("timer")}(e);if(q[t.debugId]++)return t;q[t.debugId]=1;var r=s(t);return r.inPlace(l._A,$.slice(0,2),G+Z),r.inPlace(l._A,$.slice(2,3),V+Z),r.inPlace(l._A,$.slice(3),X+Z),t.on(V+W,(function(e,t,n){e[0]=r(e[0],"fn-",null,n)})),t.on(G+W,(function(e,t,n){this.method=n,this.timerDuration=isNaN(e[1])?0:+e[1],e[0]=r(e[0],"fn-",this,n)})),t}var Y=r(50);const K={},J=["open","send"];function ee(e){var t=e||n.ee;const r=function(e){return(e||n.ee).get("xhr")}(t);if(K[r.debugId]++)return r;K[r.debugId]=1,v(t);var i=s(r),o=k.Yu.XHR,a=k.Yu.MO,c=k.Yu.PR,u=k.Yu.SI,d="readystatechange",f=["onload","onerror","onabort","onloadstart","onloadend","onprogress","ontimeout"],h=[],g=l._A.XMLHttpRequest.listeners,p=l._A.XMLHttpRequest=function(e){var t=new o(e);function n(){try{r.emit("new-xhr",[t],t),t.addEventListener(d,b,(0,P.m$)(!1))}catch(e){(0,Y.Z)("An error occured while intercepting XHR",e);try{r.emit("internal-error",[e])}catch(e){}}}return this.listeners=g?[...g,n]:[n],this.listeners.forEach((e=>e())),t};function m(e,t){i.inPlace(t,["onreadystatechange"],"fn-",E)}function b(){var e=this,t=r.context(e);e.readyState>3&&!t.resolved&&(t.resolved=!0,r.emit("xhr-resolved",[],e)),i.inPlace(e,f,"fn-",E)}if(function(e,t){for(var r in e)t[r]=e[r]}(o,p),p.prototype=o.prototype,i.inPlace(p.prototype,J,"-xhr-",E),r.on("send-xhr-start",(function(e,t){m(e,t),function(e){h.push(e),a&&(y?y.then(A):u?u(A):(w=-w,x.data=w))}(t)})),r.on("open-xhr-start",m),a){var y=c&&c.resolve();if(!u&&!c){var w=1,x=document.createTextNode(w);new a(A).observe(x,{characterData:!0})}}else t.on("fn-end",(function(e){e[0]&&e[0].type===d||A()}));function A(){for(var e=0;e<h.length;e++)m(0,h[e]);h.length&&(h=[])}function E(e,t){return t}return r}},7825:(e,t,r)=>{r.d(t,{t:()=>n});const n=r(3325).D.ajax},6660:(e,t,r)=>{r.d(t,{A:()=>i,t:()=>n});const n=r(3325).D.jserrors,i="nr@seenError"},3081:(e,t,r)=>{r.d(t,{gF:()=>o,mY:()=>i,t9:()=>n,vz:()=>s,xS:()=>a});const n=r(3325).D.metrics,i="sm",o="cm",a="storeSupportabilityMetrics",s="storeEventMetrics"},4649:(e,t,r)=>{r.d(t,{t:()=>n});const n=r(3325).D.pageAction},7633:(e,t,r)=>{r.d(t,{Dz:()=>i,OJ:()=>a,qw:()=>o,t9:()=>n});const n=r(3325).D.pageViewEvent,i="firstbyte",o="domcontent",a="windowload"},9251:(e,t,r)=>{r.d(t,{t:()=>n});const n=r(3325).D.pageViewTiming},3614:(e,t,r)=>{r.d(t,{BST_RESOURCE:()=>i,END:()=>s,FEATURE_NAME:()=>n,FN_END:()=>u,FN_START:()=>c,PUSH_STATE:()=>d,RESOURCE:()=>o,START:()=>a});const n=r(3325).D.sessionTrace,i="bstResource",o="resource",a="-start",s="-end",c="fn"+a,u="fn"+s,d="pushState"},7836:(e,t,r)=>{r.d(t,{BODY:()=>A,CB_END:()=>E,CB_START:()=>u,END:()=>x,FEATURE_NAME:()=>i,FETCH:()=>_,FETCH_BODY:()=>v,FETCH_DONE:()=>m,FETCH_START:()=>p,FN_END:()=>c,FN_START:()=>s,INTERACTION:()=>l,INTERACTION_API:()=>d,INTERACTION_EVENTS:()=>o,JSONP_END:()=>b,JSONP_NODE:()=>g,JS_TIME:()=>T,MAX_TIMER_BUDGET:()=>a,REMAINING:()=>f,SPA_NODE:()=>h,START:()=>w,originalSetTimeout:()=>y});var n=r(5763);const i=r(3325).D.spa,o=["click","submit","keypress","keydown","keyup","change"],a=999,s="fn-start",c="fn-end",u="cb-start",d="api-ixn-",f="remaining",l="interaction",h="spaNode",g="jsonpNode",p="fetch-start",m="fetch-done",v="fetch-body-",b="jsonp-end",y=n.Yu.ST,w="-start",x="-end",A="-body",E="cb"+x,T="jsTime",_="fetch"},5938:(e,t,r)=>{r.d(t,{W:()=>o});var n=r(5763),i=r(2177);class o{constructor(e,t,r){this.agentIdentifier=e,this.aggregator=t,this.ee=i.ee.get(e,(0,n.OP)(this.agentIdentifier).isolatedBacklog),this.featureName=r,this.blocked=!1}}},9144:(e,t,r)=>{r.d(t,{j:()=>m});var n=r(3325),i=r(5763),o=r(5546),a=r(2177),s=r(7894),c=r(8e3),u=r(3960),d=r(385),f=r(50),l=r(3081),h=r(8632);function g(){const e=(0,h.gG)();["setErrorHandler","finished","addToTrace","inlineHit","addRelease","addPageAction","setCurrentRouteName","setPageViewName","setCustomAttribute","interaction","noticeError","setUserId"].forEach((t=>{e[t]=function(){for(var r=arguments.length,n=new Array(r),i=0;i<r;i++)n[i]=arguments[i];return function(t){for(var r=arguments.length,n=new Array(r>1?r-1:0),i=1;i<r;i++)n[i-1]=arguments[i];let o=[];return Object.values(e.initializedAgents).forEach((e=>{e.exposed&&e.api[t]&&o.push(e.api[t](...n))})),o.length>1?o:o[0]}(t,...n)}}))}var p=r(2587);function m(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},m=arguments.length>2?arguments[2]:void 0,v=arguments.length>3?arguments[3]:void 0,{init:b,info:y,loader_config:w,runtime:x={loaderType:m},exposed:A=!0}=t;const E=(0,h.gG)();y||(b=E.init,y=E.info,w=E.loader_config),(0,i.Dg)(e,b||{}),(0,i.GE)(e,w||{}),(0,i.sU)(e,x),y.jsAttributes??={},d.v6&&(y.jsAttributes.isWorker=!0),(0,i.CX)(e,y),g();const T=function(e,t){t||(0,c.R)(e,"api");const h={};var g=a.ee.get(e),p=g.get("tracer"),m="api-",v=m+"ixn-";function b(t,r,n,o){const a=(0,i.C5)(e);return null===r?delete a.jsAttributes[t]:(0,i.CX)(e,{...a,jsAttributes:{...a.jsAttributes,[t]:r}}),x(m,n,!0,o||null===r?"session":void 0)(t,r)}function y(){}["setErrorHandler","finished","addToTrace","inlineHit","addRelease"].forEach((e=>h[e]=x(m,e,!0,"api"))),h.addPageAction=x(m,"addPageAction",!0,n.D.pageAction),h.setCurrentRouteName=x(m,"routeName",!0,n.D.spa),h.setPageViewName=function(t,r){if("string"==typeof t)return"/"!==t.charAt(0)&&(t="/"+t),(0,i.OP)(e).customTransaction=(r||"http://custom.transaction")+t,x(m,"setPageViewName",!0)()},h.setCustomAttribute=function(e,t){let r=arguments.length>2&&void 0!==arguments[2]&&arguments[2];if("string"==typeof e){if(["string","number"].includes(typeof t)||null===t)return b(e,t,"setCustomAttribute",r);(0,f.Z)("Failed to execute setCustomAttribute.\nNon-null value must be a string or number type, but a type of <".concat(typeof t,"> was provided."))}else(0,f.Z)("Failed to execute setCustomAttribute.\nName must be a string type, but a type of <".concat(typeof e,"> was provided."))},h.setUserId=function(e){if("string"==typeof e||null===e)return b("enduser.id",e,"setUserId",!0);(0,f.Z)("Failed to execute setUserId.\nNon-null value must be a string type, but a type of <".concat(typeof e,"> was provided."))},h.interaction=function(){return(new y).get()};var w=y.prototype={createTracer:function(e,t){var r={},i=this,a="function"==typeof t;return(0,o.p)(v+"tracer",[(0,s.z)(),e,r],i,n.D.spa,g),function(){if(p.emit((a?"":"no-")+"fn-start",[(0,s.z)(),i,a],r),a)try{return t.apply(this,arguments)}catch(e){throw p.emit("fn-err",[arguments,this,"string"==typeof e?new Error(e):e],r),e}finally{p.emit("fn-end",[(0,s.z)()],r)}}}};function x(e,t,r,i){return function(){return(0,o.p)(l.xS,["API/"+t+"/called"],void 0,n.D.metrics,g),i&&(0,o.p)(e+t,[(0,s.z)(),...arguments],r?null:this,i,g),r?void 0:this}}function A(){r.e(439).then(r.bind(r,7438)).then((t=>{let{setAPI:r}=t;r(e),(0,c.L)(e,"api")})).catch((()=>(0,f.Z)("Downloading runtime APIs failed...")))}return["actionText","setName","setAttribute","save","ignore","onEnd","getContext","end","get"].forEach((e=>{w[e]=x(v,e,void 0,n.D.spa)})),h.noticeError=function(e,t){"string"==typeof e&&(e=new Error(e)),(0,o.p)(l.xS,["API/noticeError/called"],void 0,n.D.metrics,g),(0,o.p)("err",[e,(0,s.z)(),!1,t],void 0,n.D.jserrors,g)},d.il?(0,u.b)((()=>A()),!0):A(),h}(e,v);return(0,h.Qy)(e,T,"api"),(0,h.Qy)(e,A,"exposed"),(0,h.EZ)("activatedFeatures",p.T),T}},3325:(e,t,r)=>{r.d(t,{D:()=>n,p:()=>i});const n={ajax:"ajax",jserrors:"jserrors",metrics:"metrics",pageAction:"page_action",pageViewEvent:"page_view_event",pageViewTiming:"page_view_timing",sessionReplay:"session_replay",sessionTrace:"session_trace",spa:"spa"},i={[n.pageViewEvent]:1,[n.pageViewTiming]:2,[n.metrics]:3,[n.jserrors]:4,[n.ajax]:5,[n.sessionTrace]:6,[n.pageAction]:7,[n.spa]:8,[n.sessionReplay]:9}}},n={};function i(e){var t=n[e];if(void 0!==t)return t.exports;var o=n[e]={exports:{}};return r[e](o,o.exports,i),o.exports}i.m=r,i.d=(e,t)=>{for(var r in t)i.o(t,r)&&!i.o(e,r)&&Object.defineProperty(e,r,{enumerable:!0,get:t[r]})},i.f={},i.e=e=>Promise.all(Object.keys(i.f).reduce(((t,r)=>(i.f[r](e,t),t)),[])),i.u=e=>(({78:"page_action-aggregate",147:"metrics-aggregate",242:"session-manager",317:"jserrors-aggregate",348:"page_view_timing-aggregate",412:"lazy-feature-loader",439:"async-api",538:"recorder",590:"session_replay-aggregate",675:"compressor",733:"session_trace-aggregate",786:"page_view_event-aggregate",873:"spa-aggregate",898:"ajax-aggregate"}[e]||e)+"."+{78:"ac76d497",147:"3dc53903",148:"1a20d5fe",242:"2a64278a",317:"49e41428",348:"bd6de33a",412:"2f55ce66",439:"30bd804e",538:"1b18459f",590:"cf0efb30",675:"ae9f91a8",733:"83105561",786:"06482edd",860:"03a8b7a5",873:"e6b09d52",898:"998ef92b"}[e]+"-1.236.0.min.js"),i.o=(e,t)=>Object.prototype.hasOwnProperty.call(e,t),e={},t="NRBA:",i.l=(r,n,o,a)=>{if(e[r])e[r].push(n);else{var s,c;if(void 0!==o)for(var u=document.getElementsByTagName("script"),d=0;d<u.length;d++){var f=u[d];if(f.getAttribute("src")==r||f.getAttribute("data-webpack")==t+o){s=f;break}}s||(c=!0,(s=document.createElement("script")).charset="utf-8",s.timeout=120,i.nc&&s.setAttribute("nonce",i.nc),s.setAttribute("data-webpack",t+o),s.src=r),e[r]=[n];var l=(t,n)=>{s.onerror=s.onload=null,clearTimeout(h);var i=e[r];if(delete e[r],s.parentNode&&s.parentNode.removeChild(s),i&&i.forEach((e=>e(n))),t)return t(n)},h=setTimeout(l.bind(null,void 0,{type:"timeout",target:s}),12e4);s.onerror=l.bind(null,s.onerror),s.onload=l.bind(null,s.onload),c&&document.head.appendChild(s)}},i.r=e=>{"undefined"!=typeof Symbol&&Symbol.toStringTag&&Object.defineProperty(e,Symbol.toStringTag,{value:"Module"}),Object.defineProperty(e,"__esModule",{value:!0})},i.j=364,i.p="https://js-agent.newrelic.com/",(()=>{var e={364:0,953:0};i.f.j=(t,r)=>{var n=i.o(e,t)?e[t]:void 0;if(0!==n)if(n)r.push(n[2]);else{var o=new Promise(((r,i)=>n=e[t]=[r,i]));r.push(n[2]=o);var a=i.p+i.u(t),s=new Error;i.l(a,(r=>{if(i.o(e,t)&&(0!==(n=e[t])&&(e[t]=void 0),n)){var o=r&&("load"===r.type?"missing":r.type),a=r&&r.target&&r.target.src;s.message="Loading chunk "+t+" failed.\n("+o+": "+a+")",s.name="ChunkLoadError",s.type=o,s.request=a,n[1](s)}}),"chunk-"+t,t)}};var t=(t,r)=>{var n,o,[a,s,c]=r,u=0;if(a.some((t=>0!==e[t]))){for(n in s)i.o(s,n)&&(i.m[n]=s[n]);if(c)c(i)}for(t&&t(r);u<a.length;u++)o=a[u],i.o(e,o)&&e[o]&&e[o][0](),e[o]=0},r=window.webpackChunkNRBA=window.webpackChunkNRBA||[];r.forEach(t.bind(null,0)),r.push=t.bind(null,r.push.bind(r))})();var o={};(()=>{i.r(o);var e=i(3325),t=i(5763);const r=Object.values(e.D);function n(e){const n={};return r.forEach((r=>{n[r]=function(e,r){return!1!==(0,t.Mt)(r,"".concat(e,".enabled"))}(r,e)})),n}var a=i(9144);var s=i(5546),c=i(385),u=i(8e3),d=i(5938),f=i(3960),l=i(50);class h extends d.W{constructor(e,t,r){let n=!(arguments.length>3&&void 0!==arguments[3])||arguments[3];super(e,t,r),this.auto=n,this.abortHandler,this.featAggregate,this.onAggregateImported,n&&(0,u.R)(e,r)}importAggregator(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{};if(this.featAggregate||!this.auto)return;const r=c.il&&!0===(0,t.Mt)(this.agentIdentifier,"privacy.cookies_enabled");let n;this.onAggregateImported=new Promise((e=>{n=e}));const o=async()=>{let t;try{if(r){const{setupAgentSession:e}=await Promise.all([i.e(860),i.e(242)]).then(i.bind(i,3228));t=e(this.agentIdentifier)}}catch(e){(0,l.Z)("A problem occurred when starting up session manager. This page will not start or extend any session.",e)}try{if(!this.shouldImportAgg(this.featureName,t))return void(0,u.L)(this.agentIdentifier,this.featureName);const{lazyFeatureLoader:r}=await i.e(412).then(i.bind(i,8582)),{Aggregate:o}=await r(this.featureName,"aggregate");this.featAggregate=new o(this.agentIdentifier,this.aggregator,e),n(!0)}catch(e){(0,l.Z)("Downloading and initializing ".concat(this.featureName," failed..."),e),this.abortHandler?.(),n(!1)}};c.il?(0,f.b)((()=>o()),!0):o()}shouldImportAgg(r,n){return r!==e.D.sessionReplay||!1!==(0,t.Mt)(this.agentIdentifier,"session_trace.enabled")&&(!!n?.isNew||!!n?.state.sessionReplay)}}var g=i(7633),p=i(7894);class m extends h{static featureName=g.t9;constructor(r,n){let i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];if(super(r,n,g.t9,i),("undefined"==typeof PerformanceNavigationTiming||c.Tt)&&"undefined"!=typeof PerformanceTiming){const n=(0,t.OP)(r);n[g.Dz]=Math.max(Date.now()-n.offset,0),(0,f.K)((()=>n[g.qw]=Math.max((0,p.z)()-n[g.Dz],0))),(0,f.b)((()=>{const t=(0,p.z)();n[g.OJ]=Math.max(t-n[g.Dz],0),(0,s.p)("timing",["load",t],void 0,e.D.pageViewTiming,this.ee)}))}this.importAggregator()}}var v=i(1117),b=i(1284);class y extends v.w{constructor(e){super(e),this.aggregatedData={}}store(e,t,r,n,i){var o=this.getBucket(e,t,r,i);return o.metrics=function(e,t){t||(t={count:0});return t.count+=1,(0,b.D)(e,(function(e,r){t[e]=w(r,t[e])})),t}(n,o.metrics),o}merge(e,t,r,n,i){var o=this.getBucket(e,t,n,i);if(o.metrics){var a=o.metrics;a.count+=r.count,(0,b.D)(r,(function(e,t){if("count"!==e){var n=a[e],i=r[e];i&&!i.c?a[e]=w(i.t,n):a[e]=function(e,t){if(!t)return e;t.c||(t=x(t.t));return t.min=Math.min(e.min,t.min),t.max=Math.max(e.max,t.max),t.t+=e.t,t.sos+=e.sos,t.c+=e.c,t}(i,a[e])}}))}else o.metrics=r}storeMetric(e,t,r,n){var i=this.getBucket(e,t,r);return i.stats=w(n,i.stats),i}getBucket(e,t,r,n){this.aggregatedData[e]||(this.aggregatedData[e]={});var i=this.aggregatedData[e][t];return i||(i=this.aggregatedData[e][t]={params:r||{}},n&&(i.custom=n)),i}get(e,t){return t?this.aggregatedData[e]&&this.aggregatedData[e][t]:this.aggregatedData[e]}take(e){for(var t={},r="",n=!1,i=0;i<e.length;i++)t[r=e[i]]=A(this.aggregatedData[r]),t[r].length&&(n=!0),delete this.aggregatedData[r];return n?t:null}}function w(e,t){return null==e?function(e){e?e.c++:e={c:1};return e}(t):t?(t.c||(t=x(t.t)),t.c+=1,t.t+=e,t.sos+=e*e,e>t.max&&(t.max=e),e<t.min&&(t.min=e),t):{t:e}}function x(e){return{t:e,min:e,max:e,sos:e*e,c:1}}function A(e){return"object"!=typeof e?[]:(0,b.D)(e,E)}function E(e,t){return t}var T=i(8632),_=i(4402),D=i(4351);var N=i(7956),O=i(3239),j=i(9251);class S extends h{static featureName=j.t;constructor(e,r){let n=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,r,j.t,n),c.il&&((0,t.OP)(e).initHidden=Boolean("hidden"===document.visibilityState),(0,N.N)((()=>(0,s.p)("docHidden",[(0,p.z)()],void 0,j.t,this.ee)),!0),(0,O.bP)("pagehide",(()=>(0,s.p)("winPagehide",[(0,p.z)()],void 0,j.t,this.ee))),this.importAggregator())}}var P=i(3081);class C extends h{static featureName=P.t9;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,P.t9,r),this.importAggregator()}}var R,I=i(2210),k=i(1214),H=i(2177),L={};try{R=localStorage.getItem("__nr_flags").split(","),console&&"function"==typeof console.log&&(L.console=!0,-1!==R.indexOf("dev")&&(L.dev=!0),-1!==R.indexOf("nr_dev")&&(L.nrDev=!0))}catch(e){}function z(e){try{L.console&&z(e)}catch(e){}}L.nrDev&&H.ee.on("internal-error",(function(e){z(e.stack)})),L.dev&&H.ee.on("fn-err",(function(e,t,r){z(r.stack)})),L.dev&&(z("NR AGENT IN DEVELOPMENT MODE"),z("flags: "+(0,b.D)(L,(function(e,t){return e})).join(", ")));var M=i(6660);class B extends h{static featureName=M.t;constructor(r,n){let i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(r,n,M.t,i),this.skipNext=0;try{this.removeOnAbort=new AbortController}catch(e){}const o=this;o.ee.on("fn-start",(function(e,t,r){o.abortHandler&&(o.skipNext+=1)})),o.ee.on("fn-err",(function(t,r,n){o.abortHandler&&!n[M.A]&&((0,I.X)(n,M.A,(function(){return!0})),this.thrown=!0,(0,s.p)("err",[n,(0,p.z)()],void 0,e.D.jserrors,o.ee))})),o.ee.on("fn-end",(function(){o.abortHandler&&!this.thrown&&o.skipNext>0&&(o.skipNext-=1)})),o.ee.on("internal-error",(function(t){(0,s.p)("ierr",[t,(0,p.z)(),!0],void 0,e.D.jserrors,o.ee)})),this.origOnerror=c._A.onerror,c._A.onerror=this.onerrorHandler.bind(this),c._A.addEventListener("unhandledrejection",(t=>{const r=function(e){let t="Unhandled Promise Rejection: ";if(e instanceof Error)try{return e.message=t+e.message,e}catch(t){return e}if(void 0===e)return new Error(t);try{return new Error(t+(0,D.P)(e))}catch(e){return new Error(t)}}(t.reason);(0,s.p)("err",[r,(0,p.z)(),!1,{unhandledPromiseRejection:1}],void 0,e.D.jserrors,this.ee)}),(0,O.m$)(!1,this.removeOnAbort?.signal)),(0,k.gy)(this.ee),(0,k.BV)(this.ee),(0,k.em)(this.ee),(0,t.OP)(r).xhrWrappable&&(0,k.Kf)(this.ee),this.abortHandler=this.#e,this.importAggregator()}#e(){this.removeOnAbort?.abort(),this.abortHandler=void 0}onerrorHandler(t,r,n,i,o){"function"==typeof this.origOnerror&&this.origOnerror(...arguments);try{this.skipNext?this.skipNext-=1:(0,s.p)("err",[o||new F(t,r,n),(0,p.z)()],void 0,e.D.jserrors,this.ee)}catch(t){try{(0,s.p)("ierr",[t,(0,p.z)(),!0],void 0,e.D.jserrors,this.ee)}catch(e){}}return!1}}function F(e,t,r){this.message=e||"Uncaught error with no additional information",this.sourceURL=t,this.line=r}let U=1;const q="nr@id";function G(e){const t=typeof e;return!e||"object"!==t&&"function"!==t?-1:e===c._A?0:(0,I.X)(e,q,(function(){return U++}))}function V(e){if("string"==typeof e&&e.length)return e.length;if("object"==typeof e){if("undefined"!=typeof ArrayBuffer&&e instanceof ArrayBuffer&&e.byteLength)return e.byteLength;if("undefined"!=typeof Blob&&e instanceof Blob&&e.size)return e.size;if(!("undefined"!=typeof FormData&&e instanceof FormData))try{return(0,D.P)(e).length}catch(e){return}}}var X=i(7243);class W{constructor(e){this.agentIdentifier=e,this.generateTracePayload=this.generateTracePayload.bind(this),this.shouldGenerateTrace=this.shouldGenerateTrace.bind(this)}generateTracePayload(e){if(!this.shouldGenerateTrace(e))return null;var r=(0,t.DL)(this.agentIdentifier);if(!r)return null;var n=(r.accountID||"").toString()||null,i=(r.agentID||"").toString()||null,o=(r.trustKey||"").toString()||null;if(!n||!i)return null;var a=(0,_.M)(),s=(0,_.Ht)(),c=Date.now(),u={spanId:a,traceId:s,timestamp:c};return(e.sameOrigin||this.isAllowedOrigin(e)&&this.useTraceContextHeadersForCors())&&(u.traceContextParentHeader=this.generateTraceContextParentHeader(a,s),u.traceContextStateHeader=this.generateTraceContextStateHeader(a,c,n,i,o)),(e.sameOrigin&&!this.excludeNewrelicHeader()||!e.sameOrigin&&this.isAllowedOrigin(e)&&this.useNewrelicHeaderForCors())&&(u.newrelicHeader=this.generateTraceHeader(a,s,c,n,i,o)),u}generateTraceContextParentHeader(e,t){return"00-"+t+"-"+e+"-01"}generateTraceContextStateHeader(e,t,r,n,i){return i+"@nr=0-1-"+r+"-"+n+"-"+e+"----"+t}generateTraceHeader(e,t,r,n,i,o){if(!("function"==typeof c._A?.btoa))return null;var a={v:[0,1],d:{ty:"Browser",ac:n,ap:i,id:e,tr:t,ti:r}};return o&&n!==o&&(a.d.tk=o),btoa((0,D.P)(a))}shouldGenerateTrace(e){return this.isDtEnabled()&&this.isAllowedOrigin(e)}isAllowedOrigin(e){var r=!1,n={};if((0,t.Mt)(this.agentIdentifier,"distributed_tracing")&&(n=(0,t.P_)(this.agentIdentifier).distributed_tracing),e.sameOrigin)r=!0;else if(n.allowed_origins instanceof Array)for(var i=0;i<n.allowed_origins.length;i++){var o=(0,X.e)(n.allowed_origins[i]);if(e.hostname===o.hostname&&e.protocol===o.protocol&&e.port===o.port){r=!0;break}}return r}isDtEnabled(){var e=(0,t.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.enabled}excludeNewrelicHeader(){var e=(0,t.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.exclude_newrelic_header}useNewrelicHeaderForCors(){var e=(0,t.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!1!==e.cors_use_newrelic_header}useTraceContextHeadersForCors(){var e=(0,t.Mt)(this.agentIdentifier,"distributed_tracing");return!!e&&!!e.cors_use_tracecontext_headers}}var Z=i(7825),$=["load","error","abort","timeout"],Q=$.length,Y=t.Yu.REQ,K=c._A.XMLHttpRequest;class J extends h{static featureName=Z.t;constructor(r,n){let i=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(r,n,Z.t,i),(0,t.OP)(r).xhrWrappable&&(this.dt=new W(r),this.handler=(e,t,r,n)=>(0,s.p)(e,t,r,n,this.ee),(0,k.u5)(this.ee),(0,k.Kf)(this.ee),function(r,n,i,o){function a(e){var t=this;t.totalCbs=0,t.called=0,t.cbTime=0,t.end=E,t.ended=!1,t.xhrGuids={},t.lastSize=null,t.loadCaptureCalled=!1,t.params=this.params||{},t.metrics=this.metrics||{},e.addEventListener("load",(function(r){_(t,e)}),(0,O.m$)(!1)),c.IF||e.addEventListener("progress",(function(e){t.lastSize=e.loaded}),(0,O.m$)(!1))}function s(e){this.params={method:e[0]},T(this,e[1]),this.metrics={}}function u(e,n){var i=(0,t.DL)(r);i.xpid&&this.sameOrigin&&n.setRequestHeader("X-NewRelic-ID",i.xpid);var a=o.generateTracePayload(this.parsedOrigin);if(a){var s=!1;a.newrelicHeader&&(n.setRequestHeader("newrelic",a.newrelicHeader),s=!0),a.traceContextParentHeader&&(n.setRequestHeader("traceparent",a.traceContextParentHeader),a.traceContextStateHeader&&n.setRequestHeader("tracestate",a.traceContextStateHeader),s=!0),s&&(this.dt=a)}}function d(e,t){var r=this.metrics,i=e[0],o=this;if(r&&i){var a=V(i);a&&(r.txSize=a)}this.startTime=(0,p.z)(),this.listener=function(e){try{"abort"!==e.type||o.loadCaptureCalled||(o.params.aborted=!0),("load"!==e.type||o.called===o.totalCbs&&(o.onloadCalled||"function"!=typeof t.onload)&&"function"==typeof o.end)&&o.end(t)}catch(e){try{n.emit("internal-error",[e])}catch(e){}}};for(var s=0;s<Q;s++)t.addEventListener($[s],this.listener,(0,O.m$)(!1))}function f(e,t,r){this.cbTime+=e,t?this.onloadCalled=!0:this.called+=1,this.called!==this.totalCbs||!this.onloadCalled&&"function"==typeof r.onload||"function"!=typeof this.end||this.end(r)}function l(e,t){var r=""+G(e)+!!t;this.xhrGuids&&!this.xhrGuids[r]&&(this.xhrGuids[r]=!0,this.totalCbs+=1)}function h(e,t){var r=""+G(e)+!!t;this.xhrGuids&&this.xhrGuids[r]&&(delete this.xhrGuids[r],this.totalCbs-=1)}function g(){this.endTime=(0,p.z)()}function m(e,t){t instanceof K&&"load"===e[0]&&n.emit("xhr-load-added",[e[1],e[2]],t)}function v(e,t){t instanceof K&&"load"===e[0]&&n.emit("xhr-load-removed",[e[1],e[2]],t)}function b(e,t,r){t instanceof K&&("onload"===r&&(this.onload=!0),("load"===(e[0]&&e[0].type)||this.onload)&&(this.xhrCbStart=(0,p.z)()))}function y(e,t){this.xhrCbStart&&n.emit("xhr-cb-time",[(0,p.z)()-this.xhrCbStart,this.onload,t],t)}function w(e){var t,r=e[1]||{};"string"==typeof e[0]?t=e[0]:e[0]&&e[0].url?t=e[0].url:c._A?.URL&&e[0]&&e[0]instanceof URL&&(t=e[0].href),t&&(this.parsedOrigin=(0,X.e)(t),this.sameOrigin=this.parsedOrigin.sameOrigin);var n=o.generateTracePayload(this.parsedOrigin);if(n&&(n.newrelicHeader||n.traceContextParentHeader))if("string"==typeof e[0]||c._A?.URL&&e[0]&&e[0]instanceof URL){var i={};for(var a in r)i[a]=r[a];i.headers=new Headers(r.headers||{}),s(i.headers,n)&&(this.dt=n),e.length>1?e[1]=i:e.push(i)}else e[0]&&e[0].headers&&s(e[0].headers,n)&&(this.dt=n);function s(e,t){var r=!1;return t.newrelicHeader&&(e.set("newrelic",t.newrelicHeader),r=!0),t.traceContextParentHeader&&(e.set("traceparent",t.traceContextParentHeader),t.traceContextStateHeader&&e.set("tracestate",t.traceContextStateHeader),r=!0),r}}function x(e,t){this.params={},this.metrics={},this.startTime=(0,p.z)(),this.dt=t,e.length>=1&&(this.target=e[0]),e.length>=2&&(this.opts=e[1]);var r,n=this.opts||{},i=this.target;"string"==typeof i?r=i:"object"==typeof i&&i instanceof Y?r=i.url:c._A?.URL&&"object"==typeof i&&i instanceof URL&&(r=i.href),T(this,r);var o=(""+(i&&i instanceof Y&&i.method||n.method||"GET")).toUpperCase();this.params.method=o,this.txSize=V(n.body)||0}function A(t,r){var n;this.endTime=(0,p.z)(),this.params||(this.params={}),this.params.status=r?r.status:0,"string"==typeof this.rxSize&&this.rxSize.length>0&&(n=+this.rxSize);var o={txSize:this.txSize,rxSize:n,duration:(0,p.z)()-this.startTime};i("xhr",[this.params,o,this.startTime,this.endTime,"fetch"],this,e.D.ajax)}function E(t){var r=this.params,n=this.metrics;if(!this.ended){this.ended=!0;for(var o=0;o<Q;o++)t.removeEventListener($[o],this.listener,!1);r.aborted||(n.duration=(0,p.z)()-this.startTime,this.loadCaptureCalled||4!==t.readyState?null==r.status&&(r.status=0):_(this,t),n.cbTime=this.cbTime,i("xhr",[r,n,this.startTime,this.endTime,"xhr"],this,e.D.ajax))}}function T(e,t){var r=(0,X.e)(t),n=e.params;n.hostname=r.hostname,n.port=r.port,n.protocol=r.protocol,n.host=r.hostname+":"+r.port,n.pathname=r.pathname,e.parsedOrigin=r,e.sameOrigin=r.sameOrigin}function _(e,t){e.params.status=t.status;var r=function(e,t){var r=e.responseType;return"json"===r&&null!==t?t:"arraybuffer"===r||"blob"===r||"json"===r?V(e.response):"text"===r||""===r||void 0===r?V(e.responseText):void 0}(t,e.lastSize);if(r&&(e.metrics.rxSize=r),e.sameOrigin){var n=t.getResponseHeader("X-NewRelic-App-Data");n&&(e.params.cat=n.split(", ").pop())}e.loadCaptureCalled=!0}n.on("new-xhr",a),n.on("open-xhr-start",s),n.on("open-xhr-end",u),n.on("send-xhr-start",d),n.on("xhr-cb-time",f),n.on("xhr-load-added",l),n.on("xhr-load-removed",h),n.on("xhr-resolved",g),n.on("addEventListener-end",m),n.on("removeEventListener-end",v),n.on("fn-end",y),n.on("fetch-before-start",w),n.on("fetch-start",x),n.on("fn-start",b),n.on("fetch-done",A)}(r,this.ee,this.handler,this.dt),this.importAggregator())}}var ee=i(3614);const{BST_RESOURCE:te,RESOURCE:re,START:ne,END:ie,FEATURE_NAME:oe,FN_END:ae,FN_START:se,PUSH_STATE:ce}=ee;var ue=i(7836);const{FEATURE_NAME:de,START:fe,END:le,BODY:he,CB_END:ge,JS_TIME:pe,FETCH:me,FN_START:ve,CB_START:be,FN_END:ye}=ue;var we=i(4649);class xe extends h{static featureName=we.t;constructor(e,t){let r=!(arguments.length>2&&void 0!==arguments[2])||arguments[2];super(e,t,we.t,r),this.importAggregator()}}new class{constructor(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:(0,_.ky)(16);c._A?(this.agentIdentifier=t,this.sharedAggregator=new y({agentIdentifier:this.agentIdentifier}),this.features={},this.desiredFeatures=new Set(e.features||[]),this.desiredFeatures.add(m),Object.assign(this,(0,a.j)(this.agentIdentifier,e,e.loaderType||"agent")),this.start()):(0,l.Z)("Failed to initial the agent. Could not determine the runtime environment.")}get config(){return{info:(0,t.C5)(this.agentIdentifier),init:(0,t.P_)(this.agentIdentifier),loader_config:(0,t.DL)(this.agentIdentifier),runtime:(0,t.OP)(this.agentIdentifier)}}start(){const t="features";try{const r=n(this.agentIdentifier),i=[...this.desiredFeatures];i.sort(((t,r)=>e.p[t.featureName]-e.p[r.featureName])),i.forEach((t=>{if(r[t.featureName]||t.featureName===e.D.pageViewEvent){const n=function(t){switch(t){case e.D.ajax:return[e.D.jserrors];case e.D.sessionTrace:return[e.D.ajax,e.D.pageViewEvent];case e.D.sessionReplay:return[e.D.sessionTrace];case e.D.pageViewTiming:return[e.D.pageViewEvent];default:return[]}}(t.featureName);n.every((e=>r[e]))||(0,l.Z)("".concat(t.featureName," is enabled but one or more dependent features has been disabled (").concat((0,D.P)(n),"). This may cause unintended consequences or missing data...")),this.features[t.featureName]=new t(this.agentIdentifier,this.sharedAggregator)}})),(0,T.Qy)(this.agentIdentifier,this.features,t)}catch(e){(0,l.Z)("Failed to initialize all enabled instrument classes (agent aborted) -",e);for(const e in this.features)this.features[e].abortHandler?.();const r=(0,T.fP)();return delete r.initializedAgents[this.agentIdentifier]?.api,delete r.initializedAgents[this.agentIdentifier]?.[t],delete this.sharedAggregator,r.ee?.abort(),delete r.ee?.get(this.agentIdentifier),!1}}}({features:[J,m,S,class extends h{static featureName=oe;constructor(t,r){if(super(t,r,oe,!(arguments.length>2&&void 0!==arguments[2])||arguments[2]),!c.il)return;const n=this.ee;let i;(0,k.QU)(n),this.eventsEE=(0,k.em)(n),this.eventsEE.on(se,(function(e,t){this.bstStart=(0,p.z)()})),this.eventsEE.on(ae,(function(t,r){(0,s.p)("bst",[t[0],r,this.bstStart,(0,p.z)()],void 0,e.D.sessionTrace,n)})),n.on(ce+ne,(function(e){this.time=(0,p.z)(),this.startPath=location.pathname+location.hash})),n.on(ce+ie,(function(t){(0,s.p)("bstHist",[location.pathname+location.hash,this.startPath,this.time],void 0,e.D.sessionTrace,n)}));try{i=new PerformanceObserver((t=>{const r=t.getEntries();(0,s.p)(te,[r],void 0,e.D.sessionTrace,n)})),i.observe({type:re,buffered:!0})}catch(e){}this.importAggregator({resourceObserver:i})}},C,xe,B,class extends h{static featureName=de;constructor(e,r){if(super(e,r,de,!(arguments.length>2&&void 0!==arguments[2])||arguments[2]),!c.il)return;if(!(0,t.OP)(e).xhrWrappable)return;try{this.removeOnAbort=new AbortController}catch(e){}let n,i=0;const o=this.ee.get("tracer"),a=(0,k._L)(this.ee),s=(0,k.Lg)(this.ee),u=(0,k.BV)(this.ee),d=(0,k.Kf)(this.ee),f=this.ee.get("events"),l=(0,k.u5)(this.ee),h=(0,k.QU)(this.ee),g=(0,k.Gm)(this.ee);function m(e,t){h.emit("newURL",[""+window.location,t])}function v(){i++,n=window.location.hash,this[ve]=(0,p.z)()}function b(){i--,window.location.hash!==n&&m(0,!0);var e=(0,p.z)();this[pe]=~~this[pe]+e-this[ve],this[ye]=e}function y(e,t){e.on(t,(function(){this[t]=(0,p.z)()}))}this.ee.on(ve,v),s.on(be,v),a.on(be,v),this.ee.on(ye,b),s.on(ge,b),a.on(ge,b),this.ee.buffer([ve,ye,"xhr-resolved"],this.featureName),f.buffer([ve],this.featureName),u.buffer(["setTimeout"+le,"clearTimeout"+fe,ve],this.featureName),d.buffer([ve,"new-xhr","send-xhr"+fe],this.featureName),l.buffer([me+fe,me+"-done",me+he+fe,me+he+le],this.featureName),h.buffer(["newURL"],this.featureName),g.buffer([ve],this.featureName),s.buffer(["propagate",be,ge,"executor-err","resolve"+fe],this.featureName),o.buffer([ve,"no-"+ve],this.featureName),a.buffer(["new-jsonp","cb-start","jsonp-error","jsonp-end"],this.featureName),y(l,me+fe),y(l,me+"-done"),y(a,"new-jsonp"),y(a,"jsonp-end"),y(a,"cb-start"),h.on("pushState-end",m),h.on("replaceState-end",m),window.addEventListener("hashchange",m,(0,O.m$)(!0,this.removeOnAbort?.signal)),window.addEventListener("load",m,(0,O.m$)(!0,this.removeOnAbort?.signal)),window.addEventListener("popstate",(function(){m(0,i>1)}),(0,O.m$)(!0,this.removeOnAbort?.signal)),this.abortHandler=this.#e,this.importAggregator()}#e(){this.removeOnAbort?.abort(),this.abortHandler=void 0}}],loaderType:"spa"})})(),window.NRBA=o})();</script><meta charset="UTF-8"/><meta name="robots" content="index, follow"/><meta name="DC.Creator" content="Manas Sajjan"/><meta name="DC.Creator" content="Junxu Li"/><meta name="DC.Creator" content="Raja Selvarajan"/><meta name="DC.Creator" content="Shree Hari Sureshbabu"/><meta name="DC.Creator" content="Sumit Suresh Kale"/><meta name="DC.Creator" content="Rishabh Gupta"/><meta name="DC.Creator" content="Vinit Singh"/><meta name="DC.Creator" content="Sabre Kais"/><meta name="DC.title" content="Quantum machine learning for chemistry and physics "/><meta name="DC.publisher" content="Royal Society of Chemistry"/><meta name="DC.Date" content="2022/08/01"/><meta name="DC.Identifier" scheme="doi" content="10.1039/D2CS00203E"/><meta name="DC.Language" content="en"/><meta name="citation_title" content="Quantum machine learning for chemistry and physics "/><meta name="citation_author" content="Manas Sajjan"/><meta name="citation_author" content="Junxu Li"/><meta name="citation_author" content="Raja Selvarajan"/><meta name="citation_author" content="Shree Hari Sureshbabu"/><meta name="citation_author" content="Sumit Suresh Kale"/><meta name="citation_author" content="Rishabh Gupta"/><meta name="citation_author" content="Vinit Singh"/><meta name="citation_author" content="Sabre Kais"/><meta name="citation_online_date" content="2022/07/18"/><meta name="citation_date" content="2022"/><meta name="citation_journal_title" content="Chemical Society Reviews"/><meta name="citation_volume" content="51"/><meta name="citation_issue" content="15"/><meta name="citation_firstpage" content="6475"/><meta name="citation_lastpage" content="6573"/><meta name="citation_doi" content="10.1039/D2CS00203E"/><meta name="citation_pdf_url" content="https://pubs.rsc.org/en/content/articlepdf/2022/cs/d2cs00203e"/><meta name="citation_abstract_html_url" content="https://pubs.rsc.org/en/content/articlelanding/2022/cs/d2cs00203e"/><meta name="citation_fulltext_html_url" content="https://pubs.rsc.org/en/content/articlehtml/2022/cs/d2cs00203e"/><link rel="shortcut icon" href=""/><link type="text/css" rel="stylesheet" href="/content/stylesheets/rschtml2.css?ver=6_2_1"/><link href="https://www.rsc-cdn.org/oxygen/assets/webfonts/fonts.min.css" rel="stylesheet" type="text/css"/><link type="text/css" rel="stylesheet" href="/content/stylesheets/pubs-ui.min.css"/><meta name="viewport" content="width=device-width, initial-scale=1"/><script type="text/javascript" src="/content/scripts/JQueryPlugins.min.js"> </script><script type="text/javascript" src="/content/scripts/GetAnchorText.js"> </script><script type="text/javascript">
    
      $(function() {
      $("table.tgroup tfoot th").attr("colspan", "100");
      $("table.tgroup.rtable").each( function (idx, el) {
      var tw = $(this).width();
      $(this).parent().css("min-width", tw+"px");
      });

      });
      
    </script><!--6_2_1--></head><body class="oxy-ui pubs-ui ahtml-page"><!--Google Tag Manager (noscript)--><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-56FZ7G" height="0" width="0" style="display:none;visibility:hidden"> </iframe></noscript><!--End Google Tag Manager (noscript)--><div class="viewport autopad"><div class="pnl pnl--drop autopad"><span id="top"/><div id="wrapper"><div class="left_head"><a class="simple" href="/"><img class="rsc-logo" border="0" src="/content/NewImages/royal-society-of-chemistry-logo.png" alt="Royal Society of Chemistry"/></a><br/><span class="btnContainer"><a class="btn btn--tiny btn--primary" target="_blank" title="Link to PDF version" href="/en/content/articlepdf/2022/cs/d2cs00203e">View PDF Version</a></span><span class="btnContainer"><a class="btn btn--tiny btn--nobg" title="Link to previous article (id:d2cs00139j)" href="/en/content/articlehtml/2022/cs/d2cs00139j" target="_BLANK">Previous Article</a></span><span class="btnContainer"><a class="btn btn--tiny btn--nobg" title="Link to next article (id:d1cs00970b)" href="/en/content/articlehtml/2022/cs/d1cs00970b" target="_BLANK">Next Article</a></span></div><div class="right_head"><div id="crossmark_container"><div id="crossmark-content"><a id="open-crossmark" href="#" data-target="crossmark"><img style="max-width:100px" id="crossmark-logo" src="https://crossmark-cdn.crossref.org/widget/v2.0/logos/CROSSMARK_Color_square.svg" alt="Check for updates"/></a><script src="https://crossmark-cdn.crossref.org/widget/v2.0/widget.js"> </script></div></div><br/><span class="oa"><img src="/content/newimages/open_access_blue.png" alt=""/> Open Access Article<br/><img src="/content/newimages/CCBY.svg" alt=""/>
This Open Access Article is licensed under a <br/><a rel="license" href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution 3.0 Unported Licence</a></span></div><div class="article_info"> DOI: <a target="_blank" title="Link to landing page via DOI" href="https://doi.org/10.1039/D2CS00203E">10.1039/D2CS00203E</a>
(Review Article)
<span class="italic"><a title="Link to journal home page" href="https://doi.org/10.1039/1460-4744/1972">Chem. Soc. Rev.</a></span>, 2022, <strong>51</strong>, 6475-6573</div><h1 id="sect432"><span class="title_heading">Quantum machine learning for chemistry and physics</span></h1><p class="header_text">
      <span class="bold">
        
          
            Manas 
            Sajjan
          
          
        
      </span><span class="orcid"><a target="_blank" title="Select to open ORCID record for Manas Sajjan (orcid.org/0000-0001-7436-5422) in a new window" id="connect-orcid-link" href="http://orcid.org/0000-0001-7436-5422"><img id="orcid-id-logo" src="/content/NewImages/orcid_16x16.png" alt="ORCID logo"/></a></span><sup><span class="sup_ref italic">ab</span></sup>, 
      <span class="bold">
        
          
            Junxu 
            Li<a title="Select to navigate to footnote" href="#fn1">†</a>
          
          
        
        
      </span><span class="orcid"><a target="_blank" title="Select to open ORCID record for Junxu Li (orcid.org/0000-0002-2189-0626) in a new window" id="connect-orcid-link" href="http://orcid.org/0000-0002-2189-0626"><img id="orcid-id-logo" src="/content/NewImages/orcid_16x16.png" alt="ORCID logo"/></a></span><sup><span class="sup_ref italic">bc</span></sup>, 
      <span class="bold">
        
          
            Raja 
            Selvarajan<a title="Select to navigate to footnote" href="#fn1">†</a>
          
          
        
      </span><sup><span class="sup_ref italic">bc</span></sup>, 
      <span class="bold">
        
          
            Shree Hari 
            Sureshbabu<a title="Select to navigate to footnote" href="#fn2">‡</a>
          
          
        
        
      </span><span class="orcid"><a target="_blank" title="Select to open ORCID record for Shree Hari Sureshbabu (orcid.org/0000-0002-6265-8268) in a new window" id="connect-orcid-link" href="http://orcid.org/0000-0002-6265-8268"><img id="orcid-id-logo" src="/content/NewImages/orcid_16x16.png" alt="ORCID logo"/></a></span><sup><span class="sup_ref italic">bd</span></sup>, 
      <span class="bold">
        
          
            Sumit Suresh 
            Kale<a title="Select to navigate to footnote" href="#fn2">‡</a>
          
          
        
      </span><span class="orcid"><a target="_blank" title="Select to open ORCID record for Sumit Suresh Kale (orcid.org/0000-0002-4537-1628) in a new window" id="connect-orcid-link" href="http://orcid.org/0000-0002-4537-1628"><img id="orcid-id-logo" src="/content/NewImages/orcid_16x16.png" alt="ORCID logo"/></a></span><sup><span class="sup_ref italic">ab</span></sup>, 
      <span class="bold">
        
          
            Rishabh 
            Gupta<a title="Select to navigate to footnote" href="#fn2">‡</a>
          
          
        
      </span><sup><span class="sup_ref italic">ab</span></sup>, 
      <span class="bold">
        
          
            Vinit 
            Singh<a title="Select to navigate to footnote" href="#fn2">‡</a>
          
          
        
      </span><sup><span class="sup_ref italic">ab</span></sup><span class="bold"> and </span>
      <span class="bold">
        
          
            Sabre 
            Kais
          
          
        
      </span><span class="orcid"><a target="_blank" title="Select to open ORCID record for Sabre Kais (orcid.org/0000-0003-0574-5346) in a new window" id="connect-orcid-link" href="http://orcid.org/0000-0003-0574-5346"><img id="orcid-id-logo" src="/content/NewImages/orcid_16x16.png" alt="ORCID logo"/></a></span>*<sup><span class="sup_ref italic">abcd</span></sup>
      <br/><a id="affa"><sup><span class="sup_ref italic">a</span></sup></a><span class="italic">Department of Chemistry, Purdue University, West Lafayette, IN-47907, USA. E-mail: <a href="mailto:kais@purdue.edu">kais@purdue.edu</a></span>
      <br/><a id="affb"><sup><span class="sup_ref italic">b</span></sup></a><span class="italic">Purdue Quantum Science and Engineering Institute, Purdue University, West Lafayette, Indiana 47907, USA</span>
      <br/><a id="affc"><sup><span class="sup_ref italic">c</span></sup></a><span class="italic">Department of Physics and Astronomy, Purdue University, West Lafayette, IN-47907, USA</span>
      <br/><a id="affd"><sup><span class="sup_ref italic">d</span></sup></a><span class="italic">Elmore Family School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN-47907, USA</span>
    </p><div id="art-admin"><span class="italic bold">Received 
      13th March 2022
    </span></div><p class="bold italic">First published on 18th July 2022</p><hr/><div class="abstract"><h2>Abstract</h2><p>Machine learning (ML) has emerged as a formidable force for identifying hidden but pertinent patterns within a given data set with the objective of subsequent generation of automated predictive behavior. In recent years, it is safe to conclude that ML and its close cousin, deep learning (DL), have ushered in unprecedented developments in all areas of physical sciences, especially chemistry. Not only classical variants of ML, even those trainable on near-term quantum hardwares have been developed with promising outcomes. Such algorithms have revolutionized materials design and performance of photovoltaics, electronic structure calculations of ground and excited states of correlated matter, computation of force-fields and potential energy surfaces informing chemical reaction dynamics, reactivity inspired rational strategies of drug designing and even classification of phases of matter with accurate identification of emergent criticality. In this review we shall explicate a subset of such topics and delineate the contributions made by both classical and quantum computing enhanced machine learning algorithms over the past few years. We shall not only present a brief overview of the well-known techniques but also highlight their learning strategies using statistical physical insight. The objective of the review is not only to foster exposition of the aforesaid techniques but also to empower and promote cross-pollination among future research in all areas of chemistry which can benefit from ML and in turn can potentially accelerate the growth of such algorithms.</p></div><hr/><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p1_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p1.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p1.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Manas Sajjan</span>
                </span></p></td><td><i><p>Dr. Sajjan received his PhD from the Department of Chemistry, University of Chicago wherein he worked on extending reduced density matrix techniques, commonly used for electronic structure, to non-equilibrium problems like bias-induced electronic transport across tunnel-coupled junctions. He is currently a post-doctoral scholar at Purdue University working on the intersection of quantum computing-based algorithms and machine learning frameworks for electronic structure and property prediction of 2D materials and correlated molecular systems.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p2_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p2.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p2.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Junxu Li</span>
                </span></p></td><td><i><p>Mr. Junxu Li received his BS degree in Physics from the University of Science and Technology of China in 2018. He is now pursuing his PhD studies at the Department of Physics and Astronomy, Purdue University. His current research mainly focuses on quantum simulation and quantum computing.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p3_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p3.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p3.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Raja Selvarajan</span>
                </span></p></td><td><i><p>Mr. Raja Selvarajan received his Bachelor of Engineering (BE) degree in Computer Engineering from Indian Institute of Technology, Patna, India. For a brief period following that he was a software developer at Amazon, Bangalore, India. He is currently a PhD student in the Physics Department at Purdue University. His research entails the development of quantum machine learning algorithms towards classification and optimization problems.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p4_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p4.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p4.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Shree Hari Sureshbabu</span>
                </span></p></td><td><i><p>Mr. Shree Hari Sureshbabu received his Bachelor of Engineering (BE) degree in Electrical and Electronics Engineering from Ramaiah Institute of Technology, Bangalore, India. He is currently a PhD student in the Elmore Family School of Electrical and Computer Engineering at Purdue University. His research entails the development of novel classical and quantum machine learning algorithms for Physics and Chemistry simulations.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p5_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p5.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p5.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Sumit Suresh Kale</span>
                </span></p></td><td><i><p>Mr. Sumit Suresh Kale received his BTech degree in Chemical Science and Tech from the Indian Institute of Technology, Guwahati, in 2019. He is currently pursuing his PhD in Prof Sabre Kais’ group at Purdue University. His research interests include coherent control and prediction of chemical reactions using quantum mechanical properties such as quantum superposition, interference and entanglement.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p6_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p6.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p6.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Rishabh Gupta</span>
                </span></p></td><td><i><p>Mr. Rishabh Gupta is a PhD candidate in the Department of Chemistry at Purdue University. He received his BS-MS from the Indian Institute of Science Education and Research, Mohali, India. He is currently working in the field of Quantum Information and Computation with the prime focus on the use of maximal entropy formalism as an alternative approach to quantum state tomography for implementation in near-term quantum devices.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p7_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p7.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p7.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Vinit Singh</span>
                </span></p></td><td><i><p>Mr. Vinit Kumar Singh received his Master of Science (MSc) degree from the Department of Physics, Indian Institute of Technology, Kharagpur. He is currently a PhD candidate at the Department of Chemistry at Purdue University. He is researching quantum-computing algorithms for machine learning and quantum simulations and understanding quantum entanglement in higher dimensions using Tensor Networks.</p></i></td></tr></table><table><tr><td class="biogPlate"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-p8_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-p8.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-p8.gif"/></a><p><span class="graphic_title">
                  <span class="bold">Sabre Kais</span>
                </span></p></td><td><i><p>Sabre Kais is a Distinguished Professor of Chemistry with full professor courtesy appointments in Physics, Computer Science, and Electrical and Computer Engineering. He was the director of the NSF-funded center of innovation on “Quantum Information for Quantum Chemistry” (2010–2013) and served as an External Research Professor at Santa Fe Institute (2013–2019). He is a Fellow of the American Physical Society, Fellow of the American Association for the Advancement of Science, Guggenheim Fellow, and Purdue University Faculty Scholar Award Fellow, and has received the 2012 Sigma Xi Research Award, and 2019 Herbert Newby McCoy Award. He published over 260 peer-reviewed papers and for the last twenty years his research has been focused on quantum information and quantum computing for complex chemical systems.</p></i></td></tr></table><hr/>
    
      
      <h2 id="sect573"><span class="a_heading">1 Introduction</span></h2>
      <span>The 21st century data revolution sparked by machine learning (ML) has yielded unprecedented applications in several domains of technology like natural language processing,<a title="Select to navigate to reference" href="#cit1"><sup><span class="sup_ref">1–3</span></sup></a> translation,<a title="Select to navigate to reference" href="#cit4"><sup><span class="sup_ref">4,5</span></sup></a> autonomous vehicles,<a title="Select to navigate to reference" href="#cit6"><sup><span class="sup_ref">6–8</span></sup></a> robotics,<a title="Select to navigate to reference" href="#cit9"><sup><span class="sup_ref">9,10</span></sup></a> image-recognition,<a title="Select to navigate to reference" href="#cit11"><sup><span class="sup_ref">11–13</span></sup></a> recommender systems,<a title="Select to navigate to references" href="#cit14"><sup><span class="sup_ref">14</span></sup></a> web-searching<a title="Select to navigate to references" href="#cit15"><sup><span class="sup_ref">15</span></sup></a> and fraudulent email filtering<a title="Select to navigate to reference" href="#cit16"><sup><span class="sup_ref">16,17</span></sup></a> and in medical sciences like bio-informatics,<a title="Select to navigate to reference" href="#cit18"><sup><span class="sup_ref">18,19</span></sup></a> medical imaging,<a title="Select to navigate to references" href="#cit20"><sup><span class="sup_ref">20</span></sup></a> brain-computer interfacing<a title="Select to navigate to references" href="#cit21"><sup><span class="sup_ref">21</span></sup></a> and in social sciences<a title="Select to navigate to references" href="#cit22"><sup><span class="sup_ref">22</span></sup></a> and finance<a title="Select to navigate to references" href="#cit23"><sup><span class="sup_ref">23</span></sup></a> and even in problems like refugee integration.<a title="Select to navigate to references" href="#cit24"><sup><span class="sup_ref">24</span></sup></a> The primary reason for such prodigious advances is the uncanny ability of ML based protocols to detect and recognize unforeseen patterns in the data analyzed and integrate the acquired knowledge into decision-making, a process fancifully coined as ‘learning’. The fruitful use of this ability has been further accelerated by not only large-scale availability of shared databases and exponential growth of computing resources but also ingenuous algorithmic advances over the past few decades that precipitated in efficient dimensionality reduction<a title="Select to navigate to references" href="#cit25"><sup><span class="sup_ref">25</span></sup></a> and data-manipulation. Needless to say, this positively disruptive methodology has also fruitfully impacted several domains of physical sciences.<a title="Select to navigate to references" href="#cit26"><sup><span class="sup_ref">26</span></sup></a> Applications ranging from astronomy,<a title="Select to navigate to reference" href="#cit27"><sup><span class="sup_ref">27,28</span></sup></a> particle-physics,<a title="Select to navigate to references" href="#cit29"><sup><span class="sup_ref">29</span></sup></a> atomic and molecular physics,<a title="Select to navigate to references" href="#cit30"><sup><span class="sup_ref">30</span></sup></a> optical manipulations of matter,<a title="Select to navigate to references" href="#cit31"><sup><span class="sup_ref">31</span></sup></a> forecasting of weather patterns and climate dynamics<a title="Select to navigate to reference" href="#cit32"><sup><span class="sup_ref">32,33</span></sup></a> and even identification of evolutionary information from fossil records in paleontology<a title="Select to navigate to reference" href="#cit34"><sup><span class="sup_ref">34–36</span></sup></a> have been recorded with an unforeseen success ratio. Chemical applications like understanding the electronic properties of matter,<a title="Select to navigate to reference" href="#cit37"><sup><span class="sup_ref">37,38</span></sup></a> materials discovery with optimal properties,<a title="Select to navigate to reference" href="#cit39"><sup><span class="sup_ref">39,40</span></sup></a> retrosynthetic design and control of chemical reactions,<a title="Select to navigate to reference" href="#cit41"><sup><span class="sup_ref">41–44</span></sup></a> understanding reaction pathways<a title="Select to navigate to reference" href="#cit45"><sup><span class="sup_ref">45,46</span></sup></a> on a potential energy surface, and cheminformatics<a title="Select to navigate to references" href="#cit47"><sup><span class="sup_ref">47</span></sup></a> have been analyzed using the newly acquired lens of ML and continue to register a meteoric rise. Simulations performed in a recent review<a title="Select to navigate to references" href="#cit48"><sup><span class="sup_ref">48</span></sup></a> bear testimony to this fact by highlighting that keywords based on ML have made steady appearances (≥10<small><sup>2</sup></small>) across all divisions of chemistry over the last 20 years in technical journals of a particular publishing house. The number of such occurrences has specifically increased steadily for applications in physical chemistry/chemical physics. While ML based algorithms were enjoying this attention, much along the same time, the world was also witnessing the rapid emergence of another computing revolution which is fundamentally different from the familiar classical bit-based architecture. The new paradigm, called quantum computing,<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> leverages the power of quantum parallelism and non-classical correlations like quantum entanglement to offer a platform that has shown algorithmic speed-up over the classical version in many instances.<a title="Select to navigate to reference" href="#cit50"><sup><span class="sup_ref">50–54</span></sup></a> The natural question which has been posed in the community is whether quantum computing can also expand the horizon for predicting and identifying relevant features in a given data-set,<a title="Select to navigate to references" href="#cit55"><sup><span class="sup_ref">55</span></sup></a> lead to newer, more efficient algorithms for machine learning or even record algorithmic speed-up for some of the established toolkits that are now routinely employed by ML practitioners in physics and chemistry.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> In this review, we shall try to explore this exciting intersection.</span>
      
        
        <h3 id="sect610"><span class="b_heading">1.1 Scope of the review</span></h3>
        <span>The scope and philosophy of this review would thus be the following:</span>
        <p class="otherpara">1. Ref. <a title="Select to navigate to references" href="#cit48">48</a> highlights that a survey has indicated that ML algorithms are increasingly becoming opaque to human comprehension. We feel that a part of the reason for this is the under-emphasis on the various methodologies that inform the basic building blocks of ML in recent reviews. Although such topics are usually covered elaborately in data-science textbooks<a title="Select to navigate to reference" href="#cit57"><sup><span class="sup_ref">57–59</span></sup></a> yet the resources lack domain-specific examples/applications which a new researcher in the field may find beneficial. Thus a holistic yet detailed account which focuses on both the basic tools used by ML practitioners and how such tools are enabling various physico-chemical applications, consolidated in one place for researchers to use synergistically, is lacking. This review will try to address this gap.</p>
        <p class="otherpara">2. We shall not only discuss the common tools that are used by traditional ML practitioners in theoretical and computational physics and chemistry but also delineate the analogues of these algorithms trainable on a quantum computer. This will be attained in two steps. First, we shall discuss the underlying theoretical framework of quantum versions of each of the vanilla ML algorithms in detail along with their classical counterparts. Second, the contributions made by both the classical and the quantum versions would be discussed separately while exploring each of the respective applications in subsequent parts of the review. To this end, it is important to clearly define certain terms which will set the tone for the review. All applications to be discussed in this review will entail deploying ML based algorithms on datasets involving features or representations of molecules/atoms and/or nanomaterials. Due to the specific nature of the data, we shall broadly call all such examples as instances of quantum machine learning (as is commonly done in this domain<a title="Select to navigate to reference" href="#cit60"><sup><span class="sup_ref">60,61</span></sup></a>) even if the analysis is performed on a classical computer. However, to distinguish examples wherein quantum computers have been used as a part of the training process for the ML algorithm we shall specifically call such applications as ‘quantum computing enhanced’. To the best of our knowledge, explicating such quantum computing enhanced variants in the physico-chemical domain has not been attempted in any of the recent reviews which distinctively sets this one apart from the rest in terms of coverage and focus.</p>
        <p class="otherpara">3. We shall also discuss five different domains of physico-chemical applications which includes tomographic preparation of quantum states in the matter, classification of states and phases of matter, electronic structure of matter, force field parameterization for molecular dynamics and drug discovery pipeline. For each of these applications, we shall discuss ML algorithms (both the classical and quantum computing enhanced variants) that have been successfully used in recent literature focusing on as many different architectures as possible. The objective of treating such a diverse portfolio of applications is to ensure that the reader is aware of the many distinct domains in physical chemistry that have benefited immensely from ML over the past decade. Since the quantum-computing variants are still a nascent variety, bulk of the applications to be discussed will involve classical ML algorithms on quantum data even though the focus will certainly be on how the capabilities in each domain can be augmented with the former in the arsenal. To the best of our knowledge, such a diverse and comprehensive portfolio of applications consolidated in one place has not been presented in any single review most of which have been topical and focused on a single domain only. It must also be emphasized that the aforesaid list is by no means exhaustive. Indeed we shall enlist several other domains later which have not been discussed in this review. Topical reviews on such applications will be duly referenced which the interested reader may consult.</p>
        <p class="otherpara">4. Lastly, another reason for the obscurity in the interpretation of machine learning algorithms especially those involving neural networks is the lack of clarity in the underlying learning process. Indeed, physicists and chemists are motivated to design computational tools which explicitly use physical laws and scientifically guided domain intuition to understand a given natural phenomenon. However, in most of machine learning algorithms, the models are initially agnostic to such physical principles. Instead they identify pertinent features and/or strategies directly from the data without the need for human intervention. While this process is intriguing, certain researchers may be reluctant to reap the full benefits of ML due to this fundamental difference in the operational paradigm. In this review we strive to address this issue by discussing several statistical physical tools which have been used in recent years to demystify the learning process. This is either completely absent or is less emphasized in recent reviews which we believe also fuels the increasing opacity as highlighted in ref. <a title="Select to navigate to references" href="#cit48">48</a></p>
      
      
        
        <h3 id="sect622"><span class="b_heading">1.2 Organization of the review</span></h3>
        <span>The organization of the review is as follows. In Section 2 we offer a glimpse of some basic notions in quantum computing to be used for understanding the subsequent portions of the review. In Section 3 we discuss in detail each of the commonly used architectures in ML and DL (both the classical and the quantum computing enhanced variants). The basic theoretical framework discussed in this section for each of the methods will be frequently referred to subsequently. In Section 4, we enlist and discuss in detail some of the recent reports wherein the power of quantum computers for machine learning tasks has been explicitly demonstrated or theoretically proven to be superior to that of classical models. In Section 5, we discuss the applications of ML in five different domains of physics and chemistry. In Section 6, we discuss several different models for explaining the learning mechanisms of deep learning algorithms using statistical physics. In Section 7, we conclude with a foray into emerging domains not discussed in this review.</span>
      
    
    
      
      <h2 id="sect626"><span class="a_heading">2 A short primer on quantum computing</span></h2>
      <span>In this section, we shall discuss some of the basic terminologies and conceptual foundations of quantum computing that will be used in the rest of the review. This is not only being done for completeness but with the motivation that since quantum computing as a paradigm is relatively new, it may be unfamiliar to traditional ML practitioners and/or new entrants into the field. To appreciate the quantum analogues of commonly used machine learning algorithms, a basic understanding of some of the operational concepts and terms used in this domain would be beneficial. This section would attempt to familiarize the reader with this knowledge. We shall visit the common operational paradigms of computing using quantum devices that are widely used. Just as in classical computers where one has binary bits encoded as {0,1} used for all logical operations, on a quantum computer the primary unit of information is commonly encoded within a qubit. To define a qubit, one would need two two-dimensional vectors commonly denoted as |0〉 and |1〉 and are referred to as computational basis states. Physically these two states can be the two hyperfine energy levels of an ion like in trapped-ion based quantum computing platforms<a title="Select to navigate to reference" href="#cit62"><sup><span class="sup_ref">62,63</span></sup></a> or can be energy levels corresponding to different number of Cooper pairs in a superconducting island created between a Josephson junction and a capacitor plate<a title="Select to navigate to references" href="#cit64"><sup><span class="sup_ref">64</span></sup></a> or can be the highly excited electronic energy levels of a Rydberg atom based cold atomic-ensembles<a title="Select to navigate to reference" href="#cit65"><sup><span class="sup_ref">65,66</span></sup></a> or polar molecules in pendular states<a title="Select to navigate to reference" href="#cit67"><sup><span class="sup_ref">67–71</span></sup></a> to name a few. Mathematically the two states can be represented as |0〉 = (1 0)<small><sup><span class="italic">T</span></sup></small> and |1〉 = (0 1)<small><sup><span class="italic">T</span></sup></small> and collectively form a basis for the two-dimensional state space <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t1_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t1.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t1.gif"/></a> of the system.</span>
      
        
        <h3 id="sect639"><span class="b_heading">2.1 Single qubit state</span></h3>
        <span>The state of the qubit in the two-dimensional basis of (|0〉,|1〉) is defined by the unit trace positive semi-definite operator (denoted as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t2_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t2.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t2.gif"/></a>) as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn1"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t3_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t3.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t3.gif"/></a></td><td class="rightEqn">(1)</td></tr></table>wherein <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t4_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t4.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t4.gif"/></a> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t5_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t5.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t5.gif"/></a> and the operators of the form |<span class="italic">i</span>〉〈<span class="italic">j</span>| ∀ (<span class="italic">i</span>,<span class="italic">j</span>) ∈ (0,1) correspond to familiar outer-product of two vectors.<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> Positive semi-definiteness of the matrix in <a title="" href="#eqn1">eqn (1)</a> guarantees that <span class="italic">n</span><small><sub><span class="italic">x</span></sub></small><small><sup>2</sup></small> + <span class="italic">n</span><small><sub><span class="italic">y</span></sub></small><small><sup>2</sup></small> + <span class="italic">n</span><small><sub><span class="italic">z</span></sub></small><small><sup>2</sup></small> ≤ 1, which allows the vector (<span class="italic">n</span><small><sub><span class="italic">x</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">y</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">z</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> to reside within a Bloch sphere.<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> For pure states which are defined by the additional idempotency constraint of <span class="italic">ρ</span><small><sup>2</sup></small> = <span class="italic">ρ</span>, the inequality is saturated. One can then parameterize (<span class="italic">n</span><small><sub><span class="italic">x</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">y</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">z</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> = (cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>sin<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">ϕ</span>,sin<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">ϕ</span>,cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span>)<small><sup><span class="italic">T</span></sup></small> and establish a bijective correspondence with a vector (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t6_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t6.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t6.gif"/></a>) defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn2"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t7_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t7.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t7.gif"/></a></td><td class="rightEqn">(2)</td></tr></table></span>
        <p class="otherpara">The parametric angles {<span class="italic">θ</span> ∈ [0,π], <span class="italic">ϕ</span> ∈ [0,2π]} are geometrically defined in the Bloch sphere in <a title="Select to navigate to figure" href="#imgfig1">Fig. 1</a> Such states of the system defined by <a title="" href="#eqn2">eqn (2)</a> will be extensively used in this review and will be exclusively referred to for single-qubit states unless otherwise specified. For certain magnitudes of the parameter <span class="italic">θ</span> wherein both <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t8_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t8.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t8.gif"/></a> acquire non-zero values, the state of the system in <a title="" href="#eqn2">eqn (2)</a> is said to be in a superposition of the two basis states. Realization of such superpositions presents one of the fundamental differences between qubit paradigm of computing and the bit paradigm of computing as used in classical processors. The parametric angle <span class="italic">ϕ</span> controls the relative phase difference between the computational basis in such superposition states. However, a superposition even though is responsible for quantum parallelism would not survive a projective measurement protocol.<a title="Select to navigate to reference" href="#cit49"><sup><span class="sup_ref">49,72</span></sup></a> Such measurements would collapse the state in <a title="" href="#eqn2">eqn (2)</a> in either the computational basis state |0〉 with probability <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t9_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t9.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t9.gif"/></a> or in the computational basis state |1〉 with probability <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t10_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t10.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t10.gif"/></a>.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig1"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f1_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f1.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f1.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 1 </b> <span id="fig1"><span class="graphic_title">The Bloch sphere (blue) and the parametric angles <span class="italic">θ</span> and <span class="italic">ϕ</span> as used in <a title="" href="#eqn2">eqn (2)</a>. The point <span class="italic">P</span> marked in red lies on the surface of the Bloch sphere and has (<span class="italic">n</span><small><sub><span class="italic">x</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">y</span></sub></small>,<span class="italic">n</span><small><sub><span class="italic">z</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> = (cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>sin<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">ϕ</span>,sin<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">ϕ</span>,cos<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">θ</span>)<small><sup><span class="italic">T</span></sup></small>. Such states can be represented as in <a title="" href="#eqn2">eqn (2)</a>. On the other hand, states like point <span class="italic">Q</span> (marked in black) lies inside the Bloch sphere <span class="italic">n</span><small><sub><span class="italic">x</span></sub></small><small><sup>2</sup></small> + <span class="italic">n</span><small><sub><span class="italic">y</span></sub></small><small><sup>2</sup></small> + <span class="italic">n</span><small><sub><span class="italic">z</span></sub></small><small><sup>2</sup></small> ≤ 1 and cannot be represented as in <a title="" href="#eqn2">eqn (2)</a>. The only way to denote such states would be using <a title="" href="#eqn1">eqn (1)</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
      
      
        
        <h3 id="sect757"><span class="b_heading">2.2 Multi-qubit state</span></h3>
        <span>For multiple qubits (say <span class="italic">N</span>), the corresponding state space is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t11_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t11.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t11.gif"/></a>.<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> One can thus define a computational basis using the Kronecker product such as |<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>〉 ⊗ |<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>〉,…,|<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>〉, where the labels (<span class="italic">A</span>, <span class="italic">B</span>, <span class="italic">C</span>,…, <span class="italic">N</span>) are physically used to demarcate the state-space of each qubit. There are now 2<small><sup><span class="italic">N</span></sup></small> basis states generated from two choices (|0〉,|1〉) for each of <span class="italic">i</span><small><sub><span class="italic">j</span></sub></small>, <span class="italic">j</span> ∈ {<span class="italic">A</span>, <span class="italic">B</span>,…, <span class="italic">N</span>}. Let us denote this set collectively as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t12_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t12.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t12.gif"/></a>. For notational convenience such multi-qubit basis states will often be abbreviated in this review such as |<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>, <span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>,…, <span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>〉 ≡ |<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>〉, |<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>〉,…, |<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>〉 ≡ |<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>〉 ⊗ |<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>〉,…, |<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>〉. A general state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t13_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t13.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t13.gif"/></a> of the multi-qubit system would again correspond to a positive semi-definite operator with unit trace defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn3"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t14_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t14.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t14.gif"/></a></td><td class="rightEqn">(3)</td></tr></table>where the elements <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t15_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t15.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t15.gif"/></a>. One can also define a reduced state for each of the sub-system qubits (say for the <span class="italic">K</span>-th qubit) through partial tracing of the state <span class="italic">ρ</span><small><sub><span class="italic">A</span>,<span class="italic">B</span>,<span class="italic">C</span>,…,<span class="italic">N</span></sub></small> over computational basis states of the remaining qubits as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn4"/><span id="eqn4"><span class="italic">ρ</span><small><sub><span class="italic">K</span></sub></small> = Tr<small><sub><span class="italic">A</span>,<span class="italic">B</span>,…,<span class="italic">J</span>,<span class="italic">L</span>,…,<span class="italic">N</span></sub></small> (<span class="italic">ρ</span><small><sub><span class="italic">A</span>,<span class="italic">B</span>,<span class="italic">C</span>,…,<span class="italic">N</span></sub></small>)</span></td><td class="rightEqn">(4)</td></tr></table>where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t16_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t16.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t16.gif"/></a>. Such operations are completely positive trace-preserving (CPTP) maps and hence generate valid states<a title="Select to navigate to reference" href="#cit49"><sup><span class="sup_ref">49,72</span></sup></a> of the sub-system (often called the reduced density operator of the <span class="italic">K</span>-th qubit). Just like in the case of single qubits, if the general state in <a title="" href="#eqn3">eqn (3)</a> is pure (<span class="italic">ρ</span><small><sub><span class="italic">A</span>,<span class="italic">B</span>,<span class="italic">C</span>,…,<span class="italic">N</span></sub></small><small><sup>2</sup></small> = <span class="italic">ρ</span><small><sub><span class="italic">A</span>,<span class="italic">B</span>,<span class="italic">C</span>,…,<span class="italic">N</span></sub></small>) one can associate a vector (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t17_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t17.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t17.gif"/></a>) which in the multi-qubit computational basis is denoted as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn5"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t18_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t18.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t18.gif"/></a></td><td class="rightEqn">(5)</td></tr></table>The coefficients <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t19_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t19.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t19.gif"/></a>. For a normalized state as is usually considered in this review, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t20_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t20.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t20.gif"/></a>.</span>
        <p class="otherpara">Other than the possibility of superposition over all basis states similar to the case of single-qubit as discussed in the previous section, it is also possible now to encounter a new phenomenon which has to do with non-classical correlation. The pure state in <a title="" href="#eqn5">eqn (5)</a> will be termed separable if ∃ scalars <span class="italic">ζ</span><small><sub><span class="italic">i</span><small><sub><span class="italic">A</span></sub></small></sub></small>, <span class="italic">γ</span><small><sub><span class="italic">i</span><small><sub><span class="italic">B</span></sub></small></sub></small>,…, <span class="italic">ω</span><small><sub><span class="italic">i</span><small><sub><span class="italic">N</span></sub></small></sub></small> for each sub-system such that <span class="italic">C</span><small><sub><span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>,<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>,<span class="italic">i</span><small><sub><span class="italic">C</span></sub></small>,…,<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small></sub></small> = <span class="italic">ζ</span><small><sub><span class="italic">i</span><small><sub><span class="italic">A</span></sub></small></sub></small>,<span class="italic">γ</span><small><sub><span class="italic">i</span><small><sub><span class="italic">B</span></sub></small></sub></small>,…,<span class="italic">ω</span><small><sub><span class="italic">i</span><small><sub><span class="italic">N</span></sub></small></sub></small> ∀ (<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>, <span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>,…, <span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> ∈ {0,1}<small><sup><span class="italic">N</span></sup></small>, <span class="italic">i.e.</span>, if <span class="italic">every</span> coefficient is multiplicatively factorizable into scalars characterizing the 2D basis states of each sub-system qubit.<a title="Select to navigate to reference" href="#cit49"><sup><span class="sup_ref">49,72,73</span></sup></a> For such a pure separable state it is possible to express <a title="" href="#eqn5">eqn (5)</a> as |<span class="italic">ψ</span>〉<small><sub><span class="italic">A</span>,<span class="italic">B</span>,<span class="italic">C</span>,…,<span class="italic">N</span></sub></small> = |<span class="italic">ϕ</span><small><sub>1</sub></small>〉<small><sub><span class="italic">A</span></sub></small>⊗|<span class="italic">ϕ</span><small><sub>2</sub></small>〉<small><sub><span class="italic">B</span></sub></small>…⊗|<span class="italic">ϕ</span><small><sub><span class="italic">N</span></sub></small>〉<small><sub><span class="italic">N</span></sub></small> wherein <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t38_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t38.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t38.gif"/></a>, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t39_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t39.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t39.gif"/></a>. If a state in <a title="" href="#eqn5">eqn (5)</a> is not separable then it is said to be entangled which is a non-classical correlation.</p>
        <p class="otherpara">The presence of entanglement is another feature wherein computation using qubits can be different from that of the classical bit counterparts and is often leveraged in many different algorithms as a useful resource.<a title="Select to navigate to reference" href="#cit50"><sup><span class="sup_ref">50,51</span></sup></a> Similar to that of the case of a single qubit, the probabilistic interpretation of a projective measurement on the state in <a title="" href="#eqn5">eqn (5)</a> is retained with the probability of collapsing onto a computational basis state |<span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>,<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>,…,<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small>〉 is |<span class="italic">C</span><small><sub><span class="italic">i</span><small><sub><span class="italic">A</span></sub></small>,<span class="italic">i</span><small><sub><span class="italic">B</span></sub></small>,<span class="italic">i</span><small><sub><span class="italic">C</span></sub></small>,…,<span class="italic">i</span><small><sub><span class="italic">N</span></sub></small></sub></small>|<small><sup>2</sup></small>. Unless otherwise stated by multi-qubit states in this review we shall almost always exclusively mean pure states of the kind given in <a title="" href="#eqn5">eqn (5)</a>. Such states as we shall see can not only provide an efficient representation of the many-body states of any interacting quantum system in quantum simulations of stationary/time-independent processes but also for real and imaginary time evolution<a title="Select to navigate to reference" href="#cit74"><sup><span class="sup_ref">74,75</span></sup></a> in quantum dynamics either through Lie–Trotter–Suzuki expansion<a title="Select to navigate to references" href="#cit76"><sup><span class="sup_ref">76</span></sup></a> or through variational frameworks.<a title="Select to navigate to references" href="#cit77"><sup><span class="sup_ref">77</span></sup></a></p>
      
      
        
        <h3 id="sect983"><span class="b_heading">2.3 Quantum gates and quantum circuit based paradigm</span></h3>
        <span>Now that we know how to define quantum states of single and many qubits, it is important to learn how such states are transformed or manipulated. In the gate-model of quantum computing paradigm, transformations between states are achieved using unitary matrices which are represented as ‘quantum gates’. Since all quantum gates are unitary, the inverse of such gates necessarily exists and hence transformations using quantum gates alone are always reversible. The way to incorporate irreversibility into the paradigm is through making projective measurements as that disturbs the state vector irrevocably making it loose its present memory (interactions with the environment induces irreversibility too in the form of qubit decoherence.<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> We shall return to this point later). Commonly used quantum gates and their matrix representation in the computational basis are given in <a title="Select to navigate to table" href="#tab1">Table 1</a>. These gates act on either one, two or three qubits as has been indicated in the table. For visualization of the operations of single-qubit gates, in <a title="Select to navigate to figure" href="#imgfig2">Fig. 2,</a> we plot the corresponding operations for most commonly-used single qubit gates in the Bloch sphere. We see that for <span class="italic">R</span><small><sub><span class="italic">n</span></sub></small>(<span class="italic">θ</span>) the axis of rotation <span class="italic">n</span> can be either {<span class="italic">x</span>,<span class="italic">y</span>,<span class="italic">z</span>} and that decides the accessible state-space for a given initial state. For Hadamard transformation, the operation can be viewed as rotation about the axis <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t50_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t50.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t50.gif"/></a> through an angle of π and hence creates the state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t51_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t51.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t51.gif"/></a> starting from |0〉. The <span class="italic">S</span>-gate <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t52_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t52.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t52.gif"/></a> and <span class="italic">T</span>-gate <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t53_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t53.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t53.gif"/></a> control the relative phases of |0〉 and |1〉 as shown in <a title="Select to navigate to figure" href="#imgfig2">Fig. 2</a>. <a title="Select to navigate to table" href="#tab1">Table 1</a> also discusses several commonly used multi-qubit gates. These operations are commonly used to entangle two or more qubits in a quantum circuit. For example one of the most celebrated two-qubit gate CNOT (see <a title="Select to navigate to table" href="#tab1">Table 1</a>) can be interpreted as the following:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn6"/><span id="eqn6">CNOT = |0〉〈0|<small><sub><span class="italic">c</span></sub></small> ⊗ <span class="italic">I</span><small><sub><span class="italic">t</span></sub></small> + |1〉〈1|<small><sub><span class="italic">c</span></sub></small> ⊗ <span class="italic">X</span><small><sub><span class="italic">t</span></sub></small></span></td><td class="rightEqn">(6)</td></tr></table>where the subscript <span class="italic">c</span> indicates the control qubit whose state is not changed and the subscript <span class="italic">t</span> indicates the target qubit whose state is altered conditioned on the state of the controlling qubit. In this case, if the state of the control is |1〉<small><sub><span class="italic">c</span></sub></small> the target qubit is flipped but it is left unchanged if the state of the control is |0〉<small><sub><span class="italic">c</span></sub></small>. Similarly using CPHASE(<span class="italic">α</span>) (see <a title="Select to navigate to table" href="#tab1">Table 1</a>) one imparts a relative phase of <span class="italic">α</span> between the basis states of the target qubit if the state of the control qubit is |1〉<small><sub><span class="italic">c</span></sub></small>. It must be emphasized that gates wherein a non-trivial operation on a target qubit is initiated if the control qubit is in state |0〉 are also routinely used in quantum algorithms. Such a controlled two qubit gate (CU<small><sub>0</sub></small>) for an arbitrary single qubit operation <span class="italic">U</span><small><sub><span class="italic">t</span></sub></small> on the target qubit is written as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn7"/><span id="eqn7">CU<small><sub>0</sub></small> = |0〉〈0|<small><sub><span class="italic">c</span></sub></small> ⊗ <span class="italic">U</span><small><sub><span class="italic">t</span></sub></small> + |1〉〈1|<small><sub><span class="italic">c</span></sub></small> ⊗ <span class="italic">I</span><small><sub><span class="italic">t</span></sub></small>.</span></td><td class="rightEqn">(7)</td></tr></table></span>
        
<div class="table_caption"><b>Table 1</b> <span id="tab1">Commonly used single and multi-qubit gates in quantum circuits and the corresponding matrix representations in the computational basis</span>
		</div>

            <div class="rtable__wrapper"><div class="rtable__inner"><table class="tgroup rtable" border="0"><colgroup><col/><col/><col/></colgroup>
              
              
              
              <thead align="left">
                <tr align="left" valign="bottom">
                  <th align="left" valign="bottom" class="border_black">Gate type</th>
                  <th align="left" valign="bottom" class="border_black">Number of qubit(s)</th>
                  <th align="left" valign="bottom" class="border_black">Matrix representation</th>
                </tr>
              </thead>
              <tbody align="left">
                <tr align="left">
                  <td valign="top">
                    <span class="italic">R</span>
                    <small><sub>
                      <span class="italic">x</span>
                    </sub></small>(<span class="italic">θ</span>)</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t21_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t21.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t21.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">
                    <span class="italic">R</span>
                    <small><sub>
                      <span class="italic">y</span>
                    </sub></small>(<span class="italic">θ</span>)</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t22_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t22.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t22.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">
                    <span class="italic">R</span>
                    <small><sub>
                      <span class="italic">z</span>
                    </sub></small>(<span class="italic">θ</span>)</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t23_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t23.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t23.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">X</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t24_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t24.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t24.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">Y</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t25_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t25.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t25.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">Z</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t26_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t26.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t26.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">H</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t27_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t27.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t27.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">
                    <span class="italic">P</span>(<span class="italic">α</span>)</td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t28_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t28.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t28.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t29_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t29.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t29.gif"/></a>
                  </td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t30_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t30.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t30.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t31_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t31.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t31.gif"/></a>
                  </td>
                  <td valign="top">1</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t32_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t32.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t32.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">CNOT</td>
                  <td valign="top">2</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t33_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t33.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t33.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">CPHASE(<span class="italic">α</span>)</td>
                  <td valign="top">2</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t34_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t34.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t34.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">SWAP</td>
                  <td valign="top">2</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t35_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t35.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t35.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">CZ = CPHASE(π)</td>
                  <td valign="top">2</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t36_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t36.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t36.gif"/></a>
                  </td>
                </tr>
                <tr align="left">
                  <td valign="top">Toffoli</td>
                  <td valign="top">3</td>
                  <td valign="top">
                    <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t37_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t37.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t37.gif"/></a>
                  </td>
                </tr>
              </tbody>
            </table></div></div>
          <hr class="hrule"/><br/>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig2"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f2_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f2.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f2.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 2 </b> <span id="fig2"><span class="graphic_title">(a) The operation <span class="italic">R</span><small><sub><span class="italic">x</span></sub></small>(<span class="italic">θ</span>) which involves rotation about the <span class="italic">x</span>-axis (marked in black) as shown in the Bloch sphere. The initial state is |0〉 (marked in red). (b) The operation <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small>(<span class="italic">θ</span>) which involves rotation about the <span class="italic">y</span>-axis (marked in black) as shown in the Bloch sphere. The initial state is |0〉 (marked in red) (c) same as in (a), (b) but with <span class="italic">R</span><small><sub><span class="italic">z</span></sub></small>(<span class="italic">θ</span>) wherein the axis of rotation is <span class="italic">z</span> (marked in black). The initial state chosen here is (marked in red) <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t40_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t40.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t40.gif"/></a>. (d) Hadamard transformation of the initial state |0〉 (marked in red) as visualized in the Bloch sphere. The operation can be viewed as rotation around the axis <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t41_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t41.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t41.gif"/></a> shown in black through an angle of π. Note that unlike the rotation gates in (a)–(c), Hadamard transformation does not have a variable user-defined angle of rotation and hence the final state starting from the said initial state is always fixed <span class="italic">i.e.</span><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t42_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t42.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t42.gif"/></a>. (e) The transformation of the initial state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t43_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t43.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t43.gif"/></a> under phase-gate <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t44_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t44.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t44.gif"/></a> (see <a title="Select to navigate to table" href="#tab1">Table 1</a>). The operation produces a final state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t45_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t45.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t45.gif"/></a>. (f) The transformation of the initial state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t46_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t46.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t46.gif"/></a> under <span class="italic">T</span>-gate where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t47_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t47.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t47.gif"/></a> (see <a title="Select to navigate to table" href="#tab1">Table 1</a>). The operation produces a final state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t48_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t48.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t48.gif"/></a>. The matrix representations of the operators in (a)–(f) are given in <a title="Select to navigate to table" href="#tab1">Table 1</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">This interpretation extends to multi-qubit gates beyond two as well, except that the size of the control register now is more than one and many more possibilities of multiple controls are realizable (for example for a three qubit control unitary, the two controlling qubits can be in any of the four states |00〉<small><sub><span class="italic">c</span></sub></small>, |01〉<small><sub><span class="italic">c</span></sub></small>, |10〉<small><sub><span class="italic">c</span></sub></small>, and |11〉<small><sub><span class="italic">c</span></sub></small>, to initiate a non-trivial operation on the target). In the well-known Toffoli gate (see <a title="Select to navigate to table" href="#tab1">Table 1</a>) the state of the target is flipped by an X operation conditioned on the joint state of two-qubits instead of one unlike in the CNOT gate. This means the operation is non-trivial only if this joint state is |11〉<small><sub><span class="italic">c</span></sub></small>. Intuitively, one-qubit gates are required to initiate superposition between the two-basis states of individual qubits as depicted within the Bloch sphere shown in <a title="Select to navigate to figure" href="#imgfig2">Fig. 2</a> but multi-qubit gates are required to initiate correlation between the joint-states of several qubits. Both these operations are therefore necessary to create non-trivial many-body quantum states.</p>
        <p class="otherpara">A certain subset of gates forms a universal set<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> in the sense that any arbitrary <span class="italic">n</span>-qubit unitary operation can be approximately modelled as a finite sequence of gates from this set within a preset user-defined precision. The choice of this set is not unique and is largely determined by which gates are operationally convenient for implementation on a given platform used for constructing the quantum hardware. One popular choice is the ({<span class="italic">R</span><small><sub><span class="italic">x</span></sub></small>(<span class="italic">θ</span>), <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small>(<span class="italic">θ</span>), <span class="italic">R</span><small><sub><span class="italic">z</span></sub></small>(<span class="italic">θ</span>), <span class="italic">P</span>(<span class="italic">α</span>), CNOT}) gate-set. Equivalent yet a minimalistic choice can be <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t54_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t54.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t54.gif"/></a>.<a title="Select to navigate to reference" href="#cit49"><sup><span class="sup_ref">49,73</span></sup></a> One must emphasize that the use of universal gate-sets only guarantees reachability, <span class="italic">i.e.</span>, the ability to approximately implement any desired unitary using a finite-sequence of gates from the set without placing any restriction on the number of gates inhabiting the sequence.<a title="Select to navigate to references" href="#cit73"><sup><span class="sup_ref">73</span></sup></a> Indeed it may so happen that implementation of certain <span class="italic">n</span>-qubit unitaries would require gate-sequences from the universal set with length scaling as <span class="italic">O</span>(<span class="italic">c</span><small><sup><span class="italic">n</span></sup></small>), <span class="italic">i.e.</span>, exponential. On the other hand, for certain other operations, the length of gate-sequences scaling as <span class="italic">O</span>(<span class="italic">n</span><small><sup><span class="italic">k</span></sup></small>) (polynomial) is seen. Only the latter kind of unitaries can be hoped to be efficiently simulated on a quantum computer.</p>
        <p class="otherpara">A quantum circuit is essentially an assembly of quantum gates which transforms the initial state of a multi-qubit system to the final desired state. The set of quantum gates operationally represents a user-defined unitary transformation. Such operations are frequently followed by measurement either on a computational basis or on the basis of an operator whose statistics in the prepared state are desired.<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> The circuit representation of the commonly used gates is given in <a title="Select to navigate to figure" href="#imgfig3">Fig. 3(a)–(d)</a>. A representative example of a quantum circuit built using some of the gates in <a title="Select to navigate to table" href="#tab1">Table 1</a> is given in <a title="Select to navigate to figure" href="#imgfig3">Fig. 3(e)</a>. The circuit shows the preparation of a typical Bell state of the kind <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t55_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t55.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t55.gif"/></a> in a 2-qubit system with <span class="italic">α</span> being the relative phase difference between the two basis states (|00〉, |11〉). One practically useful way to interpret such a circuit is to probe the state of the system at various junctions. We have divided the circuit into four junctions. At the first junction labelled as (I), the joint state of the two qubits is the initial computational basis |00〉. At junction (II), the effect of Hadamard (H) on the first qubit yields a separable state wherein the first qubit is in an equal superposition of the single-qubit basis states and the second qubit is still in |0〉. The CNOT gate with the first qubit as the control and the second qubit as the target yields the state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t56_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t56.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t56.gif"/></a> at junction (III). At junction (IV), the controlled-phase gate (CPHASE(<span class="italic">α</span>)) selectively creates a phase difference of <span class="italic">α</span> between the states |00〉 and |11〉 which results in the target state. Measurements on the target state on a computational basis would yield equal probability (½) of observing either the state |00〉 or |11〉 and zero probability of observing |01〉 or |10〉. Circuit representations of the quantum-enhanced machine learning algorithms shall appear throughout the review. Interpretations of each of them can be done analogously.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig3"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f3_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f3.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f3.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 3 </b> <span id="fig3"><span class="graphic_title">Commonly used circuit representation of (a) 1-qubit gates and (b) 2-qubit gates. Special gates in this category like CNOT and CZ gates have slightly different representations than the rest as has been highlighted within the oval windows. One must note that the solid dot indicates the control qubit and the hollow dot with a plus, <span class="italic">i.e.</span>, ⊕ indicates the target qubit. Its the target qubit whose state is actually altered conditioned on the state of the control qubit being |1〉 in this case. The operation need not always be controlled on state |1〉 for the control qubit. Indeed two-qubit gates where the non-trivial operations on the target initiated by the control is |0〉 are also routinely used (see text for more details). (c) 3-Qubit gates: special gates in this category like Toffoli gate and CCZ gate have slightly different representations than the rest as has been highlighted within the oval window. A similar interpretation as in (b) for the solid and hollow dots (⊕) must be followed in terms of the control and target qubits. (d) A generic <span class="italic">n</span>-qubit parameterized unitary. This is very often used to describe quantum circuits as we shall see later in this review. The explicit construction of gates in <span class="italic">U</span>(<span class="italic">θ</span>) is often omitted but is implied to be made up of elementary gates from (a) and (b) and occasionally even (c). The measurement protocol for any qubit will be denoted by boxes of the kind shown in green symbolically representing a monitoring device/meter. (e) A simple representative quantum circuit for the preparation of the Bell state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t49_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t49.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t49.gif"/></a>. To facilitate interpretation of the circuit, the state of both the two-qubits is illustrated at junctions (I), (II), (III), and (IV) after the operation of each elementary gate. To evaluate the states one can also use the matrix representation of the respective gates given in <a title="Select to navigate to table" href="#tab1">Table 1</a> and apply it to the initial state |00〉 with the unitaries on the left acting first.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">Development of quantum computing has been underway since the 1980s,<a title="Select to navigate to reference" href="#cit78"><sup><span class="sup_ref">78,79</span></sup></a> but it gained unprecedented attention with the exponential speed-up reported in prime factorization by Peter Shor in the last decade of the 20th century.<a title="Select to navigate to references" href="#cit53"><sup><span class="sup_ref">53</span></sup></a> It was quickly realized, however, that uncontrolled interactions of the qubit register with the environmental bath lead to the loss of coherence of the initialized state. Moreover, for the experimental implementation of a digital quantum-computing platform, the gate-operations (unitary gates defined above) may be imperfect too.<a title="Select to navigate to references" href="#cit80"><sup><span class="sup_ref">80</span></sup></a> The collective effect of both of these would be to introduce noise or errors thereby hampering the performance of the algorithm. Quantum error-correction (QEC) schemes were proposed<a title="Select to navigate to references" href="#cit81"><sup><span class="sup_ref">81</span></sup></a> which can act to mitigate the effect of these noises. However scalable implementation of such protocols is challenging<a title="Select to navigate to reference" href="#cit82"><sup><span class="sup_ref">82,83</span></sup></a> and is currently under development. In the current era, operational quantum devices are already a reality consisting of around 10–100 qubits but without any error-correction. This era of quantum computers is therefore termed noisy intermediate-scale quantum devices (NISQ).<a title="Select to navigate to references" href="#cit84"><sup><span class="sup_ref">84</span></sup></a> Due to the inherently erroneous gate operations, the algorithms developed for NISQ devices are designed to use shallow-circuit depth and usually variational and delegate a part of the computation to a classical processor.<a title="Select to navigate to references" href="#cit85"><sup><span class="sup_ref">85</span></sup></a> Such algorithms are meant to reap the maximum benefits from noisy hardwares and look for potential advantages. Such algorithms will be a key player in this review for understanding some of the near-term ML applications. These algorithms have proven to be advantageous for applications in chemistry/chemical physics,<a title="Select to navigate to reference" href="#cit86"><sup><span class="sup_ref">86–90</span></sup></a> condensed-matter physics and materials science,<a title="Select to navigate to references" href="#cit91"><sup><span class="sup_ref">91</span></sup></a> atomic physics,<a title="Select to navigate to references" href="#cit92"><sup><span class="sup_ref">92</span></sup></a> high-energy physics,<a title="Select to navigate to reference" href="#cit93"><sup><span class="sup_ref">93,94</span></sup></a> bio-chemistry,<a title="Select to navigate to references" href="#cit95"><sup><span class="sup_ref">95</span></sup></a> and finance.<a title="Select to navigate to references" href="#cit96"><sup><span class="sup_ref">96</span></sup></a> In contrast, there are algorithms like quantum phase estimation<a title="Select to navigate to reference" href="#cit97"><sup><span class="sup_ref">97,98</span></sup></a> which have a provable exponential advantage but require high-circuit depth and hence are amenable to be implemented in fault-tolerant devices.</p>
      
      
        
        <h3 id="sect1262"><span class="b_heading">2.4 Quantum annealing based paradigm</span></h3>
        <span>This paradigm is particularly useful for solving optimization problems wherein the optimal solution can be encoded within the ground state of a given Hamiltonian of a system (say <span class="italic">H</span><small><sub>2</sub></small>). The key working principle of the hardware operating under the annealing model is to prepare the ground state of a system which is efficiently prepared (say for a Hamiltonian <span class="italic">H</span><small><sub>1</sub></small>), from which the ground state of the target Hamiltonian <span class="italic">H</span><small><sub>2</sub></small> is subsequently retrieved.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn8"/><span id="eqn8"><span class="italic">H</span>(<span class="italic">s</span>) = <span class="italic">A</span>(<span class="italic">s</span>)<span class="italic">H</span><small><sub>1</sub></small> + <span class="italic">B</span>(<span class="italic">s</span>)<span class="italic">H</span><small><sub>2</sub></small></span></td><td class="rightEqn">(8)</td></tr></table></span>
        <p class="otherpara">To be more specific, let the Hamiltonian of the system be <span class="italic">H</span><small><sub>1</sub></small>, <span class="italic">i.e.</span>, <span class="italic">A</span>(<span class="italic">s</span>) = 1, <span class="italic">B</span>(<span class="italic">s</span>) = 0, in <a title="" href="#eqn8">eqn (8),</a> whose ground state can be easily constructed. Thereafter the switching parameter <span class="italic">s</span> is varied until (<span class="italic">A</span>(<span class="italic">s</span>) = 0, <span class="italic">B</span>(<span class="italic">s</span>) = 1). If the variations are sufficiently ‘slow’ then the quantum adiabatic theorem<a title="Select to navigate to references" href="#cit99"><sup><span class="sup_ref">99</span></sup></a> guarantees that the evolution trajectory would be traversing the instantaneous ground states of Hamiltonian <span class="italic">H</span>(<span class="italic">s</span>) with high probability. Under such circumstances this implies that one would yield the ground state of the target Hamiltonian <span class="italic">H</span><small><sub>2</sub></small> at the end of the protocol with high fidelity (see <a title="" href="#eqn8">eqn (8)</a>). A popular quantum annealer D-wave uses ground states of Ising type Hamiltonians<a title="Select to navigate to references" href="#cit100"><sup><span class="sup_ref">100</span></sup></a> for encoding the solution to the problem being investigated. Optimization schemes like quadratic unconstrained binary optimization (QUBO), combinatoric problems <span class="italic">etc.</span> which can be mapped to such Hamiltonians can thus be efficiently solved using this paradigm.<a title="Select to navigate to reference" href="#cit101"><sup><span class="sup_ref">101–104</span></sup></a> Except in a very small number of examples, this paradigm of quantum computing will not be explored much in this review. Interested readers may consult topical reviews like ref. <a title="Select to navigate to reference" href="#cit101">101,105</a>.</p>
      
    
    
      
      <h2 id="sect1309"><span class="a_heading">3 A short primer on the commonly used toolkits in machine learning</span></h2>
      
        
        <h3 id="sect1312"><span class="b_heading">3.1 Overview</span></h3>
        <span>Broadly problems tackled in machine learning can be categorized into 3 classes: supervised, unsupervised and reinforcement learning. We start off by discussing each of the categories independently and introduce commonly used terminologies within the machine learning community.</span>
        <div>
          
          <span id="sect1316"/><span class="c_heading_indent">3.1.1 Supervised learning. </span>
          <span>We are given a dataset of the form {(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>)|<span class="italic">i</span> ∈ [<span class="italic">N</span>]}, where <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>s are inputs sampled from some fixed distribution, <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> is the corresponding label and <span class="italic">N</span> is the size of the dataset. Typically <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> is an element in <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t57_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t57.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t57.gif"/></a> and <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> belongs to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t58_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t58.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t58.gif"/></a>. The task is to identify the correct label for <span class="italic">y</span>* for a randomly chosen sample <span class="italic">x</span>* from that distribution. The dataset {(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>)|<span class="italic">i</span> ∈ [<span class="italic">N</span>]} is referred to the training dataset. A loss function <span class="italic">L</span>(<span class="italic">h</span>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">w</span>),<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>) is defined based on the problem at hand that quantifies the error in the learning. Here <span class="italic">h</span>(<span class="italic">x</span>,<span class="italic">w</span>) refers to the hypothesis function that the learning procedure outputs and <span class="italic">w</span> refers to the parameters or weights over which the optimization is performed. An empirical risk minimization is carried over <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t59_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t59.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t59.gif"/></a> to output <span class="italic">h</span>(<span class="italic">x</span>,<span class="italic">w</span>*), where <span class="italic">w</span>* are the parameters output at the end of learning. A test data set is finally used to output the performance of <span class="italic">h</span>(<span class="italic">x</span>,<span class="italic">w</span>*) and used as a metric of comparison across several learning methods. The labelled dataset being used is manually subdivided into two subsets. One of the subset is used for training and the other for final validation and testing. The process of learning thus comprises 2 parts: trainability (empirical risk minimization over training data) and generalization (how well it performs on unseen data). Typically the optimization of parameters involves computing gradients of the loss function with respect to these parameters.</span>
          <p class="otherpara">Apart from the parameters that are trained definitively through optimization schemes, other parameters referred to as hyperparameters become critically important for neural-network based supervised learning schemes (to be explored soon). Such parameters/variables are fixed manually by hand a priori. These may include, the learning technique employed, the number of parameters, the optimization procedure<a title="Select to navigate to references" href="#cit106"><sup><span class="sup_ref">106</span></sup></a> (standard gradient descent, stochastic gradient descent, and Adam optimizer), the parameter initialization scheme, the learning rate, the stopping conditions for training (the threshold for convergence or the number of parameter update iterations), batch sizes, choice of the loss function, <span class="italic">etc.</span><a title="Select to navigate to references" href="#cit107"><sup><span class="sup_ref">107</span></sup></a></p>
          <p class="otherpara">Examples of problems in a supervised learning procedure include classification, where the labels <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> are discrete, and regression, where the labels <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> are continuous and extrapolation to unknown cases is sought. Some of the techniques used exclusively for problems in this domain include, a support vector machine, kernel ridge regression wherein data are mapped to a higher-dimensional space for manipulation, Gaussian process regression, decision trees, and a Naive Bayes classifier. Other techniques not exclusive to this learning model include Neural networks whose applications have spanned every field of industry and research. We will discuss more about each of the above learning models in the subsequent section.</p>
        </div>
        <div>
          
          <span id="sect1385"/><span class="c_heading_indent">3.1.2 Unsupervised learning. </span>
          <span>Unlike supervised learning, here we are provided with data points that do not have any continuous or discrete labels associated with them. The task is to learn something intrinsic to the distribution of the data points. Some commonly tackled tasks under this learning scheme include clustering with respect to some metric on the space, learning the given probability distribution by training latent variables (<span class="italic">e.g.</span> Boltzmann machine, restricted Boltzmann machine (RBM), and generative adversarial networks), and dimensionality reduction that allows reduction in the size of the feature space with little information loss (<span class="italic">e.g.</span>, autoencoders, principal component analysis, and RBMs). Unsupervised learning mainly tries to solve the problem of learning an arbitrary probability distribution by minimizing some loss function that quantifies the divergence between the given distribution to be learnt and the model distribution being trained (<span class="italic">e.g.</span>, cross entropy, KL divergence, and Renyi entropy).<a title="Select to navigate to references" href="#cit108"><sup><span class="sup_ref">108</span></sup></a> We would like to point out that the techniques mentioned above are different variations to making use of a neural network, whose functionality depends on the exact form of cost function being employed in the training.</span>
          <p class="otherpara">One is not restricted to using methods from either supervised or unsupervised learning exclusively for solving a problem. In practice we notice that a mix of methods are employed to solve a given problem. For instance, one might require dimensionality reduction or noise filtering or distribution learning using unsupervised methods prior to introducing labels and solving a classification problem with supervised methods. These methods are commonly referred to as semi-supervised learning<a title="Select to navigate to reference" href="#cit109"><sup><span class="sup_ref">109,110</span></sup></a> or hybrid learning methods.</p>
        </div>
        <div>
          
          <span id="sect1395"/><span class="c_heading_indent">3.1.3 Reinforcement learning. </span>
          <span>Unlike the above two learning models, here we take a totally different stand on the setting in which the learning happens. An artificial agent is made to interact with an environment through actions so as to maximize the reward function that has been identified. This type of learning is employed when the agent can learn about its surroundings only through interaction which is limited by a finite set of actions that the agent is provided with. Due to the unbounded sequence of actions that the agent can explore, one needs to employ good heuristics with regards to designing reward functions that help accept or reject the outcome of a certain action in exploring this space. Thus optimal control theory<a title="Select to navigate to references" href="#cit111"><sup><span class="sup_ref">111</span></sup></a> plays an important role in this learning method. Some of the most popular applications involve self driving cars (Tesla Autopilot), training bots in a game (Alpha zero for chess, Alpha Go zero for Go) and smart home robots (vacuum cleaning bots and companion bots) for an extensive introduction to reinforcement learning, refer ref. <a title="Select to navigate to references" href="#cit112">112</a>.</span>
        </div>
      
      
        
        <h3 id="sect1401"><span class="b_heading">3.2 Classical and quantum variants of commonly used algorithms</span></h3>
        <span>In this section we shall elaborate on some of the commonly encountered machine and deep learning techniques that have been used extensively for physico-chemical studies. We shall discuss both the classical implementation of the algorithms and also the appropriate quantum versions.</span>
        <div>
          
          <span id="sect1405"/><span class="c_heading_indent">3.2.1 Kernel based learning theory. </span>
          <span>The concept of kernels is very important in machine learning, both quantum and classical.<a title="Select to navigate to reference" href="#cit113"><sup><span class="sup_ref">113–115</span></sup></a> Let us imagine a dataset <span class="italic">D</span> = {(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>,<span class="bold">y</span><small><sub><span class="bold">i</span></sub></small>)| <span class="bold">x</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">χ</span>, <span class="bold">y</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">Ω</span> ∀ <span class="italic">i</span> ∈ [<span class="italic">m</span>]} as described in the supervised learning section. In set <span class="italic">D</span>, <span class="bold">x</span><small><sub><span class="italic">i</span></sub></small> are the feature vectors sampled from the set <span class="italic">χ</span> whereas the labels <span class="bold">y</span><small><sub><span class="italic">i</span></sub></small> are sampled from another set <span class="italic">Ω</span>. In the cases frequently encountered, one usually finds <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t60_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t60.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t60.gif"/></a> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t61_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t61.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t61.gif"/></a>. <span class="italic">m</span> is the sample size of the training data-set <span class="italic">D</span> or the number of observations. It is often convenient to define a map <span class="italic">ϕ</span> such that <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t62_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t62.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t62.gif"/></a> such that the new feature-space <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t63_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t63.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t63.gif"/></a> is usually a higher-dimensional space equipped with an inner product. For example if <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t64_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t64.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t64.gif"/></a> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t65_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t65.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t65.gif"/></a> then <span class="italic">p</span> ≥ <span class="italic">d</span>. The Kernel <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t66_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t66.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t66.gif"/></a> of the map <span class="italic">ϕ</span>(<span class="bold">x</span>) is then defined as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn9"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t67_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t67.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t67.gif"/></a></td><td class="rightEqn">(9)</td></tr></table>where (·,·)<small><sub><span class="italic">F</span></sub></small> symbolizes an inner product on <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t68_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t68.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t68.gif"/></a>. For example, if <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t69_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t69.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t69.gif"/></a> then the inner product can be familiar <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t70_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t70.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t70.gif"/></a>.</span>
          <p class="otherpara">The importance of kernels lies in the fact that since the space <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t71_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t71.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t71.gif"/></a> is high-dimensional, direct computation of the feature map <span class="italic">ϕ</span>(<span class="italic">x</span>) in that space might be intractable and/or expensive. However most algorithms using the kernel trick are designed such that the only quantity required would be the inner product <span class="italic">K</span>(<span class="bold">x</span>,<span class="bold">x</span>′) (see <a title="" href="#eqn9">eqn (9)</a>) without explicit construction or manipulation of <span class="italic">ϕ</span>(<span class="bold">x</span>) or <span class="italic">ϕ</span>(<span class="bold">x</span>′). Thus several popular kernel functions have been reported in the literature<a title="Select to navigate to reference" href="#cit113"><sup><span class="sup_ref">113,114</span></sup></a> which can be computed directly from the entries <span class="bold">x</span> in the dataset <span class="italic">D</span>. Some of them are displayed below:<br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgugt72"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t72_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t72.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t72.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td><a id="ugt72"/><span id="ugt72"/></td><td class="pushTitleLeft"> </td></tr></table></div></p>
          <p class="otherpara">The success of the kernel trick has been extended to several important supervised machine learning algorithms like kernel-ridge regression, dimensionality reduction techniques like kernel-based principal component analysis, classification routines like <span class="italic">k</span>-nearest neighbor (see Section 3.2.4), support-vector machines (SVM) (see Section 3.2.7), <span class="italic">etc</span>. For classification tasks like in SVM the effect is more conventionally described as the inability of a hyperplane for linearly discriminating the data entries which can be ameliorated through the kernel trick of transporting the feature vectors <span class="italic">x</span> to a higher dimension <span class="italic">ϕ</span>(<span class="italic">x</span>) wherein such a separability is easily attainable. Both regression and classification algorithms will be discussed in detail in appropriate sections. In this section, we shall first discuss the kernel theory developed recently for quantum-computing enhanced machine learning techniques.</p>
          
            
            <span id="sect1480"/><br/><span class="d_heading_indent">3.2.1.1 Quantum enhanced variants. </span>
            <span>The theory of quantum kernels has been formalized in ref. <a title="Select to navigate to references" href="#cit116">116</a> and <a title="Select to navigate to references" href="#cit117">117</a>. For a given classical data set <span class="italic">D</span> = {(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>,<span class="bold">y</span><small><sub><span class="bold">i</span></sub></small>)|<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">χ</span>, <span class="bold">y</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">Ω</span> ∀ <span class="italic">i</span> ∈ [<span class="italic">m</span>]} as defined above wherein <span class="bold">x</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">χ</span>, a data domain, ref. <a title="Select to navigate to references" href="#cit116">116</a> defines a data-encoding feature map as a quantum state <span class="italic">ρ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>) = |<span class="italic">ϕ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>)〉〈<span class="italic">ϕ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>)| which is created from a data-encoding unitary <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t73_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t73.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t73.gif"/></a> as |<span class="italic">ϕ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>)〉 = <span class="bold">U</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>)|0〉<small><sup><span class="italic">n</span></sup></small>. This unitary <span class="italic">U</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>) thus embeds each feature vector of the dataset within a quantum state <span class="italic">ρ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>). The state <span class="italic">ρ</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>) is part of a Hilbert space <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t74_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t74.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t74.gif"/></a> which is thereby equipped with an inner product defined as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t75_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t75.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t75.gif"/></a>. The quantum variant of the kernel matrix entries from the dataset <span class="italic">D</span> is thus computed from this inner product as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn10"/><span id="eqn10"><span class="italic">K</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>,<span class="bold">x</span><small><sub><span class="bold">j</span></sub></small>) = <span class="bold">Tr</span>(<span class="italic">ρ</span>(<span class="bold">x</span>)<small><sub><span class="bold">i</span></sub></small><span class="italic">ρ</span>(<span class="bold">x</span><small><sub><span class="bold">j</span></sub></small>)).</span></td><td class="rightEqn">(10)</td></tr></table></span>
            <p class="otherpara">The authors prove that such a definition of a quantum kernel indeed satisfies Mercer's conditions<a title="Select to navigate to references" href="#cit118"><sup><span class="sup_ref">118</span></sup></a> of positive-semi definiteness. The authors then define a reproducing kernel Hilbert space (RKHS) which is a span of basis functions <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t76_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t76.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t76.gif"/></a> where the function <span class="italic">f</span>(<span class="bold">x</span>) = <span class="italic">K</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>,<span class="bold">x</span>), <span class="italic">i.e.</span>, each such basis function in the spanning set comprises quantum kernel matrix elements <span class="italic">K</span>(<span class="bold">x</span><small><sub><span class="bold">i</span></sub></small>,<span class="bold">x</span>) as defined in <a title="" href="#eqn10">eqn (10)</a> with one input argument of the matrix element being made from a particular datum (say <span class="bold">x</span><small><sub><span class="bold">i</span></sub></small> ∈ <span class="italic">χ</span>) of the dataset <span class="italic">D</span>. Any arbitrary function (say <span class="italic">g</span>(<span class="bold">x</span>)) that lives in the RKHS is thus a linear combination of such basis functions and is expressed as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn11"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t77_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t77.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t77.gif"/></a></td><td class="rightEqn">(11)</td></tr></table>where <span class="italic">α</span><small><sub><span class="italic">i</span></sub></small> are the linear combination coefficients. The author proves that any hypothesis function (say <span class="italic">h</span>(<span class="bold">x</span>) = <span class="bold">Tr</span>(<span class="bold">M</span><span class="italic">ρ</span>(<span class="bold">x</span>)) where <span class="bold">M</span> is the measurement operator) which the supervised learning task ‘learns’ on the quantum computer by minimizing a loss function is essentially a member of RKHS. In ref. <a title="Select to navigate to references" href="#cit117">117</a>, the authors propose two different approaches for utilizing quantum Kernel entries as defined in <a title="" href="#eqn10">eqn (10)</a>. The first approach which the authors call the implicit approach requires the quantum processor to just estimate entries of the Kernel matrix. The classical processor then performs the usual machine learning algorithm using this quantum-enhanced kernel. The second approach which the authors call the explicit approach involves performing the entire machine learning algorithm on the quantum computer using parameterized unitaries. We shall analyze examples of these approaches in Sections 4 and 5.2.</p>
          
        </div>
        <div>
          
          <span id="sect1604"/><span class="c_heading_indent">3.2.2 Ridge regression (RR) – linear and kernel based. </span>
          <span>This is a form of supervised machine learning which allows us to determine and construct an explicit functional dependence of the variates/labels and the feature vectors <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> based on certain tunable parameters.<a title="Select to navigate to reference" href="#cit119"><sup><span class="sup_ref">119–122</span></sup></a> The dependence can later be extrapolated and interpolated to learn values associated with unknown feature vectors not a part of the training set. Let us start with the feature vectors <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t78_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t78.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t78.gif"/></a> in dataset <span class="italic">D</span> defined in the above section. Using these vectors, one can define a design matrix often designated as <span class="italic">X</span> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn12"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t79_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t79.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t79.gif"/></a></td><td class="rightEqn">(12)</td></tr></table>Using the design matrix above and the training data label <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t80_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t80.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t80.gif"/></a> the objective is to fit a linear model of the kind <span class="italic">X<img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t81_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t81.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t81.gif"/></a> to the data and obtain the optimal fitting parameters. This can be done through the minimization of the following mean-squared error (MSE) loss<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn13"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t82_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t82.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t82.gif"/></a></td><td class="rightEqn">(13)</td></tr></table>In the expression above, the second term is the regularization to prevent over-fitting and also, in case if the column space of the design matrix <span class="italic">X</span> is not linearly independent, the presence of this term can facilitate inversion of <span class="italic">X</span><small><sup><span class="italic">T</span></sup></small><span class="italic">X</span>. The solution to <a title="" href="#eqn13">eqn (13)</a> (say <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span>*) is the following:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn14"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t83_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t83.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t83.gif"/></a></td><td class="rightEqn">(14)</td></tr></table>One must emphasize that the formulation is quite general and can be extended to cases wherein a constant term within the <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span> is necessary. This can be tackled by augmenting the design matrix as <span class="italic">X</span> → [<img class="charmap" src="https://www.rsc.org/images/entities/char_0031_20d1.gif" alt="[1 with combining right harpoon above (vector)]"/>|<span class="italic">X</span>]<small><sup><span class="italic">T</span></sup></small>. Also extension to polynomial regression is straightforward as one can create a design matrix treating higher powers of <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> as independent variables in each row of the design matrix as <span class="italic">X</span><small><sub><span class="italic">i</span></sub></small> → [<span class="italic">x</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">i</span></sub></small>(<span class="italic">x</span><small><sup>2</sup></small><small><sub><span class="italic">i</span></sub></small>)<small><sup><span class="italic">T</span></sup></small>, (<span class="italic">x</span><small><sup>3</sup></small><small><sub><span class="italic">i</span></sub></small>)<small><sup><span class="italic">T</span></sup></small>,…, (<span class="italic">x</span><small><sup><span class="italic">k</span></sup></small><small><sub><span class="italic">i</span></sub></small>)<small><sup><span class="italic">T</span></sup></small>] where <span class="italic">x</span><small><sup><span class="italic">k</span></sup></small><small><sub><span class="italic">i</span></sub></small> denotes raising <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> element-wise to the <span class="italic">k</span>th power.<a title="Select to navigate to reference" href="#cit123"><sup><span class="sup_ref">123,124</span></sup></a></span>
          <p class="otherpara">For the kernel variant of ridge-regression, if the prediction from the model is designated as <span class="italic">ỹ</span>(<span class="italic">x</span>,<span class="italic">α</span>), then the formalism represents the function <span class="italic">ỹ</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>,<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span>) as<a title="Select to navigate to reference" href="#cit125"><sup><span class="sup_ref">125,126</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn15"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t84_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t84.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t84.gif"/></a></td><td class="rightEqn">(15)</td></tr></table>where <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span> are trainable parameters and <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0062_20d1.gif" alt="[b with combining right harpoon above (vector)]"/></span> are the hyper-parameters associated with the kernel <span class="italic">K</span>(<span class="italic">x</span>,<span class="italic">x</span><small><sub><span class="italic">j</span></sub></small>,<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0062_20d1.gif" alt="[b with combining right harpoon above (vector)]"/></span>). These hyperparameters are fixed at the beginning of the optimization and can be tuned for separate runs to modify accuracy. Using the labels <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> of the training data set <span class="italic">D</span> defined before and <a title="" href="#eqn15">eqn (15)</a> one can now formulate a mean-squared error (MSE) loss (similar to <a title="" href="#eqn13">eqn (13)</a>) to learn the parameters <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span> as below<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn16"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t85_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t85.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t85.gif"/></a></td><td class="rightEqn">(16)</td></tr></table>The matrix <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_004b_0303.gif" alt="[K with combining tilde]"/></span> is called the Gram matrix of the kernel<a title="Select to navigate to references" href="#cit125"><sup><span class="sup_ref">125</span></sup></a> with entries as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn17"/><span id="eqn17"><span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_004b_0303.gif" alt="[K with combining tilde]"/></span><small><sub><span class="italic">ij</span></sub></small> = <span class="italic">K</span>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">x</span><small><sub><span class="italic">j</span></sub></small>,<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0062_20d1.gif" alt="[b with combining right harpoon above (vector)]"/></span>).</span></td><td class="rightEqn">(17)</td></tr></table>The minimizer of <a title="" href="#eqn16">eqn (16)</a> can be easily shown to be<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn18"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t86_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t86.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t86.gif"/></a></td><td class="rightEqn">(18)</td></tr></table>Another alternative formulation which leads to the same minimizer <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e118.gif" alt="[small alpha, Greek, vector]"/></span>* is the dual formulation of the problem which involves minimizing the following Langrangian:<a title="Select to navigate to references" href="#cit127"><sup><span class="sup_ref">127</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn19"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t87_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t87.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t87.gif"/></a></td><td class="rightEqn">(19)</td></tr></table>One can prove that the minimizer of <a title="" href="#eqn19">eqn (19)</a> is actually <a title="" href="#eqn18">eqn (18)</a>. This is a form of supervised machine learning which allows us to determine and construct an explicit functional dependence of the variates/labels and the feature vectors <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> based on certain tunable parameters.<a title="Select to navigate to reference" href="#cit119"><sup><span class="sup_ref">119,120,122</span></sup></a> The dependence can later be extrapolated and interpolated to learn values associated with unknown feature vectors that are not a part of the time dynamics<a title="Select to navigate to references" href="#cit128"><sup><span class="sup_ref">128</span></sup></a> or excited state dynamics.<a title="Select to navigate to references" href="#cit129"><sup><span class="sup_ref">129</span></sup></a> We shall return to a subset of these topics in Section 5.3.</p>
          
            
            <span id="sect1746"/><br/><span class="d_heading_indent">3.2.2.1 Quantum enhanced variants. </span>
            <span>Several quantum algorithms have been proposed in the last decade for solving linear systems which can be directly extended to the solution of the vanilla linear least-square fitting. The earliest was by Weibe <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit130"><sup><span class="sup_ref">130</span></sup></a> and is based on the Harrow, Hassidim, Lloyd (HHL) algorithm.<a title="Select to navigate to reference" href="#cit131"><sup><span class="sup_ref">131,132</span></sup></a> The technique starts with a non-hermitian <span class="italic">X</span> matrix (say <span class="italic">m</span> × <span class="italic">d</span> as in the design matrix in our example above) which is required to be sparse. The algorithm assumes oracular access to a quantum state encoding its row space and also a state encoding the <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0079_20d1.gif" alt="[y with combining right harpoon above (vector)]"/></span>. The key point of the routine is to expand the non-hermitian design matrix into a (<span class="italic">m</span> + <span class="italic">d</span>) × (<span class="italic">m</span> + <span class="italic">d</span>) dimensional matrix with which the quantum-phase estimation algorithm<a title="Select to navigate to references" href="#cit52"><sup><span class="sup_ref">52</span></sup></a> is performed. The ultimate product of this algorithm is a quantum state that encodes the fitted values. Even though extraction of the exact fitting parameters from the state might be exponentially hard yet prediction of a new <span class="italic">y</span> value for a given test-input can be made effortlessly through overlap with the register containing the fitted values. Variants of this algorithm for detecting statistic leverage score and matrix coherence have also been reported.<a title="Select to navigate to references" href="#cit133"><sup><span class="sup_ref">133</span></sup></a> Wang reported a quantum algorithm which can actually yield the fitted values as a vector just as in classical least squares.<a title="Select to navigate to references" href="#cit134"><sup><span class="sup_ref">134</span></sup></a> Both the method have query complexity which is <span class="italic">O</span>(log(<span class="italic">m</span>)). A subset of these algorithms has also been experimentally implemented on a variety of platforms like NMR,<a title="Select to navigate to references" href="#cit135"><sup><span class="sup_ref">135</span></sup></a> superconducting qubits,<a title="Select to navigate to references" href="#cit136"><sup><span class="sup_ref">136</span></sup></a> and photonic.<a title="Select to navigate to references" href="#cit137"><sup><span class="sup_ref">137</span></sup></a> Schuld <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit138"><sup><span class="sup_ref">138</span></sup></a> have also designed an algorithm which does not require the design matrix <span class="italic">X</span> to be sparse. The only requirement is that <span class="italic">X</span><small><sup>†</sup></small><span class="italic">X</span> should be well-represented by a low rank approximation, <span class="italic">i.e.</span>, should be dominated by few eigenvalues only. The key point in the technique is to perform quantum-phase estimation with a density matrix <span class="italic">ρ</span> encoding <span class="italic">X</span><small><sup>†</sup></small><span class="italic">X</span>. The algorithm also returns the fitted values encoded within a quantum state with which efficient overlap of a new input can be initiated. An algorithm by Yigit <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit139"><sup><span class="sup_ref">139</span></sup></a> which solves for a linear system of equation through adiabatic Hamiltonian evolution has also been demonstrated recently. A variational algorithm amenable to the NISQ era for linear equation solver has also been reported.<a title="Select to navigate to references" href="#cit140"><sup><span class="sup_ref">140</span></sup></a> The algorithm takes as input a gate sequence <span class="italic">U</span> that prepares and encodes the state <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0079_20d1.gif" alt="[y with combining right harpoon above (vector)]"/></span> and a design matrix <span class="italic">X</span> that is decomposable into implementable unitaries. The method implements a trainable unitary <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t88_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t88.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t88.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t89_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t89.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t89.gif"/></a> is a variational parameter. The aim of the unitary is to prepare a candidate state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t90_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t90.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t90.gif"/></a> which encodes a prospective solution to the least-square problem. The prospective solution is tuned using a cost-function which measures the overlap of the state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t91_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t91.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t91.gif"/></a> with the orthogonal subspace of the vector <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0079_20d1.gif" alt="[y with combining right harpoon above (vector)]"/></span> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn20"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t92_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t92.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t92.gif"/></a></td><td class="rightEqn">(20)</td></tr></table>The cost function above is minimized with respect to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t93_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t93.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t93.gif"/></a> on a classical computer and the parameter vector is fed into the trainable unitary <span class="italic">V</span> for the next iteration until the desired convergence is met. The authors show that the above cost-function being a global one suffers from barren plateaus and is rendered untrainable for the size of design matrix <span class="italic">X</span> being close to 2<small><sup>50</sup></small> × 2<small><sup>50</sup></small>. To evade the issue, they define local merit functions which remain faithful throughout. The ansatz used for encoding <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t94_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t94.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t94.gif"/></a> is the hardware-efficient ansatz and the algorithm showed logarithmic dependence on the error tolerance but near linear dependence on the condition number of the design matrix. The dependence on qubit requirements was found to be poly-logarithmic. The algorithm was implemented on an actual hardware for a design matrix of size 2<small><sup>10</sup></small> × 2<small><sup>10</sup></small>. Recently, Yu <span class="italic">et al.</span> reported an algorithm for ridge-regression (linear variant)<a title="Select to navigate to references" href="#cit141"><sup><span class="sup_ref">141</span></sup></a> which like the one reported by Weibe requires oracular access to elements of the design matrix and <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0079_20d1.gif" alt="[y with combining right harpoon above (vector)]"/></span>. The design matrix is expanded to make it hermitian and quantum-phase estimation is performed as before with respect to e<small><sup>−<span class="italic">iXt</span></sup></small> as the unitary to encode the eigenvalues onto an extra register. The difference comes at this stage when an ancillary qubit is added and rotated to invert the eigenvalues. The rotation angles are dependant on the Ridge parameter <span class="italic">λ</span>. Like previous algorithm this also yields the final optimal parameters as a quantum state. The authors also propose another quantum algorithm (which can be used along with this) for the choice of the Ridge parameter which is similar in principle to the <span class="italic">K</span>-fold cross validation technique.<a title="Select to navigate to references" href="#cit142"><sup><span class="sup_ref">142</span></sup></a> To the best of our knowledge, no quantum algorithm has been proposed that directly attempts to implement the kernelized variant of Ridge-regression but any of the aforesaid ones can be trivially extended with the replacement of the design matrix with the Gram matrix of an appropriate kernel.</span>
          
        </div>
        <div>
          
          <span id="sect1811"/><span class="c_heading_indent">3.2.3 Principal component analysis – linear and kernel based. </span>
          <span>Dimensionality reduction without sacrificing the variance of the data-set is very important for most machine learning tasks that have large number of features and comparatively fewer training samples. One starts with a dataset (<span class="italic">D</span> as discussed before where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t95_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t95.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t95.gif"/></a>). Here we define a design matrix (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t96_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t96.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t96.gif"/></a>) as in <a title="" href="#eqn12">eqn (12)</a> Formally the goal of PCA is to replace the matrix <span class="italic">X</span> with another matrix <span class="italic">Z</span> such that <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t97_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t97.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t97.gif"/></a> where <span class="italic">R</span> ≤ <span class="italic">d</span>.<a title="Select to navigate to reference" href="#cit119"><sup><span class="sup_ref">119,143,144</span></sup></a> To do this for the usual linear variant of the PCA one defines a mean-centered data matrix (say <span class="italic">B</span> = <span class="italic">X</span> − <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0058_0302.gif" alt="[X with combining circumflex]"/></span>) where <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0058_0302.gif" alt="[X with combining circumflex]"/></span> is the stacked row-wise mean of the data matrix (mean of each feature over the samples). One then constructs the covariance matrix<a title="Select to navigate to references" href="#cit144"><sup><span class="sup_ref">144</span></sup></a> (Cov(<span class="italic">B</span>) = <span class="italic">B</span><small><sup><span class="italic">T</span></sup></small><span class="italic">B</span>, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t98_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t98.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t98.gif"/></a>) and diagonalizes it to get the <span class="italic">d</span> eigenvectors <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t99_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t99.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t99.gif"/></a>. From the set <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t100_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t100.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t100.gif"/></a> one picks up the <span class="italic">R</span> eigenvectors with the largest eigenvalues to form a new matrix (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t101_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t101.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t101.gif"/></a>) as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn21"/><span id="eqn21"><span class="italic">V</span> = (<span class="italic">ν</span><small><sub>1</sub></small>, <span class="italic">ν</span><small><sub>2</sub></small>, <span class="italic">ν</span><small><sub>3</sub></small>,…, <span class="italic">ν</span><small><sub><span class="italic">R</span></sub></small>).</span></td><td class="rightEqn">(21)</td></tr></table>The principal component matrix <span class="italic">Z</span> defined before is the projection of the data matrix onto the space of matrix <span class="italic">V</span> as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn22"/><span id="eqn22"><span class="italic">Z</span> = <span class="italic">XV</span>.</span></td><td class="rightEqn">(22)</td></tr></table></span>
          <p class="otherpara">The kernel-based variant of PCA<a title="Select to navigate to references" href="#cit145"><sup><span class="sup_ref">145</span></sup></a> becomes important when the data need to be expressed in a high-dimensional subspace induced by the map <span class="italic">ϕ</span> as defined before, <span class="italic">i.e.</span>, ∀ <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t102_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t102.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t102.gif"/></a>, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t103_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t103.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t103.gif"/></a> where <span class="italic">p</span> ≥ <span class="italic">d</span>. One can then construct the covariance matrix in this new feature space of the data as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn23"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t104_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t104.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t104.gif"/></a></td><td class="rightEqn">(23)</td></tr></table>In principle, one can simply do a PCA in the feature space <span class="italic">ϕ</span>(<span class="italic">X</span>) to get the eigenvectors <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t105_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t105.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t105.gif"/></a> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn24"/><span id="eqn24">Cov(<span class="italic">ϕ</span>(<span class="italic">X</span>))<span class="italic">ν</span><small><sub><span class="italic">k</span></sub></small> = <span class="italic">λ</span><small><sub><span class="italic">k</span></sub></small><span class="italic">ν</span><small><sub><span class="italic">k</span></sub></small></span></td><td class="rightEqn">(24)</td></tr></table>where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t106_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t106.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t106.gif"/></a>. However, since this space is high-dimensional, computation can be expensive. It is thus desirable to use the power of the kernel and design an algorithm wherein the explicit construction and/or manipulation of <span class="italic">ϕ</span>(<span class="italic">X</span>) is evaded. To do so, one can expand the eigenvectors <span class="italic">ν</span><small><sub><span class="italic">k</span></sub></small> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn25"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t107_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t107.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t107.gif"/></a></td><td class="rightEqn">(25)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn26"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t108_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t108.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t108.gif"/></a></td><td class="rightEqn">(26)</td></tr></table>It is easy to show that the coefficient vector <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub><span class="italic">k</span></sub></small> ∀ <span class="italic">k</span> ∈ {1, 2,…, <span class="italic">p</span>} satisfies the eigenvalue equation for the Gram matrix (see <a title="" href="#eqn17">eqn (17)</a>) of the kernel as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn27"/><span id="eqn27"><span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_004b_0303.gif" alt="[K with combining tilde]"/><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub><span class="italic">k</span></sub></small> = (<span class="italic">λ</span><small><sub><span class="italic">k</span></sub></small><span class="italic">m</span>)<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub><span class="italic">k</span></sub></small>.</span></td><td class="rightEqn">(27)</td></tr></table>Thus one can simply diagonalize the Gram matrix of the kernel <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t109_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t109.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t109.gif"/></a> to get the coefficients of the eigenvectors of Cov(<span class="italic">ϕ</span>(<span class="italic">X</span>)) (see <a title="" href="#eqn24">eqn (24)</a>) without explicitly constructing <span class="italic">ϕ</span>(<span class="italic">X</span>) or even the covariance. Since valid kernels need to be positive-semi-definite as a condition imposed by Mercer's theorem,<a title="Select to navigate to reference" href="#cit146"><sup><span class="sup_ref">146,147</span></sup></a> one can choose a subset (say <span class="italic">R</span>) from <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t110_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t110.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t110.gif"/></a> in the decreasing order of their eigenvalues <span class="italic">λ</span><small><sub><span class="italic">k</span></sub></small> and construct a matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t111_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t111.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t111.gif"/></a> as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn28"/><span id="eqn28"><span class="italic">V</span> = (<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub>1</sub></small>, <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub>2</sub></small>,…, <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub><span class="italic">R</span></sub></small>)</span></td><td class="rightEqn">(28)</td></tr></table>thereby affording dimensionality reduction. Any projection onto <span class="italic">ν</span><small><sub><span class="italic">k</span></sub></small> can be computed using the vector <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e101.gif" alt="[small alpha, Greek, circumflex]"/></span><small><sub><span class="italic">k</span></sub></small> and the Kernel Gram matrix only as follows:<a title="Select to navigate to references" href="#cit148"><sup><span class="sup_ref">148</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn29"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t112_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t112.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t112.gif"/></a></td><td class="rightEqn">(29)</td></tr></table>We shall return to the applications of PCA in Section 5.5.</p>
          
            
            <span id="sect1954"/><br/><span class="d_heading_indent">3.2.3.1 Quantum enhanced variants. </span>
            <span>The very first instance of performing principal component analysis on a quantum computer was put forward by Lloyd <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit149"><sup><span class="sup_ref">149</span></sup></a> The algorithm starts with a matrix say <span class="italic">P</span> which is positive-semi-definite. Even the usual linear variant of the PCA using the covariance matrices of the mean-centered data was discussed as an application for the method, yet the algorithm can be extended to any positive-semi-definite matrix including the Gram matrices of Kernels. Any such matrix <span class="italic">P</span> can be written as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t113_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t113.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t113.gif"/></a> where |<span class="italic">e</span><small><sub><span class="italic">i</span></sub></small>〉 is the computational basis and |<span class="italic">a</span><small><sub><span class="italic">j</span></sub></small>〉 are column vectors of P normalized to 1<a title="Select to navigate to references" href="#cit149"><sup><span class="sup_ref">149</span></sup></a> and |<span class="italic">a</span><small><sub><span class="italic">j</span></sub></small>| is the corresponding norm. The algorithm assumes that an oracular object exists that encodes the column space onto a quantum state as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t114_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t114.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t114.gif"/></a> where |<span class="italic">e</span><small><sub><span class="italic">i</span></sub></small>〉 is an orthonormal basis usually the standard computational basis. The corresponding reduced density matrix for the first register in this state is exactly the positive semi-definite matrix <span class="italic">P</span>. The crux of the method is to prepare the unitary e<small><sup>−<span class="italic">iPt</span></sup></small>. This is done on a qubit register that encodes the reduced density matrix <span class="italic">P</span> as described above and also another density matrix (say <span class="italic">σ</span>). The repeated applications of e<small><sup>−<span class="italic">iSδ</span></sup></small> on the joint state <span class="italic">P</span> ⊗ <span class="italic">σ</span> with <span class="italic">n</span> copies of <span class="italic">P</span> yield e<small><sup>−<span class="italic">iPnδ</span></sup></small><span class="italic">σ</span>e<small><sup>−<span class="italic">iPnδ</span></sup></small> where <span class="italic">S</span> is the efficiently implementable SWAP operator. Once efficient preparation of the unitary e<small><sup>−<span class="italic">iPnδ</span></sup></small> has been conducted one can thereafter use standard phase estimation algorithms<a title="Select to navigate to references" href="#cit52"><sup><span class="sup_ref">52</span></sup></a> to measure the first say <span class="italic">R</span> eigenvalues and eigenvectors of the desired matrix <span class="italic">K</span> and cast the data matrix <span class="italic">X</span> in the form of <a title="" href="#eqn2">eqn (2)</a>. The algorithm produces the <span class="italic">R</span> eigenvectors with a time complexity of <span class="italic">O</span>(<span class="italic">R</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log(<span class="italic">m</span>)) where <span class="italic">m</span> is the column dimension of the Gram matrix defined in <a title="" href="#eqn17">(17)</a>. Since the above formulation relies on quantum-phase estimation to extract eigenvalues and eigenvectors of the Gram matrix, application of the procedure to near-term quantum devices is cost-prohibitive due to high-qubit and gate requirements. Recently Li <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit150"><sup><span class="sup_ref">150</span></sup></a> reported a new algorithm for the extraction of principal components of any positive-semi-definite matrix using a single ancillary qubit. The algorithm encodes a joint initial state of an ancillary qubit and the <span class="italic">n</span>-qubit positive semi-definite matrix (<span class="italic">P</span> in the notation and it could be a Gram matrix of the kernel as well as defined in <a title="" href="#eqn17">eqn (17)</a>) as |0〉〈0| ⊗ <span class="italic">P</span>. This state is evolved under the effect of the following Hamiltonian for a time δ<span class="italic">t</span> (see circuit for implementing Trotterized evolution in <a title="Select to navigate to figure" href="#imgfig4">Fig. 4</a>)<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn30"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t115_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t115.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t115.gif"/></a></td><td class="rightEqn">(30)</td></tr></table>where <span class="italic">c</span> is the strength of the drive on the probe ancillary qubit and <span class="italic">Δ</span> is its natural frequency. The probability of finding the probe ancillary qubit in state |1〉 after δ<span class="italic">t</span> is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn31"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t116_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t116.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t116.gif"/></a></td><td class="rightEqn">(31)</td></tr></table>where the index <span class="italic">i</span> is for any of the eigenvalues <span class="italic">ω</span><small><sub><span class="italic">i</span></sub></small> of the matrix <span class="italic">P</span>. The quantity <span class="italic">D</span><small><sub><span class="italic">i</span></sub></small> is defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn32"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t117_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t117.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t117.gif"/></a></td><td class="rightEqn">(32)</td></tr></table>So from <a title="" href="#eqn32">eqn (32)</a> one can directly see that a resonance condition is reached when <span class="italic">ω</span><small><sub><span class="italic">i</span></sub></small> ≈ <span class="italic">Δ</span>, <span class="italic">i.e.</span>, by sweeping the probe qubit frequency <span class="italic">Δ</span> one can enhance the probability of finding the probe qubit in state |1〉 which gives an estimate of the eigenvalue <span class="italic">ω</span><small><sub><span class="italic">i</span></sub></small>. Near such a resonance if the qubit is measured then it would collapse to |1〉 with high probability and the corresponding state in the <span class="italic">n</span>-qubit register would be |<span class="italic">ν</span><small><sub><span class="italic">i</span></sub></small>〉. Thus the entire spectrum of matrix <span class="italic">P</span> can be ascertained from which the PCA procedure as described above can be performed. The group conducted successful experimental implementation of the algorithm on a nitrogen vacancy center for a system of 2-qubits<a title="Select to navigate to references" href="#cit150"><sup><span class="sup_ref">150</span></sup></a> with dynamical decoupling sequences to prevent qubit de-phasing. Recently a direct kernel-based quantum-enhanced algorithm for PCA has been demonstrated too.<a title="Select to navigate to references" href="#cit151"><sup><span class="sup_ref">151</span></sup></a> The algorithm starts with an initial state which encodes the elements of the Gram matrix of the kernel (see <a title="" href="#eqn17">eqn (17)</a>). The register encoding the row vector of the Kernel is then conceived to be expanded on the basis of its eigenvectors (preparation of which is the target, see <a title="" href="#eqn27">eqn (27)</a>). The use of quantum-phase estimation followed by controlled rotations then prepares an auxillary qubit in a superposition state. Measurement of this ancillary qubit (with a probability proportional to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t118_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t118.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t118.gif"/></a> where <span class="italic">λ</span><small><sub><span class="italic">k</span></sub></small> are the eigenvalues of the Gram matrix) encodes columns of the target matrix <span class="italic">V</span> (see <a title="" href="#eqn28">eqn (28)</a>) onto the register scaled with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t119_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t119.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t119.gif"/></a> due to phase-kickback from the measurement. Note the final state thus prepared is not entirely the columns of <a title="" href="#eqn28">eqn (28)</a> but is scaled by the square root of the corresponding eigenvalues <span class="italic">λ</span><small><sub><span class="italic">k</span></sub></small>.</span>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig4"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f4_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f4.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f4.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 4 </b> <span id="fig4"><span class="graphic_title">The schematic of the quantum circuit used for the Trotterized evolution as illustrated in ref. <a title="Select to navigate to references" href="#cit150">150</a>. The eigenvalues and eigenvectors of the positive-semi definite matrix <span class="italic">P</span> are desired. At the end of the evolution, the ancilla qubit is measured and the algorithm is considered successful if it collapses to state |1〉 (see text for more details).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
        </div>
        <div>
          
          <span id="sect2069"/><span class="c_heading_indent">3.2.4 
            <span class="italic">k</span>-Nearest neighbors algorithm (<span class="italic">k</span>-NN). </span>
          <span>The kNN approach is based on the principle that the instances within a dataset will generally exist in close proximity to other instances that have similar properties. If the instances are tagged with a classification label, then the value of the label of an unclassified instance can be determined by observing the class of its nearest neighbours. The kNN locates the <span class="italic">k</span> nearest instances to the query instance and determines its class by identifying the single most frequent class label.<a title="Select to navigate to references" href="#cit152"><sup><span class="sup_ref">152</span></sup></a> In ML, instances are generally considered as points within an <span class="italic">n</span>-dimensional instance space, where each of the <span class="italic">n</span>-dimensions corresponds to one of the <span class="italic">n</span>-features. To classify a new test instance with the kNN method, the first step is to find the <span class="italic">k</span> most nearest instances of the training set according to some distance metrics. Then the resulting class is the most frequent class label of the <span class="italic">k</span> nearest instances. <a title="Select to navigate to figure" href="#imgfig5">Fig. 5</a> is a simple example of the kNN algorithm, where the blue dots and red triangles represent the training instances with two labels, and the grey diamond is a new test instance. In this example <span class="italic">k</span> is set as 5, and the 5 nearest instances are included in the black circle. For simplicity, here the relative distance <span class="italic">D</span>(<span class="italic">x</span>,<span class="italic">x</span>′) between two instances <span class="italic">x</span> and <span class="italic">x</span>′ is calculated by the Euclidean metric,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn33"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t120_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t120.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t120.gif"/></a></td><td class="rightEqn">(33)</td></tr></table>As there are 4 red triangles and only 1 blue dot, the class of the test instance is classified as a red triangle.</span>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig5"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f5_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f5.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f5.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 5 </b> <span id="fig5"><span class="graphic_title">A simple example of the kNN algorithm approach. The blue dots and red triangles represent the training instances with two labels, and the grey diamond is a new test instance. In this example <span class="italic">k</span> is set as 5, and the 5 nearest instances are included in the black circle. As there are 4 red triangles and only 1 blue dot, the class of the test instance is classified as a red triangle.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Generally, the relative distance is determined by using a distance metric instead of the absolute position of the instances. Apart from the Euclidean metric, some significant metrics are presented as follows:<br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgugt121"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t121_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t121.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t121.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td><a id="ugt121"/><span id="ugt121"/></td><td class="pushTitleLeft"> </td></tr></table></div>An ideal distance metric should be chosen to minimize the distance between two similarly classified instances, meanwhile maximizing the distance between instances of different classes. Sometimes even these typical metrics do not lead to satisfactory results; then, one might consider to learn a distance metric for kNN classification.<a title="Select to navigate to references" href="#cit153"><sup><span class="sup_ref">153</span></sup></a> In other words, the metric is optimized with the goal that the <span class="italic">k</span>-nearest neighbors always belong to the same class while examples from different classes are separated. When there are plenty of training instances, the centroid method<a title="Select to navigate to references" href="#cit154"><sup><span class="sup_ref">154</span></sup></a> could be applied initially, where the instances in different labels are clustered into several groups, and the kNN approach works on the centroid of each group instead of the original instances. Additionally, for more accurate classification, various weighting schemes<a title="Select to navigate to references" href="#cit155"><sup><span class="sup_ref">155</span></sup></a> could be included that alter the distance measurements and voting influence of each instance. We shall return to examples of <span class="italic">k</span>-NN in Section 5.2.</p>
          
            
            <span id="sect2102"/><br/><span class="d_heading_indent">3.2.4.1 Quantum enhanced variants. </span>
            <span>In 2013, Lloyd and coworkers proposed a quantum clustering algorithm for supervised or unsupervised QML,<a title="Select to navigate to references" href="#cit156"><sup><span class="sup_ref">156</span></sup></a> relying on the fact that estimating distances and inner products between post-processed vectors in <span class="italic">N</span>-dimensional vector spaces takes time <span class="italic">O</span>(log<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">N</span>) on a quantum computer whereas on a classical computer it would take <span class="italic">O</span>(<span class="italic">N</span>) time for sampling and estimating such distances and inner products, thereby apparently providing an exponential advantage.<a title="Select to navigate to references" href="#cit157"><sup><span class="sup_ref">157</span></sup></a> More discussion on this speedup on a quantum computer can be found in Section 5.2. The significant speedup of estimating distances provokes enormous enthusiasm for studying QML, particularly the quantum instance-based learning algorithms. Wiebe and coworkers developed the quantum nearest neighbor algorithm based on the Euclidean distance, and studied the performance on several real-world binary classification tasks.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> Moreover, assorted quantum kNN methods<a title="Select to navigate to reference" href="#cit153"><sup><span class="sup_ref">153,158,159</span></sup></a> are proposed with heterogeneous distance metrics, assisting in solving a variety of pattern recognition problems.</span>
            <p class="otherpara">The structure of the quantum nearest-neighbor algorithm is shown in <a title="Select to navigate to figure" href="#imgfig6">Fig. 6</a>.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> The quantum nearest neighbor algorithm can be implemented briefly in three steps.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> First, for each training vector <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small>, prepare a state that encodes the distance between the test instance <span class="bold">u</span> and <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> in an amplitude using the subroutine for an appropriate distance metric. Then, use coherent amplitude amplification to store the distance estimate as a qubit string without measuring the state. Finally, find the <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> that minimizes the distance under certain distance metrics, and <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> is the nearest instance. Label of the test instance <span class="bold">u</span> is thus predicted as the same label as <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small>.</p>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig6"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f6_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f6.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f6.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 6 </b> <span id="fig6"><span class="graphic_title">Scheme of the structure of the quantum nearest-neighbor algorithm.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> First for each training vector <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small>, prepare a state that encodes the distance between the test instance <span class="bold">u</span> and <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> in the amplitudes using the subroutine for an appropriate distance metric. Then, use coherent amplitude amplification to store the distance estimate as a qubit string without measuring the state. Finally, find the <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> that minimizes the distance under certain distance metrics, and <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small> is the nearest instance. Label of the test instance <span class="bold">u</span> is thus predicted as the same label as <span class="bold">v</span><small><sub><span class="italic">j</span></sub></small>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
        </div>
        <div>
          
          <span id="sect2156"/><span class="c_heading_indent">3.2.5 Decision trees. </span>
          <span>Decision trees are a way to represent rules underlying data with hierarchical, sequential structures that recursively partition the data.<a title="Select to navigate to references" href="#cit160"><sup><span class="sup_ref">160</span></sup></a> In other words, decision trees are trees classifying instances by sorting them based on their features. Each node in a decision tree represents a feature in an instance to be classified, and each branch represents a value that the node can assume. Instances are classified starting from the root node and sorted based on their specific feature values.</span>
          <p class="otherpara">A simple example is shown in <a title="Select to navigate to figure" href="#imgfig7">Fig. 7</a>, where four chemical substances are classified using a decision tree model. Instances are classified starting from the first node, or the root node, where we study the state of matter at standard temperature and pressure (STP). If the instance is gas, then it will be assigned as hydrogen. If it is liquid, then it will be assigned as mercury. For the solid state, we further go to the next node, where we study its electrical resistivity (STP). The instance as conductor will be classified as copper, while that as an insulator is classified as silicon.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig7"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f7_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f7.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f7.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 7 </b> <span id="fig7"><span class="graphic_title">Scheme of the classification process with a decision tree. Instances are classified starting from the first node, or the root node, where we study the state of matter at standard temperature and pressure (STP). If the instance is gas, then it will be assigned as hydrogen. If it is liquid, then it will be assigned as mercury. For the solid state, we further go to the next node, where we study its electrical resistivity (STP). The instance as a conductor will be classified as copper, while that as an insulator is classified as silicon. For simplicity, we only consider these four chemical substances.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Constructing optimal binary decision trees is an NP-complete problem, making it possible to find efficient heuristics for constructing near-optimal decision trees.<a title="Select to navigate to references" href="#cit161"><sup><span class="sup_ref">161</span></sup></a> The feature that best divides the training data should be assigned as the root node of the tree. There are numerous methods for finding the feature that best divides the training data such as information gain<a title="Select to navigate to references" href="#cit162"><sup><span class="sup_ref">162</span></sup></a> and gini index.<a title="Select to navigate to references" href="#cit163"><sup><span class="sup_ref">163</span></sup></a> Comparison of individual methods may still be important when deciding which metric should be used for a particular dataset. We shall return to examples of decision trees in Sections 5.2 and 5.5.</p>
          
            
            <span id="sect2169"/><br/><span class="d_heading_indent">3.2.5.1 Quantum enhanced variants. </span>
            <span>In 1998, Farhi and coworkers proposed a design of quantum decision trees, which can be experimentally implemented on a quantum computer that consists of enough spin-½ particles. They further studied a single time-independent Hamiltonian that evolves a quantum state through the nodes of a decision tree.<a title="Select to navigate to references" href="#cit164"><sup><span class="sup_ref">164</span></sup></a> It has been proven that if the classical strategy succeeds in reaching the <span class="italic">n</span>-th level of the decision tree in runtime polynomial in <span class="italic">n</span>, then the quantum algorithm also requires time polynomial of <span class="italic">n</span> to reach the same level. Moreover, they found examples where the interference allows a class of trees to be penetrated exponentially faster by quantum evolution than by a classical random walk. However, these examples could also be solved in polynomial time by different classical algorithms.</span>
            <p class="otherpara">A quantum training dataset with <span class="italic">n</span> quantum data pairs can be described as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn34"/><span id="eqn34"><span class="italic">D</span> = {(|<span class="italic">x</span><small><sub>1</sub></small>〉,|<span class="italic">y</span><small><sub>1</sub></small>〉), (|<span class="italic">x</span><small><sub>2</sub></small>〉,|<span class="italic">y</span><small><sub>2</sub></small>〉),…, (|<span class="italic">x</span><small><sub><span class="italic">n</span></sub></small>〉,|<span class="italic">y</span><small><sub><span class="italic">n</span></sub></small>〉)}</span></td><td class="rightEqn">(34)</td></tr></table>where the quantum state |<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>〉 denotes the <span class="italic">i</span>th quantum object of the training dataset, and state |<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>〉 denotes the corresponding label. Due to the existence of superposition, the classical node splitting criteria can hardly work in the quantum world. Instead, criteria such as quantum entropy impurity<a title="Select to navigate to references" href="#cit165"><sup><span class="sup_ref">165</span></sup></a> are required to find the optimal features when designing quantum decision trees. Recently, a quantum version of the classification decision tree constructing algorithm is proposed,<a title="Select to navigate to references" href="#cit166"><sup><span class="sup_ref">166</span></sup></a> which is designed based on the classical version C5.0.<a title="Select to navigate to references" href="#cit167"><sup><span class="sup_ref">167</span></sup></a></p>
          
        </div>
        <div>
          
          <span id="sect2207"/><span class="c_heading_indent">3.2.6 Bayesian networks (BN). </span>
          <span>Bayesian networks (BN) are the most well known representative of statistical learning algorithms, which are graphical models of causal relationships in a given domain. BN is defined to consist of the following:<a title="Select to navigate to references" href="#cit168"><sup><span class="sup_ref">168</span></sup></a></span>
          <p class="otherpara">1. A set of variables and a set of directed edges between variables.</p>
          <p class="otherpara">2. Each variable has a finite set of mutually exclusive states.</p>
          <p class="otherpara">3. The variables together with the directed edges form a directed acyclic graph (DAG).</p>
          <p class="otherpara">4. To each variable <span class="italic">A</span> with parents <span class="italic">B</span><small><sub>1</sub></small>, <span class="italic">B</span><small><sub>2</sub></small>,…, <span class="italic">B</span><small><sub><span class="italic">n</span></sub></small>, there is attached the conditional probability table (CPT) <span class="italic">P</span>(<span class="italic">A</span>|<span class="italic">B</span><small><sub>1</sub></small>, <span class="italic">B</span><small><sub>2</sub></small>,…, <span class="italic">B</span><small><sub><span class="italic">n</span></sub></small>).</p>
          <p class="otherpara">The learning process of the BN methods generally contains two subtasks: the construction of the DAG network and the determination of parameters. The approach to design the structure is based on two observations.<a title="Select to navigate to references" href="#cit169"><sup><span class="sup_ref">169</span></sup></a> First, people can often readily assert causal relationships among variables. Second, causal relationships typically correspond to assertions of conditional dependence. In particular, to construct a Bayesian network for a given set of variables, we simply draw arcs from cause variables to their immediate effects. Sometimes the structure of the network is given; then the parameters in the CPT are usually learnt by estimating a locally exponential number of parameters from the data provided.<a title="Select to navigate to references" href="#cit168"><sup><span class="sup_ref">168</span></sup></a></p>
          <p class="otherpara">
            <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a> is an example of the BN assisted study of the scattering experiment between atom A and molecule beams B. Arcs should be drawn from cause to effect in the network. In chemical reactions we know that the initial states are causes, while the collected results are effects. The local probability distributions associated with a node are shown adjacent to the node. For simplicity, here we assume that all the features (nodes) are binary, such as the feature g will be set as ‘true’ or ‘yes’ as long as the kinetic energy of molecule beams B is equal or greater than some certain threshold. We shall return to applications of BN in Section 5.2.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig8"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f8_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f8.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f8.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 8 </b> <span id="fig8"><span class="graphic_title">Scheme of the BN assisted study of scattering experiment between atom A and molecule beams B. Atoms A and molecule beams B are initially prepared at certain initial states before the collision. The special patterns observed in the scattering experiment results are denoted as patterns C and D. In the network, arcs are drawn from cause to effect. In chemical reactions we know that the initial states are causes, while the collected results are effects. The local probability distribution(s) associated with a node are shown adjacent to the node. For simplicity, here we assume that all the features (nodes) are binary, such as the feature g will be set as ‘true’ or ‘yes’ as long as the kinetic energy of molecule beams B is equal or greater than some certain threshold.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
            
            <span id="sect2240"/><br/><span class="d_heading_indent">3.2.6.1 Quantum enhanced variants. </span>
            <span>In 1995, Tucci proposed the first design of quantum BN, which could be constructed by replacing real probabilities in classical BN with quantum complex amplitudes.<a title="Select to navigate to references" href="#cit170"><sup><span class="sup_ref">170</span></sup></a> Leifer and Poulin proposed another model in 2008, constructing quantum BN based on probability distributions, quantum marginal probabilities and quantum conditional probabilities.<a title="Select to navigate to references" href="#cit171"><sup><span class="sup_ref">171</span></sup></a> However, neither of these models could provide any advantage compared with the classical models, because they cannot take into account the interference effects between random variables.<a title="Select to navigate to references" href="#cit172"><sup><span class="sup_ref">172</span></sup></a> A quantum-like BN based on quantum probability amplitudes was proposed by Moreira and Wichert in 2016,<a title="Select to navigate to references" href="#cit172"><sup><span class="sup_ref">172</span></sup></a> where a similarity heuristic method was required to determine the parameters.</span>
            <p class="otherpara">On the other hand, in 2014, Low and coworkers discussed the principles of quantum circuit design to represent a Bayesian network with discrete nodes that have two states. Notably, it is reported that the graph structure of BN is able to efficiently construct a quantum state representing the intended classical distribution, and a square-root speedup time can be obtained per sample by implementing a quantum version of rejection sampling.<a title="Select to navigate to references" href="#cit173"><sup><span class="sup_ref">173</span></sup></a> Recently, Borujeni and coworkers further expanded the quantum representation of generic discrete BN with nodes that may have two or more states.<a title="Select to navigate to references" href="#cit174"><sup><span class="sup_ref">174</span></sup></a></p>
            <p class="otherpara">There are mainly three steps to construct the quantum circuit representing a BN. First, map a BN node to one or more qubits depending on the number of states. The next step is to map the marginal or conditional probabilities of nodes to probability amplitudes associated with the qubits to be in the |0〉 and |1〉 states. The final step is to realize the required probability amplitudes using single-qubit and controlled rotation gates.</p>
            <p class="otherpara">
              <a title="Select to navigate to figure" href="#imgfig9">Fig. 9</a> is the quantum circuit representing the BN shown in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>. The quantum circuit shown in <a title="Select to navigate to figure" href="#imgfig9">Fig. 9</a> is constructed based on the three steps discussed above. The first step is to assign qubits for each node shown in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>. Here for simplicity, we only assign one qubit for the corresponding node. There are in total four qubits, <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, <span class="italic">q</span><small><sub>3</sub></small>, and <span class="italic">q</span><small><sub>4</sub></small>, corresponding to the four nodes, g, h, c, and d, all of which are initially set at the |0〉 state. Next, we need to map the conditional probabilities of nodes to probability amplitudes. In the BN shown in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>, there are only two possible results for each node, yes or no. Here, we use quantum state |0〉 to represent no, and state |1〉 to represent yes. Then we need to realize the required probability amplitudes using single-qubit and controlled rotation gates. Single-qubit rotation gates are applied to construct the independent probability for nodes g and h, as <span class="italic">p</span>(g = yes) and <span class="italic">p</span>(h = yes) have nothing to do with the states of other nodes. Node g is a root node. In other words, there are no arcs pointing to node g in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>, so that a single <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small>(<span class="italic">θ</span><small><sub>1</sub></small>) gate is applied on <span class="italic">q</span><small><sub>1</sub></small>. The value of <span class="italic">θ</span><small><sub>1</sub></small> can be derived from the constraint,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn35"/><span id="eqn35"><span class="italic">p</span>(g = yes) = |〈1|<small><sub><span class="italic">q</span>1</sub></small>|<span class="italic">Ψ</span>〉|<small><sup>2</sup></small></span></td><td class="rightEqn">(35)</td></tr></table>where we denote the final state as |<span class="italic">Ψ</span>〉, and use state |1〉 to represent ‘yes’ as mentioned before. Similarly the value of <span class="italic">θ</span><small><sub>2</sub></small> can be calculated, as h is also a root node. The controlled rotation gates are used to construct conditional probabilities. For example, to construct <span class="italic">p</span> (c = yes|g = yes), we need to build a controlled rotation gate between <span class="italic">q</span><small><sub>1</sub></small> (control qubit, representing node g) and <span class="italic">q</span><small><sub>3</sub></small> (target qubit, representing node c). As the condition is g = yes, the controlled rotation gate works when the control qubit is at state |1〉, thus there is a solid dot in the corresponding operation in <a title="Select to navigate to figure" href="#imgfig9">Fig. 9</a>. On the other hand, when the condition is g = no, then the controlled rotation gate will work when the control qubit is at state |0〉, leading to a hollow dot in the quantum circuit. As there are only two arcs in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>, there are two controlled-<span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates involving c and g, in one of which g = yes and in the other g = no. The value of <span class="italic">θ</span><small><sub>3</sub></small> can be obtained from,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn36"/><span id="eqn36"><span class="italic">p</span>(c = yes|g = yes) = |(〈1|<small><sub><span class="italic">q</span><small><sub>1</sub></small></sub></small> ⊗ 〈1|<small><sub><span class="italic">q</span><small><sub>3</sub></small></sub></small>)|<span class="italic">Ψ</span>〉|<small><sup>2</sup></small>.</span></td><td class="rightEqn">(36)</td></tr></table>Similarly we can obtain <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t122_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t122.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t122.gif"/></a>. To construct the condition probabilities with more than one condition, we need to include control rotation gates with more than one control qubit. For example, there are two arcs pointing to node d in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>, one of which comes from node g and the other comes from h. Thus, in the circuit there are four control–control–<span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates where <span class="italic">q</span><small><sub>1</sub></small>(g) and <span class="italic">q</span><small><sub>2</sub></small>(h) are the control qubits and <span class="italic">q</span><small><sub>4</sub></small>(d) is the target qubit corresponding to 4 different choices of configurations between g, h, <span class="italic">i.e.</span>, when both are ‘yes’, both are ‘no’, and one of them is ‘yes’ and the other is ‘no’ and <span class="italic">vice versa</span>. The value of <span class="italic">θ</span><small><sub>4</sub></small> can be obtained from<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn37"/><span id="eqn37"><span class="italic">p</span>(d = yes|g = yes, h = yes) = |(〈1|<small><sub><span class="italic">q</span><small><sub>1</sub></small></sub></small> ⊗ 〈1|<small><sub><span class="italic">q</span><small><sub>2</sub></small></sub></small> ⊗ 〈1|<small><sub><span class="italic">q</span><small><sub>4</sub></small></sub></small>)|<span class="italic">Ψ</span>〉|<small><sup>2</sup></small>.</span></td><td class="rightEqn">(37)</td></tr></table>So that all parameters in the quantum gates are determined by the probability distribution from the DAG in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>. On the other hand, one could obtain the conditional probability from a given quantum BN by measuring all qubits and estimating the corresponding frequency. For instance, the probability <span class="italic">p</span>(d = yes|g = yes, h = yes) could be estimated by the frequency that <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, and <span class="italic">q</span><small><sub>4</sub></small> are all at state |1〉. For simplicity, in the example we demonstrate a quantum representation of BN with nodes that have only two states (‘yes’ or ‘no’). The quantum representation of a more intricate BN structure is discussed thoroughly in Borujeni and coworkers’ recent work.<a title="Select to navigate to references" href="#cit174"><sup><span class="sup_ref">174</span></sup></a></p>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig9"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f9_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f9.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f9.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 9 </b> <span id="fig9"><span class="graphic_title">Quantum circuit for the BN shown in <a title="Select to navigate to figure" href="#imgfig8">Fig. 8</a>. For quantum versions of more complex BN structures refer to ref. <a title="Select to navigate to references" href="#cit174">174</a>. There are in total four qubits, <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, <span class="italic">q</span><small><sub>3</sub></small>, and <span class="italic">q</span><small><sub>4</sub></small>, corresponding to the four nodes, g, h, c, and d, all of which are initially set at the |0〉 state. <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates are applied directly on root nodes to prepare the quantum states corresponding to the probability amplitudes. For example, node g is a root node (in other words, there are no arcs pointing to node g), so that a single <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small>(<span class="italic">θ</span><small><sub>1</sub></small>) gate is applied on <span class="italic">q</span><small><sub>1</sub></small>. Control-<span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates correspond to the arcs in the BN. For example, there is only a single arc pointing to node c, which comes from node g. Thus, in the circuit there are two control-<span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates where <span class="italic">q</span><small><sub>1</sub></small> is the control qubit and <span class="italic">q</span><small><sub>3</sub></small> is the gate qubit. All the parameters can be derived from the DAG.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
        </div>
        <div>
          
          <span id="sect2384"/><span class="c_heading_indent">3.2.7 Support vector machines (SVMs). </span>
          <span>Support vector machines (SVMs) revolve around the margin that separates two data classes. Implementation of SVM contains two main steps: first, map the input data into a high-dimensional feature space using some nonlinear methods, and then construct an optimal separating hyperplane. Support vector machines (SVMs) can deal with both regression and classification tasks. Mathematically, if the dataset <span class="bold">x</span> is linearly separable and is capable of being assigned into groups denoted by two labels <span class="italic">A</span> and <span class="italic">B</span>, there exist a weight vector <span class="bold">w</span> and a bias constant <span class="italic">b</span>, ensuring that<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn38"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t123_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t123.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t123.gif"/></a></td><td class="rightEqn">(38)</td></tr></table>Thereby, the classification rule for test instance <span class="bold">x</span><small><sub><span class="italic">t</span></sub></small> is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn39"/><span id="eqn39"><span class="italic">y</span><small><sub><span class="bold">w</span>,<span class="italic">b</span></sub></small>(<span class="bold">x</span><small><sub><span class="italic">t</span></sub></small>) = sgn(<span class="bold">w</span><small><sup><span class="italic">T</span></sup></small><span class="bold">x</span><small><sub><span class="italic">t</span></sub></small> + <span class="italic">b</span>).</span></td><td class="rightEqn">(39)</td></tr></table>Finding the optimal hyperplane is equivalent to a convex quadratic programming problem that minimizes the functional<a title="Select to navigate to references" href="#cit175"><sup><span class="sup_ref">175</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn40"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t124_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t124.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t124.gif"/></a></td><td class="rightEqn">(40)</td></tr></table></span>
          <p class="otherpara">
            <a title="Select to navigate to figure" href="#imgfig10">Fig. 10</a> is a scheme of the SVM classification with a hyperplane. Blue dots and red triangles represent the training instances with two labels. The black line represents the optimal hyperplane, which maximizes the margin between the blue and red instances. The red and blue dash lines are hyperlines that can separate the two groups apart, though the corresponding margin is less than the optimal one.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig10"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f10_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f10.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f10.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 10 </b> <span id="fig10"><span class="graphic_title">Scheme of the SVM classification with a hyperplane. Blue dots and red triangles represent the training instances with two labels. The black line represents the optimal hyperplane, which maximizes the margin between the blue and red instances. The red and blue dash lines are hyperlines that can separate the two groups apart, though the corresponding margin is less than the optimal one.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Sometimes due to the misclassified instances SVMs are not able to find any separating hyperplane that can perfectly separate two groups apart. Then the soft margin and penalty functions could be applied where some misclassifications of the training instances are accepted.<a title="Select to navigate to references" href="#cit175"><sup><span class="sup_ref">175</span></sup></a></p>
          <p class="otherpara">Moreover, real-world problems often involve non-separable data, where there is no separating hyperplane initially even without misclassifications. Then the training data should be first mapped onto a higher dimensional space, where the separating hyperplane would be constructed. This higher-dimensional space is generally denoted as the transformed feature space, while the training instances occupy the input space. Instead of repeating the mapping process <span class="italic">Φ</span>(<span class="italic">x</span>) explicitly, the more popular approach is to calculate the Kernel functions defined in <a title="" href="#eqn9">eqn (9)</a> which allow inner products to be calculated directly in the feature space.<a title="Select to navigate to references" href="#cit175"><sup><span class="sup_ref">175</span></sup></a> After successfully constructing the hyperplane, new instances are mapped into the feature space by Kernel functions for classification.</p>
          <p class="otherpara">SVM methods perform binary classification; thus, to solve multi-class problems we must reduce the problem into a set of multiple binary classification problems. A core advantage of SVM is that training the optimization problem of the SVM necessarily reaches a global minimum, instead of being trapped in a local minimum. We shall return to applications in Sections 5.2, 5.5 and 4.</p>
          
            
            <span id="sect2432"/><br/><span class="d_heading_indent">3.2.7.1 Quantum enhanced variants. </span>
            <span>Enthused by the success of SVM assisted big data classification, Rebentrost and coworkers proposed the implementation of quantum SVM.<a title="Select to navigate to references" href="#cit177"><sup><span class="sup_ref">177</span></sup></a></span>
            <p class="otherpara">Rewrite the weight vector <span class="bold">w</span> in <a title="" href="#eqn38">eqn (38) and (39)</a> as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn41"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t125_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t125.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t125.gif"/></a></td><td class="rightEqn">(41)</td></tr></table>where <span class="italic">α</span><small><sub><span class="italic">j</span></sub></small> is the weight of the <span class="italic">i</span>th training instance <span class="bold">x</span><small><sub><span class="italic">j</span></sub></small>, and there are <span class="italic">M</span> training instances in total. In the SVM with least-squares approximation, the optimal parameters <span class="italic">α</span><small><sub><span class="italic">j</span></sub></small> and <span class="italic">b</span> can be obtained by solving the linear equation<a title="Select to navigate to references" href="#cit177"><sup><span class="sup_ref">177</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn42"/><span id="eqn42"><span class="italic">F</span>(<span class="italic">b</span>, <span class="italic">α</span><small><sub>1</sub></small>, <span class="italic">α</span><small><sub>2</sub></small>,…, <span class="italic">α</span><small><sub><span class="italic">M</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> = (0, <span class="italic">y</span><small><sub>1</sub></small>, <span class="italic">y</span><small><sub>2</sub></small>,…, <span class="italic">y</span><small><sub><span class="italic">M</span></sub></small>)<small><sup><span class="italic">T</span></sup></small></span></td><td class="rightEqn">(42)</td></tr></table>where <span class="italic">F</span> is a (<span class="italic">M</span> + 1) × (<span class="italic">M</span> + 1) matrix with the essential part as the kernel. <a title="Select to navigate to figure" href="#imgfig11">Fig. 11</a> is a diagram of the quantum SVM.<a title="Select to navigate to references" href="#cit176"><sup><span class="sup_ref">176</span></sup></a> We can rewrite the classification rule as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn43"/><span id="eqn43"><span class="italic">y</span>(<span class="italic">x</span>′) = sgn[〈<span class="italic">ψ</span>|<span class="italic">Ô</span>|<span class="italic">ψ</span>〉]</span></td><td class="rightEqn">(43)</td></tr></table>where |<span class="italic">ψ</span>〉 is the final quantum state. The big picture is that if the expectation value in the above equation is greater than zero, then the test instance <span class="italic">x</span>′ will be assigned as label positive (<span class="italic">y</span>(<span class="italic">x</span>′) = 1). Otherwise, it will be predicted with a negative label (<span class="italic">y</span>(<span class="italic">x</span>′) = −1).</p>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig11"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f11_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f11.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f11.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 11 </b> <span id="fig11"><span class="graphic_title">The schematic diagram of quantum SVM illustrated in ref. <a title="Select to navigate to references" href="#cit176">176</a>. The qubits can be assigned into three groups: training registers (blue) that represent the training instances, label qubit (green) that takes the label, and ancillary qubit (grey). The matrix inversion is employed to acquire hyperplane parameters. Then, the training-data oracle is applied to prepare the training-data state. Classification of new test instance <span class="bold">x</span>′ is introduced by operation <span class="italic">U</span>(<span class="bold">x</span>′).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
            <p class="otherpara">The circuit has three primary components in a nutshell as shown in <a title="Select to navigate to figure" href="#imgfig11">Fig. 11</a>: matrix inversion operation (green) is designed to acquire the hyperplane parameters; training-data oracle (blue) is included to prepare the training-data state; and <span class="italic">U</span>(<span class="italic">x</span>′) is to map the test instance <span class="italic">x</span>′ into quantum states. In a classical SVM, the hyperplane is obtained by minimizing the functional as shown in <a title="" href="#eqn39">eqn (39)</a>, while in qSVM, the hyperplane is obtained <span class="italic">via</span> solving linear equations, which leads to an exponential speedup.</p>
            <p class="otherpara">Let us now get into the details of the quantum version of the algorithm. The qubits can be assigned into three groups: training registers (blue) that represent the training instances, label qubit (green) that takes the label, and ancillary qubit (grey). The matrix inversion is employed to acquire hyperplane parameters. Then, the training-data oracle is applied to prepare the training-data state. Classification of a new test instance <span class="bold">x</span>′ is introduced by operation <span class="italic">U</span>(<span class="bold">x</span>′).</p>
            <p class="otherpara">The training-data oracles are designed to return the quantum counterpart of the training data <span class="bold">x</span><small><sub><span class="italic">i</span></sub></small>,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn44"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t126_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t126.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t126.gif"/></a></td><td class="rightEqn">(44)</td></tr></table>where (<span class="bold">x</span><small><sub><span class="italic">i</span></sub></small>)<small><sub><span class="italic">j</span></sub></small> is the <span class="italic">j</span>th component of the training instance <span class="bold">x</span><small><sub><span class="italic">i</span></sub></small>. The training-data oracles will convert the initial state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t127_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t127.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t127.gif"/></a> into state |<span class="italic">χ</span>〉, where<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn45"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t128_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t128.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t128.gif"/></a></td><td class="rightEqn">(45)</td></tr></table>with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t129_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t129.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t129.gif"/></a> is the normalization factor.</p>
            <p class="otherpara">Optimization is implemented by the quantum algorithm solving linear equations, which provide exponential speedup compared to the classical version.<a title="Select to navigate to references" href="#cit178"><sup><span class="sup_ref">178</span></sup></a> Registers are initialized into state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t130_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t130.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t130.gif"/></a>. After applying the matrix inversion operation, the quantum state is transformed to<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn46"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t131_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t131.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t131.gif"/></a></td><td class="rightEqn">(46)</td></tr></table>With the optimal parameters <span class="italic">α</span><small><sub><span class="italic">j</span></sub></small>, <span class="italic">b</span>, the classification rule corresponding to <a title="" href="#eqn39">eqn (39)</a> can be written as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn47"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t132_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t132.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t132.gif"/></a></td><td class="rightEqn">(47)</td></tr></table>where for simplicity, the linear Kernel is considered. The classification result will be derived by measuring the expectation value of the coherent term <span class="italic">Ô</span> = |00〉〈⊗(|1〉〈0|)<small><sub><span class="italic">A</span></sub></small>, where subscript <span class="italic">A</span> denotes the ancillary qubit.</p>
            <p class="otherpara">In spite of constructing quantum circuits to acquire the hyperplane, researchers further developed quantum kernel methods which harness the computational power of quantum devices. In 2019, researchers from Xanadu proposed to compute a classically intractable kernel by estimating the inner products of quantum states,<a title="Select to navigate to references" href="#cit179"><sup><span class="sup_ref">179</span></sup></a> while the kernel can then be fed into any classical kernel method such as the SVM. The crucial component of quantum kernel methods is quantum feature maps, which map a classical data point <span class="italic">x</span> as an <span class="italic">n</span>-qubit quantum state |<span class="italic">ϕ</span>(<span class="italic">x</span>)〉 nonlinearly, where the feature state |<span class="italic">ϕ</span>(<span class="italic">x</span>)〉 = <span class="italic">U</span>(<span class="italic">x</span>)|0〉 is obtained by a parameterized circuit family {<span class="italic">U</span>(<span class="italic">x</span>)}.<a title="Select to navigate to references" href="#cit180"><sup><span class="sup_ref">180</span></sup></a> In the learning process, quantum feature maps take the position of pattern recognition. More details about the quantum kernel methods can be found in Section 4 (<a title="Select to navigate to figure" href="#imgfig12">Fig. 12 and 13</a>).</p>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig12"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f12_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f12.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f12.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 12 </b> <span id="fig12"><span class="graphic_title">Schematic representing quantum-enhanced kernel for Gaussian process regression as described in ref. <a title="Select to navigate to references" href="#cit181">181</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig13"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f13_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f13.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f13.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 13 </b> <span id="fig13"><span class="graphic_title">(a) A schematic of a generalized perceptron. The input is a vector <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t151_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t151.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t151.gif"/></a> and the output is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t152_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t152.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t152.gif"/></a>. The parameters of the model are <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t153_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t153.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t153.gif"/></a> (often called <span class="italic">weights</span>) and b ∈ R (often called <span class="italic">bias</span>). The layer in between performs an affine transformation to yield a variable <span class="italic">z</span> and passes <span class="italic">z</span> as the argument of the non-linear activation function <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t154_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t154.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t154.gif"/></a>. Note that for Rosenblatt's perceptron<a title="Select to navigate to references" href="#cit187"><sup><span class="sup_ref">187</span></sup></a><span class="italic">σ</span>(<span class="italic">z</span>) = 1 if <span class="italic">z</span> ≥ 0 and 0 otherwise but any generalized activation function would be fine (see text for more details). (b) A feed-forward neural network obtained by stacking many neurons in several layers. The layers have an all to all connectivity pattern that may not necessarily be the case. The input is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t155_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t155.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t155.gif"/></a> and the output, unlike in the case of a perceptron, is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t156_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t156.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t156.gif"/></a> (in the figure <span class="italic">m</span> = <span class="italic">d</span> is shown but this may not be necessarily true). Each layer much like in the case of a perceptron performs an affine transform and then a non-linear activation. The case for the <span class="italic">k</span>-th layer is shown wherein the affine transformed variable is <span class="italic">z</span><small><sup><span class="italic">k</span></sup></small> which is subsequently passed into an activation function (see text for details).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
        </div>
        <div>
          
          <span id="sect2594"/><span class="c_heading_indent">3.2.8 Gaussian process regression. </span>
          <span>Gaussian process regression (GPR)<a title="Select to navigate to references" href="#cit182"><sup><span class="sup_ref">182</span></sup></a> is a non-parametric and supervised learning method that has become quite popular in the ML setting for Chemistry applications.<a title="Select to navigate to references" href="#cit183"><sup><span class="sup_ref">183</span></sup></a> It is based on the Bayesian approach, where a probability distribution over all possible values for the parameters is inferred by the ML model. Considering the input vector <span class="bold">x</span> and output <span class="bold">y</span>, a function of <span class="bold">x</span>, <span class="italic">f</span>(<span class="italic">x</span>) with its functional form unknown, maps the <span class="italic">d</span>-dimensional vector to a scalar value: <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t133_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t133.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t133.gif"/></a>. The training set <span class="italic">D</span> is made up of <span class="italic">n</span> observations, <span class="italic">D</span> = {(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>)|<span class="italic">i</span> = 1,…, <span class="italic">n</span>}. Performing regression to predict the form of <span class="italic">y</span> can be obtained in two ways:</span>
          <p class="otherpara">• Weight-space picture: having parameterized the function <span class="italic">f</span>, a prior is placed on the parameters of the model. Using the Bayes’ Rule, the probabilities are modified based on the observed data and the distribution is updated (called the posterior distribution). Then, the predictive posterior distribution on points <span class="italic">x</span><small><sub><span class="italic">n</span></sub></small> is calculated by weighting all the possible predictions by their respective calculated posterior distributions. In order to improve the expressiveness of the model, the inputs are projected into a high dimensional space using a set of <span class="italic">M</span> basis functions to approximate <span class="italic">y</span>(<span class="italic">x</span>) by <span class="italic">ỹ</span>(<span class="italic">x</span>):<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn48"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t134_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t134.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t134.gif"/></a></td><td class="rightEqn">(48)</td></tr></table>where <span class="italic">k</span> is the kernel of choice placed on the representative set of input points and <span class="italic">c</span><small><sub><span class="italic">m</span></sub></small> are the associated weights. By choosing the Gaussian kernel, the model is fit to the data by finding the coefficients <span class="italic">c</span> = (<span class="italic">c</span><small><sub>1</sub></small>,…,<span class="italic">c</span><small><sub><span class="italic">M</span></sub></small>), that minimize the loss:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn49"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t135_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t135.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t135.gif"/></a></td><td class="rightEqn">(49)</td></tr></table>where the second term is the Tikhonov regularization.</p>
          <p class="otherpara">• Function-space picture: the prior in this case is specified in the function space. For every <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t136_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t136.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t136.gif"/></a> the distribution of <span class="italic">f</span>(<span class="italic">x</span>) along with the structure of covariance <span class="italic">k</span>(<span class="italic">x</span>,<span class="italic">x</span>′) = cov(<span class="italic">f</span>(<span class="italic">x</span>),<span class="italic">f</span>(<span class="italic">x</span>′)) is characterized. A Gaussian process (GP) is used to describe a distribution over functions. A GP is completely specified by its mean function <span class="italic">m</span>(<span class="italic">x</span>) and covariance function (<span class="italic">k</span>(<span class="italic">x</span>,<span class="italic">x</span>′)):<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn50"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t137_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t137.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t137.gif"/></a></td><td class="rightEqn">(50)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn51"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t138_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t138.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t138.gif"/></a></td><td class="rightEqn">(51)</td></tr></table>The GP can be written as:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn52"/><span id="eqn52"><span class="italic">f</span>(<span class="italic">x</span>) ∼ GP(<span class="italic">m</span>(<span class="italic">x</span>),<span class="italic">k</span>(<span class="italic">x</span>,<span class="italic">x</span>′))</span></td><td class="rightEqn">(52)</td></tr></table><span class="italic">ỹ</span>(<span class="italic">x</span>) in this case is written as:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn53"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t139_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t139.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t139.gif"/></a></td><td class="rightEqn">(53)</td></tr></table>where <span class="italic">ϕ</span> represents the basis functions that are fixed, which are independent of data and indicate the probability distribution of functions, and <span class="italic">w</span> are the weights drawn from independent, identically distributed (i.i.d) Gaussian probability distributions. Considering the squared exponential as the covariance function:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn54"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t140_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t140.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t140.gif"/></a></td><td class="rightEqn">(54)</td></tr></table>which corresponds to a Bayesian linear regression model with an infinite number of basis functions. Samples are drawn from the distribution of functions evaluated at a specified number of points and the corresponding covariance matrix is written elementwise. Then, a random Gaussian vector is generated with the covariance matrix and values are generated as a function of inputs.</p>
          <p class="otherpara">We shall return to applications of SVM in Sections 5.2 and 5.5.</p>
          
            
            <span id="sect2688"/><br/><span class="d_heading_indent">3.2.8.1 Quantum enhanced variants. </span>
            <span>Matthew Otten <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit181"><sup><span class="sup_ref">181</span></sup></a> proposed a procedure to build quantum enhanced kernels while still capturing the relevant features of the classical kernels. As can be seen from the weight-space picture above, the quality of regression results is directly influenced by the choice of the kernel. Quantum computing enhanced kernels have the potential of being powerful in terms of performing higher dimensional regression tasks since quantum computers can represent functions that classical computers might not calculate efficiently. As coherent states approximate the squared exponential kernel, the classical feature maps corresponding to the squared exponential kernel can be first approximated using coherent states, which leads to a corresponding quantum kernel. A generic coherent state with parameter <span class="italic">α</span> can be written as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn55"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t141_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t141.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t141.gif"/></a></td><td class="rightEqn">(55)</td></tr></table></span>
            <p class="otherpara">The input data are encoded as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t142_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t142.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t142.gif"/></a> leading to the coherent state kernel:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn56"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t143_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t143.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t143.gif"/></a></td><td class="rightEqn">(56)</td></tr></table></p>
            <p class="otherpara">Since the coherent state can be written in terms of the displacement operator applied to the vacuum state, and truncating the Hilbert space at some maximum number of levels <span class="italic">N</span>, gives rise to the s finite-dimensional displacement operator <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t144_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t144.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t144.gif"/></a> where <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0062_0303.gif" alt="[b with combining tilde]"/></span><small><sup>†</sup></small><small><sub><span class="italic">N</span></sub></small> is the bosonic creation operator in the finite-dimensional Hibert space.</p>
            <p class="otherpara">The finite-dimensional coherent state based kernels are first prepared on a qubit system by decomposing the <span class="italic">N</span> level displacement operator into log<small><sub>2</sub></small>(<span class="italic">N</span>) Pauli operators and then using Trotterization up to <span class="italic">m</span> steps on the qubit Hamiltonian. This defines the quantum feature map that approximates the feature map of the classical exponential squared kernel. Classically inspired quantum feature maps can then be applied to solve the requisite regression task.</p>
            <p class="otherpara">In order to show a quantum advantage, an entanglement enhanced kernel can be prepared by using a multi-mode squeezing operator to entangle the different data dimensions for a multi-dimensional regression problem. Thereby, smaller quantum devices with only a few operations can perform higher-dimensional regression tasks. Following this, the GP-based ML task is performed on the classical hardware.</p>
          
        </div>
      
      
        
        <h3 id="sect2717"><span class="b_heading">3.3 Artificial neural networks</span></h3>
        <span>In this section, we briefly review the various architectures of neural networks or deep learning algorithms that are commonly used for applications in physics and chemistry. As before, we not only focus on the training of each such architecture on a classical computer but also on the quantum algorithms proposed wherever applicable. Applications of NN are discussed in all sections from Sections 4 and 5.1–5.5.</span>
        <div>
          
          <span id="sect2721"/><span class="c_heading_indent">3.3.1 Perceptron and feed forward-neural networks. </span>
          <span>A perceptron is a single artificial neuron which models a non-linear function of the kind <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t145_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t145.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t145.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t146_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t146.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t146.gif"/></a> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t147_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t147.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t147.gif"/></a>.<a title="Select to navigate to reference" href="#cit184"><sup><span class="sup_ref">184–186</span></sup></a> The <span class="italic">d</span>-dimensional vector <span class="italic">x</span> is an input and the single number <span class="italic">y</span> is the output. The perceptron layer in between first makes an affine transformation on the input <span class="italic">x</span> using tunable parameters <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t148_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t148.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t148.gif"/></a> (often called <span class="italic">weights</span>) and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t149_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t149.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t149.gif"/></a> (often called <span class="italic">bias</span>). This transformation is as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn57"/><span id="eqn57"><span class="italic">z</span> = <span class="italic">w</span><small><sup><span class="italic">T</span></sup></small><span class="italic">x</span> + <span class="italic">b</span>.</span></td><td class="rightEqn">(57)</td></tr></table>From the above transformation, it is clear that the weight vector <span class="italic">w</span> strengthens or weakens the importance of each element in the input through multiplicative scaling. The bias <span class="italic">b</span> physically sets a threshold when the neuron would ‘fire’ as would be clarified soon. Non-linearity is thereafter introduced by passing this affine transformed variable <span class="italic">z</span> as an input argument through an activation function (say <span class="italic">σ</span>). The output so obtained is the final output of the perceptron <span class="italic">y</span> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn58"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t150_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t150.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t150.gif"/></a></td><td class="rightEqn">(58)</td></tr></table>In Rosenblatt's model of perceptron<a title="Select to navigate to references" href="#cit187"><sup><span class="sup_ref">187</span></sup></a> the activation function used was a step function, <span class="italic">i.e.</span>, <span class="italic">σ</span>(<span class="italic">z</span>) = 1 if <span class="italic">z</span> ≥ 0 but <span class="italic">σ</span>(<span class="italic">z</span>) = 0 otherwise. It was essentially a linear classifier. However, more sophisticated and continuous activation functions commonly used now are as follows:</span>
          
            <span id="sect2761"/><br/><span class="d_heading_indent">Logistic activation function. </span>
            <span>The original idea of a perceptron is to model a biological neuron in the central nervous system. The activation function serves the purpose of mimicking the biological neuron activation rate. Logistic functions are typical activation functions having a similar representation to a biological neuron activation rate.<a title="Select to navigate to references" href="#cit188"><sup><span class="sup_ref">188</span></sup></a> Sigmoid function is a traditional type of logistic functions. The sigmoid function is an increasing function with ‘S’ shape, assuming a continuous range of values from 0 to 1, as described in <a title="" href="#eqn59">eqn (59)</a>.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn59"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t157_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t157.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t157.gif"/></a></td><td class="rightEqn">(59)</td></tr></table>where <span class="italic">α</span> is the slope parameter. Notice that the sigmoid function centers at 0.5, which might slow down the learning process. Besides, the gradient of sigmoid function for the data fallen in the region of either 0 or 1 is almost zero, which causes the network performance to degrade.<a title="Select to navigate to references" href="#cit189"><sup><span class="sup_ref">189</span></sup></a> Therefore, the hyperbolic tangent (tanh) function is introduced as another type of logistic activation function, which is the rescaled and biased version of the sigmoid function. The tanh function is defined as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn60"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t158_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t158.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t158.gif"/></a></td><td class="rightEqn">(60)</td></tr></table>Furthermore, there is an adaptive hyperbolic tangent activation function with two trainable parameters <span class="italic">β</span> and <span class="italic">α</span> to adjust the slope and amplitude of the tanh activation function throughout the training process. The adaptive tanh activation function is defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn61"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t159_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t159.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t159.gif"/></a></td><td class="rightEqn">(61)</td></tr></table>Both the sigmoid function and tanh function are saturated activation functions, as they squeeze the input (sigmoid function squashes real numbers to range between [0,1], while tanh function squashes real numbers to range between [−1,1]).</span>
          
          
            <span id="sect2779"/><br/><span class="d_heading_indent">Rectified linear unit (ReLU). </span>
            <span>Rectified linear unit (ReLU) is defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn62"/><span id="eqn62"><span class="italic">σ</span><small><sub>ReLU</sub></small>(<span class="italic">z</span>) = max(0,<span class="italic">z</span>).</span></td><td class="rightEqn">(62)</td></tr></table>Due to its simplicity, ReLU is a popular activation function in ANN. ReLU is more efficient than other functions as all the neurons are not activated at the same time, rather a certain number of neurons are activated at a time.<a title="Select to navigate to references" href="#cit190"><sup><span class="sup_ref">190</span></sup></a> If we would like to activate the neuron in the negative region, the Leaky ReLU (LReLU) might be an appropriate choice, where we could set the negative region with a small constant value.<a title="Select to navigate to references" href="#cit191"><sup><span class="sup_ref">191</span></sup></a> The LReLU is defined as,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn63"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t160_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t160.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t160.gif"/></a></td><td class="rightEqn">(63)</td></tr></table>Both the ReLU and LReLU are non-saturating activation functions.</span>
          
          
            <span id="sect2794"/><br/><span class="d_heading_indent">Exponential linear unit. </span>
            <span>Exponential linear unit (ELU) is defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn64"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t161_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t161.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t161.gif"/></a></td><td class="rightEqn">(64)</td></tr></table>ELU has a similar shape with LReLU; however, it performs better than ReLU in batch normalization.</span>
          
          
            <span id="sect2800"/><br/><span class="d_heading_indent">Multistate activation function (MSAF). </span>
            <span>Instead of combining numerous perceptrons with simple logistic activation functions, it is a simple way to achieve an N-state neuron by using an <span class="italic">N</span>-level activation function for real-valued neuronal states. Thus multistate activation functions (MSAF) are applied in ANN, which are generally multilevel step functions. As an example of MSAF, the <span class="italic">N</span>-level complex-signum activation function is defined as<a title="Select to navigate to references" href="#cit192"><sup><span class="sup_ref">192</span></sup></a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn65"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t162_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t162.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t162.gif"/></a></td><td class="rightEqn">(65)</td></tr></table>where <span class="italic">θ</span><small><sub><span class="italic">N</span></sub></small> = 2π/<span class="italic">N</span>, and<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn66"/><span id="eqn66">CSIGN<small><sub><span class="italic">N</span></sub></small>(<span class="italic">z</span>) = exp(<span class="italic">inθ</span><small><sub><span class="italic">N</span></sub></small>), arg(<span class="italic">z</span>) ∈ [(<span class="italic">n</span> − 1)<span class="italic">θ</span><small><sub><span class="italic">N</span></sub></small>,<span class="italic">nθ</span><small><sub><span class="italic">N</span></sub></small>), <span class="italic">n</span> = 1, 2,…, <span class="italic">N</span>.</span></td><td class="rightEqn">(66)</td></tr></table>The complex-signum activation function is often applied in the associative memory models based on Hopfield-type neural networks.<a title="Select to navigate to references" href="#cit193"><sup><span class="sup_ref">193</span></sup></a> Picking up an appropriate activation function is always essential in the classical ML. More discussion of the performance analysis of various activation functions can be found in ref. <a title="Select to navigate to references" href="#cit194">194</a> and <a title="Select to navigate to references" href="#cit195">195</a>.</span>
            <p class="otherpara">A perceptron is trained by seeing if the output value <span class="italic">y</span> matches with the true or expected value. If such a matching did not happen based on some pre-defined metric then the parameters of the neuron (<span class="italic">w</span>,<span class="italic">b</span>) are optimized so that the output of the network matches up to the desired value. In Rosenblatt's perceptron,<a title="Select to navigate to references" href="#cit187"><sup><span class="sup_ref">187</span></sup></a> this optimization was done by simply adding the input vector <span class="italic">x</span> to the weights <span class="italic">w</span> if the perceptron underestimated the output value compared to the true label and subtracting the <span class="italic">x</span> if the perceptron over-estimated the output value compared to the true label. The bias <span class="italic">b</span> was updated by ±1 in the two cases, respectively, as well. Once the output of the perceptron agrees with the label, the neuron is said to have ‘learnt’ to perform the task.</p>
            <p class="otherpara">Each such perceptron described is essentially equivalent to a biological neuron. A feed-forward neural network is obtained by stacking many such neurons, layer by layer such that the neurons in one layer are connected to those in the other layer. Operationally, the network models a non-linear function of the kind <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t163_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t163.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t163.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t164_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t164.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t164.gif"/></a> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t165_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t165.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t165.gif"/></a>. The <span class="italic">d</span>-dimensional vector <span class="italic">x</span> is an input and the <span class="italic">m</span> dimensional vector <span class="italic">y</span> is the output. If the network has <span class="italic">L</span> layers of stacked neurons, this would mean that the first (input) and the last (output) layers will have <span class="italic">d</span> and <span class="italic">m</span> neurons, respectively. The layers in between are called hidden layers. Let us concentrate on the <span class="italic">k</span>-th and (<span class="italic">k</span> − 1)-th layers ((<span class="italic">k</span>,<span class="italic">k</span> − 1) ∈ {1, 2,…, <span class="italic">L</span>}) only. The affine transformation defined at the <span class="italic">k</span>-th layer will be parameterized by a <span class="italic">weight</span> matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t166_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t166.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t166.gif"/></a> where <span class="italic">q</span> is the number of neurons in the <span class="italic">k</span>-th layer and <span class="italic">p</span> is the number of neurons in the (<span class="italic">k</span> − 1)-th layer and also by a <span class="italic">bias</span> vector <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t167_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t167.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t167.gif"/></a>.<a title="Select to navigate to reference" href="#cit58"><sup><span class="sup_ref">58,184</span></sup></a> The transformation acts on the activation response of the (<span class="italic">k</span> − 1)-th layer, <span class="italic">i.e.</span>, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t168_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t168.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t168.gif"/></a> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn67"/><span id="eqn67"><span class="italic">z</span><small><sup><span class="italic">k</span></sup></small> = <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><span class="italic">σ</span>(<span class="italic">z</span><small><sup><span class="italic">k</span>−1</sup></small>) + <span class="italic">b</span><small><sup><span class="italic">k</span></sup></small>.</span></td><td class="rightEqn">(67)</td></tr></table>The transformation thus yields a new vector <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t169_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t169.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t169.gif"/></a> which is passed through an activation process using any of the activation functions defined before and fed into the next layer. This process is repeated until one reaches the last layer. At the last <span class="italic">L</span>-th layer the activation response is <span class="italic">z</span><small><sup><span class="italic">L</span></sup></small> = <span class="italic">y</span>. This is compared with the true values/labels of the data (say <span class="italic">y</span>*). Many such metrics for the comparison can be defined, one simple example being the <span class="italic">L</span><small><sup>2</sup></small> norm of the difference vector ||<span class="italic">y</span> − <span class="italic">y</span>*||<small><sub>2</sub></small> or even cross-entropy<a title="Select to navigate to references" href="#cit196"><sup><span class="sup_ref">196</span></sup></a> if both <span class="italic">y</span> and <span class="italic">y</span>* are probability distributions <span class="italic">etc.</span> Such metrics are often called merit-functions or cost functions. Once a cost-function is defined, the error in the metric is decided and this is used to evaluate the gradient of the cost-function with respect to the <span class="italic">bias</span> parameters of the <span class="italic">L</span>-th layer and the interconnecting <span class="italic">weights</span> between the <span class="italic">L</span>-th and (<span class="italic">L</span> − 1)-th layers. The process is then repeated for all the layers up until one reaches the first layer. In the end, one then has access to the gradient of the cost function with respect to the tunable parameters of all the layers. This method of acquiring the gradient is called back-propagation.<a title="Select to navigate to reference" href="#cit197"><sup><span class="sup_ref">197,198</span></sup></a> Once all such gradients are obtained, one can update the parameters of the entire network using a simple gradient descent<a title="Select to navigate to references" href="#cit199"><sup><span class="sup_ref">199</span></sup></a> or sophisticated optimizers like ADAGRAD,<a title="Select to navigate to references" href="#cit200"><sup><span class="sup_ref">200</span></sup></a> RMSprop,<a title="Select to navigate to references" href="#cit199"><sup><span class="sup_ref">199</span></sup></a> ADAM,<a title="Select to navigate to reference" href="#cit199"><sup><span class="sup_ref">199,201</span></sup></a> and NADAM.<a title="Select to navigate to references" href="#cit202"><sup><span class="sup_ref">202</span></sup></a> When the error metric has decreased below a certain preset threshold, the network is said to have been ‘trained’ to perform the task. At this point, predictions of the network are usually cross-validated using the data outside that of the labelled training examples. It must be noted that often the term multi-layer perceptron is used interchangeably for feed-forward neural networks even though historically as mentioned above the training algorithm of perceptrons is slightly different. For fairly large neural-networks with many neurons stacked within each layer, the risk of overfitting the data exists. This can be handled using appropriate regularization techniques<a title="Select to navigate to reference" href="#cit203"><sup><span class="sup_ref">203,204</span></sup></a> or dropout.<a title="Select to navigate to references" href="#cit205"><sup><span class="sup_ref">205</span></sup></a></p>
          
          
            
            <span id="sect2918"/><br/><span class="d_heading_indent">3.3.1.1 Quantum enhanced variants. </span>
            <span>Difficulties arise inevitably when attempting to include nonlinear activation functions into quantum circuits. The nonlinear activation functions do not immediately correspond to the mathematical framework of quantum theory, which describes system evolution with linear operations and probabilistic observation. Conventionally, it is thus extremely difficult to generate these nonlinearities with a simple quantum circuit. Researchers could build up quantum-classical hybrid neural networks, where the linear part corresponds to the quantum unitary operations in quantum layers, while the nonlinear part corresponds to the classical layers. In other words, the classical layer in the quantum-classical hybrid neural network is to serve as the activation function connecting different quantum layers. <a title="Select to navigate to figure" href="#imgfig14">Fig. 14(a)</a> is a scheme of the quantum-classical hybrid neural networks, where the linear part in the classical neural network is replaced by the quantum circuits. <a title="Select to navigate to figure" href="#imgfig15">Fig. 15(b)</a> shows an example construction of the hybrid quantum-classical neural network for 3 qubits, <a title="Select to navigate to figure" href="#imgfig14">Fig. 14</a> (see ref. <a title="Select to navigate to references" href="#cit206">206</a>). The quantum-classical hybrid neural networks generally work as follows: firstly, the classical data are converted into the quantum state <span class="italic">via</span> a certain mapping process. Then, the quantum unitary operations will implement the linear calculation. Next, the qubits are all measured and the estimation value is sent out to the classical layer. The classical layer will implement the nonlinear calculation (serve as the activation function), and the output will be sent to the next quantum layer to repeat the steps above. Based on the hybrid quantum-classical neural networks, researchers could construct quantum deep neural networks to calculate ground state energies of molecules,<a title="Select to navigate to references" href="#cit206"><sup><span class="sup_ref">206</span></sup></a> to study the barren plateaus in the training process,<a title="Select to navigate to references" href="#cit207"><sup><span class="sup_ref">207</span></sup></a> and to recognize figures with transfer learning.<a title="Select to navigate to references" href="#cit208"><sup><span class="sup_ref">208</span></sup></a> More details of the hybrid quantum-classical neural networks for various tasks could be found in these applications.</span>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig14"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f14_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f14.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f14.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 14 </b> <span id="fig14"><span class="graphic_title">(a) A scheme of the structure of a quantum-classical hybrid neural network for the realization of ANN. The linear part is accomplished with parameterized unitaries which defines the tunable <span class="italic">weights</span> and <span class="italic">biases</span> of the network whereas the non-linear activation is obtained from measurements in the quantum-classical hybrid neural network. (b) An example of the construction of the hybrid quantum-classical neural network for 3 qubits. Stage (I) refers to state-encoding with unitary <span class="italic">U</span><small><sub>1</sub></small>, stage 2 is the actual variational circuit with parameterized unitary <span class="italic">U</span><small><sub>2</sub></small> and stage 3 is the measurement to reproduce the effect of non-linear activation. In the next iteration, the measurement results <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0062_20d1.gif" alt="[b with combining right harpoon above (vector)]"/></span> is used in the unitary <span class="italic">U</span><small><sub>1</sub></small> for state-encoding. This way the full feed-forward neural network proceeds. Training is done by variation of the parameters of <span class="italic">U</span><small><sub>2</sub></small> (see ref. <a title="Select to navigate to references" href="#cit206">206</a> for details).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig15"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f15_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f15.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f15.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 15 </b> <span id="fig15"><span class="graphic_title">(a) Shows the main structure of quantum circuits estimating arbitrary periodic functions. There are two main modules. The first one contains <span class="italic">U</span><small><sub>pre</sub></small> acting on the auxiliary qubits <span class="italic">q</span>′, and Hadamard gates acting on <span class="italic">q</span>′′. The succeeding module is formed by <span class="italic">N</span> controlled unitary operations denoted as <span class="italic">U</span><small><sub><span class="italic">n</span></sub></small>. <span class="italic">q</span>′ (blue color) are control qubits. <span class="italic">q</span>′ are converted to the state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t170_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t170.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t170.gif"/></a> under the operation <span class="italic">U</span><small><sub>pre</sub></small>, where <span class="italic">γ</span> is determined by <span class="italic">F</span><small><sub><span class="italic">N</span></sub></small>. In (b), the blue curve represents the final output of the quantum circuit estimating square wave functions. Meanwhile, the red curve is the original shape of square wave functions. (a) and (b) are reproduced from ref. <a title="Select to navigate to references" href="#cit209">209</a> under Creative Common CC BY license.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
            <p class="otherpara">Though the unitary operations always correspond to linear calculation, the measurements could lead to nonlinearity. The repeat-until-success (RUS) circuit is a typical method implementing activation functions based on special measurements.<a title="Select to navigate to reference" href="#cit210"><sup><span class="sup_ref">210,211</span></sup></a> In the RUS circuit, an ancillary qubit is connected with the input and the output qubit. After certain operations, the ancillary qubit is measured. If result |0〉 is obtained, then the desired output is generated. Otherwise, we need to correct the operation and apply it on the qubits, and then measure the ancillary qubit once again. The steps above should be repeated until we get the result |0〉. Thus the circuit is named repeat-until-success (RUS) circuit. In 2017, Cao and coworkers developed both the quantum feed forward neural network and quantum Hopfield network based on the RUS circuit.<a title="Select to navigate to references" href="#cit212"><sup><span class="sup_ref">212</span></sup></a> Sometimes in the hybrid quantum-classical neural networks, researchers also use special intermediate measurements to implement certain nonlinear functions.<a title="Select to navigate to references" href="#cit206"><sup><span class="sup_ref">206</span></sup></a></p>
            <p class="otherpara">There are some other approaches to implement the activation functions in quantum neural networks. Activation functions can be implemented <span class="italic">via</span> the mapping process. In 2018, Daskin developed a simple quantum neural network with a periodic activation function,<a title="Select to navigate to references" href="#cit213"><sup><span class="sup_ref">213</span></sup></a> where the simple cos function is used as an activation function. There are also methods to implement activation function with the assistance of the phase estimation algorithm.<a title="Select to navigate to references" href="#cit214"><sup><span class="sup_ref">214</span></sup></a> Recently, our group also developed a quantum circuit to implement periodic nonlinear activation functions with multi copies of input.<a title="Select to navigate to references" href="#cit209"><sup><span class="sup_ref">209</span></sup></a><a title="Select to navigate to figure" href="#imgfig15">Fig. 15(a)</a> is a scheme of the circuit structure, and <a title="Select to navigate to figure" href="#imgfig15">Fig. 15(b)</a> shows the approximation of periodic square wave functions. The quantum circuit contains <span class="italic">N</span>-qubits to store the information on the different <span class="italic">N</span>-Fourier components and <span class="italic">M</span> + 2 auxiliary qubits with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t171_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t171.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t171.gif"/></a> for control operations. The desired output will be measured in the last qubit <span class="italic">q</span><small><sub><span class="italic">N</span></sub></small> with a time complexity of the computation of <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t172_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t172.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t172.gif"/></a> which leads to polynomial speedup under certain circumstances. In conclusion, it is an essential but still open question to find an optimal approach to implement nonlinear activation functions in quantum neural networks.</p>
          
        </div>
        <div>
          
          <span id="sect2983"/><span class="c_heading_indent">3.3.2 Convolutional neural network (CNN). </span>
          <span>This is a specific kind of neural network architecture that is widely used for image classification and computer-vision problems.<a title="Select to navigate to reference" href="#cit216"><sup><span class="sup_ref">216,217</span></sup></a> To understand the basic algorithm let us consider a grayscale image composed of (<span class="italic">n</span><small><sub>1</sub></small> × <span class="italic">n</span><small><sub>2</sub></small>) pixels. The image can be numerically represented as a matrix of intensity I such that <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t173_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t173.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t173.gif"/></a>. For colored images, the only difference in the intensity distribution at the (<span class="italic">i</span>,<span class="italic">j</span>)th pixel (position in the intensity matrix) will be that instead of a scalar value, the intensity will be a vector of [<span class="italic">R</span>,<span class="italic">G</span>,<span class="italic">B</span>]<small><sup><span class="italic">T</span></sup></small> entries. If the total pixel count (<span class="italic">n</span><small><sub>1</sub></small><span class="italic">n</span><small><sub>2</sub></small>) is too big, then converting the matrix into a 1D vector and using a feed-forward neural network as discussed before may be cumbersome and would require a large number of tunable parameters with the possibility of over-fitting. Besides, a 1D encoding loosens the spatial correlation in the intensity pattern among the neighboring pixels. CNN is designed to use as input the entire 2D matrix and hinges on understanding and identifying the spatial information (often called feature maps) and then condensing the information into feature vectors of reduced sizes which is then fed into a fully-connected feed-forward network for usual operations as described before.<a title="Select to navigate to reference" href="#cit218"><sup><span class="sup_ref">218–221</span></sup></a> In other words, CNN is basically a simple neural network defined before in the final layer equipped with a robust feature-extractor before the final layer to remove redundancies and decrease parameter count.</span>
          <p class="otherpara">The key components which facilitate the CNN architecture are thus grouped into two parts: (a) feature extractor and (b) fully-connected neural network. The component (a) is further made up of the repeated use of the following categories of layers.</p>
          <p class="otherpara">1. Convolutional layer:<a title="Select to navigate to references" href="#cit58"><sup><span class="sup_ref">58</span></sup></a> this is where the magic of CNN happens. For each feature the user wants to identify and extract from the image, this layer uses a spatial filter (kernel) denoted as <span class="italic">K</span> which is essentially a matrix that can slide over the output of the previous layer (or the intensity matrix of the input image if one is looking right after the first layer) and define a new feature map. In other words, the kernel acts on a chunk of the input matrix every time and the process is essentially a convolution. This feature map is obtained by a Frobenius inner product between the kernel and the chunk of the input it is acting on such that the resulting map has large entries only over the pixels (or (<span class="italic">i</span>,<span class="italic">j</span>) positions) wherein the kernel entries ‘are similar’ to the entries of the chunk <span class="italic">i.e.</span> the feature is present. This is done for every kernel (one corresponding to every feature that needs extraction) and for every feature map from the previous layer. Operationally let the input to the <span class="italic">l</span>-th layer from the (<span class="italic">l</span> − 1)-th layer comprise feature maps denoted as <span class="italic">y</span><small><sup><span class="italic">l</span>−1</sup></small><small><sub><span class="italic">p</span></sub></small> each where <span class="italic">p</span> = 1, 2,…, <span class="italic">α</span><small><sup><span class="italic">l</span>−1</sup></small> features. Each such map is of size <span class="italic">β</span><small><sup><span class="italic">l</span>−1</sup></small> × <span class="italic">γ</span><small><sup><span class="italic">l</span>−1</sup></small>. Then each of the output from the <span class="italic">l</span>-th layer denoted as <span class="italic">y</span><small><sup><span class="italic">l</span></sup></small><small><sub><span class="italic">p</span></sub></small> (<span class="italic">p</span> = 1, 2,…, <span class="italic">α</span><small><sup><span class="italic">l</span>−1</sup></small>) is a feature map of size <span class="italic">β</span><small><sup><span class="italic">l</span></sup></small> × <span class="italic">γ</span><small><sup><span class="italic">l</span></sup></small> obtained by convolving against kernels as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn68"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t174_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t174.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t174.gif"/></a></td><td class="rightEqn">(68)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn69"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t175_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t175.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t175.gif"/></a></td><td class="rightEqn">(69)</td></tr></table>where <span class="italic">b</span><small><sup><span class="italic">l</span></sup></small><small><sub><span class="italic">i</span>,<span class="italic">j</span></sub></small> are the elements of the bias matrix of the <span class="italic">l</span>-th layer. The tunable parameters within this layer are the bias matrix elements and the parameters within the kernel <span class="italic">K</span>. This convoluted feature map may be obtained by passing the kernel over the entire input without missing any row or column (without using any stride<a title="Select to navigate to references" href="#cit222"><sup><span class="sup_ref">222</span></sup></a>) or otherwise. The corresponding map so obtained may also be padded with zeros for dimensional consistency. All the feature maps so generated serve as input to the (<span class="italic">l</span> + 1)-th layer. The early convolutional layers in the network usually extract simple features with complexity increasing along the way. Feature maps can also be stacked along the third dimension to obtain compound features.</p>
          <p class="otherpara">2. Activation layer: this layer is responsible for introducing non-linearity into the model using the input of the previous layer through the following expression<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn70"/><span id="eqn70">(<span class="italic">y</span><small><sup><span class="italic">l</span></sup></small><small><sub><span class="italic">q</span></sub></small>)<small><sub><span class="italic">i</span>,<span class="italic">j</span></sub></small> = <span class="italic">σ</span>(<span class="italic">y</span><small><sup><span class="italic">l</span>−1</sup></small><small><sub><span class="italic">q</span></sub></small>)<small><sub><span class="italic">i</span>,<span class="italic">j</span></sub></small></span></td><td class="rightEqn">(70)</td></tr></table>wherein <span class="italic">σ</span> is an activation function like ReLU, sigmoid, tanh, <span class="italic">etc.</span>, and (<span class="italic">y</span><small><sup><span class="italic">l</span></sup></small><small><sub><span class="italic">q</span></sub></small>)<small><sub><span class="italic">i</span>,<span class="italic">j</span></sub></small> are defined as in the previous point. Sometimes rectification layers are also used which compute the absolute values of the input.</p>
          <p class="otherpara">3. Pooling layer: this is where dimensional reduction or downsampling of the feature maps happens. This layer takes in the feature maps from the previous layer and uses windows of pre-defined sizes within each chunk of the feature map and preserves only one value within each such window to generate a new feature map with reduced dimension. The number of feature maps remains unchanged. The one value so selected can be the maximum value of all features within the window (max-pooling<a title="Select to navigate to reference" href="#cit223"><sup><span class="sup_ref">223–225</span></sup></a>) or may be the average value (average pooling<a title="Select to navigate to reference" href="#cit224"><sup><span class="sup_ref">224,225</span></sup></a>).</p>
          <p class="otherpara">The architecture has repeated applications of these layers to ensure parameter sharing and efficient feature extraction. The output at the last layer of the feature-extractor is vectorized into a 1D format and fed into a completely connected deep-neural network at the end. This network then processes the input and generates the final output. For example, if the final desired output is a multi-label classification task, the connected neural network will have as many neurons as the number of labels with each neuron being a placeholder for a 1 or 0 denoting classification into the corresponding label or not. <a title="Select to navigate to figure" href="#imgfig16">Fig. 16(a)</a> illustrates pictorially all of these components.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig16"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f16_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f16.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f16.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 16 </b> <span id="fig16"><span class="graphic_title">(a) The schematic of a typical convolutional neural network (CNN) is illustrated. The process starts with an input image from which feature maps are extracted through an element-wise product with a kernel and then activation through any of the activation functions discussed in the previous section. Such feature maps are depicted in green. The pooling operation (blue layer) thereafter reduces the size of the feature maps by preserving the values of a prescribed choice by the user within a certain window of each feature map. The two layers are repeated many times and then fed into a fully-connected neural network as discussed in the previous section. The output is read and back-propagation is used to train the parameters of the entire network. (b) The schematic of the quantum circuit as illustrated in ref. <a title="Select to navigate to references" href="#cit215">215</a> for the realization of a CNN. The circuit receives an arbitrary input state say <span class="italic">ρ</span><small><sub>0</sub></small>. The unitaries designated as <span class="italic">U</span><small><sub><span class="italic">i</span></sub></small> are responsible for convolutional operation whereas the unitaries designated as <span class="italic">V</span><small><sub><span class="italic">i</span></sub></small> are responsible for controlled operations in the pooling layer. The unitaries <span class="italic">V</span><small><sub><span class="italic">i</span></sub></small> are conditioned on the measurement results of neighboring unitaries. Such measurements reduce the qubit pool and are similar to dimensional reduction in conventional pooling layers. The operations are repeated several times until a fully-connected unitary (denoted as <span class="italic">F</span>) acts on. Certain qubits are measured subsequently to process the output.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
            
            <span id="sect3118"/><br/><span class="d_heading_indent">3.3.2.1 Quantum enhanced variants. </span>
            <span>In 2019, Cong <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit215"><sup><span class="sup_ref">215</span></sup></a> developed a quantum circuit based on CNN architecture which is used to classify an <span class="italic">N</span>-qubit quantum state with <span class="italic">M</span>-labels. In other words, given a training data set of M states {(|<span class="italic">ψ</span><small><sub><span class="italic">i</span></sub></small>〉,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>)} where <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> are the binary classification labels associated with the states, the circuit can decide which of the training vectors the input unknown state resembles the most. This is useful in understanding whether a given state belongs to a particular phase with phase labels and shall be discussed later. The circuit architecture involves the following steps:</span>
            <p class="otherpara">1. The initial inputs are mapped to a quantum state.</p>
            <p class="otherpara">2. In the convolutional layer, the quantum state is transformed using a set of quasi-local unitaries labelled as <span class="italic">U</span><small><sub><span class="italic">i</span></sub></small> where <span class="italic">i</span> ∈ {1, 2,…}.</p>
            <p class="otherpara">3. In the pooling layer, some of the qubits are measured and, conditioned on this measurement outcome, the unitaries for the remaining qubits are decided. This reduces the width of the circuit as certain qubits whose state has been readout are no longer a part of subsequent operations. Such controlled entangling unitaries are labelled as <span class="italic">V</span><small><sub><span class="italic">i</span></sub></small> where <span class="italic">i</span> ∈ {1, 2,…}.</p>
            <p class="otherpara">4. The convolutional and the pooling layers are applied many times until the width of the circuit is reduced sufficiently.</p>
            <p class="otherpara">5. Finally, a fully-connected layer of single and two-qubit gates (labelled as say <span class="italic">F</span>) is applied on the remaining qubits analogous to the fully-connected layer in classical CNN.</p>
            <p class="otherpara">6. The final prediction from the algorithm is read by measuring certain qubits at the very end.</p>
            <p class="otherpara">The circuit is described in <a title="Select to navigate to figure" href="#imgfig16">Fig. 16(b)</a> schematically.</p>
          
        </div>
        <div>
          
          <span id="sect3152"/><span class="c_heading_indent">3.3.3 Recurrent neural networks. </span>
          <span>For data that involve time-ordered sequence as what appears frequently in natural-language processing,<a title="Select to navigate to references" href="#cit226"><sup><span class="sup_ref">226</span></sup></a> stock-price prediction,<a title="Select to navigate to references" href="#cit227"><sup><span class="sup_ref">227</span></sup></a> translation<a title="Select to navigate to references" href="#cit228"><sup><span class="sup_ref">228</span></sup></a> or any simple time-series prediction,<a title="Select to navigate to references" href="#cit229"><sup><span class="sup_ref">229</span></sup></a> it is important for the neural network architecture to preserve information about the previous entries in the sequence, <span class="italic">i.e.</span>, the notion of building memory in the architecture becomes essential. Recurrent neural networks (RNN) are specifically built to handle such tasks. The task such networks perform is usually supervised in which one has access to a sequence {<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>}<small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">i</span>=1</sub></small> where each entry in the sequence <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t176_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t176.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t176.gif"/></a> and a corresponding label <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t177_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t177.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t177.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t178_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t178.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t178.gif"/></a>. The primary goal of the network is to produce a new sequence {<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>}<small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">i</span>=1</sub></small> as the output such that each <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> is close enough to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t179_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t179.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t179.gif"/></a> ∀ <span class="italic">i</span> as computed from a chosen metric and a preset threshold. In the vanilla RNN architecture,<a title="Select to navigate to references" href="#cit230"><sup><span class="sup_ref">230</span></sup></a> the primary functional unit which is used repeatedly for each input entry in the sequence consists of three layers of stacked neurons. The first layer is an input layer having <span class="italic">d</span> neurons (as the input entries <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> are <span class="italic">d</span>-dimensional). The next layer is the hidden layer having say <span class="italic">p</span> neurons and is parameterized by <span class="italic">weight</span> matrices (<span class="italic">W</span><small><sub><span class="italic">z</span></sub></small>,<span class="italic">W</span><small><sub><span class="italic">x</span></sub></small>) and bias vector <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t180_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t180.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t180.gif"/></a>. The <span class="italic">weight</span> matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t181_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t181.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t181.gif"/></a> is responsible for the affine transformation on the input <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> as discussed in the case of feed-forward neural networks. However, the primary difference from an usual feed-forward network is the presence of a second set of <span class="italic">weight</span> matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t182_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t182.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t182.gif"/></a> which performs an affine transform on the hidden layer activation response corresponding to the entry in the previous step <span class="italic">i.e.</span> for the last but one input entry <span class="italic">x</span><small><sub><span class="italic">i</span>−1</sub></small>. If the activation response from the hidden layer for <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> is denoted as <span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span></sub></small>) and the activation response for the previous entry <span class="italic">x</span><small><sub><span class="italic">i</span>−1</sub></small> is denoted as <span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span>−1</sub></small>) then the two are related as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn71"/><span id="eqn71"><span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span></sub></small>) = <span class="italic">σ</span>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">x</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">z</span></sub></small><span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span>−1</sub></small>) + <span class="italic">b</span><small><sub><span class="italic">z</span></sub></small>).</span></td><td class="rightEqn">(71)</td></tr></table></span>
          <p class="otherpara">Using this activation response (usually tanh) the last output layer now performs another affine transformation followed by the usual introduction of non-linearity through input to the activation process as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn72"/><span id="eqn72"><span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">σ</span>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">y</span></sub></small><span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span></sub></small>) + <span class="italic">b</span><small><sub><span class="italic">y</span></sub></small>)</span></td><td class="rightEqn">(72)</td></tr></table>where the <span class="italic">weight</span> matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t183_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t183.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t183.gif"/></a> defines the inter-connections between the hidden layer and output layer and the <span class="italic">bias</span> vector <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t184_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t184.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t184.gif"/></a>. The total number of tunable parameter vectors for this unit is 5 (<span class="italic">W</span><small><sub><span class="italic">z</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">x</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">y</span></sub></small>, <span class="italic">b</span><small><sub><span class="italic">z</span></sub></small>, and <span class="italic">b</span><small><sub><span class="italic">y</span></sub></small>). For all subsequent input entries (<span class="italic">i.e. x</span><small><sub><span class="italic">i</span>+1</sub></small>, <span class="italic">x</span><small><sub><span class="italic">i</span>+2</sub></small>, <span class="italic">etc.</span>) the functional unit is repeatedly queried using the activation response (usually tanh) of the previous step as explained above. The total number of parameters (<span class="italic">W</span><small><sub><span class="italic">z</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">x</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">y</span></sub></small>, <span class="italic">b</span><small><sub><span class="italic">z</span></sub></small>, and <span class="italic">b</span><small><sub><span class="italic">y</span></sub></small>) is kept the same for all such steps which leads to reduction in the number of variables through efficient sharing. Each such iteration generates a <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> as explained. After the first pass through the entire data-set (a subset of the data-set can also be used depending on user preference), an ordered sequence {<span class="italic">y</span>}<small><sub><span class="italic">i</span></sub></small> is generated and a cost-function is defined to compare this sequence with the labelled sequence {<span class="italic">y</span>*}<small><sub><span class="italic">i</span></sub></small>. The error in this cost-function is minimized by updating the parameter set (<span class="italic">W</span><small><sub><span class="italic">z</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">x</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">y</span></sub></small>, <span class="italic">b</span><small><sub><span class="italic">z</span></sub></small>, and <span class="italic">b</span><small><sub><span class="italic">y</span></sub></small>) using the gradient of the cost-function or any other optimizer as has been described in the case of feed-forward neural networks. The architecture is pictorially depicted in <a title="Select to navigate to figure" href="#imgfig17">Fig. 17(a)</a>.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig17"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f17_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f17.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f17.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 17 </b> <span id="fig17"><span class="graphic_title">(a) A schematic of recurrent neural network (RNN) architecture. The blue layer encodes the input sequence {<span class="italic">x</span><small><sub>1</sub></small>, <span class="italic">x</span><small><sub>2</sub></small>,…, <span class="italic">x</span><small><sub><span class="italic">i</span>−1</sub></small>, <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>, <span class="italic">x</span><small><sub><span class="italic">i</span>+1</sub></small>,…}. The yellow layer is the hidden layer which processes the input and generates an activated response <span class="italic">σ</span>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>) for the input <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>. The key difference between RNN and other neural network architectures is that <span class="italic">σ</span>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>) is fed onto the network when the next entry in the time-ordered sequence <span class="italic">x</span><small><sub><span class="italic">i</span>+1</sub></small> is the input. This forms the core of the memory retention process in RNN<span class="italic">. σ</span>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>) is also used by the final layer (green) to generate the output <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>. The parameters of the network are the biases (<span class="italic">b</span><small><sub><span class="italic">y</span></sub></small> and <span class="italic">b</span><small><sub><span class="italic">z</span></sub></small>) and the interconnecting weights (<span class="italic">W</span><small><sub><span class="italic">x</span></sub></small>, <span class="italic">W</span><small><sub><span class="italic">z</span></sub></small>, and <span class="italic">W</span><small><sub><span class="italic">y</span></sub></small>) between a pair of layers which are highlighted in the figure. (b) The schematic of the quantum circuit for processing time-ordered sequence using the RNN architecture as illustrated in ref. <a title="Select to navigate to references" href="#cit231">231</a>. Two qubit registers are invoked with <span class="italic">n</span><small><sub><span class="italic">A</span></sub></small> a<span class="italic">n</span>d <span class="italic">n</span><small><sub><span class="italic">B</span></sub></small> qubits and the input entry <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> is encoded within the first register using <span class="italic">U</span><small><sub>1</sub></small>(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>). The first and second registers are entangled using a parameterized unitary <span class="italic">U</span><small><sub>2</sub></small>(<span class="italic">θ</span>) and thereafter an observable <span class="italic">O</span><small><sup><span class="italic">A</span></sup></small> on the first register is measured to yield the output <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>. The second register is left untouched and is responsible for carrying the memory for the next input <span class="italic">x</span><small><sub><span class="italic">i</span>+1</sub></small>. The circuit is adapted from ref. <a title="Select to navigate to references" href="#cit231">231</a>. The parameters of the entangling unitary gate <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t334_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t334.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t334.gif"/></a> are optimized to make the output sequence {<span class="italic">y</span><small><sub>1</sub></small>, <span class="italic">y</span><small><sub>2</sub></small>, <span class="italic">y</span><small><sub>3</sub></small>, …, <span class="italic">y</span><small><sub><span class="italic">i</span>−1</sub></small>, <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>, <span class="italic">y</span><small><sub><span class="italic">i</span>+1</sub></small>,…} to the desired.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">During back-propagation for long RNNs it is possible to encounter a situation wherein the gradient vector accrues a zero value (or grow unbounded in magnitude) with respect to parameters of nodes appearing at the beginning of the network. Such a situation is known as vanishing (exploding) gradient and if happens can render the model untrainable. Apart from changing the activation function from logistic ones like tanh to ReLU, one can adopt architectures of RNN like long-short term memory (LSTM) or gated recurrent unit (GRU)<a title="Select to navigate to reference" href="#cit230"><sup><span class="sup_ref">230,232–235</span></sup></a> in such a case. LSTM networks introduced in ref. <a title="Select to navigate to references" href="#cit236">236</a> also have successive repeating units/cells wherein the input to each cell is one entry of the ordered sequence <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> (defined before) as well as the activation response (say <span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>) which was denoted as <span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span>−1</sub></small>) for vanilla RNN. The change in notation will be clarified soon. (The two quantities are conceptually similar though.) However the key difference with the vanilla version of RNN lies in the presence of a memory channel/carousel. The response of this memory channel from the previous cell (often denoted as <span class="italic">c</span><small><sub><span class="italic">i</span>−1</sub></small>) is also fed as input into the current cell. Inside the current cell there are three different networks/gates which work to erase, update the memory of the carousel entry and generate a new output <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> as well (<span class="italic">h</span><small><sub><span class="italic">i</span></sub></small>). The latter is fed back into the next cell as before in the case of vanilla RNN. The primary components inside each cell are the following:</p>
          <p class="otherpara">(a) The forget gate: this takes in input (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>)<small><sup><span class="italic">T</span></sup></small> and performs an affine transformation with weight matrices and bias (<span class="italic">W</span><small><sub><span class="italic">xf</span></sub></small>,<span class="italic">W</span><small><sub><span class="italic">hf</span></sub></small>,<span class="italic">b</span><small><sub><span class="italic">f</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> wherein the subscript <span class="italic">f</span> stands for the forget gate. This is passed onto a sigmoid activation which outputs values between 0 and 1 only. The purpose of this gate is to read from the present input entries (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>)<small><sup><span class="italic">T</span></sup></small> what features in the memory channel need to be erased (hence the name forget gate). If the output of the forget gate is denoted as <span class="italic">f</span><small><sub><span class="italic">i</span></sub></small> the transformation is abbreviated as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn73"/><span id="eqn73"><span class="italic">f</span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small> (<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">xf</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">hf</span></sub></small><span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small> + <span class="italic">b</span><small><sub><span class="italic">f</span></sub></small>)</span></td><td class="rightEqn">(73)</td></tr></table>where <span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small> is the sigmoid activation.</p>
          <p class="otherpara">(b) The next important gate is the input gate whose purpose is to decide what new information needs to be updated into the memory channel and at what places. Two operations are performed herein. The first involves creating an affine transformation of the input (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>)<small><sup><span class="italic">T</span></sup></small> followed by sigmoid activation. The weights and biases in the process are (<span class="italic">W</span><small><sub><span class="italic">xI</span></sub></small>,<span class="italic">W</span><small><sub><span class="italic">hI</span></sub></small>,<span class="italic">b</span><small><sub><span class="italic">I</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> wherein <span class="italic">I</span> is for the input gate. This accomplishes the task of where to update the new information through the 0s and 1s of the sigmoid activation. The next operation is to create a candidate memory <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0043_0303.gif" alt="[C with combining tilde]"/></span><small><sub><span class="italic">i</span></sub></small> for the memory channel using the input entries and parameters (<span class="italic">W</span><small><sub><span class="italic">xIc</span></sub></small>,<span class="italic">W</span><small><sub><span class="italic">hIc</span></sub></small>,<span class="italic">b</span><small><sub><span class="italic">Ic</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> using a tanh activation to acquire values between ±1. The operations are as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn74"/><span id="eqn74"><span class="italic">I</span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">xI</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">hI</span></sub></small><span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small> + <span class="italic">b</span><small><sub><span class="italic">I</span></sub></small>)</span></td><td class="rightEqn">(74)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn75"/><span id="eqn75"><span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0043_0303.gif" alt="[C with combining tilde]"/></span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">σ</span><small><sub>tanh</sub></small>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">xIc</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">hIc</span></sub></small><span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small> + <span class="italic">b</span><small><sub><span class="italic">Ic</span></sub></small>)</span></td><td class="rightEqn">(75)</td></tr></table>The state in the memory channel is then updated using <a title="" href="#eqn73">eqn (73) and (75)</a> as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn76"/><span id="eqn76"><span class="italic">C</span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">f</span><small><sub><span class="italic">i</span></sub></small> × <span class="italic">C</span><small><sub><span class="italic">i</span>−1</sub></small> + <span class="italic">I</span><small><sub><span class="italic">i</span></sub></small> × <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0043_0303.gif" alt="[C with combining tilde]"/></span><small><sub><span class="italic">i</span></sub></small></span></td><td class="rightEqn">(76)</td></tr></table>where the first term erases the memory from the previous state <span class="italic">C</span><small><sub><span class="italic">i</span>−1</sub></small> using location in <span class="italic">f</span><small><sub><span class="italic">i</span></sub></small> and the second term re-builds it with new information in <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0043_0303.gif" alt="[C with combining tilde]"/></span><small><sub><span class="italic">i</span></sub></small> at the location specified by the <span class="italic">I</span><small><sub><span class="italic">i</span></sub></small> vector.</p>
          <p class="otherpara">(c) The third component is the output gate which is used to create an output <span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> to be fed into the next cell with data entry  <span class="italic">x</span><small><sub><span class="italic">i</span>+1</sub></small>. The transformation is<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn77"/><span id="eqn77"><span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> = (<span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">ox</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">oh</span></sub></small><span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small> + <span class="italic">b</span><small><sub><span class="italic">o</span></sub></small>)) × <span class="italic">σ</span><small><sub>tanh</sub></small>(<span class="italic">C</span><small><sub><span class="italic">i</span></sub></small>).</span></td><td class="rightEqn">(77)</td></tr></table>This operation can be interpreted as returning the tanh of the state of the memory channel <span class="italic">σ</span><small><sub>tanh</sub></small>(<span class="italic">C</span><small><sub><span class="italic">i</span></sub></small>) as the output at locations filtered by the vector (<span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small>(<span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">ox</span></sub></small><span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> + <span class="italic">W</span><small><sup><span class="italic">T</span></sup></small><small><sub><span class="italic">oh</span></sub></small><span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>)) which explains why the symbol was changed from <span class="italic">σ</span>(<span class="italic">z</span><small><sub><span class="italic">i</span>−1</sub></small>) as it is not just an activation output but a scaled one. The weights and the biases (<span class="italic">W</span><small><sub><span class="italic">ox</span></sub></small>,<span class="italic">W</span><small><sub><span class="italic">oh</span></sub></small>,<span class="italic">b</span><small><sub><span class="italic">o</span></sub></small>)<small><sup><span class="italic">T</span></sup></small> are parameters of this output gate. <a title="Select to navigate to figure" href="#imgfig18">Fig. 18(a)</a> displays a schematic version of a typical LSTM network.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig18"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f18_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f18.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f18.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 18 </b> <span id="fig18"><span class="graphic_title">(a) A schematic representation of a typical LSTM network as implemented in a classical processor. The three different processing gates – forget gate, input gate and output gate are illustrated (see text for details) along with the memory channel encoding <span class="italic">C</span><small><sub><span class="italic">i</span></sub></small>. ⊗ Indicates elementwise multiplication whereas ⊕ indicates elementwise addition<span class="italic">. σ</span><small><sub><span class="italic">t</span></sub></small> indicates <span class="italic">tanh</span> activation and <span class="italic">σ</span><small><sub><span class="italic">s</span></sub></small> indicates sigmoid activation. (b) The hybrid quantum-classical LSTM network implemented using parameterized quantum unitaries (PQC) in ref. <a title="Select to navigate to references" href="#cit237">237</a>. Each PQC has a data-encoding circuit and a variational circuit parameterized by angles (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t185_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t185.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t185.gif"/></a>). For say PQC1 to PQC4, the data encoder loads the concatenated vector (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small>)<small><sup><span class="italic">T</span></sup></small>. For PQC5 and PQC6, the output from PQC4 is processed and the memory channel is loaded (see text for more details).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
            
            <span id="sect3756"/><br/><span class="d_heading_indent">3.3.3.1 Quantum enhanced variants. </span>
            <span>Recently a quantum algorithm has been designed<a title="Select to navigate to references" href="#cit231"><sup><span class="sup_ref">231</span></sup></a> to implement the vanilla RNN architecture using a hybrid-variational framework. The algorithm uses a quantum circuit of two sets of qubits (say <span class="italic">n</span><small><sub>A</sub></small> and <span class="italic">n</span><small><sub>B</sub></small>). The input sequence of data is stored within the quantum states of one of the two registers through appropriate unitary operations. Both registers are then processed through unitaries with parameterized angles. The state of one of the register is measured subsequently to obtain the output whereas the other is untouched and passes onto the subsequent state to carry the memory of previous steps. The key ingredients of the protocol are:</span>
            <p class="otherpara">1. Both the registers with <span class="italic">n</span><small><sub>A</sub></small> and <span class="italic">n</span><small><sub>B</sub></small> qubits are initialized to null kets.</p>
            <p class="otherpara">2. The first input entry <span class="italic">x</span><small><sub>0</sub></small> is encoded onto the state of the register with <span class="italic">n</span><small><sub>A</sub></small> qubits. Thereafter controlled unitaries are used to entangle the register with <span class="italic">n</span><small><sub>A</sub></small> qubits and <span class="italic">n</span><small><sub>B</sub></small> qubits. Such unitaries are parameterized by variational angles.</p>
            <p class="otherpara">3. The expectation value of an operator <span class="italic">O</span><small><sup>A</sup></small> is measured using the reduced density matrix of the first set of qubits (say <span class="italic">ρ</span><small><sup>0</sup></small><small><sub>A</sub></small>). This measurement yields <span class="italic">y</span><small><sub>0</sub></small>. The second set of qubits (in the register with <span class="italic">n</span><small><sub>B</sub></small> qubits) remains untouched. The first register is re-initialized to null kets.</p>
            <p class="otherpara">4. For subsequent input entries (say <span class="italic">x</span><small><sub>1</sub></small>, <span class="italic">x</span><small><sub>2</sub></small>,…, <span class="italic">x</span><small><sub><span class="italic">t</span></sub></small>, <span class="italic">etc.</span>), the second step above is repeated with the input <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> encoded within the first register. This is followed by the third step. The state of the second register which is left pristine at each step retains the memory and information about previous input entries. This information is shared with the qubits in the first register through the parameterized entangling unitaries for each input.</p>
            <p class="otherpara">5. The sequence of {<span class="italic">y</span>}<small><sub><span class="italic">i</span></sub></small> values so generated is fed into a cost function and the parameters of the entangling unitaries are updated for the next cycle from the knowledge of the errors.</p>
            <p class="otherpara">The circuit is schematically illustrated in <a title="Select to navigate to figure" href="#imgfig17">Fig. 17(b)</a> A quantum version of the LSTM network has also been implemented recently using hybrid-variational circuits.<a title="Select to navigate to references" href="#cit237"><sup><span class="sup_ref">237</span></sup></a> The schema of the algorithm consists of 4 major components.</p>
            <p class="otherpara">1. A data loader circuit: this component serves to map the concatenated form of the input vectors of the sequence <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> and <span class="italic">h</span><small><sub><span class="italic">i</span>−1</sub></small> (defined before in the classical variant) to quantum states. The circuit consists of <span class="italic">R</span><small><sub><span class="italic">z</span></sub></small> and <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates after a conversion to an equal superposition state using Hadamard transforms.</p>
            <p class="otherpara">2. The next step involves parameterized unitaries with CNOT gates and single-qubit rotations. This block of parameterized unitary is used inside the input gate, the forget gate and the output gates (see <a title="Select to navigate to figure" href="#imgfig18">Fig. 18(b)</a>) for optimizing the response from each gate.</p>
            <p class="otherpara">3. The measurement protocol on each such block of parameterized unitary in (2) using the input encoded within the state-preparation circuit in (1) yields the necessary affine transformation which is subsequently passed through an appropriate activation function for each gate as defined before in the classical variant (see <a title="Select to navigate to figure" href="#imgfig18">Fig. 18(b)</a>).</p>
            <p class="otherpara">4. The parameters of the unitary in step (2) are updated through gradient estimates of a cost-function involving the error between the actual and the output from the network using a classical computer. The network was applied to many different problems including the dynamics of damped harmonic oscillators with good results.</p>
            <p class="otherpara">The circuit is schematically illustrated in <a title="Select to navigate to figure" href="#imgfig18">Fig. 18(b).</a></p>
          
        </div>
        <div>
          
          <span id="sect3829"/><span class="c_heading_indent">3.3.4 Autoencoders. </span>
          <span>A typical autoencoder is a type of neural network which is used to generate useful representations of the input data, to be used for unsupervised learning. The data can be thought of being generated from some distribution that represents a class spanning a subspace of the vector space in which they are represented. This is usually the case in most practically used image datasets and thus allows for dimensionality reduction. Autoencoders have helped in providing sparse representations,<a title="Select to navigate to references" href="#cit238"><sup><span class="sup_ref">238</span></sup></a> denoising,<a title="Select to navigate to references" href="#cit239"><sup><span class="sup_ref">239</span></sup></a> generating compact representations, information retrieval,<a title="Select to navigate to references" href="#cit240"><sup><span class="sup_ref">240</span></sup></a> anomaly detection,<a title="Select to navigate to references" href="#cit241"><sup><span class="sup_ref">241</span></sup></a> and image processing as a precursor to classification tasks. An autoencoder can be thought of as a feed forward neural network composed of an encoder and a decoder with a bottleneck layer providing a minimal representation separating them. The output of the encoder constructs a compact representation of the input data and is fed to the decoder which reconstructs it back (<a title="Select to navigate to figure" href="#imgfig19">Fig. 19</a>).</span>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig19"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f19_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f19.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f19.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 19 </b> <span id="fig19"><span class="graphic_title">(a) Schematic representation of a classical autoencoder. The encoder takes classical input to create a compact space representation. The decoder acts on the code to output a representation from the input space. (b) Schematic representation of a circuit used in quantum autoencoder as illustrated in ref. <a title="Select to navigate to references" href="#cit242">242</a>. The encoder <span class="italic">U</span><small><sub>AB</sub></small> acts on a circuit to create a code. The trash qubits are swapped out with a reference state and the decoder circuit works to reconstruct the input.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Like any other feed forward neural networks it is trained through back-propagation to minimize the reconstruction error defined over it. For the simplest one layer encoder–decoder circuit, with weights, biases and element wise activation function <span class="italic">W</span>,<span class="italic">b</span>,<span class="italic">σ</span> and <span class="italic">W</span>′,<span class="italic">b</span>′,<span class="italic">σ</span>′, we can construct the <span class="italic">L</span><small><sub>2</sub></small> norm loss function as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn78"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t186_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t186.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t186.gif"/></a></td><td class="rightEqn">(78)</td></tr></table>where <span class="italic">N</span> is the size of the training data set. Using a standard gradient descent one can train the parameters of the circuit to minimize the loss output. A regularization term might be added to ensure that the network isn’t overfitting to the training dataset. Here, we described an undercomplete autoencoder that made use of the no regularization term. Overfitting here is avoided by ensuring a small latent code size. Depending on the error function, inputs, and size of the latent space we can construct autoencoders that have different functionalities. A sparse encoder for instance has the same latent space size as the input and minimizes the number of activations in the latent space, implemented by an <span class="italic">L</span>1-regularization term on the latent space. A denoising encoder takes inputs that are overlayed with minimal perturbations to reconstruct the original image. A contractive autoencoder tries to ensure that samples that are close in the input space have a similar encoding representation.</p>
          
            
            <span id="sect3857"/><br/><span class="d_heading_indent">3.3.4.1 Quantum enhanced variants. </span>
            <span>To generalize a classical encoder to the quantum setting, we start with building a unitary circuit that allows information to be compressed into a smaller set of qubits with a garbage state in the remaining qubits that can be replaced with a reference state. We start with an ensemble of <span class="italic">N</span> pure states {|<span class="italic">ψ</span><small><sub><span class="italic">i</span></sub></small>〉<small><sub><small><sub>AB</sub></small></sub></small>}, where A is an <span class="italic">n</span> qubit system, and B is a <span class="italic">k</span> qubit system. Let <span class="italic">U</span> be the encoding unitary that takes as input a pure state from the ensemble. System B in the output is then swapped with a reference state and we try reconstructing the input state with a decoder given by the unitary <span class="italic">U</span><small><sup>†</sup></small>. The objective function to maximize is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn79"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t187_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t187.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t187.gif"/></a></td><td class="rightEqn">(79)</td></tr></table>where<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn80"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t188_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t188.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t188.gif"/></a></td><td class="rightEqn">(80)</td></tr></table>Here <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t189_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t189.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t189.gif"/></a><span class="italic">F</span> denotes the fidelity between the states, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t190_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t190.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t190.gif"/></a> is a swap gate that swaps the corresponding qubits and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t191_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t191.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t191.gif"/></a> represents the parameters of the unitary circuit that needs to be trained. It can be observed that a perfect fidelity is obtained when the output state of the encoder circuit produces a product circuit, <span class="italic">i.e.</span>, <span class="italic">U</span>|<span class="italic">ψ</span>〉<small><sub>AB</sub></small> = |<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e0e3.gif" alt="[small psi, Greek, tilde]"/></span><small><sub><span class="italic">i</span></sub></small>〉<small><sub><small><sub>A</sub></small></sub></small> ⊗ |<span class="italic">a</span>〉<small><sub>B</sub></small>.<a title="Select to navigate to references" href="#cit242"><sup><span class="sup_ref">242</span></sup></a> Thus, we could alternatively define the maximizing objective function as the fidelity over the trash system B with respect to the reference state as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn81"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t192_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t192.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t192.gif"/></a></td><td class="rightEqn">(81)</td></tr></table>This problem can thus be framed within the context of developing unitary circuits that work to disentangle qubits. It has been shown that by using circuits of exponential depth it is always possible to disentangle qubits.<a title="Select to navigate to references" href="#cit243"><sup><span class="sup_ref">243</span></sup></a> Other alternative implementations include using approximate quantum adders trained with genetic algorithms<a title="Select to navigate to references" href="#cit244"><sup><span class="sup_ref">244</span></sup></a> and generalization of feed forward neural networks as quantum circuits to implement autoencoders.<a title="Select to navigate to references" href="#cit245"><sup><span class="sup_ref">245</span></sup></a></span>
          
        </div>
        <div>
          
          <span id="sect3900"/><span class="c_heading_indent">3.3.5 Variational encoders. </span>
          <span>Unlike autoencoders that try and provide useful latent space representation, variational autoencoders (VAEs) are used to learn the distribution that models the latent space. The decoder thus generated can be used to sample the input space, working similar to generative adversarial networks (to be discussed shortly). They have found their use in unsupervised<a title="Select to navigate to references" href="#cit246"><sup><span class="sup_ref">246</span></sup></a> and semi-supervised learning.<a title="Select to navigate to references" href="#cit247"><sup><span class="sup_ref">247</span></sup></a> Let <span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">x</span>|<span class="italic">y</span>) be the conditional likelihood of the decoder and <span class="italic">q</span><small><sub><span class="italic">ϕ</span></sub></small>(<span class="italic">y</span>|<span class="italic">x</span>) be the approximated posterior distribution the encoder computes, where <span class="italic">x</span> is the input vector and <span class="italic">y</span> is the latent vector. We train the network on the parameters <span class="italic">θ</span>,<span class="italic">ϕ</span> to reduce the reconstruction error on the input and have <span class="italic">q</span><small><sub><span class="italic">ϕ</span></sub></small>(<span class="italic">y</span>|<span class="italic">x</span>) as close as possible to <span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">y</span>|<span class="italic">x</span>). Thus we would like to minimize the following evidence of lower bound loss function (ELBO):<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn82"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t193_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t193.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t193.gif"/></a></td><td class="rightEqn">(82)</td></tr></table>where <span class="italic">E</span> is the expectation value with respect to the specified distribution, <span class="italic">D</span><small><sub>KL</sub></small> is the KL divergence between the distributions and <span class="italic">x</span> is the input from the training set. The later equality of the above expression is obtained by expressing <span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">y</span>|<span class="italic">x</span>) using the Bayes theorem and regrouping terms. The KL divergence regularizes the expression allowing for continuity (the neighbouring points in latent space are mapped to the neighbouring points in the input space) and completeness (points in latent space map to meaningful points in the input space for any chosen distribution). At this point 2 assumptions are made to allow for training. First, <span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">x</span>|<span class="italic">y</span>) is a Gaussian distribution and <span class="italic">q</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">y</span>|<span class="italic">x</span>) is a multivariate Gaussian that can be re-expressed as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t194_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t194.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t194.gif"/></a> to allow for gradient back-propagation (the reparametrization trick), where <span class="italic">ε</span> ∼ <span class="italic">N</span>(0,<span class="italic">I</span>) and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t195_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t195.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t195.gif"/></a> is an element-wise product.</span>
          
            
            <span id="sect3957"/><br/><span class="d_heading_indent">3.3.5.1 Quantum enhanced variants. </span>
            <span>Khoshaman <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit249"><sup><span class="sup_ref">249</span></sup></a> developed a quantum variational autoencoder that makes use of a quantum Boltzmann machine (QBM)<a title="Select to navigate to references" href="#cit250"><sup><span class="sup_ref">250</span></sup></a> to evaluate the gradient updates used in training. A QBM is an energy model defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn83"/><span id="eqn83"><span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>(<span class="italic">z</span>) = Tr[<span class="italic">Λ</span><small><sub><span class="italic">z</span></sub></small>e<small><sup>−<span class="italic">H</span><small><sub>θ</sub></small></sup></small>]</span></td><td class="rightEqn">(83)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn84"/><span id="eqn84"><span class="italic">Z</span><small><sub><span class="italic">θ</span></sub></small> = Tr[e<small><sup>−<span class="italic">H</span><small><sub>θ</sub></small></sup></small>]</span></td><td class="rightEqn">(84)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn85"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t198_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t198.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t198.gif"/></a></td><td class="rightEqn">(85)</td></tr></table>where <span class="italic">θ</span> = <span class="italic">Γ</span>, <span class="italic">h</span>, <span class="italic">W</span>, <span class="italic">Λ</span> = <span class="italic">zz</span>, <span class="italic">σ</span><small><sup><span class="italic">z</span>,<span class="italic">x</span></sup></small><small><sub><span class="italic">l</span></sub></small> are Pauli operators and <span class="italic">p</span><small><sub><span class="italic">t</span></sub></small><span class="italic">heta</span> (<span class="italic">z</span>) governs the distribution of the states |<span class="italic">z</span>〉. The ELBO is defined with a cross entropy term as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn86"/><span id="eqn86"><span class="italic">H</span>(<span class="italic">p</span><small><sub><span class="italic">θ</span></sub></small>,<span class="italic">q</span><small><sub><span class="italic">ϕ</span></sub></small>) = −<span class="italic">E</span><small><sub><span class="italic">z</span>∼<span class="italic">q</span><small><sub><span class="italic">ϕ</span></sub></small></sub></small>[log(Tr[<span class="italic">Λ</span><small><sub><span class="italic">z</span></sub></small>e<small><sup>−<span class="italic">H</span><small><sub>θ</sub></small></sup></small>])] + log(<span class="italic">Z</span><small><sub><span class="italic">θ</span></sub></small>)</span></td><td class="rightEqn">(86)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn87"/><span id="eqn87">≥<span class="italic">E</span><small><sub><span class="italic">z</span>∼<span class="italic">q</span><small><sub><span class="italic">ϕ</span></sub></small></sub></small>[log(Tr[e<small><sup>−<span class="italic">H</span><small><sub>θ</sub></small>+ln<span class="italic">Λ</span><small><sub><span class="italic">z</span></sub></small></sup></small>])] + log(<span class="italic">Z</span><small><sub><span class="italic">θ</span></sub></small>)</span></td><td class="rightEqn">(87)</td></tr></table>where in the second line we have used Golden–Thompson inequality (Tr[e<small><sup>A</sup></small>e<small><sup>B</sup></small>] ≥ Tr[e<small><sup>A+B</sup></small>]) to express the intractable first term with a lower bound. Similar to the classical case, a reparametrization trick is employed to effectively evaluate gradients and the trace is taken to be concentrated at the state |<span class="italic">z</span>〉. See ref. <a title="Select to navigate to references" href="#cit249">249</a> for the reparametrization trick in the continuous and discrete case setting.</span>
          
        </div>
        <div>
          
          <span id="sect4056"/><span class="c_heading_indent">3.3.6 Generative adversarial network (GAN). </span>
          <span>Generative adversarial network (GAN) was introduced by Ian Goodfellow <span class="italic">et al.</span> in 2014<a title="Select to navigate to references" href="#cit251"><sup><span class="sup_ref">251</span></sup></a> and is considered to be one of the major milestones of machine learning in the last 10 years. Its applications extend to art,<a title="Select to navigate to references" href="#cit252"><sup><span class="sup_ref">252</span></sup></a> science,<a title="Select to navigate to references" href="#cit253"><sup><span class="sup_ref">253</span></sup></a> video games,<a title="Select to navigate to references" href="#cit254"><sup><span class="sup_ref">254</span></sup></a> deepfakes and transfer learning.<a title="Select to navigate to references" href="#cit255"><sup><span class="sup_ref">255</span></sup></a> A GAN consists of a generator and a discriminator that are trained simultaneously to learn a given distribution by competing against each other. The goal of the generator is to generate fake samples that cannot be distinguished from the true samples of the input data. The goal of the discriminator is to correctly distinguish the fake samples from true samples, thus solving a well understood classification problem. This game has a Nash equilibrium point that is attained when the generator is able to generate samples that are distinguished with a probability of 1/2, making it no better than a random guess. Let the generator <span class="italic">G</span> take a random input from a distribution <span class="italic">p</span><small><sub><span class="italic">z</span></sub></small> (usually taken to be a Gaussian distribution) to generate samples from the distribution <span class="italic">p</span><small><sub><span class="italic">f</span></sub></small>. Let <span class="italic">D</span> be the discriminator that takes inputs equally likely sampled from <span class="italic">p</span><small><sub><span class="italic">t</span></sub></small> (true distribution) and <span class="italic">p</span><small><sub><span class="italic">f</span></sub></small> to output the probability of data coming from <span class="italic">p</span><small><sub><span class="italic">t</span></sub></small>. The objective function for the discriminator is thus given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn88"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t199_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t199.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t199.gif"/></a></td><td class="rightEqn">(88)</td></tr></table>where the first term is the error in determining the true samples to be fake and the second term is the error in determining the generated samples to be true. The generator on the other hand tries to maximize this loss function against the trained discriminator, <span class="italic">i.e.</span>, the objective function of the generator is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn89"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t200_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t200.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t200.gif"/></a></td><td class="rightEqn">(89)</td></tr></table></span>
          <p class="otherpara">The parameters of the generator and discriminator are trained alternatively till the discriminator no longer is able to differentiate <span class="italic">p</span><small><sub><span class="italic">f</span></sub></small> from <span class="italic">p</span><small><sub><span class="italic">t</span></sub></small>. Thus the distributions are equal in the eyes of the discriminator and we have managed to create a generative model for the given training samples. The discriminator is discarded after the training. <span class="italic">p</span><small><sub><span class="italic">z</span></sub></small> can be thought of as the distribution that represents the domain set of the problem and thus the generator works to extract features of the input vector. The trained generator can be further re-purposed for transfer learning on similar input data. A conditional extension referred to as cGAN (conditional GAN) allows for generating inputs from a specific class by imposing additional restrictions on the random input vector provided. This restriction can be envisioned as selecting from a specific class within the input domain. To train a cGAN the discriminator also needs to be provided with this additional label input to constrain classification within the chosen class.</p>
          
            
            <span id="sect4100"/><br/><span class="d_heading_indent">3.3.6.1 Quantum enhanced variants. </span>
            <span>In the quantum generative adversarial network (QGAN),<a title="Select to navigate to references" href="#cit256"><sup><span class="sup_ref">256</span></sup></a> we have a source <span class="italic">U</span><small><sub><span class="italic">S</span></sub></small> that outputs true samples and a generator <span class="italic">U</span><small><sub><span class="italic">G</span></sub></small> that outputs fake samples. Both <span class="italic">U</span><small><sub><span class="italic">S</span></sub></small> and <span class="italic">U</span><small><sub><span class="italic">G</span></sub></small> take an input state |0〉<small><sup>⊗</sup></small><span class="italic">n</span>, label |<span class="italic">λ</span>〉 and random noise |<span class="italic">z</span>〉 to output a density matrix in the system qubits. The noise vector supports to provide a distribution for the generator on the output qubit state corresponding to a given input label. With equal probability we choose between <span class="italic">U</span><small><sub><span class="italic">G</span></sub></small> and <span class="italic">U</span><small><sub><span class="italic">S</span></sub></small> to create a density matrix that is fed into the discriminator. Alongside the sample output from <span class="italic">U</span><small><sub><span class="italic">S</span></sub></small> or <span class="italic">U</span><small><sub><span class="italic">G</span></sub></small> provided, the discriminator takes as input the label |<span class="italic">λ</span>〉 used to generate the state, a bath |0〉<small><sup>⊗</sup></small><span class="italic">d</span> that works as a scratchpad and an output qubit to measure the probability of the source of the sample. <a title="Select to navigate to figure" href="#imgfig20">Fig. 20</a> provides a sketch for the working of the QGAN. The objective function can thus be given by,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn90"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t201_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t201.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t201.gif"/></a></td><td class="rightEqn">(90)</td></tr></table>where <span class="italic">ρ</span><small><sup><span class="italic">S</span></sup></small><small><sub><span class="italic">λ</span></sub></small> = <span class="italic">U</span><small><sub><span class="italic">S</span></sub></small><span class="italic">ρ</span><small><sup>0</sup></small><small><sub><span class="italic">λ</span></sub></small><span class="italic">U</span><small><sup>†</sup></small><small><sub><span class="italic">S</span></sub></small> is the state output by the source and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t202_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t202.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t202.gif"/></a> is the state output by the generator and <span class="italic">Z</span> represents the measurement made at the output qubit of the discriminator. The first and second terms in the trace come from the discriminators’ success in correctly predicting states from source and generator, respectively. The cost function has been derived using measurement probabilities to keep the expression linear, unlike the maximum likelihood optimization used for the classical case. Given the optimization function, gradients can be computed using a parameter shift trick or re-expressing it as a sum of simple unitary operations.<a title="Select to navigate to references" href="#cit257"><sup><span class="sup_ref">257</span></sup></a> For a complete discussion on the derivation of cost function, analyzing limiting cases and computing gradients corresponding to the parameters, refer ref. <a title="Select to navigate to references" href="#cit248">248</a>.</span>
            <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig20"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f20_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f20.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f20.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 20 </b> <span id="fig20"><span class="graphic_title">(a) Schematic representation of classical GAN. The generator takes as input some noise to produce a sample. Samples from the real source and generator are fed to the discriminator. These work as labelled data for the discriminator to distinguish. The errors are used to backpropagate and train the generator and discriminator. (b) Schematic representation of the quantum circuit used in quantum GAN as illustrated in ref. <a title="Select to navigate to references" href="#cit248">248</a>. Samples are output from the real source <span class="italic">S</span> or the generator <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t196_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t196.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t196.gif"/></a> that takes as input some noise and label. This is then fed to the discriminator <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t197_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t197.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t197.gif"/></a> along with the label qubits and scratch space(bath) to work on. The measured output qubit is used to backpropagate the errors through classical updates to give new circuit parameters.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          
        </div>
      
      
        
        <h3 id="sect4169"><span class="b_heading">3.4 Tensor networks</span></h3>
        <span>Tensor network states constitute an important set of variational quantum states for numerical studies of strongly correlated systems in physics and chemistry as they attempt to construct global quantum states from tensors associated with local degrees of freedom.<a title="Select to navigate to reference" href="#cit258"><sup><span class="sup_ref">258,259</span></sup></a> Expressing a quantum many-body system defined on <span class="italic">n</span> qubits requires 2<small><sup><span class="italic">n</span></sup></small> complex coefficients. Storing and manipulating these numbers of coefficients on a classical computer pose a big challenge while simulating strongly correlated systems. Luckily physically relevant quantum states often possess a limited amount of entanglement wherein only a subset of these coefficients are necessary to describe these states efficiently. Tensor networks provide a natural language to model complex quantum systems (states and operators) on which the amount of entanglement (or correlations in the case of mixed-state dynamics) is conveniently restricted. The representation is such that the complexity of the structure grows linearly with qubit but exponentially with the amount of entanglement in the system. It thereby allows manipulation of quantum states residing in large Hilbert spaces with polynomial amount of resources. The classical simulation of low entangled systems (whose entanglement grows at most polynomially with system size <span class="italic">n</span>) becomes tractable using tensor network algorithms like Density Matrix Renormalization Group (DMRG) [discussed in detail in Section 3.4.5] and Time-Evolving Block Decimation (TEBD).<a title="Select to navigate to reference" href="#cit260"><sup><span class="sup_ref">260,261</span></sup></a></span>
        <p class="otherpara">Tensor networks are the graphic representation of tensors in Einstein notation such that a rank-<span class="italic">n</span> tensor is represented by a box with <span class="italic">n</span> indices projecting out of it. The connections between tensors signify the set of indices of tensors which are contracted. Hence the final rank of a tensor network is determined by the number of free edges. A quantum state |<span class="italic">ψ</span>〉 in the <span class="italic">n</span>-dimensional Hilbert space is basically a rank-<span class="italic">n</span> tensor and can be written as,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn91"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t203_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t203.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t203.gif"/></a></td><td class="rightEqn">(91)</td></tr></table>where |<span class="italic">l</span><small><sub><span class="italic">i</span></sub></small>〉 represent the local basis states and the coefficients <span class="italic">m</span><small><sub>1<small><sub>1</sub></small></sub></small>,<small><sub><span class="italic">l</span><small><sub>2</sub></small></sub></small>,<small><sub>…</sub></small>,<small><sub><span class="italic">l</span><small><sub><span class="italic">n</span></sub></small></sub></small> are the amplitude of wave function in a given basis state |<span class="italic">l</span><small><sub>1</sub></small>〉, |<span class="italic">l</span><small><sub>2</sub></small>〉,…,|<span class="italic">l</span><small><sub><span class="italic">n</span></sub></small>〉.</p>
        <p class="otherpara">The idea of using tensor networks to represent quantum systems is motivated from the very famous Area Law<a title="Select to navigate to references" href="#cit262"><sup><span class="sup_ref">262</span></sup></a> which states that the ground state of the Hamiltonian resides in the low entangled space such that the entanglement entropy between any two partitions of the system grows as the area of the surface separating them. The entangled entropy is usually quantified in terms of the von Neumann entropy of a quantum many-body system which is defined as <span class="italic">S</span>(<span class="italic">ρ</span>) = −tr[<span class="italic">ρ</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">ρ</span>], where <span class="italic">ρ</span> is the density matrix of the state. It serves as a befitting measure of the degree of quantum correlation that exists between any of the two-partitions of the system under consideration. The area law has been proven only for gapped Hamiltonians in one-dimension by Hastings<a title="Select to navigate to references" href="#cit263"><sup><span class="sup_ref">263</span></sup></a> and has been studied intensively for higher dimensions [see section IV of review<a title="Select to navigate to references" href="#cit262"><sup><span class="sup_ref">262</span></sup></a> for a detailed discussion on the area law in higher dimensions]. The area law guarantees an efficient description of ground states by a matrix product state and justifies the density-matrix renormalization group algorithm. Each of these two methods will be discussed in detail in Sections 3.4.1 and 3.4.5, respectively.</p>
        <p class="otherpara">Tensor networks can be broadly classified into two main groups: those based on the matrix product state (MPS), the tree tensor network state (TTN) and their higher dimension analogues (ex. PEPS); and those based on the multiscale entanglement renormalization ansatz (MERA). We shall discuss applications of TN in Section 5.2 and in Sections 4 and 5.3.</p>
        <div>
          
          <span id="sect4219"/><span class="c_heading_indent">3.4.1 Matrix product state (MPS). </span>
          <span>A pure quantum state |<span class="italic">ψ</span>〉 of an <span class="italic">n</span>-qubit system can be described by the sum of tensor products of orthogonal states in two subsystems. The Schmidt decomposition<a title="Select to navigate to references" href="#cit49"><sup><span class="sup_ref">49</span></sup></a> of <span class="italic">ψ</span> with respect to the partition reads as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn92"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t204_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t204.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t204.gif"/></a></td><td class="rightEqn">(92)</td></tr></table>where |<span class="italic">u</span><small><sub><span class="italic">i</span></sub></small>〉<small><sub>A</sub></small> and |<span class="italic">v</span><small><sub><span class="italic">i</span></sub></small>〉<small><sub>B</sub></small> are the state of the subsystems A and B and <span class="italic">λ</span><small><sub><span class="italic">i</span></sub></small>s are the Schmidt coefficients of the quantum state with respect to the partition. The Schmidt rank <span class="italic">χ</span><small><sub>A</sub></small> is defined by the number of non-zero Schmidt coefficients. It is a natural measure of the entanglement between the qubits in A and B, popularly known as bond dimensions in the tensor network community. The von-Neumann entropy between the two partitions is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn93"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t205_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t205.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t205.gif"/></a></td><td class="rightEqn">(93)</td></tr></table></span>
          <p class="otherpara">Sometimes entanglement entropy is measured in ebits where one ebit is the amount of entanglement possessed by a maximally entangled two-qubit Bell state. Now, if subsystems A and B are further partitioned into smaller subsystems, we are ultimately left with single-qubit subsystems. Let the states on these single-qubit subsystems be denoted by <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small> and the diagonal matrices containing the Schmidt coefficients be denoted by <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small>. Let these states be denoted by <span class="italic">Λ</span><small><sup>[<span class="italic">i</span>]</sup></small> and the diagonal matrix containing the Schmidt coefficient be denoted by <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small>. Then the quantum state reads as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn94"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t206_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t206.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t206.gif"/></a></td><td class="rightEqn">(94)</td></tr></table>where |<span class="italic">Λ</span><small><sub><span class="italic">s</span><small><sub><span class="italic">i</span></sub></small></sub></small>〉 are complex square matrices of order <span class="italic">χ</span> (the bond dimension). <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> represent the state indices in the computational basis (physical indices). This format of representing quantum states is known as the matrix product state.<a title="Select to navigate to references" href="#cit264"><sup><span class="sup_ref">264</span></sup></a> In the tensor network notation it can be described as shown in <a title="Select to navigate to figure" href="#imgfig21">Fig. 21(a)</a>. Operators can similarly be represented in the matrix product form known as the matrix product operator (MPO).</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig21"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f21_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f21.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f21.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 21 </b> <span id="fig21"><span class="graphic_title">Schematic representation of different types of tensor networks: (a) matrix product state, (b) tree-tensor networks, (c) multi-scale entanglement renormalization ansatz, and (d) projected entangled pair states on a square lattice. Each solid object represents a tensor while the black lines denote the indices. The triangles in (b) and (c) are isometric tensors while the circles in (c) are unitary disentanglers.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Matrix product states in theory can represent a maximally entangled state but the bond dimension at the middle cut would grow as <span class="italic">O</span>(2<small><sup><span class="italic">n</span></sup></small>).<a title="Select to navigate to references" href="#cit264"><sup><span class="sup_ref">264</span></sup></a> For an MPS with a fixed bond dimension <span class="italic">χ</span>, the quantum state residing in the <span class="italic">n</span>-qubit Hilbert space can now be represented using just <span class="italic">O</span>(<span class="italic">nχ</span><small><sup>2</sup></small>) parameters. The area law limits the bond dimension of the ground state of local gapped Hamiltonians making them the best candidates for MPS representation. Evaluating inner products of two quantum states in MPS form takes <span class="italic">O</span>(<span class="italic">nχ</span><small><sup>2</sup></small>) time.</p>
          <p class="otherpara">The <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small> matrices in the <a title="" href="#eqn94">eqn (94)</a> are usually absorbed into the nearby local tensor <span class="italic">λ</span>. The matrix product state is invariant of the contraction of <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small> either to left or right. This gives MPS a gauge degree of freedom. Usually the gauge is fixed by choosing either of the two directions for multiplying <span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small> giving rise to the left and right canonical forms of MPS. The process is known as canonicalization.<a title="Select to navigate to reference" href="#cit264"><sup><span class="sup_ref">264,265</span></sup></a> There is another kind of canonical form known as the mixed canonical form<a title="Select to navigate to references" href="#cit266"><sup><span class="sup_ref">266</span></sup></a> which is obtained by combining each (<span class="italic">λ</span><small><sup>[<span class="italic">i</span>]</sup></small>) to the left (right) of a given special site to its left (right) neighbouring (<span class="italic">Λ</span>).</p>
        </div>
        <div>
          
          <span id="sect4306"/><span class="c_heading_indent">3.4.2 Tree tensor networks (TTNs). </span>
          <span>Tree tensor networks provide another approach to model quantum states by arranging the local tensors in a tree-like pattern (see <a title="Select to navigate to figure" href="#imgfig21">Fig. 21(b)</a>). A TTN can be formed from an <span class="italic">n</span>-qubit quantum state using the tree-Tucker decomposition.<a title="Select to navigate to reference" href="#cit267"><sup><span class="sup_ref">267,268</span></sup></a> Like other tensor networks, TTNs are used as an ansatz to simulate the ground state of local Hamiltonian.<a title="Select to navigate to reference" href="#cit269"><sup><span class="sup_ref">269–271</span></sup></a> Tensors in TTNs form the nodes of the tree which are connected to each other through bond indices. The physical indices appear on the leaf nodes. On contracting the bond indices, the TTN has <span class="italic">n</span> free indices which represent the physical degree of freedom of the state. TTNs are a generalization of MPS and can in principle be non-binary as well.<a title="Select to navigate to reference" href="#cit272"><sup><span class="sup_ref">272,273</span></sup></a> An MPS can be thought of as a flattened TTN such that each parent node has one successor (bond indices of MPS) and another leaf node (physical indices of MPS).</span>
          <p class="otherpara">The structure of TTN is inspired from the spatial renormalization group.<a title="Select to navigate to references" href="#cit274"><sup><span class="sup_ref">274</span></sup></a> At every layer of TTN, coarse-graining is carried out between neighbouring sub-trees. Unlike MPS, the local tensors with access to physical indices in TTN are not connected directly to each other, the correlation between qubits is represented through the layers. The local correlation information is stored in the lower layers while the upper layers store long-range correlation information.</p>
          <p class="otherpara">Each node in a TTN is a three-dimensional tensor (except the root/uppermost node) with at most one upper index <span class="italic">α</span> and two lower indices <span class="italic">β</span><small><sub>1</sub></small> and <span class="italic">β</span><small><sub>2</sub></small>. The tensors can be written as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t207_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t207.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t207.gif"/></a>. The space required to store a TTN grows as <span class="italic">O</span>(<span class="italic">ND</span><small><sup>3</sup></small>) (see theorem 4.1<a title="Select to navigate to references" href="#cit267"><sup><span class="sup_ref">267</span></sup></a>), where <span class="italic">N</span> is the number of physical indices and <span class="italic">D</span> is the bond dimension of the local tensors. Each tensor in TTN is an isometry satisfying the following condition:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn95"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t208_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t208.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t208.gif"/></a></td><td class="rightEqn">(95)</td></tr></table></p>
          <p class="otherpara">Choosing an isometric tensor as in <a title="" href="#eqn95">eqn (95)</a> is advantageous in numerous ways. It simplifies the optimization of TTN and calculation of the expectation values of local observables and it is also known to provide numerical stability to TTN algorithms.<a title="Select to navigate to references" href="#cit275"><sup><span class="sup_ref">275</span></sup></a> TTN can very well be generalized to higher dimensions by appropriately placing isometries across local physical indices and hierarchically merging sub-trees through more isometries. Tagliacozzo <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit275"><sup><span class="sup_ref">275</span></sup></a> in their studies demonstrate simulation of the transverse-field Ising model on the square lattice using a two-dimensional TTN. Their approach takes advantage of the area law which reduces their simulation cost to exp(<span class="italic">N</span>) instead of exp(<span class="italic">N</span><small><sup>2</sup></small>).</p>
          <p class="otherpara">Tree tensor networks form the basis of the multi-layer multi-configuration time-dependent Hartree (ML-MCTDH) methods which are used to perform quantum molecular dynamics simulations. In the report<a title="Select to navigate to references" href="#cit276"><sup><span class="sup_ref">276</span></sup></a> authors compute the vibrational eigenstates of acetonitrile using TTNs. ML-MCTDH methods are a generalization of the MCTDH methods which can be optimized using MPS as shown in the report.<a title="Select to navigate to references" href="#cit277"><sup><span class="sup_ref">277</span></sup></a> Authors make use of the DMRG algorithm to efficiently evaluate the mean-field operators represented in the MPS format. The runtime of the MCTDH methods scales exponentially with the system size, hence multi-layered MCTDH is used which makes use of Tucker decomposition to reduce the dimensionality of the problem and enables it to simulate larger systems.</p>
        </div>
        <div>
          
          <span id="sect4345"/><span class="c_heading_indent">3.4.3 Projected entangled pair states (PEPSs). </span>
          <span>PEPS is a generalization of MPS in higher dimensions or for arbitrary graphs.<a title="Select to navigate to references" href="#cit264"><sup><span class="sup_ref">264</span></sup></a> It get its name from the way it is constructed. Let a vertex of a graph contain <span class="italic">k</span> edges; each edge can be represented by a virtual spin of dimension <span class="italic">D</span> (bond dimension). The edges are described by a maximally entangled state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t209_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t209.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t209.gif"/></a>. Now the vertex can be defined by a <span class="italic">k</span>-rank tensor containing entangled states. Ultimately this tensor is projected onto the physical spin through a linear map, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t210_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t210.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t210.gif"/></a>, where <span class="italic">d</span> is the local dimension of the physical state.</span>
          <p class="otherpara">In one dimension (<span class="italic">k</span> = 2), an entangled pair of states is projected onto the physical index. While in a square lattice, each local tensor has at most four neighbours [see <a title="Select to navigate to figure" href="#imgfig21">Fig. 21(d)</a>]. Hence, the local tensor can be written as <span class="italic">Λ</span><small><sup><span class="italic">α</span>,<span class="italic">β</span>,<span class="italic">γ</span>,<span class="italic">δ</span></sup></small><small><sub><span class="italic">s</span></sub></small>, where <span class="italic">s</span> is the physical index and <span class="italic">α</span>, <span class="italic">β</span>, <span class="italic">γ</span>, and <span class="italic">δ</span> are bond indices. Hence storing a PEPS requires <span class="italic">O</span>(<span class="italic">N</span><small><sup>2</sup></small><span class="italic">dD</span><small><sup>4</sup></small>) space, where <span class="italic">N</span> is the number of qubits along a side of the square, <span class="italic">d</span> is the dimension of the physical local state and <span class="italic">D</span> is the bond dimension. Performing computations on PEPSs is difficult;<a title="Select to navigate to references" href="#cit278"><sup><span class="sup_ref">278</span></sup></a> for instance, evaluating the inner products of PEPSs scales exponentially with <span class="italic">D</span>. This is because any partition which divides PEPSs into two equal parts always cuts the <span class="italic">O</span>(<span class="italic">N</span>) bonds; hence, while evaluating the inner product, one has to form a rank-<span class="italic">O</span> (<span class="italic">N</span>) tensor as an intermediate.</p>
          <p class="otherpara">PEPSs can theoretically represent any state due to their generic structure given that their bond dimension can be arbitrarily large. Due to this universality, PEPSs serve as a variational ansatz in numerical simulation of a wide variety of quantum systems. It can easily prepare physically important states like GHZ and Cluster State<a title="Select to navigate to references" href="#cit279"><sup><span class="sup_ref">279</span></sup></a> using <span class="italic">D</span> = 2. With <span class="italic">D</span> = 3, PEPSs can prepare a resonance valence bond states.<a title="Select to navigate to references" href="#cit280"><sup><span class="sup_ref">280</span></sup></a> Kitaev's Toric code which finds its application in quantum error correction and demonstrates non-trivial topological properties can be prepared using PEPSs with <span class="italic">D</span> = 2.<a title="Select to navigate to references" href="#cit281"><sup><span class="sup_ref">281</span></sup></a> It is widely known that PEPSs can efficiently approximate ground states of gapped local Hamiltonian which satisfy the area law. In the report<a title="Select to navigate to references" href="#cit282"><sup><span class="sup_ref">282</span></sup></a> authors show that they can compute the expectation values of local observables in quasi-polynomial time. Jordan <span class="italic">et al</span>. proposed algorithms to compute the ground states and time evolution of two-dimensional Hamiltonians defined on infinite-size lattice using PEPSs.<a title="Select to navigate to references" href="#cit283"><sup><span class="sup_ref">283</span></sup></a> It is known that it is difficult to simulate systems with long-range correlations on PEPSs, but Gu <span class="italic">et al.</span> extensively studied these systems to demonstrate the power and versatility of PEPSs.<a title="Select to navigate to references" href="#cit284"><sup><span class="sup_ref">284</span></sup></a> They studied both systems which exhibit symmetry breaking phase transition (transverse field Ising model) and those that show topological phase transition (<span class="italic">Z</span><small><sub>2</sub></small> gauge model and double-semion model).</p>
          <p class="otherpara">While PEPSs have been designed to study quantum systems on classical computers, there have been approaches to simulate them on a quantum computer for faster computations. Schwarz <span class="italic">et al.</span> in their report<a title="Select to navigate to references" href="#cit285"><sup><span class="sup_ref">285</span></sup></a> presented an algorithm to prepare a PEPS on a quantum computer which scales only polynomially with the spectral gap and the minimum condition number of the PEPS projectors. In the consecutive year they came up with another algorithm to prepare topologically projected entangled pair states on a quantum computer with a similar runtime.<a title="Select to navigate to references" href="#cit286"><sup><span class="sup_ref">286</span></sup></a> Specifically they simulated the resonance valence bond state which is hypothesized to contain the topological spin liquid phase.</p>
          <p class="otherpara">There also exists an infinite version of MPS (PEPS) known as iMPS (iPEPS).<a title="Select to navigate to references" href="#cit283"><sup><span class="sup_ref">283</span></sup></a> They allow working directly in the thermodynamic limit without encountering the finite size or boundary effects. There have been accurate studies of continuous quantum phase transitions using iPEPS.<a title="Select to navigate to references" href="#cit287"><sup><span class="sup_ref">287</span></sup></a></p>
        </div>
        <div>
          
          <span id="sect4408"/><span class="c_heading_indent">3.4.4 Multi-scale entanglement renormalisation ansatz (MERA). </span>
          <span>MERA<a title="Select to navigate to references" href="#cit288"><sup><span class="sup_ref">288</span></sup></a> is a powerful class of Tensor Networks which can be used to study gapless ground states and properties of systems near criticality. Despite its huge success in representing a wide variety of states, MPS is scalable only for gapped systems with exponentially decaying correlations and the area law is strictly satisfied. Owing to its hierarchical structure, MERA allows long-range correlations and shows polynomially decaying correlations [shown in <a title="" href="#eqn5">eqn (5)</a> of ref. <a title="Select to navigate to references" href="#cit289">289</a>]. The entanglement entropy of an <span class="italic">N</span>-qubit 1D gapless system grows as <span class="italic">O</span>(log(<span class="italic">N</span>)) and hence it can be naturally represented by a MERA.</span>
          <p class="otherpara">The architecture of MERA is inspired from the Renormalization Group.<a title="Select to navigate to reference" href="#cit265"><sup><span class="sup_ref">265,290</span></sup></a> Its structure is very similar to that of TTN with an additional type of tensors known as disentanglers (<span class="italic">U</span>) [shown in <a title="Select to navigate to figure" href="#imgfig21">Fig. 21(c)</a> using blue circles]. These are unitary tensors satisfying<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn96"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t211_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t211.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t211.gif"/></a></td><td class="rightEqn">(96)</td></tr></table>whereas the isometries (<span class="italic">W</span>) [depicted using red triangles in <a title="Select to navigate to figure" href="#imgfig21">Fig. 21(c)</a>] satisfy <a title="" href="#eqn95">eqn (95)</a>. To recover a TTN from MERA one can simply replace the disentangler with an identity tensor.</p>
          <p class="otherpara">Entanglement in MERA builds up due to its layered structure. To dissect a sub-system of <span class="italic">n</span>-qubits from the system requires at least <span class="italic">O</span>(log(<span class="italic">n</span>)) bonds to be broken. Hence the maximum entanglement entropy generated by MERA goes as <span class="italic">O</span>(log(<span class="italic">n</span>)log(<span class="italic">D</span>)). That is why MERA allows logarithmic divergence from the area law.<a title="Select to navigate to reference" href="#cit265"><sup><span class="sup_ref">265,289</span></sup></a></p>
          <p class="otherpara">Storing a MERA on a classical computer requires space polynomial in number of qubits and the bond dimension. Performing computations using MERA is simplified due to its structure. It can perform efficient computation of local expectation values and correlators by only contracting over the shadow (causal cone) of the local operator, <span class="italic">i.e.</span>, the tensor which are directly connected to the operator and those tensors on higher levels which are further connected to these tensors. The isometries and disentanglers which lie outside this shadow contract themselves with their conjugates to give unity.</p>
        </div>
        <div>
          
          <span id="sect4438"/><span class="c_heading_indent">3.4.5 Density matrix renormalization group (DMRG). </span>
          <span>DMRG is one of the most successful algorithms for simulation of condensed matter systems. It was introduced by White<a title="Select to navigate to references" href="#cit291"><sup><span class="sup_ref">291</span></sup></a> in the pre-tensor network era. The algorithm has changed a lot over the years and has been simplified by adapting to the language of tensor networks. In the following discussion, we will be describing the modern DMRG algorithm using matrix product state formulation.<a title="Select to navigate to references" href="#cit266"><sup><span class="sup_ref">266</span></sup></a></span>
          <p class="otherpara">Finding the ground state of a Hamiltonian is a challenging problem and yet is one of the core problem in physics, chemistry, and materials sciences. Even for one-dimensional <span class="italic">k</span>-local Hamiltonian it is known to be QMA-complete,<a title="Select to navigate to references" href="#cit292"><sup><span class="sup_ref">292</span></sup></a><span class="italic">i.e.</span>, it's difficult to solve it in polynomial time even with access to a fully functional quantum computer.</p>
          <p class="otherpara">The ground state of a gapped local Hamiltonian is known to reside in a low entanglement regime by the area law. DMRG algorithm makes use of this property to find the solution of the local Hamiltonian problem. The algorithm makes use of an ansatz which succinctly represents the state with bounded entanglement (matrix product state). The ground state of the Hamiltonian is attained by minimizing the energy of the system,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn97"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t212_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t212.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t212.gif"/></a></td><td class="rightEqn">(97)</td></tr></table></p>
          <p class="otherpara">Before starting the algorithm the Hamiltonian has to be converted into a matrix product operator so that it is compatible with MPS. A <span class="italic">k</span>-local Hamiltonian can be written as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t213_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t213.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t213.gif"/></a>, where <span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> are local hermitian operators acting on at most <span class="italic">k</span>-qubits. Each <span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> can be converted into an MPO defined on <span class="italic">k</span> physical indices using recursive singular valued decomposition as explained in Section 3.4.1. Once local operators are converted into MPO, they can be added using the MPO addition operation, which is basically a direct sum operation over the bond indices (see Section 5.2 in ref. <a title="Select to navigate to references" href="#cit266">266</a> for details).</p>
          <p class="otherpara">The initial MPS can be created using random tensors of desired dimension. At each step of DMRG a contiguous set of sites is chosen which is to be optimized and is designated as the system, while everything outside the system is called the environment which is kept fixed. By performing local optimization over the system states iteratively, the ground state of the given Hamiltonian is attained. Usually it requires several sweeps over the complete lattice to reach convergence which depend on the complexity of the Hamiltonian and also the choice of the initial state.</p>
          <p class="otherpara">To perform local optimization over the system, the environment qubits are contracted to form a reduced Hamiltonian (<span class="italic">H</span><small><sub><span class="italic">S</span></sub></small>) whose energy is then minimized.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn98"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t214_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t214.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t214.gif"/></a></td><td class="rightEqn">(98)</td></tr></table></p>
          <p class="otherpara">Energy minimization of <span class="italic">Ĥ</span><small><sub><span class="italic">S</span></sub></small> can be analogously performed by solving the following eigenvalue problem: <span class="italic">Ĥ</span><small><sub><span class="italic">S</span></sub></small>|<span class="italic">ψ</span><small><sub><span class="italic">S</span></sub></small>〉 = <span class="italic">E</span>|<span class="italic">ψ</span><small><sub><span class="italic">S</span></sub></small>〉.</p>
          <p class="otherpara">The system state |<span class="italic">ψ</span><small><sub><span class="italic">S</span></sub></small>〉 so obtained updates the current system state. The system can be defined by any number of qubits. For single qubit systems the bond dimension remains fixed while working with two or more site systems can allow the bond dimensions to be changed dynamically. Basically, the local optimization procedure for a multi-site system returns the state defined on multiple qubits. This state has to be decomposed into an MPS using recursive singular value decomposition before replacing them at the current state. Since SVD gives us the complete set of singular values we can choose to trim the bond dimensions which are below the threshold of the accuracy required. Usually a larger system size means more accurate results and the trial state converges to the ground state in lesser number of sweeps. But it also increases the overall computational cost. The computational cost heavily depends on the local optimization procedure which can be improved by using iterative algorithms like Lanczos which only computes the smallest set of eigenvalues and eigenvector of a given operator. Since we are only interested in the ground state of reduced Hamiltonian, Lanczos algorithm can massively cut down the computation cost.</p>
        </div>
        <div>
          
          <span id="sect4490"/><span class="c_heading_indent">3.4.6 Quantum enhanced tensor networks. </span>
          <span>There are numerous connections between tensor networks and quantum circuits. These relations lead to interest in two broad research directions. First one is the classical simulation of quantum circuits using a tensor network. There are studies demonstrating the implementation of quantum algorithms like Grover's algorithm and Shor's algorithm in matrix product state (MPS)<a title="Select to navigate to reference" href="#cit293"><sup><span class="sup_ref">293,294</span></sup></a> and tree tensor network (TTN) framework.<a title="Select to navigate to references" href="#cit295"><sup><span class="sup_ref">295</span></sup></a> Recently a report showed a classical simulation of the random quantum circuit using a tensor network;<a title="Select to navigate to references" href="#cit296"><sup><span class="sup_ref">296</span></sup></a> the same circuit which was implemented on the Sycamore quantum processor to demonstrate “Quantum Supremacy” by Google.<a title="Select to navigate to references" href="#cit297"><sup><span class="sup_ref">297</span></sup></a> There has been a massive improvement over the years in the runtimes for evaluating the tensor network classically. In a recent report by Huang <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit298"><sup><span class="sup_ref">298</span></sup></a> they demonstrated a new method called index slicing which can accelerate the simulation of random quantum circuits through a tensor network contraction process by up to five orders of magnitude using parallelization. Markov <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit299"><sup><span class="sup_ref">299</span></sup></a> theorized the time complexity of simulating quantum circuits using a tensor network. A quantum circuit with a treewidth <span class="italic">d</span> (a measure of how far a graph is from being a tree) and <span class="italic">T</span> gates can be deterministically simulated in <span class="italic">O</span>(poly(<span class="italic">T</span>)exp(<span class="italic">d</span>)) time. Another research direction which has been gaining traction due to advents of noisy intermediate scale quantum (NISQ) computers is the optimization of tensor networks using quantum computers.<a title="Select to navigate to reference" href="#cit300"><sup><span class="sup_ref">300,301</span></sup></a> There are quantum machine learning models which use ansatz inspired from a tensor network.<a title="Select to navigate to reference" href="#cit215"><sup><span class="sup_ref">215,302</span></sup></a> The analogy between TN and quantum circuits can be exploited to develop an efficient state preparation mechanism on a quantum computer. Efforts have been made to creating quantum states in MPS,<a title="Select to navigate to references" href="#cit303"><sup><span class="sup_ref">303</span></sup></a> TTN,<a title="Select to navigate to references" href="#cit302"><sup><span class="sup_ref">302</span></sup></a> PEPS,<a title="Select to navigate to references" href="#cit285"><sup><span class="sup_ref">285</span></sup></a> and MERA<a title="Select to navigate to references" href="#cit304"><sup><span class="sup_ref">304</span></sup></a> formats using quantum circuits.</span>
          <p class="otherpara">Since the dimensions of the associated tensor grow exponentially with the depth of the quantum circuit associated with it, it is possible to prepare certain tensor networks with a large bond dimension on a quantum computer that cannot be efficiently simulated on a classical computer. These states are of utmost importance because there is a definite quantum advantage associated with them. The authors in the report<a title="Select to navigate to references" href="#cit304"><sup><span class="sup_ref">304</span></sup></a> demonstrated the preparation of such a state called deep-MERA which can be represented by a local quantum circuit of depth <span class="italic">D</span> consisting of two-qubit gates. The expectation values of local observables of a DMERA can be computed in time <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t215_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t215.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t215.gif"/></a> on a quantum computer while a classical computer would take <span class="italic">O</span>(<span class="italic">e</span><small><sup><span class="italic">O</span>(<span class="italic">D</span>)</sup></small><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/><span class="italic">L</span>log(1/<span class="italic">η</span>)) time, where <span class="italic">η</span> is the desired precision and <span class="italic">L</span> is the number of qubits.</p>
          <p class="otherpara">Schwarz <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit285"><sup><span class="sup_ref">285</span></sup></a> demonstrate a procedure to efficiently prepare PEPSs on a quantum computer that scales polynomially with the inverse of the spectral gap of Hamiltonian. Efforts have also been made to use the advantages of tensor networks and quantum circuit simultaneously by fusing them. In the report,<a title="Select to navigate to references" href="#cit305"><sup><span class="sup_ref">305</span></sup></a> authors introduced a hybrid tree tensor network architecture to perform quantum simulation of spin lattice Hamiltonian with short-range interactions. They simulated two-dimensional spin systems as large as 9 × 8 qubits which require operations acting on at most 9 qubits. Their method can be generalized to arbitrary trees to represent the <span class="italic">N</span> = <span class="italic">O</span>(<span class="italic">g</span><small><sup><span class="italic">D</span>−1</sup></small>) qubit system, where <span class="italic">D</span> and <span class="italic">g</span> are the maximal depth and degree of the tree. It would require <span class="italic">O</span>(<span class="italic">Nk</span><small><sup>2</sup></small>) circuits for computation and the cost for measuring local expectation values would be <span class="italic">O</span>(<span class="italic">Ngk</span><small><sup>4</sup></small>), where <span class="italic">k</span> is the bond dimension of the TTN. They provide an efficient representation of a quantum state whose elements can be evaluated on a near-term quantum device. When compared against standard DMRG on MPS and imaginary-TEBD on PEPSs, they produce results more accurate by up to two orders.</p>
        </div>
      
    
    
      
      <h2 id="sect4544"><span class="a_heading">4 Case for quantum computing enhanced machine learning</span></h2>
      
        
        <h3 id="sect4547"><span class="b_heading">4.1 Universal function approximation through supervised learning on a quantum computer</span></h3>
        <span>In this section, we shall specifically highlight some of the recent claims that propose a theoretical guarantee for supervised machine learning tasks on a quantum computer, with these claims being the successful mimicking of arbitrary unknown functional dependence with high accuracy. It is thus needless to say that the benefits of these claims if realized can enhance the learning capabilities of supervised models for both quantum and classical data even beyond the precincts of physical sciences.</span>
        <p class="otherpara">Several significant proposals have been reported recently that attempt to approximate a function (say <span class="italic">f</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>)) using a quantum circuit where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t216_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t216.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t216.gif"/></a> are classical data entries. Intuitively this can be framed as a supervised learning task where one has access to a dataset <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t217_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t217.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t217.gif"/></a> which is assumed to follow the functional inter-relationship <span class="italic">f</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>). The crux of the problem is therefore to learn a hypothesis <span class="italic">h</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) which closely mimics the actual function <span class="italic">f</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) within a certain error tolerance. To perform such tasks on a quantum computer and learn the hypothesis <span class="italic">h</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) one needs to encode classical data onto a quantum state first. Mitarai <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit306"><sup><span class="sup_ref">306</span></sup></a> proposed a data-uploading scheme on a quantum circuit for such a scenario. The scheme maps <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t218_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t218.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t218.gif"/></a> with −1 ≤ <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> ≤ 1 ∀ <span class="italic">i</span> wherein one requires access to the <span class="italic">n</span><small><sub><span class="italic">k</span></sub></small>-th power for each datum <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> with <span class="italic">k</span> ∈ {1, 2, 3,…, <span class="italic">d</span>} into an <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t219_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t219.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t219.gif"/></a> qubit state as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t220_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t220.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t220.gif"/></a>. The tensor product structure of the many-qubit state creates non-linear cross terms of the kind <span class="italic">x</span><small><sub><span class="italic">m</span></sub></small><span class="italic">x</span><small><sub><span class="italic">n</span></sub></small>. Following this data-encoding, the state can be acted upon by any parameterized unitary (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t221_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t221.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t221.gif"/></a>). This will be followed by a measurement protocol using a pre-defined operator (say <span class="italic">M</span>) to learn the hypothesis function <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t222_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t222.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t222.gif"/></a>. The hypothesis function is optimized with respect to the parameters <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t223_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t223.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t223.gif"/></a> using an appropriate loss function <span class="italic">L</span>(<span class="italic">h</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>,<span class="italic">θ</span>),<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0079_20d1.gif" alt="[y with combining right harpoon above (vector)]"/></span>,<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) until the desired tolerance is reached, <span class="italic">i.e.</span>, at <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t224_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t224.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t224.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t225_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t225.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t225.gif"/></a>. The authors claim that the encoding above is capable of approximating a larger class of functions than what classical supervised learning tasks can achieve. To substantiate this claim, the authors argue that if classical processors could mimic every kind of function, which can be realized from such quantum data-encoding, then that would mean the classical device in principle learns the input–output relationship of complex computational models like quantum cellular automata<a title="Select to navigate to references" href="#cit307"><sup><span class="sup_ref">307</span></sup></a> which is known to not being achievable using polynomial resources (poly(<span class="italic">N</span>)) on a classical device. Numerical experiments for fitting the time evolution of the transverse Ising model and a binary classification task of a non-linearly separable data were performed with the above encoding with great success.</p>
        <p class="otherpara">Perez-Salinas<a title="Select to navigate to references" href="#cit308"><sup><span class="sup_ref">308</span></sup></a> demonstrated how to construct single-qubit classfiers using efficient data-reuploading which is essentially sequential loading of classical data entries. Many powerful insights into the function learning ability of a quantum circuit through data-encoders have been recently elaborated in ref. <a title="Select to navigate to references" href="#cit309">309</a>. The work explicates if the data-encoding unitary is expressed as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t226_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t226.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t226.gif"/></a> and <span class="italic">r</span> repetitions of such unitaries in the circuit are made along with parameterized unitaries (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t227_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t227.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t227.gif"/></a> as above) for training the model then the frequency components of the hypothesis function <span class="italic">h</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) when resolved in the Fourier basis are entirely controlled by the encoding Hamiltonian family {<span class="italic">H</span><small><sub><span class="italic">m</span></sub></small>}<small><sup><span class="italic">d</span></sup></small><small><sub><span class="italic">m</span>=1</sub></small>. However, the Fourier coefficients are influenced by the remaining part of the circuit <span class="italic">i.e.</span> the trainable unitaries <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t228_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t228.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t228.gif"/></a> as well as the measurement operator <span class="italic">M</span>. The authors further show that repeating the encoding in parallel or in a sequence would lead to a similar frequency spectrum. Under the assumption that the trainable part of the circuit is general enough to realize any arbitrary unitary, then it is possible to choose encoding Hamiltonians {<span class="italic">H</span><small><sub><span class="italic">m</span></sub></small>}<small><sup><span class="italic">d</span></sup></small><small><sub><span class="italic">m</span>=1</sub></small> that can generate any arbitrary frequency range asymptotically. Using this fact the authors prove that it is possible for the hypothesis function <span class="italic">h</span>(<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span>) learnt by such a quantum circuit to mimic any square integrable function within an arbitrarily preset tolerance. This thereby lends to universal –expressibility to such hypothesis functions. The importance of this result is many-fold as it allows one to not only realize that expressive power of the family of functions learnt from supervised learning task on a quantum circuit is extremely high but also allows one to design unitaries, set number of necessary repetitions, <span class="italic">etc.</span> to augment the learning process. Universality in discriminative learning wherein a hybrid quantum model to learn the parameters of an unknown unitary was used has also been illustrated recently.<a title="Select to navigate to references" href="#cit310"><sup><span class="sup_ref">310</span></sup></a> Other than these, expressive capacity of parameterized quantum circuits has been thoroughly investigated recently.<a title="Select to navigate to references" href="#cit311"><sup><span class="sup_ref">311</span></sup></a> Since most of the NISQ era quantum ML models are indeed variational, much of the insight from these studies is directly transferable.</p>
      
      
        
        <h3 id="sect4634"><span class="b_heading">4.2 Power of kernel estimation and data-classification from quantum computers</span></h3>
        <span>In this section we shall highlight some of the key results that have been demonstrated in recent years regarding the superiority of constructing kernel matrix elements from the quantum computer as opposed to a classical processor. Such kernel estimates are necessary for a variety of supervised learning algorithms like kernel-ridge regression (see Section 3.2.2) or for classification tasks like in support-vector machine or SVM (see Section 3.2.7) to name a few. Kernel-ridge regression on a classical processor has been extensively used in chemistry for estimating density functionals,<a title="Select to navigate to references" href="#cit312"><sup><span class="sup_ref">312</span></sup></a> simulating non-adiabatic dynamics across potential energy surfaces,<a title="Select to navigate to references" href="#cit45"><sup><span class="sup_ref">45</span></sup></a> dissipative quantum dynamics<a title="Select to navigate to references" href="#cit128"><sup><span class="sup_ref">128</span></sup></a> and even procuring molecular and atomic properties like atomization energies.<a title="Select to navigate to reference" href="#cit313"><sup><span class="sup_ref">313,314</span></sup></a> We shall return to a subset of these applications and explore them in detail in Section 5.3. Even for classification, kernelized variants of SVM on a classical processor have been useful for demarcating phases of matter, or for delineating malignant tumors from non-malignant ones<a title="Select to navigate to references" href="#cit315"><sup><span class="sup_ref">315</span></sup></a> which would be of use to biochemists and oncologists. We shall return to a subset of these applications in Section 5.2. Thus the learning capabilities of all the aforementioned algorithms can be augmented if quantum computing-enhanced kernel estimates are used. A kernelized SVM has also been used extensively for the drug-designing process, in drug-induced toxicity classification,<a title="Select to navigate to references" href="#cit316"><sup><span class="sup_ref">316</span></sup></a><span class="italic">etc.</span> We shall discuss some of these in Section 5.5. In fact a study has already demonstrated quantum advantage recently<a title="Select to navigate to references" href="#cit317"><sup><span class="sup_ref">317</span></sup></a> wherein a kernel SVM on an actual quantum device (<span class="italic">ibmq</span>_<span class="italic">rochester</span>) was used with classical processing to delineate active <span class="italic">vs.</span> inactive drug candidates for several diseases. The authors note a faster training time on a quantum processor than on the classical processor for larger dataset sizes. We shall discuss this specific example in detail in Section 5.5.4.</span>
        <p class="otherpara">It must be emphasized that for classification tasks, apart from the quantum Kernel methods, the quantum instance-based learning algorithms could also outperform classical learners. Estimating the distance between the test data and the training ones is always crucial in the instance-based learning algorithms. For instance, in the nearest neighbor algorithm, one of the most typical instance-based learning frameworks, the label of the test data is determined by the nearest training data. In 2013, Lloyd and coworkers proposed a quantum clustering algorithm for unsupervised QML,<a title="Select to navigate to references" href="#cit156"><sup><span class="sup_ref">156</span></sup></a> showing that estimating distances and inner products between post-processed vectors in <span class="italic">N</span>-dimensional vector spaces then takes time <span class="italic">O</span>(log<span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_2009.gif" alt="[thin space (1/6-em)]"/>N</span>) on a quantum computer. In contrast, sampling and estimating distances and inner products between post-processed vectors on a classical computer are exponentially hard.<a title="Select to navigate to references" href="#cit157"><sup><span class="sup_ref">157</span></sup></a> The significant speedup yields considerable power of the quantum instance-based learning algorithms as well. In fact a specific example of this class of algorithms which inherits the aforesaid advantage has also been recently designed by one of the authors<a title="Select to navigate to references" href="#cit318"><sup><span class="sup_ref">318</span></sup></a> and applied for phase classification of material VO<small><sub>2</sub></small> which will be of importance to materials scientists. More details on such examples can be found in Section 5.2 and will not be elaborated herein. Here we shall specifically discuss the advantages of estimating the kernel on a quantum processor that has been noted recently for certain tasks which thereby promises exciting opportunities for kernelized quantum supervised learning with applications in physics and chemistry.</p>
        <p class="otherpara">1. Quantum-enhanced feature maps and kernels are defined in Section 3.2.1. As mentioned therein, ref. <a title="Select to navigate to references" href="#cit117">117</a> provides two strategies for efficiently performing kernel-based machine learning algorithms using a quantum computer. The first is an implicit approach wherein the kernel matrix is estimated through an inner product once a quantum circuit for state preparation with encoding classical data is in place. With access to the entries of the kernel-matrix from the quantum computer, the actual ML algorithm is then performed classically. The other approach is the explicit approach, where the full ML task is performed on the quantum computer itself. Ref. <a title="Select to navigate to references" href="#cit117">117</a> adopts the first approach by encoding each entry <span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> of a given feature vector <span class="italic">x</span> ⊂ <span class="bold">R</span><small><sup><span class="italic">d</span></sup></small> in the phase information of a multi-mode squeezed state and shows that the corresponding kernel obtained through inner product of such states is expressive enough for classification tasks. To exemplify the second approach, it also used the two-mode squeezed state as a data-encoder and then applied a variational circuit (say <span class="italic">W</span>(<span class="italic">θ</span>)) followed by photon-number measurement and assigned the probability of a binary classification task to obtain two specific Fock states in the two-modes. Using the distribution obtained from the QC, the authors could linearly separate a dataset with 100% accuracy. However, the authors note that the primary data-encoding strategy adopted in the paper is through the preparation of squeezed states in continuous variable quantum computing which can be efficiently simulated classically.<a title="Select to navigate to reference" href="#cit319"><sup><span class="sup_ref">319,320</span></sup></a> They further mention that inclusion of non-Gaussian elements like cubic-phase gates,<a title="Select to navigate to references" href="#cit97"><sup><span class="sup_ref">97</span></sup></a> non-linearity in photon-number measurements,<a title="Select to navigate to references" href="#cit321"><sup><span class="sup_ref">321</span></sup></a> classically intractable continuous-variable instantaneous quantum computing or CV-IQP circuits<a title="Select to navigate to references" href="#cit322"><sup><span class="sup_ref">322</span></sup></a> may lead to a non-trivial kernel estimation task wherein the power of quantum computers can be better used. Similar results as these are also reported in ref. <a title="Select to navigate to references" href="#cit318">318</a> wherein classical data were not-only encoded within the phase information of a multi-mode squeezed state but also in the amplitude. Proper comparisons of such squeezed state encoded kernels with Gaussian kernels were also investigated using standard datasets from scikit learn.<a title="Select to navigate to references" href="#cit323"><sup><span class="sup_ref">323</span></sup></a></p>
        <p class="otherpara">2. The first work to exemplify an advantage is ref. <a title="Select to navigate to references" href="#cit324">324</a>. The algorithm in ref. <a title="Select to navigate to references" href="#cit324">324</a> performs a standard support-vector machine classification task (discussed in Section 3.2.7) with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t229_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t229.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t229.gif"/></a> (training feature vectors for input) and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t230_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t230.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t230.gif"/></a> (testing feature vectors). The labels <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t231_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t231.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t231.gif"/></a> where the set <span class="italic">y</span> = {<span class="italic">y</span><small><sub><span class="italic">t</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">tt</span></sub></small>}. The algorithm only had access to the training labels (<span class="italic">y</span><small><sub><span class="italic">t</span></sub></small>) and its job was to evaluate an approximation to the testing labels, <span class="italic">i.e.</span>, obtain <span class="italic">ỹ</span><small><sub><span class="italic">tt</span></sub></small> ∀ <span class="italic">x</span><small><sub><span class="italic">tt</span></sub></small> ∈ <span class="italic">S</span> which matches with <span class="italic">y</span><small><sub><span class="italic">tt</span></sub></small> with high probability. Unlike in the previous reference, the data-encoding feature map used did not produce product states. The specific data-encoding unitary used is the following:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn99"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t232_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t232.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t232.gif"/></a></td><td class="rightEqn">(99)</td></tr></table>where <span class="italic">n</span> is the number of qubits, <span class="italic">S</span> denotes the nature of the unitary, <span class="italic">i.e.</span>, if the unitary is <span class="italic">S</span>-local. For simulations the work used <span class="italic">S</span> = 2. Ref. <a title="Select to navigate to references" href="#cit324">324</a> argued that the above mentioned data-encoding is hard to simulate classically. The feature vector size <span class="italic">d</span> = 2, <span class="italic">i.e.</span>, <span class="italic">x</span><small><sub><span class="italic">t</span></sub></small> = [<span class="italic">x</span><small><sub>1</sub></small>,<span class="italic">x</span><small><sub>2</sub></small>]<small><sup><span class="italic">T</span></sup></small> and the feature maps are defining the unitaries in <a title="" href="#eqn99">eqn (99)</a> are<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn100"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t233_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t233.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t233.gif"/></a></td><td class="rightEqn">(100)</td></tr></table>The first classification protocol which the authors in ref. <a title="Select to navigate to references" href="#cit324">324</a> implemented is the explicit approach wherein after the above-mentioned data-encoding a variational circuit (say <span class="italic">W</span>(<span class="italic">θ</span>)) was also implemented followed by a measurement protocol. If the probability for certain specific bit-strings were higher than a tunable threshold, the algorithm was said to yield <span class="italic">ỹ</span><small><sub><span class="italic">t</span></sub></small> = 1 (or −1 otherwise). Numerical experiments were conducted on a 5-qubit superconducting circuit and the depth of the variational circuit was varied from 0–4. The training set had 20 data points for each label and so did the testing set. The success ratio as seen in <a title="Select to navigate to figure" href="#imgfig3">Fig. 3</a> of ref. <a title="Select to navigate to references" href="#cit324">324</a> was close to 100% for 4 layers of the variational circuit. In the second part of the numerical experiment, the authors followed the implicit scheme in which only the estimates of the kernel matrix were obtained from the quantum computer. The rest of the classification task was performed classically once that was done. The constructed kernel matrix from the actual hardware agreed fairly well with the ideal one (see <a title="Select to navigate to figure" href="#imgfig4">Fig. 4</a> in ref. <a title="Select to navigate to references" href="#cit324">324</a>) and classification task using it was of 100% accuracy. After this demonstration of a data-encoding scheme which is hard to simulate classically, several other numerical experiments have been initiated to validate kernelized SVM on a quantum computer in different platforms.<a title="Select to navigate to reference" href="#cit325"><sup><span class="sup_ref">325,326</span></sup></a></p>
        <p class="otherpara">3. A recent report<a title="Select to navigate to references" href="#cit327"><sup><span class="sup_ref">327</span></sup></a> using the same feature-space encoding scheme as in ref. <a title="Select to navigate to references" href="#cit324">324</a> above establishes that quantum enhanced kernels perform better for complex data classification tasks like geometric data patterns distributed according to Mersenne Twister distribution.<a title="Select to navigate to references" href="#cit328"><sup><span class="sup_ref">328</span></sup></a> Classical methods cannot achieve similar accuracy. However, if the data distribution is simple such that large differences exist between data that belong to the separating classes then classical kernels would perform as well. Also, the study claims that simpler data encoding circuits for computing entries of quantum kernels may be less effective for certain data-classification tasks. Another study<a title="Select to navigate to references" href="#cit329"><sup><span class="sup_ref">329</span></sup></a> has actually systematically studied the effect of noise and finite measurement samples and concluded that a high noise content may expectedly be detrimental to the estimation of kernel entries on the quantum computer. However, the report<a title="Select to navigate to references" href="#cit329"><sup><span class="sup_ref">329</span></sup></a> also proposes to mitigate the effect of the noise by classical pre-processing of the estimated noisy kernel like discarding the negative eigenvalues.</p>
        <p class="otherpara">4. A clear and most decisive exhibition of the power of kernelized variant of a support-vector machine on a quantum computer for a classification task was highlighted in ref. <a title="Select to navigate to references" href="#cit180">180</a>. The motivation for the work was to demonstrate a specific example wherein estimation of the kernel Gram matrix on a classical processor would not only be not efficient, but the classification task itself would be provably disadvantageous. Also, the quantum advantage would be retained even in the presence of finite sampling errors. The classification task chosen was based on the famous discrete-logarithm problem. The problem entails finding <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t234_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t234.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t234.gif"/></a>, where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t235_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t235.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t235.gif"/></a> with <span class="italic">p</span> being a large prime number and <span class="italic">g</span> being the generator of the multiplicative cyclic group <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t236_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t236.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t236.gif"/></a>. By generator one means an element <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t237_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t237.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t237.gif"/></a> such that for every element <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t238_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t238.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t238.gif"/></a>, one can write <span class="italic">x</span> = <span class="italic">g</span><small><sup><span class="italic">m</span></sup></small>. In such a case, <span class="italic">m</span> is said to be the discrete-logarithm to base <span class="italic">g</span> of <span class="italic">x</span> in <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t239_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t239.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t239.gif"/></a> and is denoted by <span class="italic">m</span> = log<small><sub><span class="italic">g</span></sub></small>(<span class="italic">x</span>). It is believed that no classical algorithm can compute the discrete-logarithm in time which is polynomial in <span class="italic">n</span> = log<small><sub>2</sub></small>(<span class="italic">p</span>) even though quantum algorithms like Shor's algorithm is known to do it.<a title="Select to navigate to references" href="#cit53"><sup><span class="sup_ref">53</span></sup></a> The classifier function makes the following decision:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn101"/><span id="eqn101"><span class="italic">f</span><small><sub><span class="italic">s</span></sub></small>(<span class="italic">x</span>) = +1 if log<small><sub><span class="italic">g</span></sub></small>(<span class="italic">x</span>) ∈ [<span class="italic">s</span>,<span class="italic">s</span>+(<span class="italic">p</span> − 3)/2] = −1 (otherwise).</span></td><td class="rightEqn">(101)</td></tr></table>This classifier thus divides the set <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t240_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t240.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t240.gif"/></a> into two equal halves by mapping each of the halves to {+1, −1}. The authors prove that a classical learning algorithm for this task cannot achieve an accuracy more than 0.5 + 1/poly(<span class="italic">n</span>) indicating that the best classical algorithm can only do random guessing for unseen test data. For the kernelized quantum SVM, however, the authors propose the following quantum feature map/state:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn102"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t241_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t241.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t241.gif"/></a></td><td class="rightEqn">(102)</td></tr></table>where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t242_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t242.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t242.gif"/></a>, <span class="italic">k</span> = <span class="italic">n</span> − <span class="italic">t</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log(<span class="italic">n</span>) for <span class="italic">t</span> being some constant.<a title="Select to navigate to references" href="#cit180"><sup><span class="sup_ref">180</span></sup></a> The state-preparation circuit which prepares the above state is shown to be efficient using Shor's algorithm.<a title="Select to navigate to references" href="#cit53"><sup><span class="sup_ref">53</span></sup></a> Using the above state-preparation strategy, the authors can estimate the kernel matrix for each entry in the training set as <span class="italic">K</span>(<span class="italic">x</span>,<span class="italic">x</span>′) = |〈<span class="italic">ϕ</span>(<span class="italic">x</span>)|<span class="italic">ϕ</span>(<span class="italic">x</span>′)〉|<small><sup>2</sup></small>. Using this kernel, the authors rely on the usual SVM algorithm on a classical processor to construct a separating hyperplane and optimize the parameters for it. Once the parameters for the hyperplane are determined, classification of new test data also requires kernel matrix elements when new kernel estimates from the QC are invoked again. The authors call this procedure support vector machine with quantum kernel estimation (SVM-QKE) indicating that the quantum computer is only involved in constructing the entries of the kernel. The authors prove that SVM-QKE yields a classifier that can segregate the data in testing and training set with an accuracy of 0.99 in polynomial time and with a probability of at least <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t243_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t243.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t243.gif"/></a> over even random training samples. They further show that even when the QKE entries have a small additive perturbation due to finite sampling; the separating hyperplane so obtained is close to the exact one with high probability and so is the accuracy of the corresponding classifier. Since the kernel estimates from a classical processor cannot provably do better than random guessing, this classification task clearly explicates the superiority of quantum feature maps. Since then, several interesting demonstrations of kernel estimates on the quantum computer have emerged like using a linear combination of multiple quantum enhanced kernels with a variational circuit to enhance accuracy and expressivity over and beyond a single kernel for complex datasets,<a title="Select to navigate to references" href="#cit330"><sup><span class="sup_ref">330</span></sup></a> a fidelity based quantum kernel estimate between the members of the training dataset and the testing samples<a title="Select to navigate to references" href="#cit331"><sup><span class="sup_ref">331</span></sup></a> or even distinguishing the classical data entries directly after mapping to quantum states in the quantum feature space using metrics with a shallow circuit depth in a process which the authors call quantum metric learning.<a title="Select to navigate to references" href="#cit332"><sup><span class="sup_ref">332</span></sup></a> Like in the above cases, numerical experiments have also been reported on real devices like a 17-qubit classification task<a title="Select to navigate to references" href="#cit333"><sup><span class="sup_ref">333</span></sup></a> performed on Google's Sycamore to segregate data in a 67-dimensional space with appropriate noise-mitigation strategies.</p>
      
      
        
        <h3 id="sect4807"><span class="b_heading">4.3 Power of quantum-neural networks</span></h3>
        <span>In this section, we shall highlight some of the recent reports wherein the superiority of quantum computing enhanced neural network models has been demonstrated or theoretically proven in terms of its generalizability and expressive power, training capacity, resource and parameter requirements to mention a few. Neural networks in a classical processor have become the standard go-to method for many applications in chemistry and physics like in efficient state-preparation protocols using generative adversarial networks (see Section 3.3.6).<a title="Select to navigate to reference" href="#cit334"><sup><span class="sup_ref">334,335</span></sup></a> Networks like CNN (see Section 3.3.2) have been used for the classification of phases of matter,<a title="Select to navigate to reference" href="#cit336"><sup><span class="sup_ref">336,337</span></sup></a> in quantum state-tomography,<a title="Select to navigate to references" href="#cit338"><sup><span class="sup_ref">338</span></sup></a> and in structure and ligand based drug-designing protocols.<a title="Select to navigate to references" href="#cit339"><sup><span class="sup_ref">339</span></sup></a> Deep neural networks (see Section 3.3) have been also used for predicting molecular properties even with non-bonding interactions,<a title="Select to navigate to references" href="#cit340"><sup><span class="sup_ref">340</span></sup></a> in drug-induced toxicity detection,<a title="Select to navigate to references" href="#cit341"><sup><span class="sup_ref">341</span></sup></a> many-body structure of correlated quantum matter like molecules and materials,<a title="Select to navigate to reference" href="#cit342"><sup><span class="sup_ref">342,343</span></sup></a> and even in molecular dynamics.<a title="Select to navigate to reference" href="#cit344"><sup><span class="sup_ref">344,345</span></sup></a> Generative models like restricted Boltzmann machine based neural-network representation of many-body quantum states<a title="Select to navigate to references" href="#cit346"><sup><span class="sup_ref">346</span></sup></a> have been used for classification and understanding ground and excited state properties of quantum systems. We shall return to a subset of these applications in Sections 5.1–5.5. It is thus apparent that all the aforesaid algorithms stand to benefit from any quantum advantage seen in the development of neural network based models on a quantum processor. In fact, in certain cases, direct advantages have already been reported. For example, the authors have reported a quantum circuit-based implementation of a restricted Boltzmann machine based ansatz for any of the electronic states of molecules and materials<a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> which requires polynomial resources for its construction. Similarly for quantum version of CNN which has been used for the classification of phases in the Ising model,<a title="Select to navigate to references" href="#cit215"><sup><span class="sup_ref">215</span></sup></a> the authors claim a more parameter reduction. We shall return to these applications and their description in Sections 5.2 and 5.3. Herein we enlist some of the recent examples wherein quantum superiority has been seen or theoretically conjectured thereby promising many novel applications in chemistry and physics which can be realized in the future. More theoretical insight into the learning mechanisms and generalizability of quantum computing enhanced neural networks are discussed in detail in Section 6.4.</span>
        <p class="otherpara">1. Quantum-neural networks (QNN) have been discussed in Section 3.3. Each such network has three generic components – a data encoding circuit (often called feature map) which accepts classical data as the input and usually encodes them into the amplitudes of a quantum state (other encoding schemes are also possible; see ref. <a title="Select to navigate to references" href="#cit116">116</a>) followed by a layer of parameterized unitaries. Finally, measurement protocol is exercised whose outcome is post-processed on a classical computer to minimize a loss function and alter the parameters of the last layer of unitaries variationally until the desired convergence is reached. A recent report has suggested that such networks can be more expressive and faster trainable than the corresponding classical networks if the data encoding circuit possess non-trivial entangling gates which can identify hidden correlation among data entries.<a title="Select to navigate to references" href="#cit348"><sup><span class="sup_ref">348</span></sup></a> The work used an input data-set <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t244_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t244.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t244.gif"/></a> and a parameter vector <span class="italic">θ</span> ⊆ [−1,1]<small><sup><span class="italic">d</span></sup></small>. The input distribution <span class="italic">p</span>(<span class="italic">x</span>) is the prior distribution and <span class="italic">p</span>(<span class="italic">y</span>|<span class="italic">x</span>;<span class="italic">θ</span>) is the output distribution from the QNN given the input and specific parameter set. Using this they constructed the empirical Fisher information matrix (∈<span class="bold">R</span><small><sup><span class="italic">d</span>×<span class="italic">d</span></sup></small>)<a title="Select to navigate to references" href="#cit349"><sup><span class="sup_ref">349</span></sup></a> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn103"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t245_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t245.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t245.gif"/></a></td><td class="rightEqn">(103)</td></tr></table>where <span class="italic">k</span> denotes the sample size. The authors found that the eigenvalues of the Fisher information matrix for 100 samples with (<span class="italic">d</span> = 40, <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> = 4, <span class="italic">s</span><small><sub><span class="italic">o</span></sub></small> = 2) in the case of the QNN were fairly uniformly distributed contrary to that in the classical neural network wherein the eigenvalues were largely concentrated near zero indicating the relative flatness of the optimization surface and difficulty in trainability of the model with gradient-based schemes.<a title="Select to navigate to references" href="#cit350"><sup><span class="sup_ref">350</span></sup></a> They used an ‘easy-quantum’ model as well with data-encoding scheme without any entangling gates and found the Fisher information spectrum to be within the two limiting cases of a classical NN and a full quantum NN. The results are retained for (<span class="italic">d</span> = 60, <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> = 6, <span class="italic">s</span><small><sub><span class="italic">o</span></sub></small> = 2), (<span class="italic">d</span> = 80, <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> = 8, <span class="italic">s</span><small><sub><span class="italic">o</span></sub></small> = 2), and (<span class="italic">d</span> = 100, <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> = 10, <span class="italic">s</span><small><sub><span class="italic">o</span></sub></small> = 2). The authors in ref. <a title="Select to navigate to references" href="#cit348">348</a> thereafter promised a metric for effective dimension defined below as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn104"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t246_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t246.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t246.gif"/></a></td><td class="rightEqn">(104)</td></tr></table>where <span class="italic">n</span> is the number of data samples, <span class="italic">F</span>(<span class="italic">θ</span>) is the normalized Fisher information matrix and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t247_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t247.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t247.gif"/></a> is the volume in parameter space and <span class="italic">γ</span> ∈ (0,1]. The physical motivation of defining an effective dimension is to quantify the expressibility of the model, <span class="italic">i.e.</span>, estimate the size of the space all possible functions which the model class can successfully mimic with the Fisher information as the metric.<a title="Select to navigate to references" href="#cit351"><sup><span class="sup_ref">351</span></sup></a> Using the above definition of the effective dimension, the authors show that the full QNN has the highest effective dimension compared to the easy quantum model (without entangling gates in the circuit encoding the features) and even the classical neural network for (<span class="italic">d</span> = 40, <span class="italic">s</span><small><sub><span class="italic">i</span></sub></small> = 4, <span class="italic">s</span><small><sub><span class="italic">o</span></sub></small> = 2) and size of data <span class="italic">n</span> = 10<small><sup>5</sup></small>–10<small><sup>6</sup></small> (see <a title="Select to navigate to figure" href="#imgfig3">Fig. 3(a)</a> in ref. <a title="Select to navigate to references" href="#cit348">348</a>). They also demonstrated that the full QNN trains faster and achieves lesser loss function values within smaller number of iterations compared to the other two (see <a title="Select to navigate to figure" href="#imgfig3">Fig. 3(b)</a> in ref. <a title="Select to navigate to references" href="#cit348">348</a>). The conclusion remains invariant to training even on the real hardware.</p>
        <p class="otherpara">2. Recently, a new report has been published<a title="Select to navigate to references" href="#cit352"><sup><span class="sup_ref">352</span></sup></a> which extends the famous no-free lunch theorem<a title="Select to navigate to reference" href="#cit353"><sup><span class="sup_ref">353,354</span></sup></a> to the learning process in a quantum neural network (QNN) wherein the training data-set may be intrinsically entangled with a third accessible register. The no-free lunch theorem (NFL) for classical learning task is deduced for an unknown map, say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t248_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t248.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t248.gif"/></a> where the size of the set <span class="italic">χ</span> is <span class="italic">d</span><small><sub><span class="italic">x</span></sub></small> and that of set <span class="italic">Y</span> is <span class="italic">d</span><small><sub><span class="italic">y</span></sub></small>. One generates a training set <span class="italic">S</span> consisting of <span class="italic">t</span> points from this function defined as <span class="italic">S</span> = {(<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>)|<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small> ∈ <span class="italic">χ</span>, <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> = <span class="italic">f</span> (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>) ∈ <span class="italic">Y</span>}<small><sup><span class="italic">t</span></sup></small><small><sub><span class="italic">i</span>=1</sub></small>. In general in the supervised learning setup, this set <span class="italic">S</span> is used to construct a merit-function <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t249_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t249.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t249.gif"/></a> where <span class="italic">h</span><small><sub><span class="italic">S</span></sub></small> (<span class="italic">x</span><small><sub><span class="italic">i</span></sub></small>) is the hypothesis function that is expected to mimic the unknown function <span class="italic">f</span>. The merit-function is minimized to obtain the parameters defining the hypothesis function <span class="italic">h</span><small><sub><span class="italic">S</span></sub></small>(<span class="italic">x</span>) which can then be used to make predictions for unseen <span class="italic">x</span> ∈ (<span class="italic">χ</span> ∩ <span class="italic">S</span><small><sup><span class="italic">c</span></sup></small>). To quantify how well the approximate function <span class="italic">h</span><small><sub><span class="italic">S</span></sub></small>(<span class="italic">x</span>) resembles the actual one <span class="italic">f</span> one can define a risk function as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn105"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t250_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t250.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t250.gif"/></a></td><td class="rightEqn">(105)</td></tr></table>where <span class="italic">P</span>(<span class="italic">x</span>) is the prior probability distribution of sampling the input <span class="italic">x</span> ∈ <span class="italic">χ</span> and <span class="italic">P</span>(<span class="italic">h</span><small><sub><span class="italic">S</span></sub></small>(<span class="italic">x</span>) ≠ <span class="italic">f</span>(<span class="italic">x</span>)) is the probability that the output of the hypothesis function differs from the actual output for the specific input. The statement of NFL which the authors used is the following:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn106"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t251_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t251.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t251.gif"/></a></td><td class="rightEqn">(106)</td></tr></table>where the averaging of <a title="" href="#eqn105">eqn (105)</a> has been done over many training sets <span class="italic">S</span> and many different functional maps <span class="italic">f</span>. The result in <a title="" href="#eqn106">eqn (106)</a> indicates that the average risk can be minimized if the number of training samples <span class="italic">t</span> = <span class="italic">d</span><small><sub><span class="italic">x</span></sub></small> and hence is entirely determined by the training set <span class="italic">S</span> independent of the specific details of the optimization scheme. In the quantum setting, the authors deduce a version of NFL wherein the data-set contains entries of quantum states that are entangled with an accessible auxillary quantum system. The setup of the deduction involves a unitary map <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t252_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t252.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t252.gif"/></a> both of which are <span class="italic">d</span>-dimensional. The user herein has access to another auxillary quantum system (say <span class="italic">R</span> ∈ <span class="italic">H</span><small><sub><span class="italic">R</span></sub></small>). The training set <span class="italic">S</span><small><sub><span class="italic">Q</span></sub></small> contains <span class="italic">t</span> pairs of states which are entangled with <span class="italic">R</span> as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn107"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t253_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t253.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t253.gif"/></a></td><td class="rightEqn">(107)</td></tr></table>All input states |<span class="italic">ψ</span><small><sup><span class="italic">i</span></sup></small><small><sub>in</sub></small>〉 ∈ <span class="italic">H</span><small><sub><span class="italic">X</span></sub></small> ⊗ <span class="italic">H</span><small><sub><span class="italic">R</span></sub></small> are entangled with the same Schmidt rank <span class="italic">r</span> = {1, 2,…, <span class="italic">d</span>}. The learning task is to design an unitary <span class="italic">V</span> such that |<span class="italic">ϕ</span><small><sup><span class="italic">i</span></sup></small><small><sub>hyp</sub></small>〉 = (<span class="italic">V</span> ⊗ <span class="italic">I</span><small><sub><span class="italic">R</span></sub></small>)|<span class="italic">ψ</span><small><sup><span class="italic">i</span></sup></small><small><sub>in</sub></small>〉 and |〈<span class="italic">ϕ</span><small><sup><span class="italic">i</span></sup></small><small><sub>hyp</sub></small>|<span class="italic">ψ</span><small><sup><span class="italic">i</span></sup></small><small><sub>out</sub></small>〉| ≈ 1. The risk function in this case is defined as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn108"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t254_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t254.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t254.gif"/></a></td><td class="rightEqn">(108)</td></tr></table>where <span class="italic">ρ</span><small><sub><span class="italic">y</span></sub></small> = |<span class="italic">y</span>〉〈<span class="italic">y</span>| = <span class="italic">U</span>|<span class="italic">x</span>〉〈<span class="italic">x</span>|<span class="italic">U</span><small><sup>†</sup></small> and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t255_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t255.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t255.gif"/></a> and |<span class="italic">x</span>〉 ∈ <span class="italic">H</span><small><sub><span class="italic">X</span></sub></small> (not necessarily within <span class="italic">S</span><small><sub><span class="italic">Q</span></sub></small>) and |<span class="italic">y</span>〉, |<span class="italic">y</span>′〉 ∈ <span class="italic">H</span><small><sub><span class="italic">Y</span></sub></small> and the measure d<span class="italic">μ</span> is over the Haar measure of states. The averaging of the above risk function over all training sets <span class="italic">S</span><small><sub><span class="italic">Q</span></sub></small> and unitary maps <span class="italic">U</span> as before yields<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn109"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t256_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t256.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t256.gif"/></a></td><td class="rightEqn">(109)</td></tr></table></p>
        <p class="otherpara">The bound actually holds for each <span class="italic">S</span><small><sub><span class="italic">Q</span></sub></small> and hence averaging over <span class="italic">S</span><small><sub><span class="italic">Q</span></sub></small> is unnecessary. It is derived under the assumption that over the training samples the outcomes of <span class="italic">V</span> and <span class="italic">U</span> match perfectly. Implication of the above bound is that for <span class="italic">r</span> = 1, the bound vanishes and the average risk can be minimized only if <span class="italic">t</span> = <span class="italic">d</span> = 2<small><sup><span class="italic">n</span></sup></small> where 2<small><sup><span class="italic">n</span></sup></small> is the dimension of <span class="italic">H</span><small><sub><span class="italic">X</span></sub></small>. This indicates that if the input states are product states with the quantum system characterized by <span class="italic">H</span><small><sub><span class="italic">R</span></sub></small> then exponentially many training samples might be required in <span class="italic">n</span>. This result was previously obtained in ref. <a title="Select to navigate to references" href="#cit355">355</a>. However for <span class="italic">r</span> ≠ 1 this is not the case. Specifically, in the limiting case of <span class="italic">r</span> = <span class="italic">d</span>, a single training example would suffice to saturate the lower bound on the average. For any <span class="italic">r</span> in between one can easily see that the number of training samples <span class="italic">t</span> can be set to be far fewer than <span class="italic">d</span>. The authors show numerical experiments on Rigetti's quantum processor for 2 × 2 unitaries and demonstrate that the average risk can be minimized well below the classically accessible limit by controlling r which alters entanglement with the third quantum system <span class="italic">R</span>. Similar results were obtained even in the simulator. The concrete proof of the theorem restores hope that the size of the training set can be small yet a quantum advantage can be retained as long as entanglement is used as a useful resource which is not possible classically. This result joins the group of other results wherein entanglement has served similar roles like in superdense coding<a title="Select to navigate to reference" href="#cit356"><sup><span class="sup_ref">356,357</span></sup></a> or quantum teleportation.<a title="Select to navigate to reference" href="#cit358"><sup><span class="sup_ref">358,359</span></sup></a></p>
        <p class="otherpara">3. This example is different in spirit than the previous ones as it demonstrates how the quantum convolutional neural network (QCNN)<a title="Select to navigate to references" href="#cit215"><sup><span class="sup_ref">215</span></sup></a> which is discussed in detail in Section 3.3.2 can lead to efficient parameter reduction compared to other methods of quantum phase classification. The power of a quantum classifier over and beyond that of a classical classifier has already been demonstrated for certain tasks in some of the points above. The task for which the authors tested the QCNN was to segregate quantum states belonging to a particular phase given a training data-set <span class="italic">S</span> = {|<span class="italic">ψ</span>〉<small><sub><span class="italic">i</span></sub></small>,<span class="italic">y</span><small><sub><span class="italic">i</span></sub></small>}<small><sup><span class="italic">M</span></sup></small><small><sub><span class="italic">i</span>=1</sub></small> where <span class="italic">y</span><small><sub><span class="italic">i</span></sub></small> = {0,1} ∀ <span class="italic">i</span> are the corresponding labels. The specific example was the symmetry-protected topological phase of a 1D spin-chain. The algorithm requires only <span class="italic">O</span>(log(<span class="italic">n</span>)) parameters to classify such an <span class="italic">n</span>-qubit quantum state which the authors claim is a two-fold exponential reduction in parameter space compared to other quantum classifiers. This essentially means that the QCNN is more expressive compared to other classifiers as it solves a relatively low-dimensional optimization problem without sacrificing the accuracy of the task at hand. Similarly, they also show that the sample complexity of QCNN, which is defined as the number of copies of the input state that need to be accessed by the algorithm for correctly identifying the phase, is lesser than from other techniques like direct evaluation of expectation values of certain operators, which can also act as a marker of the phase. We shall return to this example in Section 5.2. Pesah <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit360"><sup><span class="sup_ref">360</span></sup></a> have demonstrated that in the conventional QCNN architecture there is an absence of barren plateaus as the gradient estimates vanish polynomially (and not exponentially) in the size of the system for random initialization. Recently, MacCormack <span class="italic">et al.</span> have extended the concept to introduce a variant of QCNN which is called the branching quantum convolutional network (bQCNN).<a title="Select to navigate to references" href="#cit361"><sup><span class="sup_ref">361</span></sup></a> In QCNN as discussed in Section 3.3.2, the primary idea is reduction of the number of qubits while preserving important features. The convolutional layers perform multi-qubit unitaries for generating entanglement among the qubits whereas in pooling layers certain number of qubits are discarded through measurements and controlled rotations on nearby qubits are performed conditioned on the measurement results of the discarded qubits. The new reduced set of qubits is then fed into the convolutional unitaries again and the process is repeated. For bQCNN, the authors make a deviation at the pooling layer. Instead of using certain measurement outcomes only of the discarded qubits, the authors use all possible outcomes or binary bit combinations to design several different channels/branches for subsequent convolutional operations each of which is realized for a given bit string. This enhances the parameter requirement drastically as noted in the report. However, the authors also demonstrate that the expressibility of the ansatz from bQCNN is higher than that of QCNN at a similar circuit depth.</p>
      
      
        
        <h3 id="sect5129"><span class="b_heading">4.4 Power of quantum computers for tensor-network based machine learning tasks</span></h3>
        <span>In this section, we highlight an advantage that has been recently reported for a classification task performed using a tensor network ansatz on a quantum computer. Such classification can be extended to the physico-chemical domain like in ligand selectivity for structure-based drug designing (see Section 5.5.1) with the quantum benefit reaped. An example of another tensor network based classification of phases of a spin-model can be found in Section 5.2. Besides, tensor networks on a classical processor have also been aggressively used for representing many-body states for a variety of applications like for spin-liquids,<a title="Select to navigate to references" href="#cit362"><sup><span class="sup_ref">362</span></sup></a> excitonic states in materials<a title="Select to navigate to references" href="#cit363"><sup><span class="sup_ref">363</span></sup></a> and molecules.<a title="Select to navigate to references" href="#cit364"><sup><span class="sup_ref">364</span></sup></a> Quantum advantages as have been noted for the example below can be extended to such applications. We shall discuss such prospects with concrete examples in Section 5.3.2.</span>
        <p class="otherpara">Recently, a report has illustrated the use of quantum architectures of tree and matrix product state tensor networks<a title="Select to navigate to references" href="#cit302"><sup><span class="sup_ref">302</span></sup></a> to demonstrate the working of a discriminative machine-learning model on MNIST dataset.<a title="Select to navigate to references" href="#cit365"><sup><span class="sup_ref">365</span></sup></a> The primary objective of the study was to perform classification and recognition of hand-written digits using a variational optimization procedure that can be efficiently carried out on a quantum hardware in the NISQ era. Classical data-entries (say <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t257_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t257.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t257.gif"/></a>) from the data-set are mapped to an <span class="italic">N</span>-qubit quantum state using the data-encoding protocol described below in <a title="" href="#eqn110">eqn (110)</a><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn110"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t258_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t258.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t258.gif"/></a></td><td class="rightEqn">(110)</td></tr></table></p>
        <p class="otherpara">State |<span class="italic">ϕ</span>〉 is a product state which can easily be prepared by applying single qubit rotation gates on the |0〉<small><sup>⊗<span class="italic">N</span></sup></small> state. After state preparation, each set of qubits (say 2<span class="italic">V</span> qubits where <span class="italic">V</span> is the number of virtual states) is acted upon by a parameterized unitary gate. The scheme is inspired from coarse-graining techniques. After each parameterized unitary operation, <span class="italic">V</span> qubits are discarded/reset and the other <span class="italic">V</span> qubits proceed to the next step where they are merged with <span class="italic">V</span> qubits coming from another set of 2<span class="italic">V</span> qubits. This process is continued until the last 2<span class="italic">V</span> qubits remain which are acted upon by a unitary gate to produce output qubits. One or more output qubits are measured to determine the probability distribution of the output labels. While the model is agnostic to the grouping of the qubit sets, it is usually wise to group qubits that represent local regions in the input data. This ansatz is motivated from a tree tensor network (see <a title="Select to navigate to figure" href="#imgfig22">Fig. 22</a>). The optimization problem which they solve involves classification of hand-written digits using a loss-function that penalizes the difference in the probability of attaining the true-label from the measurement protocol as opposed to the most-probable incorrect label. The algorithm used is adapted from Simultaneous Perturbation Stochastic Approximation (SPSA)<a title="Select to navigate to references" href="#cit366"><sup><span class="sup_ref">366</span></sup></a> with momentum being included inside the gradient estimates.<a title="Select to navigate to references" href="#cit199"><sup><span class="sup_ref">199</span></sup></a> The accuracy of the method as reported is extremely high, <span class="italic">i.e.</span>, with the lowest percentage error being 87% and an average test accuracy of over 95%. The authors noted that for usual quantum algorithms, encoding such an <span class="italic">N</span>-qubit state and applying tunable unitaries would require <span class="italic">N</span>-physical qubits. However, the tree-structure in the ansatz with sequential measurements allows the authors to use ≈<span class="italic">V</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log(<span class="italic">N</span>) physical qubits indicating that the scheme is qubit efficient. It must also be emphasized that merely using a tree-tensor network approach (TTN) on a classical processor would require a space complexity of <span class="italic">O</span>(<span class="italic">N</span>2<small><sup>3<span class="italic">V</span></sup></small>)<a title="Select to navigate to references" href="#cit367"><sup><span class="sup_ref">367</span></sup></a> where <span class="italic">V</span> is as defined before the number of virtual states. This result indicates that the TTN ansatz on the quantum computer is more expressive than a corresponding classical implementation as similar accuracies are afforded even with a reduced bond-dimension (bond-dimension is usually denoted by <span class="italic">D</span> where <span class="italic">D</span> = 2<small><sup><span class="italic">V</span></sup></small> and as seen here scales logarithmically for the quantum TTN version whereas would have a cubic scaling for a classical TTN). The authors also performed a systematic analysis of the effect of noise on the algorithm and concluded that the degradation of the performance was only 0.004 with a strongly enhanced (×10) noise parameters thereby indicating the resilience of the algorithm.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig22"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f22_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f22.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f22.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 22 </b> <span id="fig22"><span class="graphic_title">The architecture for evaluating the discriminative tree tensor network model using a Qubit-efficient scheme with two virtual qubits (<span class="italic">V</span>) and 16 input states (<span class="italic">N</span>) as used in ref. <a title="Select to navigate to references" href="#cit302">302</a>. The architecture requires <span class="italic">O</span>(<span class="italic">V</span><img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>log(<span class="italic">N</span>)) = 8 qubits for its operation. The qubits indicated with hash marks are measured and reset to accept input states in the next step. IOP Publishing. Reproduced with permission from W. Huggins, P. Patil, B. Mitchell, K. B. Whaley and E. M. Stoudenmire, Towards quantum machine learning with tensor networks, <span class="italic">Quantum Sci. Technol.</span>, 2019, <span class="bold">4</span>(2), <a target="_blank" href="https://doi.org/10.1088/2058-9565/aaea94">https://doi.org/10.1088/2058-9565/aaea94</a>. All rights reserved.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
      
    
    
      
      <h2 id="sect5185"><span class="a_heading">5 Applications</span></h2>
      
        
        <h3 id="sect5188"><span class="b_heading">5.1 State preparation protocols and quantum state tomography</span></h3>
        <span>With the rapid advancement of quantum technology in the past decade, there is an increased demand for efficient methods that can verify the generated quantum states according to specific needs. This is primarily important for the validation and benchmarking of the quantum devices. To address this, quantum state tomography (QST) aims to obtain the statistical inference of the unknown quantum state of the system based on the information of the expectation values of a complete set of observables.<a title="Select to navigate to reference" href="#cit49"><sup><span class="sup_ref">49,368–371</span></sup></a> However, the traditional approach to QST has been pushed to its limits for the large quantum platforms available today.<a title="Select to navigate to references" href="#cit372"><sup><span class="sup_ref">372</span></sup></a> This is because the number of measurements required for full reconstruction of the quantum state scales exponentially with the system size which poses a critical challenge for performing QST even for moderately sized systems. Also, in order to obtain valuable insight on the physical observables of the system and to estimate them accurately, the measurement outcomes need to be stored and processed for which exponential amounts of classical memory and computing power are required which makes the technique infeasible for practical applications.</span>
        <p class="otherpara">Apart from the problem of exponential scaling for complex systems, another drawback for the accurate estimation of a quantum state is the inherent noise in the present day noisy intermediate-scale quantum (NISQ) devices.<a title="Select to navigate to references" href="#cit84"><sup><span class="sup_ref">84</span></sup></a> Because of this the measurements available are of limited fidelity and, in certain cases, some of the measurements are even not accessible. Several methods have been proposed as a means of an alternative approach to the traditional QST such as matrix product state tomography,<a title="Select to navigate to references" href="#cit371"><sup><span class="sup_ref">371</span></sup></a> neural network tomography,<a title="Select to navigate to reference" href="#cit373"><sup><span class="sup_ref">373–376</span></sup></a> quantum overlapping tomography,<a title="Select to navigate to references" href="#cit377"><sup><span class="sup_ref">377</span></sup></a> and shadow tomography.<a title="Select to navigate to reference" href="#cit378"><sup><span class="sup_ref">378,379</span></sup></a> Because of the noisy nature of the quantum systems since not all measurements are available at high fidelity, there have also been approaches that try to carry out QST based on incomplete measurements<a title="Select to navigate to reference" href="#cit380"><sup><span class="sup_ref">380–382</span></sup></a> such as maximum likelihood estimation (MLE),<a title="Select to navigate to reference" href="#cit383"><sup><span class="sup_ref">383–388</span></sup></a> Bayesian mean estimation (BME),<a title="Select to navigate to reference" href="#cit389"><sup><span class="sup_ref">389–392</span></sup></a> and maximal entropy formalism based QST.<a title="Select to navigate to reference" href="#cit393"><sup><span class="sup_ref">393,394</span></sup></a></p>
        <p class="otherpara">Quantum detection and estimation theory has been a prominent field of research in quantum information theory since the 1970s<a title="Select to navigate to reference" href="#cit395"><sup><span class="sup_ref">395–397</span></sup></a> and the rapid progress in quantum communication and computation in the past two decades motivated the use of big data for the classification of quantum systems through quantum learning and quantum matching machines.<a title="Select to navigate to references" href="#cit398"><sup><span class="sup_ref">398</span></sup></a> The notion of self-improvement of the performances of quantum machines <span class="italic">via</span> quantum learning was introduced by Ron Chrisley in 1995 in ref. <a title="Select to navigate to references" href="#cit399">399</a> through the example of a barrier/slit/plate feed-forward back-propagation network. In a feed-forward network, parameterized non-linear functions map an input state to an output state space. The interactions with the environment help the networks to modify those parameters such that each network can better approximate the resulting function. The proposed quantum implementation of such a network involved setting up a barrier with several slits in front of a particle beam. Some of the slits are designated as input slits and the rest are the weight slits. Behind the barrier is a photo-sensitive plate on which the interference pattern is observed that serves as an output for each input slit configuration. Once an interference pattern of high resolution is obtained on the plate, the error, which is a function of the desired and actual output vectors, is calculated. The gradient descent method is applied by taking the partial derivative of the error function with respect to the control variables, the weights, and the slits are adjusted accordingly for the next reading so as to minimize the error function. After sufficient training, optimum values for the weight configuration are obtained that ensured minimal error on the training set. This feed-forward network established the correspondence between the neural networks and the quantum system and is one amongst the many diverse approaches for the practical implementation of quantum neural networks.<a title="Select to navigate to reference" href="#cit400"><sup><span class="sup_ref">400,401</span></sup></a></p>
        <p class="otherpara">One of the most valuable applications of quantum tomography is the validation and testing of near-term quantum devices called the NISQ devices. Leveraging the capabilities of the quantum hardware of the gate-based NISQ devices, Alejandro <span class="italic">et al.</span> in ref. <a title="Select to navigate to references" href="#cit402">402</a> proposed a hybrid quantum-classical framework called the data-driven quantum circuit learning (DDQCL) algorithm for benchmarking and training shallow quantum circuits for generative modeling. Much like the various models encompassed within the Born machines,<a title="Select to navigate to reference" href="#cit403"><sup><span class="sup_ref">403–406</span></sup></a> the authors captured the correlations in the data set using the 2<small><sup><span class="italic">N</span></sup></small> amplitudes of wave functions obtained from an <span class="italic">N</span>-qubit quantum circuit. However, the distinctive feature in their work is the use of quantum circuits as a model for the data set that naturally works as a Born machine, thereby avoiding the dependence on tensor networks for the same. They demonstrated their approach of training quantum circuits for the preparation of the GHZ state and coherent thermal states thereby illustrating the power of Born machines for approximating Boltzmann machines.</p>
        <p class="otherpara">Finding an effective quantum circuit that can optimally carry out a desired transformation between the input and output quantum states also constitutes an important aspect of QST. In ref. <a title="Select to navigate to references" href="#cit407">407</a> the authors proposed a machine learning based optimization algorithm for quantum state preparation and gate synthesis on photonic quantum computers. They used the continuous-variable quantum neural network architecture<a title="Select to navigate to references" href="#cit408"><sup><span class="sup_ref">408</span></sup></a> as an ansatz whose optimization was carried out on the Strawberry Fields software platform for photonic quantum computation.<a title="Select to navigate to references" href="#cit409"><sup><span class="sup_ref">409</span></sup></a> Using the proposed method, the authors were able to achieve high fidelities of over 99% using short-depth circuits given only the target state as an input.</p>
        <p class="otherpara">Another quantum state preparation method was presented in ref. <a title="Select to navigate to references" href="#cit410">410</a> based on reinforcement learning which is a machine learning training architecture framework that finds an optimal solution to a problem based on the principle of rewarding the desired actions and penalizing the negative actions. The authors made a comparative study of the performances of three reinforcement learning algorithms: tabular Q-learning (TQL), deep Q-learning (DQL), and policy gradient (PG), and two traditionally used non-machine-learning methods: stochastic gradient descent (SGD) and Krotov algorithms, demonstrating their efficiency with reference to quantum state preparations under certain control constraints. Their results illustrated the effectiveness of reinforcement learning algorithms in solving complex optimization problems as the algorithms, especially DQL and PG, performed better amongst the five algorithms considered for state preparation under different types of constraints.</p>
        <p class="otherpara">Parameterized quantum circuits (PQC) are yet another machine learning model that utilizes the resources of both quantum and classical computation for application in a variety of data-driven tasks.<a title="Select to navigate to references" href="#cit411"><sup><span class="sup_ref">411</span></sup></a> Benedetti <span class="italic">et al</span>.<a title="Select to navigate to references" href="#cit411"><sup><span class="sup_ref">411</span></sup></a> presented a comprehensive review of various machine learning models involving PQC and also their application in diverse fields including quantum tomography. Another class of algorithms within the domain of quantum machine learning is hybrid quantum-classical Variational Quantum Algorithms (VQAs)<a title="Select to navigate to references" href="#cit412"><sup><span class="sup_ref">412</span></sup></a> that have been gaining popularity in recent years with numerous applications.<a title="Select to navigate to reference" href="#cit413"><sup><span class="sup_ref">413–419</span></sup></a> VQAs try to reduce the quantum resource allocation by using shallow quantum circuits for carrying out computations on a quantum device. One such algorithm was proposed in ref. <a title="Select to navigate to references" href="#cit420">420</a> by Wang <span class="italic">et al.</span> to prepare the quantum Gibbs state on near-term quantum hardware using parameterized quantum circuits. On such devices it is in general quite difficult to prepare the Gibbs state at arbitrary low temperatures just like finding the ground states of Hamiltonians.<a title="Select to navigate to references" href="#cit421"><sup><span class="sup_ref">421</span></sup></a> Preparation of the quantum Gibbs state of a given Hamiltonian has its application in a variety of fields like many-body physics, quantum simulations,<a title="Select to navigate to references" href="#cit422"><sup><span class="sup_ref">422</span></sup></a> and quantum optimization.<a title="Select to navigate to references" href="#cit423"><sup><span class="sup_ref">423</span></sup></a> In the method proposed by Wang <span class="italic">et al.</span> minimization of free energy serves as the loss function. However, within the calculation of free energy, estimation of entropy is the most challenging part.<a title="Select to navigate to references" href="#cit424"><sup><span class="sup_ref">424</span></sup></a> To tackle this problem they used truncation of the Taylor series of the von Neumann entropy at order <span class="italic">K</span> and, thus, the truncated free energy was set as the loss function whose minimum would correspond to the optimal parameters of the quantum circuit giving the Gibbs state. The estimation of the Taylor series expansion terms of entropy can be practically carried out using the well-known swap test,<a title="Select to navigate to reference" href="#cit425"><sup><span class="sup_ref">425,426</span></sup></a> and therefore, their method can be physically realized on a near-term quantum hardware. To validate the approach they numerically showed the preparation of high-fidelity Gibbs state for Ising chain and <span class="italic">XY</span> spin-½ chain models and were able to achieve fidelity of at least 95% for a range of temperatures.</p>
        <p class="otherpara">Another kind of quantum state preparation commonly termed as quantum state tomography relies on accessing experimentally measured observables. We shall discuss, here, how various techniques within the domain of machine learning/deep learning have been applied to perform QST.</p>
        <p class="otherpara">As machine learning and neural networks became increasingly popular with their application in many diverse fields, the concoction of quantum mechanics and machine learning algorithms also started surfacing.<a title="Select to navigate to reference" href="#cit156"><sup><span class="sup_ref">156,177,427,428</span></sup></a> The development of quantum annealing processors<a title="Select to navigate to references" href="#cit429"><sup><span class="sup_ref">429</span></sup></a> deemed a natural fit for testing the machine learning algorithms on a quantum hardware<a title="Select to navigate to reference" href="#cit430"><sup><span class="sup_ref">430,431</span></sup></a> to check for any quantum advantage. In the initial stages quantum mechanics was only used to facilitate the training for solving classical problems which, in fact for certain problems, did result in obtaining polynomial speed-ups relative to the classical training methods.<a title="Select to navigate to references" href="#cit432"><sup><span class="sup_ref">432</span></sup></a> Although the training of the Boltzmann machines using quantum processors did result in accurate training at a lower cost, the success of machine learning algorithms based on classical Boltzmann distribution inspired the proposition of a quantum probabilistic model for machine learning called quantum Boltzmann machines (QBMs) based on Boltzmann distribution for quantum Hamiltonian.<a title="Select to navigate to references" href="#cit433"><sup><span class="sup_ref">433</span></sup></a> In QBMs, not only the training is performed utilizing the quantum nature of the processors but also the model in itself is inherently quantum. The data modeling and training of the Boltzmann machine are carried out using the equilibrium thermal states of the transverse Ising type Hamiltonian.</p>
        <p class="otherpara">However, the training procedure used in ref. <a title="Select to navigate to references" href="#cit433">433</a> suffered from two limitations: (a) brute force techniques are required to find out the transverse field terms as they cannot be learned through classical data making it very hard to find the full Hamiltonian, and (b) quantum Monte Carlo methods can be used to efficiently simulate the transverse Ising models, and therefore, using the training method with transverse Ising models in thermal equilibrium did not show a clear quantum advantage. In ref. <a title="Select to navigate to references" href="#cit434">434</a>, the authors proposed the quantum analog of the generative training model that included quantum data sets in the training set so that their QBM is capable of learning the quantum terms along with the classical ones. The training of the Boltzmann machine to incorporate quantum data was carried out through two methods: POVM-based Golden-Thompson training and state-based relative entropy training (quantum equivalence of KL divergence). The relative entropy training method allowed the QBM to clone the quantum states within a certain level of approximation and given a considerable number of copies of the density operator for the training of QBM, it could reproduce approximate copies of the input state. Thus, although different from the traditional quantum state tomography wherein an explicit representation of the state operator is available at the output, the generative training of the QBM resulted in a quantum process that can learn Hamiltonian models for complex quantum states which in itself is a form of partial tomography.</p>
        <p class="otherpara">Machine learning techniques offer a big advantage of representing high dimensional data in the compressed form which can be really favourable for QST which in itself is a highly data-driven technique. In 2018, Torlai and colleagues<a title="Select to navigate to references" href="#cit373"><sup><span class="sup_ref">373</span></sup></a> utilized this property of the neural networks for obtaining the complete quantum mechanical description of highly entangled many-body quantum systems based on the availability of a limited set of experimental measurements. Their method involves the training of a restricted Boltzmann machine (RBM) using simulated measurement data for the true state. Their RBM architecture comprises a layer of visible neurons for encoding the measurement degrees of freedom and a hidden layer of binary neurons for encoding the tunable parameters. The training of the RBM is performed such that the generated probability distribution resembles closely the given data distribution and parameters are tuned depending on the desired degree of accuracy that is required for reconstruction. They demonstrated their approach by first benchmarking the neural network tomography of the <span class="italic">N</span>-qubit multipartite entangled <span class="italic">W</span> state. They also demonstrated QST for more complex systems consisting of quadratic spin Hamiltonians, namely the transverse-field Ising model and the XXZ spin-1/2 model.</p>
        <p class="otherpara">According to the authors, their RBM model works really well for entangled many-body systems and quantum-optic states. However, when there is unstructured data as in the case of states generated from a random unitary operation, the performance of the RBM goes down. The paper also did not highlight some of the pertinent questions such as what should be the size of the training set, or the optimal number of hidden neurons, and the dominance of RBM over other contemporary machine-learning approaches. Also, in the demonstration of their approach, both in the case of the tomographic reconstruction of the <span class="italic">W</span> state or ground states of the quadratic-spin Hamiltonians, the wavefunctions considered are real which decreases the measurement degrees of freedom required for state reconstruction and thereby dramatically reduces the tomographic overhead.</p>
        <p class="otherpara">Another neural network based quantum state estimation method empowered by machine learning techniques was presented in ref. <a title="Select to navigate to references" href="#cit435">435</a> in 2018 for full quantum state tomography. Instead of reducing the number of mean measurements required for full state reconstruction, the work focuses on speeding up the data processing in full QST without the assumption of any prior knowledge of the quantum state. Their training model is based on standard supervised learning techniques and the state estimation is carried out using a regression process wherein a parameterized function is applied for the mapping of the measurement data onto the estimated states. For a single state reconstruction, the computational complexity of their model is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t259_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t259.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t259.gif"/></a>, where <span class="italic">d</span> is the dimension of the Hilbert space, and was the fastest amongst the full QST algorithms such as MLE, LRE, BME, <span class="italic">etc.</span> at that time.</p>
        <p class="otherpara">A further application of neural network in the field of state tomography was presented in ref. <a title="Select to navigate to references" href="#cit376">376</a> where local measurements on reduced density matrices (RDMs)<a title="Select to navigate to reference" href="#cit436"><sup><span class="sup_ref">436–440</span></sup></a> were used to characterize the quantum state. Using the recent approach of measuring RDMs and thereby reconstructing the full state is a convenient alternative to the traditional QST approaches as the whole system can be characterized in polynomial number of parameters as opposed to the exponential parameters required for full reconstruction in the traditional QST techniques. However, QST <span class="italic">via</span> local measurements on RDMs is a computationally hard problem.<a title="Select to navigate to references" href="#cit441"><sup><span class="sup_ref">441</span></sup></a> In this work, the authors addressed this problem using machine learning techniques by building a fully connected feedforward neural network, as shown in <a title="Select to navigate to figure" href="#imgfig23">Fig. 23</a>, to demonstrate the full reconstruction of the states for up to 7-qubit in simulation and also reconstructed 4-qubit nuclear magnetic resonance (NMR) states in experiments. Their approach also had comparable fidelities with the MLE method but with the additional advantage in terms of speed up and better noise tolerance.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig23"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f23_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f23.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f23.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 23 </b> <span id="fig23"><span class="graphic_title">Local measurement based quantum state tomography <span class="italic">via</span> neural networks: the procedure starts with generating the training and test datasets, represented by dashed arrows, through random <span class="italic">k</span>-local Hamiltonians H to obtain the local measurement results <span class="italic">M</span> from the ground states <span class="italic">ψ</span><small><sub>H</sub></small>. The neural network is then trained using the training dataset which then produces the local measurements <span class="italic">M</span>, represented by black arrows. The measurements are used to find the Hamiltonian H followed by obtaining the ground states. The normal QST process follows the red arrow which is computationally hard. Figure is a schematic of the protocol illustrated in ref. <a title="Select to navigate to references" href="#cit376">376</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">Another machine learning assisted quantum state estimation technique based on convolutional neural networks (CNN) (basic theoretical framework discussed in Section 3.3.2) was presented in ref. <a title="Select to navigate to references" href="#cit338">338</a> in 2020 for reconstructing quantum states from a given set of coincidence measurements for both pure and mixed input states. Their approach involves feeding the noisy or incomplete set of simulated measurements to the CNN which then makes prediction of the <span class="italic">τ</span>-matrix based on the decomposition method discussed in ref. <a title="Select to navigate to references" href="#cit442">442</a>. The predicted matrix, which is the output, is then inverted to give the final density matrix. In order to compare the fidelity of the reconstructed states, the authors also implemented the Stokes reconstruction method<a title="Select to navigate to references" href="#cit442"><sup><span class="sup_ref">442</span></sup></a> and found a significant improvement in fidelity for not just noisy data sets but also when the projective measurements are incomplete and thereby demonstrating the advantage of CNN over typical reconstruction techniques.</p>
        <p class="otherpara">As an alternative means of QST several methods have been proposed, such as MLE, BME, and least-squares (LS) inversion,<a title="Select to navigate to references" href="#cit443"><sup><span class="sup_ref">443</span></sup></a> for efficient reconstruction of the quantum state. Since measurements on many copies of the state are required for efficient reconstruction of the quantum state so in order to gain maximum information from the measurements on the next copy, the measurements are adjusted based on the already available information from the measurements made thus far. This method of reconstructing the quantum state through adaptive measurements is called adaptive quantum state tomography. One such approach was introduced in ref. <a title="Select to navigate to references" href="#cit444">444</a> where self-learning algorithm was used in combination with different optimization strategies such as random selection, maximization of average information gain, and fidelity maximization for quantum state estimation. A generalization of self-learning algorithm was presented in ref. <a title="Select to navigate to references" href="#cit390">390</a> in the form of adaptive Bayesian quantum tomography (ABQT) with the aim to optimally design quantum tomographic experiments based on full Bayesian inference and Shannon information. Through their adaptive tomography strategy the authors were able to achieve significant reduction in the number of measurements required for full state reconstruction in case of two qubits pure states in comparison to the Monte Carlo simulation of the qubit systems. The experimental realization of the ABQT method was also carried out in ref. <a title="Select to navigate to references" href="#cit445">445</a> for two qubit quantum systems which did show a significant improvement in the accuracy of state estimation in comparison to the nonadaptive tomographic schemes. Recently, neural adaptive quantum state tomography (NAQT) was introduced in ref. <a title="Select to navigate to references" href="#cit446">446</a> that utilizes the neural network framework to replace the standard method of Bayes’ update in the ABQT scheme and thereby obtained orders of magnitude faster processing in estimating the quantum state while retaining the accuracy of the standard model. Basically, in the adaptive Bayesian-type tomography, the quantum space is discretized into samples and with each sample there is an associated weight that gets updated with each new measurement according to the Bayes’ rule in order to update the prior distribution to the posterior distribution of the quantum state space. However, with the increase in the number of measurements the likelihood function becomes sharply peaked with a tiny subset of the sample states having the majority weights. This can lead to a numerical singularity which is then avoided by resampling of the weights onto the space states which is computationally a very expensive process. In ref. <a title="Select to navigate to references" href="#cit446">446</a> the authors have used a machine learning algorithm to map the Bayesian update rule on a neural network to replace the traditional approach and thereby eliminate the problem of resampling of the weights saving the computational cost significantly without compromising on the accuracy of the reconstruction process. In comparison to the ABQT technique, the NAQT approach was able to speed up the reconstruction process by a factor of a million for approximately 10<small><sup>7</sup></small> measurements and is independent of the number of qubits involved and type of measurements used for estimating the quantum state.</p>
        <p class="otherpara">In 2013, another useful method was proposed in ref. <a title="Select to navigate to references" href="#cit441">441</a> wherein the linear regression estimation (LRE) model was used to identify optimal measurement sets to reconstruct the quantum state efficiently. The model was relatively straightforward where the authors converted the state estimation problem into a parameter estimation problem of a linear regression model and the LS method was employed to determine the unknown parameters. For a <span class="italic">d</span>-dimensional quantum state, the computational complexity of the method for state reconstruction is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t260_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t260.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t260.gif"/></a> and thereby saves up the cost of computation in comparison with the MLE or BME method. A natural extension to this work, in terms of both theory and experiment, was presented in ref. <a title="Select to navigate to references" href="#cit448">448</a> in order to improve the tomography accuracy by better tomographic measurements <span class="italic">via</span> an adaptive tomography protocol that does not necessarily require non-local measurements from experiments. The method is called recursively adaptive quantum state tomography (RAQST) primarily because the parameters are updated using the recursive LRE proposed in ref. <a title="Select to navigate to references" href="#cit441">441</a> and using the already collected data the measurement strategy is adaptively optimized for obtaining the state estimation. The authors also performed two-qubit state tomography experiments and showed the superiority of the RAQST method over the nonadaptive methods for quantum states with a high level of purity which is an important criterion for most forms of information processing methods.</p>
        <p class="otherpara">Another significant machine learning approach is the generative adversarial network (GAN) (basic theoretical framework is discussed in Section 3.3.6) based QST<a title="Select to navigate to reference" href="#cit334"><sup><span class="sup_ref">334,335</span></sup></a> that basically involves learning the map between the data and the quantum state unlike the RBM-based QST methods where the map yields a probability distribution. In the GAN method, two competing entities, generator G and discriminator D, engage with the objective to output a data distribution from some prior noisy distribution. Both G and D are parameterized non-linear functions consisting of multi-layered neural networks.<a title="Select to navigate to references" href="#cit251"><sup><span class="sup_ref">251</span></sup></a> With each step of optimization the generator becomes better at yielding outputs closer to the target data and the discriminator becomes better at detecting fake outputs. Inspired by the classical model, the quantum generative adversarial network (QGAN) was introduced in ref. <a title="Select to navigate to references" href="#cit449">449</a> and <a title="Select to navigate to references" href="#cit450">450</a> where quantum processors are used for running the neural nets of generator and discriminator as well as the data can be both quantum and classical. Thus, making the entire system quantum mechanical, at the end of the optimization process the generator can reproduce the true ensemble of quantum states without the discriminator being able to distinguish between the true and generated ensemble.</p>
        <p class="otherpara">The first proof-of-principle experimental demonstration of the QGAN algorithm was presented in ref. <a title="Select to navigate to references" href="#cit447">447</a> in a superconducting quantum circuit on datasets that are inherently quantum for both the input and the output. The QGAN algorithm employed was able to reproduce the statistics of the quantum data generated from a quantum channel simulator with a high level of fidelity (98.8% on average). Their experimental approach involves a superconducting quantum circuit for the generator G that outputs an ensemble of quantum states with a probability distribution to mimic the quantum true data whereas the discriminator D is used to carry out projective measurements on the true and the generated data in order to distinguish the two based on the measurement outcomes. The optimization process based on the gradient descent method consists of the adversarial learning by the discriminator and the generator in alternative steps that is terminated when the Nash equilibrium point is reached, <span class="italic">i.e.</span>, G produces the statistics such that D can no longer differentiate between the fake and the true data. The experimental protocol of the implementation of the QGAN algorithm is shown in <a title="Select to navigate to figure" href="#imgfig24">Fig. 24</a>.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig24"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f24_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f24.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f24.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 24 </b> <span id="fig24"><span class="graphic_title">Flowchart demonstrating the experimental protocol of the QGAN algorithm in ref. <a title="Select to navigate to references" href="#cit447">447</a> and the optimization scheme of the generator G and the discriminator D:G generates a random starting state <span class="italic">ρ</span><small><sub>0</sub></small>(<span class="italic">r</span><small><sub>0</sub></small>,<span class="italic">θ</span><small><sub>0</sub></small>,<span class="italic">ψ</span><small><sub>0</sub></small>) following which both D and G compete against each other by optimizing their strategies alternatively. The process is terminated when either D fails to discriminate the generated state <span class="italic">ρ</span> from the true state <span class="italic">σ</span> or the number of steps <span class="italic">c</span><small><sub>step</sub></small> reaches the limit. The optimization scheme, based on the gradient descent method, of D involves updating the parameters <span class="italic">β</span> and <span class="italic">γ</span> of the measurement operator <span class="italic">M</span> whereas <span class="italic">r</span>, <span class="italic">θ</span>, and <span class="italic">ψ</span> are updated to optimize G. The gradient estimation is carried out on a classical computer whereas field programmable gate arrays (FPGAs) are used for the measurement and control of the quantum system.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">In ref. <a title="Select to navigate to references" href="#cit451">451</a> the authors introduced the conditional generative adversarial network (CGAN) based QST as in the standard GAN approach there is no control over the output as the generator input is random which can be addressed using CGAN. Because of the improved control over the output, CGAN led to many diverse applications in a variety of fields.<a title="Select to navigate to reference" href="#cit452"><sup><span class="sup_ref">452–456</span></sup></a> With the CGAN based QST the authors were able to achieve higher fidelity of reconstruction, faster convergence and also reduced the number the measurements required for reconstructing a quantum state as compared to the standard model of state reconstruction using MLE. Also, with sufficient training on simulated data their model can even reconstruct the quantum states in a single shot. In their CGAN approach, the measurement operators ({<span class="italic">O</span><small><sub><span class="italic">i</span></sub></small>}) and the measurement statistics are the conditioning input to the generator which then outputs a density matrix <span class="italic">ρ</span><small><sub><span class="italic">G</span></sub></small> that is used to generate the measurement statistics by calculating tr(<span class="italic">O</span><small><sub><span class="italic">i</span></sub></small><span class="italic">ρ</span><small><sub><span class="italic">G</span></sub></small>). These measurement statistics and the experimental measurement statistics serve as the input for the discriminator which then outputs a set of numbers to distinguish between the generated statistics and the true data. The standard gradient-based optimization techniques are then used to train the network which is completed when the discriminator is unable to differentiate between the generated statistics from the generator and the true data. With better control over the output the CGAN approach can further find its applications in efficiently eliminating noise from the experimental data by training it on noisy simulated data; in addition it can have potential advantages in adaptive tomography as well by improving the choice of adaptive measurements for better reconstruction of quantum states.</p>
        <p class="otherpara">The huge success of the attention mechanism-based neural network generative model<a title="Select to navigate to reference" href="#cit228"><sup><span class="sup_ref">228,457,458</span></sup></a> to learn long-range correlations in natural language processing (NLP)<a title="Select to navigate to references" href="#cit459"><sup><span class="sup_ref">459</span></sup></a> prompted for its application in QST owing to the entanglement among qubits that can also be learnt through the self-attention mechanism used in the former case. The self-attention mechanism computes a representation of a sequence by relating different positions of a single sequence and has been very successful in a variety of sub-fields under NLP.<a title="Select to navigate to reference" href="#cit460"><sup><span class="sup_ref">460,461</span></sup></a> In ref. <a title="Select to navigate to references" href="#cit459">459</a> the authors proposed the first purely self-attention based transduction model, the Transformer, for deriving global dependencies between input and output without the use of a recurrent neural network (RNN) or convolutions. Just like other transduction models, the Transformer also uses an architecture of fully connected layers for both the encoder and decoder<a title="Select to navigate to reference" href="#cit228"><sup><span class="sup_ref">228,462,463</span></sup></a> using stacked self-attention that results in significantly faster training than architectures based on recurrent or convolutional layers. To the best of our knowledge the quantum-enhanced version of the Transformer has not been studied yet. However, the application of Transformer on quantum data has shown tremendous potential, as discussed below in the context of QST, for future research in this field.</p>
        <p class="otherpara">The long-range correlations exhibited by entangled quantum systems can be modeled analogous to the sentences in natural language using informationally complete positive operator-valued measurements (IC-POVM). Therefore, with this motivation, recently, Cha <span class="italic">et al.</span> proposed the ‘attention-based quantum tomography’ (AQT)<a title="Select to navigate to references" href="#cit464"><sup><span class="sup_ref">464</span></sup></a> to reconstruct the mixed state density matrix of a noisy quantum system using the Transformer<a title="Select to navigate to references" href="#cit459"><sup><span class="sup_ref">459</span></sup></a> architecture for the neural network. In this work they first benchmark their approach against previous neural network based QST using RNN<a title="Select to navigate to references" href="#cit465"><sup><span class="sup_ref">465</span></sup></a> that demonstrated a high fidelity classical description of a noisy many-body quantum state. To compare the performance of AQT with other state-of-art tomographic techniques they considered the Greenberger–Horne–Zeilinger (GHZ) state as their target state for up to 90 qubits and a built-in simulated error resulting in a mixed state. They showed that the complexity of learning the GHZ state can be improved by an order of magnitude when compared with the RNN method. They also benchmark AQT against MLE for a 3-qubit system and found a superior quantum fidelity of reconstruction for the AQT with the additional advantage of being scalable to larger systems. Furthermore, they were also able to reconstruct the density matrix of a 6-qubit GHZ state using AQT, with a quantum fidelity of 0.977, which is currently beyond the tomographic capabilities of the IBM Qiskit.</p>
        <p class="otherpara">QST becomes a formidable task as the size of the quantum system increases. In order to address this problem, in this section of the review, we present research based on several machine learning driven QST techniques such as RBM based QST for highly entangled many-body quantum systems, characterizing quantum systems using local measurements on RDMs, using CNN for state reconstruction, adaptive QST through self-learning and optimization based algorithms, VQAs for QST, generative models like GANs and attention based QST. Although the use of classical machine learning algorithms and deep neural networks has proven to be very effective in finding patterns in data, but for implementation on a quantum computer, loading classical data onto quantum devices can present a serious bottleneck for the implementation of these algorithms.<a title="Select to navigate to references" href="#cit466"><sup><span class="sup_ref">466</span></sup></a> Since a large number of independent parameters are required for reconstructing the quantum state that scales exponentially with the system size, and therefore, using quantum machine learning algorithms directly on the quantum states of the system can help in the learning and optimization of these parameters much more efficiently, owing to their ability in handling larger Hilbert space. As mentioned in the above sections, generative machine learning models such as quantum Boltzmann machine (QBM) and quantum generative adversarial network (QGAN) provide valuable insights into exploiting the full capabilities of the present day quantum computers in order to reproduce the desired quantum states. The improvement of quantum hardware with time also calls for their validation and benchmarking and QML can play a major role in the design of cost-effective quantum tomography techniques and protocols that can be easily implemented on the NISQ devices.</p>
      
      
        
        <h3 id="sect5353"><span class="b_heading">5.2 State classification protocols</span></h3>
        <span>Classification is always one of the most significant applications for classical machine learning (ML) or quantum machine learning (QML). Due to the fact that people are prone to make mistakes when establishing connections among various features, ML and QML are often able to improve the efficiency in dealing with classification problems. Each instance in the dataset used by machine learning algorithms should be represented with the same set of features, which could be continuous, categorical or binary.<a title="Select to navigate to references" href="#cit161"><sup><span class="sup_ref">161</span></sup></a> The learning process is then denoted as supervised machine learning if all instances are given with known labels. Generally, the classification problems can be divided as binary classification and multi-label classification. Binary classification is a classification with two possible outcomes. For example, classifying if an atom or molecule is excited or at ground state. Multi-label classification is a classification task with more than two possible outcomes. For example, classifying the electrical resistivity and conductivity of materials as a conductor, an insulator, a semiconductor or a superconductor. We shall focus extensively on the supervised QML assisted classification problems in chemical systems, specifically, where the output of instances admits only discrete un-ordered values and then discuss unsupervised learning strategies too.</span>
        <p class="otherpara">The process<a title="Select to navigate to reference" href="#cit161"><sup><span class="sup_ref">161,467</span></sup></a> of applying supervised ML or QML to a typical classification problem with physico-chemical applications is demonstrated in <a title="Select to navigate to figure" href="#imgfig25">Fig. 25</a>. The first step is to collect the dataset from chemical experiments. Due to the very existence of errors in measurement and impurities in the reaction, in most cases the raw dataset contains noise and missing feature values, and therefore significant pre-processing is required.<a title="Select to navigate to references" href="#cit468"><sup><span class="sup_ref">468</span></sup></a> The second step is to select appropriate algorithms and initialize the classifier model. Then, split the dataset using cross-validation and feed the classifier model with the training data. Next, predict the label for a test dataset, and evaluate the error rate of the classifier model. Additionally, parameter tuning should be repeated until an acceptable evaluation with the test set is obtained.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig25"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f25_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f25.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f25.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 25 </b> <span id="fig25"><span class="graphic_title">Working flowchart of a supervised classification model to a chemical problem. The first step is to collect raw data from chemical experiments, and clean the dataset with various pre-processing methods. The second step is to select appropriate algorithms and initialize the classifier model. Then, split the dataset using cross-validation and feed the classifier model with the training data. Next, predict the label for test dataset, and evaluate the error rate of the classifier model.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">Classifiers based on decision trees (see Section 3.2.5) play important roles in various chemical problems, such as toxicity prediction,<a title="Select to navigate to references" href="#cit469"><sup><span class="sup_ref">469</span></sup></a> mutagenesis analysis,<a title="Select to navigate to references" href="#cit470"><sup><span class="sup_ref">470</span></sup></a> and reaction prediction.<a title="Select to navigate to references" href="#cit471"><sup><span class="sup_ref">471</span></sup></a> However, quantum decision trees are hardly applied independently dealing with intricate chemical classification problems, since simplicity, one of the key advantages of decision tree, could be eclipsed during the complicated mapping and learning process in QML.</p>
        <p class="otherpara">Decision trees are sometimes applied along with other algorithms to analyze and demonstrate the key features of the intricate classification process. Recently, Heinen and coworkers studied two competing reactive processes mainly with a reactant-to-barrier (R2B) machine learning model,<a title="Select to navigate to references" href="#cit472"><sup><span class="sup_ref">472</span></sup></a> where the decision tree generated by the R2B method systematically extracts the information hidden in the data and the model. <a title="Select to navigate to figure" href="#imgfig26">Fig. 26</a> is a scheme of the tree generated from the R2B method. Blue dotted lines refer to an accepted change meaning that only compounds containing these substituents at the position are considered. Orange dotted lines refer to substitution declined, meaning that all compounds except the decision are kept. Vertical lines on the right of energy levels denote the minimum first (lower limit), and the third (upper limit) quartile of a box plot over the energy range. Numbers above the energy levels correspond to the number of compounds left after the decision. Lewis structures resemble the final decision.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig26"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f26_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f26.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f26.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 26 </b> <span id="fig26"><span class="graphic_title">Decision tree using extracted rules and design guidelines. The decision tree is generated using the reactant-to-barrier (R2B) method estimated activation barriers to predict changes in barrier heights by starting at all reactions (first energy level on the left) and subsequently applying changes by substituting functional groups, leaving groups and nucleophiles with E2 (see ref. <a title="Select to navigate to references" href="#cit472">472</a>). Blue dotted lines refer to an accepted change meaning that only compounds containing these substituents at the position are considered. Orange dotted lines refer to substitution declined, meaning that all compounds except the decision are kept. Vertical lines on the right of energy levels denote the minimum first (lower limit), and the third (upper limit) quartile of a box plot over the energy range. Numbers above energy levels correspond to the number of compounds left after the decision. Lewis structures resemble the final decision. Reprinted from H. Stefan and V. Rudorff, G. Falk and V. Lilienfeld, O. Anatole, <span class="italic">J. Chem. Phys.</span>, 2021, <span class="bold">155</span>, 6, 064105 with the permission of AIP Publishing.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">In recent years there have been more than a few reports where Bayesian network (BN) (see Section 3.2.6) based methods are applied to solve various chemical classification problems. The BN approach shows fruitful capability for the prediction of chemical shifts in NMR crystallography,<a title="Select to navigate to references" href="#cit473"><sup><span class="sup_ref">473</span></sup></a> simulation of the entropy driven phase transitions,<a title="Select to navigate to references" href="#cit474"><sup><span class="sup_ref">474</span></sup></a> and particularly, the simulation of quantum molecular dynamics simulation.<a title="Select to navigate to references" href="#cit475"><sup><span class="sup_ref">475</span></sup></a></p>
        <p class="otherpara">Quantum instance-based learning algorithms like <span class="italic">k</span>-NN (see Section 3.2.4) are also applied in chemical classification problems. Recently, researchers have studied the phase transition of VO<small><sub>2</sub></small> based on a quantum instance-based learning algorithms.<a title="Select to navigate to references" href="#cit318"><sup><span class="sup_ref">318</span></sup></a> The training instances are first assigned into several sublabels <span class="italic">via</span> the quantum clustering algorithm, based on which a quantum circuit is constructed for classification. <a title="Select to navigate to figure" href="#imgfig27">Fig. 27</a> is a scheme of the quantum circuit implementing the classification process of the quantum clustering algorithm. For training instances that are clustered into <span class="italic">N</span> sublabels, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t261_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t261.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t261.gif"/></a> qubits are required representing the sublabels in the classifier circuit. Meanwhile, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t262_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t262.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t262.gif"/></a> qubits are required to represent the test instance, where <span class="italic">d</span> is denoted as the dimension of the instance. For simplicity, here we assume that there are only 5 sublabels totally, and all instances are 2-d vectors. Thus, <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, and <span class="italic">q</span><small><sub>3</sub></small> represent the possible sublabels and <span class="italic">q</span><small><sub>4</sub></small> represents the test instance, meanwhile <span class="italic">U</span><small><sub>n</sub></small> is an operation corresponding to the centroid or mean value of the training instances under the same sublabel. Here Hadamard gates are applied on <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, and <span class="italic">q</span><small><sub>3</sub></small>, preparing a uniform distribution of the possible sublabels. To improve the accuracy, the weighting scheme can be included by assigning <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, and <span class="italic">q</span><small><sub>3</sub></small> as some certain quantum states corresponding to the weights.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig27"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f27_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f27.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f27.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 27 </b> <span id="fig27"><span class="graphic_title">Scheme of the structure of the quantum circuit implementing the classification process. For training instances that are clustered into <span class="italic">N</span> sublabels, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t263_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t263.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t263.gif"/></a> qubits are required representing the sublabels in the classifier circuit. Meanwhile, <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t264_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t264.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t264.gif"/></a>qubits are required to represent the test instance, where <span class="italic">d</span> is denoted as the dimension of the instance. For simplicity, here we assume that there are only 5 sublabels totally, and all instances are 2-d vectors. Thus, <span class="italic">q</span><small><sub>1</sub></small>, <span class="italic">q</span><small><sub>2</sub></small>, and <span class="italic">q</span><small><sub>3</sub></small> represent the possible sublabels and <span class="italic">q</span><small><sub>4</sub></small> represents the test instance, meanwhile <span class="italic">U</span><small><sub><span class="italic">n</span></sub></small> is operation corresponding to the centroid or mean value of the training instances under the same sublabel. Figure is reproduced from ref. <a title="Select to navigate to references" href="#cit318">318</a>. (Under Creative Commons Attribution 4.0 International License.)</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">The process of classification of metallic and insulating states of VO<small><sub>2</sub></small> is shown in <a title="Select to navigate to figure" href="#imgfig28">Fig. 28</a>. <a title="Select to navigate to figure" href="#imgfig28">Fig. 28a</a> demonstrates the original data used for classification. All training instances are 2-d vectors (pressure and temperature), while the label is denoted by color. Red dots represent the metallic state, and blue ones represent the insulating state. The phase transition line is indicated by the black solid curve. The sublabels left after quantum clustering algorithm is shown in <a title="Select to navigate to figure" href="#imgfig28">Fig. 28b</a>, where each sphere represents a sublabel, with center corresponding to the centroid or mean-value, and radius corresponding to the number of instances. Prediction of test instances is shown in <a title="Select to navigate to figure" href="#imgfig28">Fig. 28c</a>. Test instances in the blue part will be recognized with the label ‘insulating’, and the label of test instances in the yellow part will be predicted as ‘metallic’.</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig28"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f28_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f28.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f28.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 28 </b> <span id="fig28"><span class="graphic_title">Classification of the metallic and insulating states of VO<small><sub>2</sub></small> based on the quantum clustering algorithm. (a) Demonstrates the original data used for classification. All training instances are 2-d vectors (pressure and temperature), while the label is denoted by color. Red dots represent the metallic state, and blue ones represent the insulating state. Phase transition line is indicated by the black solid curve. The sublabels left after quantum clustering algorithm is shown in (b), where each sphere represents a sublabel, with center corresponding to the centroid or meanvalue, and radius corresponding to the number of instances. Prediction of test instances are shown in (c). Test instances in the blue part will be recognized with the label insulating, and the label of test instances in the yellow part will be predicted as metallic. Figure is reproduced from ref. <a title="Select to navigate to references" href="#cit318">318</a> (under Creative Commons Attribution 4.0 International License).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">The quantum SVM (see Section 3.2.7) has also been applied to various classification problems, such as handwritten character recognition,<a title="Select to navigate to references" href="#cit176"><sup><span class="sup_ref">176</span></sup></a> solar irradiation prediction,<a title="Select to navigate to references" href="#cit476"><sup><span class="sup_ref">476</span></sup></a> and even the study of particle decays in high energy physics.<a title="Select to navigate to references" href="#cit477"><sup><span class="sup_ref">477</span></sup></a> Additionally, in experiments provable quantum advantage has been demonstrated by recent quantum classifiers based on a variational quantum classifier and a quantum kernel estimator—build on noisy intermediate-scale (NISQ) devices.<a title="Select to navigate to references" href="#cit324"><sup><span class="sup_ref">324</span></sup></a> Though there are only a few attempts to deal with specific chemical problems with quantum SVM methods, quantum SVMs demonstrate great capacity in classification problems, while the optimization process of quantum SVM leads to exponential speedup compared with the classical version. Therefore, there exists enormous potential for the quantum SVM methods to assist chemical classification and regression problems.</p>
        <p class="otherpara">It is always one of the most crucial challenges in the study of many-body problems that the dimensionality of the Hilbert space grows exponentially with the system size, which leads to tremendous difficulty to solve the Shrödinger equations. Among the modern numerical techniques designed to study the complicated systems, neural networks (NNs) attract enormous attention due to their remarkable abilities to extract features and classify or characterize complex sets of data. Modern ANN architectures, especially the feed-forward neural networks (see Section 3.3.1) and convolutional neural networks (see Section 3.3.2) have been playing significant roles in the classification problems of various fields. It is reported that the neural network technologies can be used to discriminate phase transitions in correlated many-body systems,<a title="Select to navigate to references" href="#cit336"><sup><span class="sup_ref">336</span></sup></a> to probe the localization in many-body systems,<a title="Select to navigate to references" href="#cit478"><sup><span class="sup_ref">478</span></sup></a> and even to classify the entangled states from the separated ones.<a title="Select to navigate to references" href="#cit337"><sup><span class="sup_ref">337</span></sup></a></p>
        <p class="otherpara">On the other hand, dramatic success of classical neural networks also provokes interest in developing the quantum version. More than 20 years ago, pioneers attempted to build up quantum neural networks (QNNs), particularly, the quantum version of feed-forward neural networks<a title="Select to navigate to references" href="#cit479"><sup><span class="sup_ref">479</span></sup></a> (see Section 3.3.1). There were also reports where the QNNs were applied into real classification problems, such as the vehicle classification.<a title="Select to navigate to references" href="#cit480"><sup><span class="sup_ref">480</span></sup></a> The rapid development of hardware further provided more possibilities for designing the QNN models.</p>
        <p class="otherpara">Special-purpose quantum information processors such as quantum annealers and programmable photonic circuits are suitable fundamental implementation of deep quantum learning networks.<a title="Select to navigate to reference" href="#cit55"><sup><span class="sup_ref">55,481</span></sup></a> Researchers also developed recurrent quantum neural networks (RQNNs) (see Section 3.3.3) that can characterize nonstationary stochastic signals.<a title="Select to navigate to reference" href="#cit482"><sup><span class="sup_ref">482,483</span></sup></a> Additionally, there are some other QNN models aiming to supervised learning, such as the classifiers based on QNNs and measurements of entanglement.<a title="Select to navigate to references" href="#cit484"><sup><span class="sup_ref">484</span></sup></a> However, most of these models are actually hybrid models, as the activation functions are calculated classically, and the dataset are generally classical data.</p>
        <p class="otherpara">In 2018 Farhi and Neven proposed a specific framework for building QNNs that can be used to do supervised learning both on classical and quantum data.<a title="Select to navigate to references" href="#cit485"><sup><span class="sup_ref">485</span></sup></a> For binary classification problems, the input instance can be represented by quantum state |<span class="italic">z</span>,1〉, where <span class="italic">z</span> is an <span class="italic">n</span>-bit binary string carrying information of the inputs, and an auxiliary qubit is set as |1〉 initially. Totally there are <span class="italic">n</span> + 1 qubits, <span class="italic">n</span> qubits representing the input instance, and 1 qubit representing the label. After the unitary operation <span class="italic">U</span>(<span class="italic">θ</span>), the auxiliary qubit is measured by a Pauli operator, denoted as <span class="italic">Y</span><small><sub><span class="italic">n</span>+1</sub></small>, and the measurement result 1 or −1 indicates the prediction of the label. With multiple copies, the average of the observed outcomes can be written as 〈<span class="italic">z</span>,1|<span class="italic">U</span><small><sup>†</sup></small>(<span class="italic">θ</span>)<span class="italic">Y</span><small><sub><span class="italic">n</span>+1</sub></small><span class="italic">U</span>(<span class="italic">θ</span>)|<span class="italic">z</span>,1〉. Furthermore, we can estimate the loss function<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn111"/><span id="eqn111">loss(<span class="italic">θ</span>,<span class="italic">z</span>) = 1 − <span class="italic">l</span>(<span class="italic">z</span>)〈<span class="italic">z</span>,1|<span class="italic">U</span><small><sup>†</sup></small>(<span class="italic">θ</span>)<span class="italic">Y</span><small><sub><span class="italic">n</span>+1</sub></small><span class="italic">U</span>(<span class="italic">θ</span>)|<span class="italic">z</span>,1〉</span></td><td class="rightEqn">(111)</td></tr></table>where <span class="italic">l</span>(<span class="italic">z</span>) is the label of instance <span class="italic">z</span>, which might be 1 or −1. For a training set <span class="italic">z</span><small><sub><span class="italic">j</span></sub></small>, <span class="italic">l</span>(<span class="italic">z</span><small><sub><span class="italic">j</span></sub></small>), <span class="italic">j</span> = 1,…, <span class="italic">N</span>, the training process is to find the optimal parameters <span class="italic">q</span> minimizing the loss function <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t265_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t265.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t265.gif"/></a>. However, in the numerical simulations, they did not find any case where the QNNs could show speedup over classical competitors for supervised learning.<a title="Select to navigate to references" href="#cit485"><sup><span class="sup_ref">485</span></sup></a></p>
        <p class="otherpara">Meanwhile in 2018, researchers from Xanadu investigated the relationship between feature maps, kernel methods and quantum computing.<a title="Select to navigate to references" href="#cit179"><sup><span class="sup_ref">179</span></sup></a> There contains two main steps in the classification steps. They attempted to encode the inputs in a quantum state as a nonlinear feature map that maps data to the quantum Hilbert space. Inner products of quantum states in the quantum Hilbert space can be used to evaluate a kernel function. Then a variational quantum circuit is trained as an explicit classifier in feature space to learn a decision boundary. In the model, a vector (<span class="italic">x</span><small><sub>1</sub></small>,<span class="italic">x</span><small><sub>2</sub></small>)<small><sup><span class="italic">T</span></sup></small> from the input space <span class="italic">X</span> is mapped into the feature space <span class="italic">F</span> which is the infinite-dimensional space of the quantum system. The model circuit then implements a linear model in feature space and reduces the “infinite hidden layer” to two outputs. Though linear transformations are natural for quantum theory, nonlinearities are difficult to design in the quantum circuits. Therefore the feature map approach offers an elegant solution. Classical machine learning took many years from the original inception until the construction of a general framework for supervised learning. Therefore, towards the general implementation much efforts might be required as we are still at the exploratory stage in the design of quantum neural networks.</p>
        <p class="otherpara">In 2019, Adhikary and coworkers proposed a quantum classifier using a quantum feature space,<a title="Select to navigate to references" href="#cit486"><sup><span class="sup_ref">486</span></sup></a> with both quantum variational algorithm and hybrid quantum-classical algorithm for training. The input are encoded into a multi-level system; therefore, the required number of training parameters is significantly lesser than those of the classical ones. Simulation based on four benchmark datasets (CANCER, SONAR, IRIS and IRIS2) shows that the quantum classifier could lead to a better performance with respect to some classical machine learning classifiers. In 2020, Wiebe's group proposed a circuit-centric quantum classifier,<a title="Select to navigate to references" href="#cit487"><sup><span class="sup_ref">487</span></sup></a> which is a class of variational circuits designed for supervised machine learning. The quantum classifier contains relatively few trainable parameters, and constructed by only a small number of one- and two-qubit quantum gates, as entanglement among the qubits plays a crucial role capturing patterns in the data. The optimal parameters are obtained <span class="italic">via</span> a hybrid gradient descent method. The circuit-centric quantum classifier shows significant model size reduction comparing the classical predictive models.<a title="Select to navigate to references" href="#cit487"><sup><span class="sup_ref">487</span></sup></a></p>
        <p class="otherpara">The impressive success of these hybrid methods provide an alternative to study the chemistry classification problems. There are plenty of attempts to study the phase diagrams classification with classical machine learning methods.<a title="Select to navigate to reference" href="#cit488"><sup><span class="sup_ref">488,489</span></sup></a> It would be of great interest to study these problems with quantum or hybrid classifiers.</p>
        <p class="otherpara">Recently<a title="Select to navigate to references" href="#cit490"><sup><span class="sup_ref">490</span></sup></a> a report has proposed a variational quantum algorithm to classify phases of matter using a tensor network ansatz. The algorithm has been exemplified on the XXZ model and the transverse field Ising model. The classification circuit is composed of two parts: the first part prepares the approximate ground state of the system using a variational quantum eigensolver and feeds the state to the second part which is a quantum classifier which is used to label the phase of the state. Since the quantum state is fed directly into the classification circuit from the variational quantum eigensolver, it bypasses the data reading overhead which slows down many applications of quantum-enhanced machine learning. For both the parts, the quantum state <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t266_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t266.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t266.gif"/></a> is represented using a shallow tensor network which makes the algorithms realizable on NISQ devices. The first part of the algorithm represents the Hamiltonian matrix <span class="italic">H</span> of the system in terms of Pauli strings and variationally minimizes the average energy <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t267_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t267.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t267.gif"/></a> to prepare the ground state. A checkerboard tensor network scheme with a tunable number of layers <span class="italic">L</span> is used for the representation. For an <span class="italic">n</span>-qubit state, the ansatz requires <span class="italic">O</span>(<span class="italic">nL</span>) independent parameters, where <span class="italic">L</span> is the number of layers in the circuit. In this scheme, the maximally entangled state would require <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t268_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t268.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t268.gif"/></a> layers with periodic boundary conditions and for critical one-dimensional systems <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t269_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t269.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t269.gif"/></a> is enough. This can be contrasted with a UCCSD ansatz which requires typically <span class="italic">O</span>(<span class="italic">n</span><small><sup>4</sup></small>) parameters<a title="Select to navigate to references" href="#cit491"><sup><span class="sup_ref">491</span></sup></a> thereby necessitating a higher dimensional optimization. As mentioned before, the second part of the circuit, <span class="italic">i.e.</span>, the classifier receives the state from the VQE part and applies a unitary <span class="italic">U</span><small><sub>class</sub></small>(<span class="italic">ϕ</span>). The unitary is again approximated using the Checkerboard tensor network ansatz. Then the output of the circuit is measured in Z-basis to determine the phase of the state using majority voting. For the transverse field Ising model the report demonstrated a 99% accuracy with a 4-layered classifier and for the XXZ model it was 94% accuracy with a 6-layered classifier (<a title="Select to navigate to figure" href="#imgfig29">Fig. 29</a>).</p>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig29"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f29_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f29.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f29.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 29 </b> <span id="fig29"><span class="graphic_title">A figure showing the prediction of the phases using a quantum classifier based on tensor network ansatz. The plot on the left (right) shows the prediction of phase II as a function of the magnetic field (<span class="italic">J</span><small><sub><span class="italic">z</span></sub></small>) for the transverse-field Ising (XXZ) model. The Roman numbers I and II denote the phases of the models. Reprinted figure with permission from A. V. Uvarov, A. S. Kardashin, and J. D. Biamonte, Machine learning phase transitions with a quantum processor, <span class="italic">Phys. Rev. A</span>, 2020, <span class="bold">102</span>, 012415. Copyright (2022) by the American Physical Society.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">Picking up an appropriate algorithm is always crucial when dealing with the classification problems. The classifier evaluation is often based on prediction accuracy. Here we present three techniques to estimate classifier accuracy.<a title="Select to navigate to references" href="#cit161"><sup><span class="sup_ref">161</span></sup></a> One popular technique is to split the training instances into two groups, where two-thirds are regard as the training data and the other third is regard as the test data. Another technique is known as cross-validation. The training set is manually divided into exclusive and equal-sized subsets initially. Then for each subset the classifier is trained on all the other subsets. Thus, estimation of the error rate of the classifier is obtained by calculating the average of the error rate of each subset. The last one is denoted as leave-one-out validation, which is a special case of cross validation. In leave-one-out validation there is only a single instance in each subset. If the prediction accuracy cannot reach the demand, another supervised learning algorithm should be selected, as shown in <a title="Select to navigate to figure" href="#imgfig25">Fig. 25</a>.</p>
        <p class="otherpara">Additionally, we will present some remarks about the techniques as follows. Even though the optimal solution always depends on the task at hand, these remarks can prevent the practitioners from selecting a wholly inappropriate algorithm. Logic-based systems often perform better when dealing with discrete features. Decision trees are in general resistant to noise because of their pruning strategies. In contrast, most decision tree algorithms cannot perform well when diagonal partitioning is required. Interference allows a class of quantum decision trees to be penetrated exponentially faster by quantum evolution than by a classical random walk. However, these examples could also be solved in polynomial time by different classical algorithms.<a title="Select to navigate to references" href="#cit164"><sup><span class="sup_ref">164</span></sup></a> BN methods are able to achieve their maximum prediction accuracy with a relatively small dataset. Besides, BN methods train very quickly since they require only a single pass on the data either to collect the frequencies or to compute the normal probability density functions. The graph structure of BNs can efficiently construct a quantum state representing the intended classical distribution, and a square-root speedup time can be obtained per sample by implementing a quantum version of rejection sampling.<a title="Select to navigate to references" href="#cit173"><sup><span class="sup_ref">173</span></sup></a> Lazy learning methods require zero training time as the training instance is initially stored. On the other hand, <span class="italic">k</span>-NN is quite sensitive to the irrelevant features, and is generally intolerant of noise. Quantum nearest neighbor algorithm and quantum nearest centroid algorithm both show significant speedup compared to the classical version. In certain cases, there are exponential or even super-exponential reductions over the classical analog.<a title="Select to navigate to references" href="#cit56"><sup><span class="sup_ref">56</span></sup></a> SVM methods generally perform better when dealing with classification problems with multi-dimensions and continuous features. Moreover, SVMs are still able to perform well when there exists a nonlinear relationship between the input and output features. However, a large sample size is required to achieve its maximum prediction accuracy. Optimization of quantum SVM is implemented by the quantum algorithm solving linear equations, leading to exponential speedup compared to the classical version.<a title="Select to navigate to references" href="#cit177"><sup><span class="sup_ref">177</span></sup></a></p>
        <p class="otherpara">Let us now focus on unsupervised approaches to classification briefly. The learning process is denoted as unsupervised when the given training instances are not assigned with the desired labels. Due to the absence of supervision, the unsupervised learning can hardly be applied to distinguish various types of chemicals or detect some certain structures. Instead, unsupervised learning processes can find out the boundaries that divide the instances, so that they could be beneficial in the recognition of phase transitions. For instance, unsupervised machine learning methods can be applied to identify the phase transition to non-trivial many-body phases such as superfluids,<a title="Select to navigate to references" href="#cit492"><sup><span class="sup_ref">492</span></sup></a> or to detect the topological quantum phase transitions.<a title="Select to navigate to references" href="#cit493"><sup><span class="sup_ref">493</span></sup></a></p>
        <p class="otherpara">One important approach in the unsupervised QML is clustering methods. The clustering methods can be assigned as instance-based learning algorithms. Consider the <span class="italic">k</span>-means problem of assigning given vectors to <span class="italic">k</span> clusters minimizing the average distance to the centroid of the cluster. The standard unsupervised learning method is Lloyds algorithm, which contains the following steps:<a title="Select to navigate to reference" href="#cit494"><sup><span class="sup_ref">494,495</span></sup></a> (1) pick up the initial centroid randomly; (2) assign each vector to the cluster with the closest mean; (3) recalculate the centroids of the clusters; (4) repeat steps (1–2) until a stationary assignment is attained. Based on the classical version, in 2013, Lloyd and coworkers proposed the quantum unsupervised machine learning method,<a title="Select to navigate to references" href="#cit156"><sup><span class="sup_ref">156</span></sup></a> rephrasing the <span class="italic">k</span>-means problem as a quadratic programming problem which is amenable to solution by the quantum adiabatic algorithm. In 2018, Iordanis and coworkers proposed <span class="italic">q</span>-means, a new quantum algorithm for clustering problem, which provides substantial savings compared to the classical <span class="italic">k</span>-means algorithm. In 2017, researchers implemented a hybrid quantum algorithm for clustering on a 19-qubit quantum computer,<a title="Select to navigate to references" href="#cit496"><sup><span class="sup_ref">496</span></sup></a> which shows robustness to realistic noise.</p>
        <p class="otherpara">Inspired by the success of neural network-based machine learning, Iris and coworkers proposed the quantum convolutional neural networks (QCNNs) in 2019<a title="Select to navigate to references" href="#cit215"><sup><span class="sup_ref">215</span></sup></a> (see Section 3.3.2). A paradigm is presented where the QCNN is applied to 1-D quantum phase recognition. There are less applications of unsupervised QML compared to supervised learning in chemical classification problems reported so far. However, recent advancements suggest the future that unsupervised QML could take a place in the study of complex many-body systems, especially in the recognition of phases.</p>
      
      
        
        <h3 id="sect5578"><span class="b_heading">5.3 Many-body structure and property prediction for molecules, materials, and lattice models and spin-systems</span></h3>
        <div>
          
          <span id="sect5581"/><span class="c_heading_indent">5.3.1 Machine learning techniques on a classical processor. </span>
          <span>Obtaining an electronic structure description for material systems has been a problem with continued research in Chemistry and materials science. Since this task is a many-body problem, solving it with high accuracy is crucial as numerous materials properties and chemical reactions entail quantum many-body effects. For a significant duration, electronic structure calculations were performed using Density Functional Theory (DFT), which is based on the effective single-particle Kohn–Sham equations.<a title="Select to navigate to references" href="#cit497"><sup><span class="sup_ref">497</span></sup></a> In DFT, the ground state energy of a many-electron system is written as a functional of the electron density, thereby reducing the many-body problem for an <span class="italic">N</span> particle wavefunction to just one. This has yielded accurate results with efficient computations compared to its predecessors, but the functional form of the exact solution is unknown and efficient approximations are made for practical purposes. Attempts have been therefore made to obtain such density-functionals using ML algorithms. One of the earliest studies was by Snyder <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit312"><sup><span class="sup_ref">312</span></sup></a> who constructed a kinetic energy functional for spinless fermions in a 1D box subjected to an external potential made from the linear combination of several Gaussians defined on a dense spatial grid. Many such randomly selected external potentials were chosen as the training set with the actual labelled density and kinetic energy obtained by solving the Schroedinger equation as the training data. Thereafter kernel-ridge regression was used to construct the kinetic energy functional from the aforesaid known density with excellent accuracy. The oscillations in functional derivative of the so constructed kinetic energy functional were dampened by using the principal components. From the knowledge of this functional derivative a protocol to procure a self-consistent density field that minimizes the energy was presented. Since then many report have been made which have attempted to construct density functionals especially the exchange-correlation functional.<a title="Select to navigate to reference" href="#cit498"><sup><span class="sup_ref">498–505</span></sup></a></span>
          <p class="otherpara">Kernel-ridge regression (see Section 3.2.2) has been extensively used in chemistry for a variety of other purposes too like predicting the energy of the highest occupied molecular orbital from three different molecular datasets<a title="Select to navigate to references" href="#cit506"><sup><span class="sup_ref">506</span></sup></a> using two different technique to encode structural information about the molecule or for the prediction of atomization energies<a title="Select to navigate to reference" href="#cit313"><sup><span class="sup_ref">313,507</span></sup></a> through a Coulomb matrix representation of the molecule wherein the energy is expressed as a sum of weighted Gaussian functions. Recently, many new schemes to represent structural features have also been designed<a title="Select to navigate to reference" href="#cit314"><sup><span class="sup_ref">314,508</span></sup></a> wherein information about the environment of each constituent atom is encoded within its <span class="italic">M</span>-body interaction elements each of which is a weighted sum of several Gaussian functions. The distance metric between each such interaction representation between element <span class="italic">I</span> and <span class="italic">J</span> is considered to be the usual Euclidean norm. Atomization energies, energies for even non-bonded interaction like in water clusters, predicted using such representations are extremely accurate.<a title="Select to navigate to references" href="#cit314"><sup><span class="sup_ref">314</span></sup></a> More such examples can be found in topical reviews like in ref. <a title="Select to navigate to references" href="#cit509">509</a>. Caetano <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit342"><sup><span class="sup_ref">342</span></sup></a> used Artificial Neural Networks (ANNs) (a theoretical framework discussed in Section 3.3) trained using the Genetic Algorithm (GA) to solve the Kohn–Sham equations for He and Li atoms. They used a network comprising of one neuron in the input layer, one hidden layer with eight neurons, and two neurons in the output layer. For the GA based optimization, the number of individuals <span class="italic">N</span> in the population was kept to 50. By generating the initial orbitals randomly and building the electron density, an effective Hamiltonian is constructed. The ANN is trained using GA to find the orbitals that minimize the energy functional and then the total energy is calculated, which is repeated until a convergence point. The results from the ANN were shown to be in good agreement with those of the other numerical procedures.</p>
          <p class="otherpara">Performing higher order calculations like CCSD provides accurate results but has a very high computational cost. While, methods such as semi-empirical theory PM7, Hartree–Fock (HF), or DFT provide less accurate results but scale efficiently. The work by Ramakrishnan <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit510"><sup><span class="sup_ref">510</span></sup></a> corrects the lower-order methods to provide accurate calculations by training their <span class="italic">Δ</span> model to learn enthalpies, free energies, entropies, and electron correlation energies from a dataset consisting of organic molecules. The property of interest was corrected by expressing<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn112"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t270_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t270.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t270.gif"/></a></td><td class="rightEqn">(112)</td></tr></table>where, <span class="italic">α</span><small><sub><span class="italic">i</span></sub></small> are regression coefficients, obtained through kernel ridge regression (a theoretical framework discussed in Section 3.2.8), <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t271_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t271.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t271.gif"/></a> with <span class="italic">σ</span> being a hyperparameter that is tuned, |<span class="italic">R</span><small><sub>b</sub></small> − <span class="italic">R</span><small><sub><span class="italic">i</span></sub></small>| is the Manhattan norm<a title="Select to navigate to references" href="#cit511"><sup><span class="sup_ref">511</span></sup></a> measuring the similarity between the features of target molecule <span class="italic">R</span><small><sub>b</sub></small> and molecule in the data <span class="italic">R</span><small><sub><span class="italic">i</span></sub></small>.</p>
          <p class="otherpara">Burak Himmetoglu<a title="Select to navigate to references" href="#cit512"><sup><span class="sup_ref">512</span></sup></a> constructed a dataset incorporated from PubChem comprising of the electronic structures of 16<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>242 molecules composed of CHNOPS atoms. Having constructed the Coulomb matrices as in ref. <a title="Select to navigate to references" href="#cit511">511</a> defined by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn113"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t272_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t272.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t272.gif"/></a></td><td class="rightEqn">(113)</td></tr></table>where, the atomic numbers are denoted by <span class="italic">Z</span>, and <span class="italic">R</span> is their corresponding positions. Design matrices using the eigenvalues of the Coulomb matrices are constructed and two types of ML approaches are used to predict the molecular ground state energies. First, boosted regression trees (Theoretical framework discussed in Section 3.2.5) and then ANNs are used, and their performances are compared.</p>
          <p class="otherpara">Geometry optimization is a crucial task, which directly impacts the electronic structure calculations. The total energy calculations can prove to be quite expensive depending on the choice of the electronic structure method. Hence, the number of evaluations of the potential energy surface has to be reduced considerably. A novel gradient-based geometry optimizer was developed by Denzel and Kästner,<a title="Select to navigate to references" href="#cit513"><sup><span class="sup_ref">513</span></sup></a> that exploits Gaussian Process Regression or GPR (a theoretical framework discussed in Section 3.2.8) to find the minimum structures. By comparing Matérn kernel with the squared exponential kernel, the authors show that the performance is better when Matérn kernel is used. The performance of their optimizer was tested on a set of 25 test cases along with a higher dimensional molybdenum system, molybdenum amidato bisalkyl alkylidyne complex, and it was shown that the GPR based approach can handle the optimization quite well.</p>
          <p class="otherpara">In the work by Carleo and Troyer,<a title="Select to navigate to references" href="#cit346"><sup><span class="sup_ref">346</span></sup></a> it was shown that by representing the many-body wavefunction in terms of ANNs, the quantum many-body problem can be solved. They used this idea to solve for the ground states and describe the time evolution of the quantum spin models, <span class="italic">viz.</span> transverse field Ising, and anti-ferromagnetic Heisenberg models. Expanding the quantum many-body state <span class="italic">ψ</span> in the basis |<span class="italic">x</span>〉:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn114"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t273_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t273.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t273.gif"/></a></td><td class="rightEqn">(114)</td></tr></table>where, <span class="italic">ψ</span>(<span class="italic">x</span>) is the wavefunction represented in terms of a restricted Boltzmann machine (RBM), which is the ANN architecture that was used in this study (<a title="Select to navigate to figure" href="#imgfig30">Fig. 30a</a>). The description of neural-network quantum states (NQSs) results in the wavefunction <span class="italic">ψ</span>(<span class="italic">x</span>) to be written as <span class="italic">ψ</span>(<span class="italic">x</span>;<span class="italic">θ</span>), where <span class="italic">θ</span> denotes the tunable parameters of the neural network. The quantum states can now be expressed as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn115"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t274_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t274.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t274.gif"/></a></td><td class="rightEqn">(115)</td></tr></table>where the values of <span class="italic">σ</span><small><sup><span class="italic">z</span></sup></small><small><sub><span class="italic">j</span></sub></small> &amp; <span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> ∈ {+1,−1}, <span class="italic">a</span><small><sub><span class="italic">j</span></sub></small> &amp; <span class="italic">b</span><small><sub><span class="italic">i</span></sub></small> are the bias parameters corresponding to the visible and hidden layer, respectively, and <span class="italic">w</span><small><sub><span class="italic">ij</span></sub></small> is the weight associated with the connections between <span class="italic">σ</span><small><sup><span class="italic">z</span></sup></small><small><sub><span class="italic">j</span></sub></small> &amp; <span class="italic">h</span><small><sub><span class="italic">i</span></sub></small> (see <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(a)</a>). For a given Hamiltonian, the average energy written as a statistical expectation value over |<span class="italic">ψ</span><small><sub>M</sub></small>(<span class="italic">x</span>;<span class="italic">θ</span>)|<small><sup>2</sup></small> distribution is computed. For a specific set of parameters, samples from the |<span class="italic">ψ</span><small><sub>M</sub></small>(<span class="italic">x</span>;<span class="italic">θ</span>)|<small><sup>2</sup></small> distribution are taken <span class="italic">via</span> Markov Chain Monte Carlo (MCMC) and the gradients of the expectation value are calculated. With the gradients known, the parameters are optimized in order to model the ground states of the Hamiltonian. The accuracy of the results were extremely high in comparison to the exact values. Recently Choo <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit514"><sup><span class="sup_ref">514</span></sup></a> have also extended the method to model ground states of molecular Hamiltonians. The protocol maps the molecular Hamiltonian onto the basis of Pauli strings using any of the fermion-qubit mapping like Jordan–Wigner or Bravi–Kitaev encoding that is routinely employed in quantum computing.<a title="Select to navigate to references" href="#cit515"><sup><span class="sup_ref">515</span></sup></a> Thereafter the wave-function as described in eqn (5.3.1) is used to variationally minimize the ground state energy by drawing samples using variational Monte Carlo (VMC).<a title="Select to navigate to references" href="#cit514"><sup><span class="sup_ref">514</span></sup></a> Several insightful features like the fact that support of the probability distribution in the space of spin configurations is peaked over certain dominant configurations only near the Hartree–Fock state, out-performance of RBM ansatz over more traditional quantum chemistry methods like CCSD(T) and Jastrow ansatz, the efficacy of the various fermion-qubit mapping techniques in classical quantum chemistry simnulations, <span class="italic">etc.</span> were elucidated.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig30"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f30_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f30.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f30.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 30 </b> <span id="fig30"><span class="graphic_title">The RBM architecture evolution. (a) The RBM architecture consisting of a visible layer and a hidden layer. The neurons in the visible layer represented encode a variable <span class="italic">σ</span><small><sup><span class="italic">z</span></sup></small><small><sub><span class="italic">i</span></sub></small>∈ {1, −1} have biases denoted by <span class="italic">a</span><small><sub><span class="italic">i</span></sub></small> and the neurons in the hidden layer encode a variable <span class="italic">h</span><small><sub><span class="italic">j</span></sub></small> ∈ {1,−1} have biases denoted by <span class="italic">b</span><small><sub><span class="italic">j</span></sub></small>. The weights associated with each connection between the visible node and the hidden node are denoted by <span class="italic">w</span><small><sub><span class="italic">ij</span></sub></small> (schematic of RBM as illustrated in ref. <a title="Select to navigate to references" href="#cit346">346</a>). (b) The three layered RBM architecture with the first two layers and their corresponding parameters the same as in (a). The third layer has one neuron which encodes a variable <span class="italic">s</span>(<span class="italic">x</span>) (see <a title="" href="#eqn121">eqn (121)</a> for the case with (<span class="italic">e</span>,<span class="italic">f</span><small><sub><span class="italic">i</span></sub></small>) = (0,0) ∀<span class="italic">i</span>) and is known as the sign layer. The bias unit for the sign layer is represented by <span class="italic">c</span><small><sub><span class="italic">i</span></sub></small> and the weights associated with each connection between the visible node and the sign node is denoted by <span class="italic">d</span><small><sub><span class="italic">i</span></sub></small> (schematic of RBM as illustrated in ref. <a title="Select to navigate to references" href="#cit517">517</a>). The sign layer does not share any connection with the hidden layer. (c) The three layered RBM architecture with the third layer consisting of two units. <span class="italic">c</span> and <span class="italic">e</span> denote the bias parameters of the unit representing the real part of the complex layer, and the unit representing the complex part of the complex layer, respectively; <span class="italic">f</span><small><sub><span class="italic">i</span></sub></small> indicates the weights corresponding to the connections between <span class="italic">σ</span><small><sup><span class="italic">z</span></sup></small><small><sub><span class="italic">i</span></sub></small> and the unit representing the complex part of the complex layer (schematic of RBM as illustrated in ref. <a title="Select to navigate to references" href="#cit347">347</a>, <a title="Select to navigate to references" href="#cit518">518</a> and <a title="Select to navigate to references" href="#cit519">519</a>). (d) The quantum circuit to sample Gibbs distribution on quantum hardware. In general for <span class="italic">n</span> neurons in the visible layer and <span class="italic">m</span> neurons in the hidden layer, this quantum circuit would require <span class="italic">m</span> × <span class="italic">n</span> ancilla qubits as well. The circuit thus shown is for a special case of (<span class="italic">n</span> = <span class="italic">m</span> = 2). The circuit is responsible for simulating the amplitude field <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t275_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t275.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t275.gif"/></a> by sampling from the distribution <span class="italic">P</span>(<span class="bold">x</span>) as defined in <a title="" href="#eqn120">eqn (120)</a> (schematic of a quantum circuit as illustrated in ref. <a title="Select to navigate to references" href="#cit347">347</a> and <a title="Select to navigate to references" href="#cit519">519</a>).</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">This NQS approach was then extended by Saito<a title="Select to navigate to references" href="#cit516"><sup><span class="sup_ref">516</span></sup></a> to compute the ground states of the Bose–Hubbard model. Here, the quantum state is expanded by the Fock states. One-dimensional optical lattice with 11 sites and 9 particles, and a two-dimensional lattice with 9 × 9 sites and 25 particles were studied. The optimization scheme is similar to that in ref. <a title="Select to navigate to references" href="#cit346">346</a> and the ground states calculated for the 1D and 2D cases were shown to be in good agreement with those obtained from exact diagonalization and Gutzwiller approximation, respectively.</p>
          <p class="otherpara">In the work by Coe,<a title="Select to navigate to references" href="#cit520"><sup><span class="sup_ref">520</span></sup></a> NNs were used to select important configurations in the configuration interaction (CI) method. By using just a single hidden layer, with binary occupations of the spin orbitals in the desired configurations, the transformed co-efficient of the configuration that fitted to the co-efficients in the training set is predicted. The important configurations of stretched CO and Co at its equilibrium geometry were shown to have accurate predictions. This work was extended in their follow-up paper,<a title="Select to navigate to references" href="#cit521"><sup><span class="sup_ref">521</span></sup></a> in which the potential energy curves for N<small><sub>2</sub></small>, H<small><sub>2</sub></small>O, and CO were computed with near-full CI accuracy.</p>
          <p class="otherpara">Custódio <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit522"><sup><span class="sup_ref">522</span></sup></a> developed a feedforward neural network (a theoretical framework discussed in Section 3.3) to obtain a functional form for calculations pertaining to inhomogeneous systems within DFT and Local Spin Density Approximation (LSDA) framework. The network consists of an input layer with 3 neurons, a hidden layer with 20 neurons, and an output layer consisting of one neuron. The network was trained on 20<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>891 numerically exact Lieb–Wu results for 1000 epochs. The authors test their network on non-magnetic and magnetic systems and through their results claim that the neural network functional is capable of capturing the qualitative behavior of energy and density profiles for all the inhomogeneous systems. In another closely related study,<a title="Select to navigate to references" href="#cit523"><sup><span class="sup_ref">523</span></sup></a> the authors attempt to construct the many-body wavefunction directly from 1D discretized electron density using a feed-forward neural network. The network was trained by a supervised learning scheme with the infidelity of the procured wave-function and the target wave-function within the training data. The model showed excellent performance for the Fermni–Hubbard Hamiltonian in both the metallic and the Mott-insulating phases. To bypass the need to construct the exponentially scaling many-body wavefunction, the authors also construct the density–density two-point auto-correlation function with remarkable accuracy. From such auto-correlation function, the power-law scaling parameters of different phases can be obtained.<a title="Select to navigate to references" href="#cit523"><sup><span class="sup_ref">523</span></sup></a></p>
          <p class="otherpara">A deep neural network titled SchNet was introduced<a title="Select to navigate to references" href="#cit524"><sup><span class="sup_ref">524</span></sup></a> and SchNOrd was introduced<a title="Select to navigate to references" href="#cit343"><sup><span class="sup_ref">343</span></sup></a>, to predict the wavefunction on a local basis of atomic orbitals. By treating a molecule as a collection of atoms and having a descriptor for each atom, the output properties being predicted are a sum of all these atomic descriptions. The inputs to the network are the atom types and the position of these atoms in the Cartesian coordinates. The atom types are embedded in random initializations and are convoluted with continuous filters in order to encode the distances of these atoms. The interaction layers encode the interaction between different atoms, which essentially dictates the features of these atoms. These features are now input to a factorized tensor layer, which connects the features into pairwise combinations representing every pair of atomic orbitals. Multi-layer perceptrons (a theoretical framework discussed in Section 3.3) are then used to describe the pair-wise interactions within these pair of orbitals. This model was shown to predict with good accuracy the energies, Hamiltonians, and overlap matrices corresponding to water, ethanol, malondialdehyde, and uracil.</p>
          <p class="otherpara">Hermann <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit525"><sup><span class="sup_ref">525</span></sup></a> extended the work by Schutt <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit524"><sup><span class="sup_ref">524</span></sup></a> by using SchNet for the representation of electrons in molecular environments. The representation of the wavefunction through their neural network titled PauliNet in association with the training done using variational Monte Carlo approaches very high accuracy for energies of Hydrogen molecules (H<small><sub>2</sub></small>), lithium hydride (LiH), beryllium (Be), boron (B), and a linear chain of hydrogen atoms (H<small><sub>10</sub></small>). The authors also investigated the scaling of PauliNet with the number of determinants and with system size on Li<small><sub>2</sub></small>, Be<small><sub>2</sub></small>, B<small><sub>2</sub></small>, and C<small><sub>2</sub></small> and state the high accuracy in comparison to diffusion Monte Carlo (DMC) can be achieved quite fast. Having studied the energies for systems that have benchmark results, the authors move on to high accuracy prediction of the minimum and transition-state energies of cyclobutadiene.</p>
          <p class="otherpara">Faber <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit526"><sup><span class="sup_ref">526</span></sup></a> introduced representations of atoms as a sum of multidimensional Gaussians in a given chemical compound for a Kernel-Ridge Regression (KRR) (a theoretical framework discussed in Section 3.2.8) based model to predict the electronic properties having learnt them from several datasets. By deriving analytical expressions for the distances between chemical compounds, the authors use these distances to make the KRR based model to learn the electronic ground state properties. In a follow-up paper from the same group,<a title="Select to navigate to references" href="#cit527"><sup><span class="sup_ref">527</span></sup></a> Christensen <span class="italic">et al.</span> provide a discretized representation as opposed to comparing atomic environments by solving the aforementioned analytical expression. In this work, the authors use KRR to learn the energy of chemical compounds and three other regressors, <span class="italic">viz.</span>, operator quantum machine learning (OQML), GPR, and gradient-domain machine learning (GDML), are reviewed for the learning of forces and energies of chemical compounds.</p>
          <p class="otherpara">In order to have an ML model that can generalize well in large datasets and have efficient tranferability, Huang and Lilienfeld<a title="Select to navigate to references" href="#cit528"><sup><span class="sup_ref">528</span></sup></a> introduced the atom-in-molecule (amon) approach where fragments of such amons with increasing size act as training data to predict molecular quantum properties. Considering only two and three-body interatomic potential based representations of amons, a subgraph matching procedure is adopted, which iterates over all the non-hydrogen atoms in the molecule, and identifies the relevant amons. The relevent amons are identified by converting the molecular geometry to a molecular graph with vertices specified by the nuclear charge of atoms and bond orders, then verifying if their hybridization state is preserved or not, and if such subgraphs are isomorphic to other identified subgraphs with some additional checks. Such amons are selected and sorted based on size.</p>
          <p class="otherpara">Obtaining the right material that can be explored experimentally in a large database can prove to be a daunting task but provides very rewarding results. Multilayer Perceptrons (MLPs) were used by Pyzer-Knapp <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit529"><sup><span class="sup_ref">529</span></sup></a> in the High Throughput Virtual Screening (HTVS) of the Harvard Clean Energy Project, which was developed for the discovery of organic photovoltaic materials. The network consisting of linear input and output layers, three hidden layers with 128, 64, and 32 nodes, was trained on 200<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>000 molecules and an additional 50<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>000 molecules were considered to make up the validation set.</p>
          <p class="otherpara">Having performed an FT based high throughput screening, Choudhary <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit530"><sup><span class="sup_ref">530</span></sup></a> trained a Gradient Boosting Decision Tree (GBDT) (a theoretical framework discussed in Section 3.2.5) based supervised learning model on 1557 descriptors obtained from classical force-field inspired descriptors CFID. The model was used to classify the materials based on spectroscopic limited maximum efficiency (SLME) to be greater than 10% or not. The authors use this classification to prescreen over a million materials available through large crystallographic and DFT databases, with the goal of accelerating the prediction of novel high-efficiency solar cell materials.</p>
          <p class="otherpara">By using a series of random forest models (a theoretical framework discussed in Section 3.2.5) trained at different threshold temperatures, Stanev <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit531"><sup><span class="sup_ref">531</span></sup></a> showed that the materials in the SuperCon database can be classified into two classes, above or below a critical temperature (<span class="italic">T</span><small><sub>c</sub></small>) of 10 K. The features required to be learnt by the model are obtained by using Materials Agnostic Platform for Informatics and Exploration (Magpie) based predictors from SuperCon along with a set of 26 predictors from AFLOW online Repositories. The authors also develop a regression model to predict the <span class="italic">T</span><small><sub>c</sub></small> of cuprate, iron-based and low-<span class="italic">T</span><small><sub>c</sub></small> materials.</p>
          <p class="otherpara">Barett <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit532"><sup><span class="sup_ref">532</span></sup></a> showed the expressibility power of representing the wavefunction ansatz using an Autoregressive Neural Network (ARN). ARNs are a class of generative, sequential models that are feedforward, where observations from the previous time-steps are used to predict the value at the current time step. By developing an architecture with several sub-networks, each made up of a multi-level perceptron, to model the amplitudes and a separate sub-network to model the phase of the wavefunction, the authors go on to introduce a unique sampling procedure that scales with the number of unique configurations sampled and describe the efficiency of ARNs by computing the ground states of various molecules, achieving standard full configurational interaction results.</p>
          <p class="otherpara">In order to discover non-centrosymmetric oxides, Prasanna V. Balachandran <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit533"><sup><span class="sup_ref">533</span></sup></a> developed a methodology combining group theroretic approach, ML, and DFT and applied it towards layered Ruddlesden–Popper oxides. Using group theory to establish a dataset consisting of 3253 total chemical compositions and performing PCA (a theoretical framework discussed in Section 3.2.3) and classification learning, the authors identify 242 relevant compositions that displayed potential for NCS ground state structures. Then, taking advantage of DFT, 19 compositions were predicted that were suggested for experimental synthesis. Autoencoders (a theoretical framework discussed in Section 3.3.4) are known for their ability to reduce the dimenisonality of the problem at hand and can be used to design molecules with specific desirable properties. This was used by Gómez-Bombarelli <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit534"><sup><span class="sup_ref">534</span></sup></a> by coupling an autoencoder with a neural network to generate molecular structures along with predicting the properties of molecules, which were represented by points in the latent space. Neural networks have also branched into the domain of generating complex crystalline nanomaterials, thanks to the work by Baekjun Kim <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit535"><sup><span class="sup_ref">535</span></sup></a> In this work, the authors base their approach with the use of Wasserstein GAN (WGAN) (a theoretical framework discussed in Section 3.3.6) with gradient penalty on the loss function on the critic (which is a renamed discriminator specific to the WGANs). By considering a training set consisting of 31<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>713 known zeolites, the network takes energy and materials dimensions (with the materials grid subdivided into silicon and oxygen atom grids) as the input, to produce 121 crystalline porous materials. Considering methane potential energy to be the energy dimension, and a user defined range from 18 kJ mol<small><sup>−1</sup></small> to 22 kJ mol<small><sup>−1</sup></small>, the authors were able to successfully demonstrate inverse design of zeolites. Since the combinatorial space of multi-principal element alloys (MPEAs) is extensive, it becomes necessary to have a reliable method that accurately and rapidly predicts the intermetallics and their formation enthalpies. In accordance with this necessity, an ML model using GPR (a theoretical framework discussed in Section 3.2.8) with a sum kernel, which consists of the square exponential kernel and a white noise kernel, was developed.<a title="Select to navigate to references" href="#cit536"><sup><span class="sup_ref">536</span></sup></a> In this work, the ML model is trained on 1538 stable binary intermetallics from the Materials Project database and uses elemental properties as descriptors in order to learn the distribution that maps these descriptors to the formation enthalpies. By doing so, the authors show that stable intermetallics can be predicted using this method. They also perform transfer learning to predict ternary intermetallics in the aforementioned database, thereby informing about the generalizability of the model. With growing interest in superhard materials for various industrial applications, Efim Mazhni <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit537"><sup><span class="sup_ref">537</span></sup></a> developed a neural network on graphs model, with a linear embedding layer, followed by three convolutional layers, a pooling layer, and two linear transformations with softmax activation layers, to calculate hardness and fracture toughness. By training their network on the database of crystal structures by Materials Project, consisting of 8033 structures, the authors predict the bulk modulus and the shear modulus, from which Young's modulus and Poisson's ratio is obtained.</p>
          <p class="otherpara">Tensor networks have been discussed in Section 3.4. Tensor networks have been applied to solve numerous problems in physics, chemistry and materials science (to review tensor network algorithms for simulating strongly correlated systems refer ref. <a title="Select to navigate to references" href="#cit538">538</a>). Recently Kshetrimayum <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit539"><sup><span class="sup_ref">539</span></sup></a> published a report developing tensor network models for strongly correlated systems in two spatial dimensions and extending them to more realistic examples. The specific material the authors chose is that of a quantum magnet Ca<small><sub>10</sub></small>Cr<small><sub>7</sub></small>O<small><sub>28</sub></small> which is known to show properties of a quantum spin liquid in inelastic neutron scattering experiments.<a title="Select to navigate to references" href="#cit540"><sup><span class="sup_ref">540</span></sup></a> The material possesses a distorted breathing bilayer Kagome lattice crystal structure that is composed of Cr<small><sup>5+</sup></small> ions with spin-1/2 moments. Despite having even number of spin-1/2 Cr<small><sup>5+</sup></small> ions, the interactions lack the tendency to form a singlet state. The description of the lattice is shown in <a title="Select to navigate to figure" href="#imgfig31">Fig. 31</a>. The two Kagome layers are different from each other and each of them consists of two non-equivalent alternating triangles. The Hamiltonian of this compound consists of five distinct Heisenberg type interactions and is as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn116"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t276_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t276.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t276.gif"/></a></td><td class="rightEqn">(116)</td></tr></table>where 〈<span class="italic">i</span>,<span class="italic">j</span>〉 corresponds to the nearest-neighbor interaction between Cr<small><sup>5+</sup></small> ions only in the lattice. The first term with an interaction strength of <span class="italic">J</span><small><sub>inter</sub></small> is ferromagnetic and defines the coupling between two layers in the lattice (see <a title="Select to navigate to figure" href="#imgfig31">Fig. 31</a>). The second term <span class="italic">J</span><small><sub><span class="italic">Δ</span>1</sub></small> is responsible for coupling spins within the ‘up’-triangles in first layer and is ferromagnetic too. The third term <span class="italic">J</span><small><sub>∇1</sub></small> is similar to the second term but for the ‘down’triangles and is anti-ferromagnetic. Terms with interaction matrix elements labelled by <span class="italic">J</span><small><sub><span class="italic">Δ</span>2</sub></small> and <span class="italic">J</span><small><sub>∇2</sub></small> are similar to the first two terms but for the second layer. The combination of ferromagnetic and anti-ferromagnetic interactions leads to spin-frustration. The magnetization curve of the material along the <span class="italic">z</span>-direction is obtained by adding the following term:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn117"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t277_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t277.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t277.gif"/></a></td><td class="rightEqn">(117)</td></tr></table>where <span class="italic">g</span><small><sub>s</sub></small> ≈ 2 and <span class="italic">μ</span><small><sub>B</sub></small> is the Bohr magneton and is equal to 5.7883818012 × 10<small><sup>−5</sup></small> eV per Tesla and <span class="italic">B</span><small><sub><span class="italic">z</span></sub></small> is the strength of the external field along the <span class="italic">z</span>-direction. The ground state of the above model was investigated using the Projected Entangled Simplex State (PESS) algorithm in the thermodynamic limit.<a title="Select to navigate to references" href="#cit541"><sup><span class="sup_ref">541</span></sup></a> as a function of bond dimensions. Trends in the ground state energy, heat capacity and magnetization and magnetic susceptibility indicated a gap-less spectrum of a spin-liquid.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig31"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f31_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f31.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f31.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 31 </b> <span id="fig31"><span class="graphic_title">The crystal structure showing only the Cr<small><sup>5+</sup></small> ions in Ca<small><sub>10</sub></small>Cr<small><sub>7</sub></small>O<small><sub>28</sub></small>. Five different interaction matrix elements as illustrated in <a title="" href="#eqn116">eqn (116)</a> are depicted. Reprinted from A. Kshetrimayum, C. Balz, B. Lake and J. Eisert, Tensor network investigation of the double layer Kagome compound Ca<small><sub>10</sub></small>Cr<small><sub>7</sub></small>O<small><sub>28</sub></small>, <span class="italic">Ann. Phys.</span>, 2020, <span class="bold">421</span>, 168292, Copyright (2022), with permission from Elsevier.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">The DMRG algorithm (see Section 3.4.5) has been extremely useful in finding the ground state of one-dimensional systems. Despite the success of DMRG, the method is not suitable for simulating a high-dimensional system. It requires projecting the multi-dimensional state into one dimension which causes the computation time to grow many-fold. Murg <span class="italic">et al.</span> in their report<a title="Select to navigate to references" href="#cit542"><sup><span class="sup_ref">542</span></sup></a> demonstrate a general approach to simulate ground states of systems in higher dimension using the tree tensor network ansatz (see Section 3.4.2). By exploiting the advantages of correlation scaling of TTN (correlations in TTN only deviates polynomially from the mean-field value compared to MPS which deviates exponentially)<a title="Select to navigate to references" href="#cit542"><sup><span class="sup_ref">542</span></sup></a> they efficiently simulate the two-dimensional Heisenberg model and interacting spinless fermions on a square lattice. They also demonstrate its working on the simulation of the ground state of Beryllium atom. Another study by Barthel <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit543"><sup><span class="sup_ref">543</span></sup></a> proposed an ansatz, Fermionic Operator Circuit (FOC), to simulate fermionic systems in higher dimension by mapping the fermionic operators onto known tensor network architectures, namely, MPS, PEPS, and MERA (see Sections 3.4.1, 3.4.3 and 3.4.4, respectively). FOC is composed of products of fermionic operators and are known to be parity symmetric. The biggest challenge in formulating a FOC is to manage the sign factors while reordering the fermionic operators. Authors propose an efficient scheme to contract an FOC which computationally scale similar to the contraction of standard TN architectures. The efficiency of the scheme emerges from the fact that while contracting Jordan–Wigner string within a causal cone (specifically referring to the MERA based FOC), the strings outside it are not accounted. The scheme provides a method to formulate fermionic problems in the tensor network notation so that they can be benefited from the existing tensor network algorithms. In another study,<a title="Select to navigate to references" href="#cit544"><sup><span class="sup_ref">544</span></sup></a> authors propose a PEPS based algorithm to classify quantum phases of matter in a fermionic system with a specific emphasis on topological phases. Authors introduce a Grassman number tensor network ansatz to study the exemplary Toric code model and fermionic twisted quantum double model which support topological phases. While working with fermionic problems in quantum chemistry, the choice of the most suitable orbital is very crucial. Author in the report<a title="Select to navigate to references" href="#cit545"><sup><span class="sup_ref">545</span></sup></a> propose a tool to optimize the orbitals using a tensor network.</p>
          <p class="otherpara">Thermal state or Gibbs state provides efficient description of systems in equilibrium (see ref. <a title="Select to navigate to references" href="#cit546">546</a> for a detailed review). Simulating these states at a finite temperature for higher dimensions is computationally challenging. Authors in the report<a title="Select to navigate to references" href="#cit547"><sup><span class="sup_ref">547</span></sup></a> propose an algorithm based on projected entangled pair states (see Section 3.4.3) to compute the thermal state of two-dimensional Ising model and Bose–Hubbard model on infinite square lattice. They use a method akin to annealing, <span class="italic">i.e.</span>, cool down the state from a high temperature to attain the desired finite-temperature state. A thermal state can be described as, <span class="italic">ρ</span> = e<small><sup>−<span class="italic">βH</span></sup></small>, where <span class="italic">H</span> is the Hamiltonian of the system and <span class="italic">β</span> is the inverse temperature. In the infinite temperature limit, the thermal state is described by just an identity matrix. The evolution of state can be performed by the operator e<small><sup>−Δ<span class="italic">βH</span></sup></small> which can be approximated by Suzuki–Trotter expansion. The final state's description reads as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn118"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t278_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t278.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t278.gif"/></a></td><td class="rightEqn">(118)</td></tr></table>where <span class="italic">m</span>Δ<span class="italic">β</span> = <span class="italic">β</span> (<span class="italic">m</span> is the number of temperature steps). The state evolution operator is represented in the Projected Entangled Pair Operator (PEPO) notation.</p>
          <p class="otherpara">Moving a step further, there are states which do not thermalize due to many-body localization (MBL) and studying them is both interesting and difficult. In the report,<a title="Select to navigate to references" href="#cit548"><sup><span class="sup_ref">548</span></sup></a> authors propose an algorithm based on infinite PEPSs to simulate time evolution of disordered spin-1/2 XXZ Hamiltonian on a square lattice. They initialize the system in the Néel state |<span class="italic">ψ</span><small><sub>0</sub></small>〉 and update it with the time evolution operator as<br/><table><tr><td class="eqnText">|<span class="italic">ψ</span>(<span class="italic">t</span>)〉 = e<small><sup>−<span class="italic">iHt</span></sup></small>|<span class="italic">ψ</span><small><sub>0</sub></small>〉.</td></tr></table>They estimated the expectation value of the local particle number to determine the phases of the system for varying disorder dimensions and disorder strengths.</p>
        </div>
        <div>
          
          <span id="sect5919"/><span class="c_heading_indent">5.3.2 Quantum-computing enhanced machine learning techniques. </span>
          <span>The above studies involved the usage of ML in its classical sense. Implementing neural networks where a quantum computer is involved to either in part (hybrid classical-quantum approach) or in entirety is suspected to achieve speed-ups that could be very beneficial to many fields of study, in particular chemistry. A parameterized quantum circuit-based quantum-classical hybrid neural network was proposed by Xia <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit206"><sup><span class="sup_ref">206</span></sup></a> This study uses the fundamental idea that a neural network is composed of a linear part and a non-linear activation part (a theoretical framework discussed in Section 3.3.1). The linear part is now made of a quantum layer and the non-linear part is composed of measurements, which is also the classical part of the hybrid quantum-classical method. The quantum layer is constructed using parameterized quantum circuit (PQC). Having encoded the input data as quantum states, the PQC is optimized to approximate the output state, and the outputs are extracted as the expectation values by measurements. The encoding of the input along with the usage of PQC and computing the outputs can be repeated several times to construct a hybrid multi-layer neural network and is trained in an unsupervised fashion for the ground state energies on a few bond lengths. The trained network is now used to predict the energies for other bond lengths. The authors show high accuracy for obtaining the ground state energies of H<small><sub>2</sub></small>, LiH, and BeH<small><sub>2</sub></small>.</span>
          <p class="otherpara">Xia and Kais<a title="Select to navigate to references" href="#cit517"><sup><span class="sup_ref">517</span></sup></a> also developed a hybrid quantum machine learning algorithm, which involves a three-layered RBM approach as shown in <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(b)</a>. The first two layers encode the amplitude field similar to ref. <a title="Select to navigate to references" href="#cit346">346</a>, while the third layer consisting of one unit is to deal with the lack of sign (±1) in the electronic structure problems. Now the ansatz for state-vector |<span class="italic">ψ</span>〉 is given by:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn119"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t279_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t279.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t279.gif"/></a></td><td class="rightEqn">(119)</td></tr></table>where<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn120"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t280_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t280.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t280.gif"/></a></td><td class="rightEqn">(120)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn121"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t281_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t281.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t281.gif"/></a></td><td class="rightEqn">(121)</td></tr></table></p>
          <p class="otherpara">In order to simulate the distribution <span class="italic">P</span>(<span class="bold">x</span>), a quantum algorithm is proposed to sample from Gibb's distribution. The quantum circuit mainly consists of two types of operations:</p>
          <p class="otherpara">• Single-qubit <span class="italic">R</span><small><sub><span class="italic">y</span></sub></small> gates that correspond to a rotational operation whose angle is determined by the bias parameters <span class="italic">a</span><small><sub><span class="italic">i</span></sub></small> (visible) and <span class="italic">b</span><small><sub><span class="italic">j</span></sub></small> (hidden) and is responsible for simulating the non-interaction of the distribution in <a title="" href="#eqn120">eqn (120)</a>.</p>
          <p class="otherpara">• A three-qubit gate C1–C2–R<small><sub><span class="italic">y</span></sub></small> (efficiently representable by two-qubit and single-qubit operations) that is a controlled–controlled–rotation whose angle is determined by the connection parameter <span class="italic">w</span><small><sub><span class="italic">ij</span></sub></small> and is responsible for simulating the interaction between the visible and hidden layers of the distribution in <a title="" href="#eqn120">eqn (120)</a>. The target qubit of each such controlled operations is an ancillary qubit which was re-initialized and re-used post-measurement.</p>
          <p class="otherpara">A Boltzmann distribution for all configurations of the visible and hidden layers can be generated through the quantum circuit similar to as shown in <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(d)</a>. This algorithm is based on sequential applications of controlled-rotation operations, and tries to calculate the interacting part of the distribution <span class="italic">P</span>(<span class="bold">x</span>) with an ancillary qubit. The ancillary qubit was thereafter measured and conditioned on the measurement results sampling from <span class="italic">P</span>(<span class="bold">x</span>) is deemed successful. With <span class="italic">P</span>(<span class="bold">x</span>) computed through the quantum circuit and <span class="italic">s</span>(<span class="bold">x</span>) computed classically, |<span class="italic">ψ</span>〉 can now be minimized by using a gradient descent. Having described the method, the authors show the results corresponding to the ground states of H<small><sub>2</sub></small>, LiH, and H<small><sub>2</sub></small>O molecules (<a title="Select to navigate to figure" href="#imgfig32">Fig. 32</a>).</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig32"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f32_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f32.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f32.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 32 </b> <span id="fig32"><span class="graphic_title">(a–c) Ground state energies of H<small><sub>2</sub></small>, LiH, and H<small><sub>2</sub></small>O, respectively. (d) Ground state energy of LiH having used a warm starting procedure. The inner panels indicate the error with respect to exact diagonalization values. The panel is reproduced from ref. <a title="Select to navigate to references" href="#cit517">517</a> with permission under Creative Commons CC BY license.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">An extension to the work by Xia <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit517"><sup><span class="sup_ref">517</span></sup></a> was proposed in the study by Kanno,<a title="Select to navigate to references" href="#cit518"><sup><span class="sup_ref">518</span></sup></a> wherein an additional unit in the third layer of the RBM was introduced in order to tackle periodic systems and take into account the complex value of the wavefunction. So, the sign layer contains two units, one to encode the real part and the other to encode the complex part of the wavefunction as shown in <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(c)</a>. The authors construct the global electronic structure using DFT, then an effective model with just the target band. By using the maximally localized Wannier function for basis construction for the effective model, a Hubbard Hamiltonian for the target bands is built. The choice of the material is graphene with the target band being the valence band contributed by the 2p<small><sub><span class="italic">z</span></sub></small> orbitals of the two carbon atoms within an unit cell. Using the algorithm by Xia and Kais,<a title="Select to navigate to references" href="#cit517"><sup><span class="sup_ref">517</span></sup></a> this study shows the pristine valence band of graphene in the presence and absence of band splitting for a finite repulsion <span class="italic">U</span> parameter within the Hubbard Hamiltonian.<a title="Select to navigate to references" href="#cit518"><sup><span class="sup_ref">518</span></sup></a></p>
          <p class="otherpara">In the above, ref. <a title="Select to navigate to references" href="#cit517">517</a> and <a title="Select to navigate to references" href="#cit518">518</a>, the efficacy of the quantum circuit was tested by simulating it on a classical computer. To benchmark the performance on an actual quantum device, repeated use of a single ancilla qubit would not be operationally convenient. A slightly modified variant of the quantum circuit with (<span class="italic">m</span> × <span class="italic">n</span>) qubits in the ancilla register has been used thereafter to act as targets of the controlled operations<a title="Select to navigate to reference" href="#cit347"><sup><span class="sup_ref">347,519</span></sup></a> as shown in <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(d)</a>. One must note that <span class="italic">m</span> denotes the number of neurons in the hidden layer and <span class="italic">n</span> denotes the number of neurons in the visible layer. Sureshbabu <span class="italic">et al.</span> in ref. <a title="Select to navigate to references" href="#cit519">519</a> uses this circuit to benchmark implementation on two 27 qubit IBM-Q devices for the valence bands (within the tight-binding framework) of hexagonal boron nitride (h-BN) and monolayer graphene, respectively. A Hubbard Hamiltonian similar to ref. <a title="Select to navigate to references" href="#cit518">518</a> was used to explore band-splitting as shown in <a title="Select to navigate to figure" href="#imgfig33">Fig. 33(a and b)</a>. Excellent results were obtained even on an actual NISQ device through the use of measurement-error mitigation and repeated warm starting with well converged results for nearby <span class="italic">k</span> points in the energy trajectory.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig33"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f33_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f33.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f33.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 33 </b> <span id="fig33"><span class="graphic_title">Band structures of 2D materials as computed from RBM implementation on classical computer, quantum simulator (qasm) and actual IBM-Q devices.<a title="Select to navigate to references" href="#cit519"><sup><span class="sup_ref">519</span></sup></a> (a) Valence band of graphene with the Hubbard <span class="italic">U</span> interaction = 0 eV. (b) Valence band of graphene with the Hubbard <span class="italic">U</span> interaction = 9.3 eV (Reprinted (adapted) with permission from S. H. Sureshbabu, M. Sajjan, S. Oh and S. Kais, <span class="italic">J. Chem. Inf. Model.</span>, 2021, <span class="bold">61</span>(6), 2667. Copyright 2021 American Chemical Society). (c) Valence (VB) and conduction bands (CB) of MoS<small><sub>2</sub></small> obtained along with the infidelity of the target state learnt by the network in each flavor of RBM implementation. CB is obtained through the constrained minimization procedure.<a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> (d) The energy, constraint violation error, energy error and state infidelity comparison corresponding to symmetry filtering for MoS<small><sub>2</sub></small> with the operator (<span class="italic">L</span><small><sup>2</sup></small>) using the constrained minimization procedure.<a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> The eigenspace of this operator is labelled by <span class="italic">ω</span> = 0. (e) The energy, constraint violation error, energy error and state infidelity comparison corresponding to symmetry filtering for MoS<small><sub>2</sub></small> with the operator (<span class="italic">L</span><small><sup>2</sup></small>) and <span class="italic">ω</span> = 4 a.u. (Reprinted (adapted) with permission from M. Sajjan, S. H. Sureshbabu, and S. Kais, <span class="italic">J. Am. Chem. Soc.</span>, 2021, DOI: 10.1021/jacs.1c06246. Copyright 2021 American Chemical Society.)</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">The classical-quantum hybrid algorithms described above focus their attention on only computing the ground states of molecules and materials. In the work by Sajjan <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> the authors use the idea of constrained optimization to obtain any arbitrary energy eigenstates of molecules and materials through a user-defined choice. The authors define a quadratic minimization problem with a penalty procedure to achieve the target state.<a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> The procedure is capable of producing a minimum energy state in the orthogonal complement sub-space of a given user-defined state. The latter state can be obtained from a prior run of the same algorithm. The authors also elucidate the protocol to systematically filter states using a symmetry operator (say <span class="italic">S</span>) of the Hamiltonian by sampling the symmetry eigenspace labelled by the eigenvalue (say <span class="italic">ω</span>). Apart from this in the same reference Sajjan <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit347"><sup><span class="sup_ref">347</span></sup></a> also deduce a generic lower bound for the successful sampling of the quantum circuit and thoroughly discuss special limiting cases. The lower bound can be surpassed with a tunable parameter which the user can set to ensure the ancilla register collapses into the favorable state enough number of times on repeated measurements of the quantum circuit as shown in <a title="Select to navigate to figure" href="#imgfig30">Fig. 30(d)</a>. Only such measurements are important in constructing the distribution in <a title="" href="#eqn120">eqn (120)</a>. The role of measurement error mitigation and warm-initialization on the algorithm, measurement statistics of the algorithm, transferability of the algorithm to related tasks, and the effect of hidden node density to name a few were thoroughly explored. Specific examples used were important categories of 2D-materials like transition metal di-chalcogenides (TMDCs) whose entire set of frontier bands (both valence and conduction), band-splitting due to spin–orbit coupling (SOC), <span class="italic">etc.</span>, were accurately obtained even when implemented on 27-qubit IBMQ processors. Representative data for monolayer Molybdenum di-Sulfide (MoS<small><sub>2</sub></small>) for valence and conduction bands are shown in <a title="Select to navigate to figure" href="#imgfig33">Fig. 33(c)</a> and for symmetry filtering using squared-orbital angular momentum (<span class="italic">L</span><small><sup>2</sup></small>) operator in <a title="Select to navigate to figure" href="#imgfig33">Fig. 33(d and e)</a>. Molecular examples to study the effect of multi-reference correlation were explored both in the ground and excited states. In each case the performance of the algorithm was benchmarked with metric like energy errors, infidelity of the target state trained on the neural network, constraint violation, <span class="italic">etc.</span></p>
          <p class="otherpara">Tensor networks as described in Section 3.4 have been used as an ansatz to classically simulate numerous quantum systems with limited entanglement. One can map a quantum many-body state represented on a tensor network to quantum circuits so that it can harness the quantum advantage. The goal is to prepare variational states using quantum circuits which are more expressive than tensor networks or any other classical ansatz and also are difficult to simulate on classical computers. In a recent report, the authors<a title="Select to navigate to references" href="#cit549"><sup><span class="sup_ref">549</span></sup></a> use this idea to represent quantum states with variational parameters of quantum circuit defined on a few qubits instead of standard parameterized tensor used in DMRG (see Section 3.4.5). They show that sparsely parameterized quantum circuit tensor networks are capable of representing physical states more efficiently than the dense tensor networks. Authors refer to the standard tensor networks with all variable elements as Dense with a prefix ‘<span class="italic">D</span>’ while the quantum circuit tensor networks are referred with their names prefixed with ‘<span class="italic">q</span>’. In theory, the number of gates required to exactly recover a <span class="italic">q</span>-qubit unitary grows exponentially with <span class="italic">q</span>. The bond dimensions (<span class="italic">D</span>) between local tensors of tensor networks are encoded into quantum circuit using an unitary operator defined on <span class="italic">q</span> qubits where <span class="italic">q</span> = log<small><sub>2</sub></small>(<span class="italic">D</span>). Contrary to this, the authors claim that only a polynomial number of variational parameters of two qubit unitaries and isometries are sufficient to express the quantum state. Authors work with three types of local circuits to represent the unitary:</p>
          <p class="otherpara">• Brick-wall circuit: it has a layered structure with successive layers fully connected <span class="italic">via</span> two-body unitary gates. In <a title="Select to navigate to figure" href="#imgfig34">Fig. 34(a)</a> a brick-wall circuit is shown with depth <span class="italic">τ</span> = 6. The effective correlation length is proportional to the depth; hence, the correlations in brick-wall circuit are known to spread slowly with increasing depth.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig34"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f34_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f34.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f34.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 34 </b> <span id="fig34"><span class="graphic_title">A schematic diagram showing architectures of different local circuits. (a) A brick-wall circuit, (b) ladder circuit, and (c) MERA circuit. Each rank-4 tensor is represented by a circle denoting two-qubit unitary gates. The different colours represent different layers of the circuit. The arrows in the circuits show the direction of canonicalization. Depths of the circuits are six, four, and five, respectively. Reproduced from ref. <a title="Select to navigate to references" href="#cit549">549</a> under Creative Commons Attribution 4.0 International license.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">• Ladder circuit: it also has a layered structure with denser connections. The first and the last qubits are entangled to each other in the first layer itself. This structure allows efficient propagation of correlations. <a title="Select to navigate to figure" href="#imgfig34">Fig. 34(b)</a> shows a ladder circuit with depth <span class="italic">τ</span> = 4.</p>
          <p class="otherpara">• MERA circuit: its architecture is inspired from the MERA tensor networks (described in Section 3.4.4). It has isometric tensors and unitary disentanglers arranged in alternate layers. <a title="Select to navigate to figure" href="#imgfig34">Fig. 34(c)</a> shows a MERA circuit with depth <span class="italic">τ</span> = 5.</p>
          <p class="otherpara">Two different paradigmatic Hamiltonians – Heisenberg and Fermi–Hubbard model have been used to test the efficacy of the technique using both local DMRG like optimization and global gradient based optimization. The minimization of the energy (<span class="italic">E</span> = 〈<span class="italic">ψ</span>|<span class="italic">Ĥ</span>|<span class="italic">ψ</span>〉) is performed using conjugate gradient<a title="Select to navigate to references" href="#cit550"><sup><span class="sup_ref">550</span></sup></a> and LBFGS-B.<a title="Select to navigate to references" href="#cit551"><sup><span class="sup_ref">551</span></sup></a></p>
          <p class="otherpara">Empirically the relative error δ<span class="italic">E</span> in the ground state energy was found to be inversely proportional to the polynomial of number of variational parameters <span class="italic">n</span>.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn122"/><span id="eqn122">δ<span class="italic">E</span>(<span class="italic">n</span>) ∼ <span class="italic">an</span><small><sup>−<span class="italic">b</span></sup></small></span></td><td class="rightEqn">(122)</td></tr></table>The results obtained by implementing different local quantum circuit tensor networks are shown in <a title="Select to navigate to figure" href="#imgfig35">Fig. 35</a>. Fitting the <a title="" href="#eqn122">eqn (122)</a> on the data generates set of (<span class="italic">a</span>,<span class="italic">b</span>) parameters which are summarized in <a title="Select to navigate to table" href="#tab2">Table 2</a>.</p>
          
<div class="table_caption"><b>Table 2</b> <span id="tab2">The (<span class="italic">a</span>, <span class="italic">b</span>) values obtained numerically from the various ansatz employed in ref. <a title="Select to navigate to references" href="#cit549">549</a></span>
		</div>

              <div class="rtable__wrapper"><div class="rtable__inner"><table class="tgroup rtable" border="0"><colgroup><col/><col/><col/></colgroup>
                
                
                
                <thead align="left">
                  <tr align="left" valign="bottom">
                    <th align="left" valign="bottom" class="border_black">Ansatz</th>
                    <th align="left" valign="bottom" class="border_black">Heisenberg (<span class="italic">a</span>, <span class="italic">b</span>)</th>
                    <th align="left" valign="bottom" class="border_black">Hubbard (<span class="italic">a</span>, <span class="italic">b</span>)</th>
                  </tr>
                </thead>
                <tbody align="left">
                  <tr align="left">
                    <td valign="top">qMPS-b</td>
                    <td valign="top">(20, 4)</td>
                    <td valign="top">(9, 1.9)</td>
                  </tr>
                  <tr align="left">
                    <td valign="top">qMPS-l</td>
                    <td valign="top">(14, 3.1)</td>
                    <td valign="top">(10, 1.9)</td>
                  </tr>
                  <tr align="left">
                    <td valign="top">qMERA-b</td>
                    <td valign="top">(15, 3.1)</td>
                    <td valign="top">(6.0, 1.4)</td>
                  </tr>
                  <tr align="left">
                    <td valign="top">dMPS (DMRG)</td>
                    <td valign="top">(15, 2.9)</td>
                    <td valign="top">(8.0, 1.5)</td>
                  </tr>
                </tbody>
              </table></div></div>
            <hr class="hrule"/><br/>
          <p class="otherpara">The parameter <span class="italic">b</span> gives the asymptotic behaviour of accuracy of the circuit depending on the number of parameters. A higher <span class="italic">b</span> indicates that for the same number of parameters, the model is approximating the ground state better. Hence, it is quite evident that ansatz based on quantum circuit tensor networks is more expressive compared to the standard classical variants studied in the report as the former yields comparable accuracy to the latter with a lower parameter cost. This clearly explicates the need for simulating tensor networks on a quantum-hardware to realize its optimum potential for understanding many-body quantum systems.</p>
          <p class="otherpara">Being limited by the number of noiseless qubits available on any quantum hardware in the NISQ era, a recent report<a title="Select to navigate to references" href="#cit552"><sup><span class="sup_ref">552</span></sup></a> has illustrated how to decompose a 2<span class="italic">N</span>-qubit circuit into multiple <span class="italic">N</span>-qubit circuits. These <span class="italic">N</span>-qubit circuits can be run on NISQ devices while their results can be post-processed on a classical computer. In this formulation, the state |<span class="italic">ψ</span>〉 on 2<span class="italic">N</span> qubit system can be partitioned into smaller states defined on <span class="italic">N</span> qubits using Schmidt decomposition (similar to MPS defined in Section 3.4.1).<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn123"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t282_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t282.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t282.gif"/></a></td><td class="rightEqn">(123)</td></tr></table>where |<span class="italic">b</span><small><sub><span class="italic">n</span></sub></small>〉 are the <span class="italic">N</span>-qubits states in the computational basis, <span class="italic">U</span> and <span class="italic">V</span> are unitary operators acting on the two subsystems that transform the computational basis to the desired state and <span class="italic">λ</span><small><sub><span class="italic">n</span></sub></small>s are the Schmidt coefficients which determine the degree of correlation present within the system. Using the above state, the expectation of a 2<span class="italic">N</span>-qubit operator defined as <span class="italic">O</span> = <span class="italic">O</span><small><sub>1</sub></small> ⊗ <span class="italic">O</span><small><sub>2</sub></small> can be written as<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn124"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t283_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t283.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t283.gif"/></a></td><td class="rightEqn">(124)</td></tr></table>where <span class="italic">Õ</span><small><sub>1</sub></small> = <span class="italic">U</span><small><sup>†</sup></small><span class="italic">O</span><small><sub>1</sub></small><span class="italic">U</span> and <span class="italic">Õ</span><small><sub>2</sub></small> = <span class="italic">V</span><small><sup>†</sup></small><span class="italic">O</span><small><sub>2</sub></small><span class="italic">V</span>, and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t284_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t284.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t284.gif"/></a> with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t285_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t285.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t285.gif"/></a>.</p>
          <p class="otherpara">Authors use this approach to simulate the ground state of water molecule using VQE simulation. They use the frozen core approximation and to enforce spin symmetry set <span class="italic">U</span> = <span class="italic">V</span>. This yields ten spin orbitals of the water molecule in the STO-6G basis set which using the aforesaid scheme can be encoded into five qubits on the quantum processor. The formalism yields excellent values of energies for geometries which are distorted through stretching of the O–H bond in H<small><sub>2</sub></small>O molecules by using just 3 Schmidt coefficients even though the results degrade from the exact value for deformations of the H–O–H bond angle.</p>
          <p class="otherpara">Another algorithm has been proposed based on tensor networks to solve for any eigenvector of a unitary matrix <span class="italic">Q</span>, given black-box access to the matrix. When working with a given Hamiltonian for a physical system, the black-box unitary can be prepared by applying the unitaries of the problem Hamiltonian and the mixing Hamiltonian alternatively for different times. In that case <span class="italic">Q</span> will be characterized by time-parameters. A parameterized unitary <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t286_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t286.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t286.gif"/></a> is used to define the state ansatz. The loss function for estimating the ground state of <span class="italic">Q</span> is simply maximizing the probability of projecting each qubit to |0〉. If <span class="italic">k</span> denotes a <span class="italic">k</span>-ebit matrix product state (MPS) then the value <span class="italic">k</span> value is iteratively increased until the desired accuracy is achieved. <span class="italic">k</span> ranges from 1 to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t287_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t287.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t287.gif"/></a> because the maximum entanglement produced by an <span class="italic">n</span>-qubit quantum circuit of depth <span class="italic">m</span> is <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t288_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t288.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t288.gif"/></a> ebits. The implementation complexity of the algorithm (measured in terms of number of CNOT gates used) scales as <span class="italic">O</span>(<span class="italic">l</span>·<span class="italic">n</span>·<span class="italic">r</span><small><sup>2</sup></small>) where <span class="italic">n</span> is the number of blocks and <span class="italic">r</span> is the rank of tensor network and <span class="italic">l</span> is the number of steps required to terminate the optimization routine. This algorithm has a significant advantage over other variational techniques in that it terminates when the state reaches the eigenstate of system.</p>
        </div>
      
      
        
        <h3 id="sect6197"><span class="b_heading">5.4 Estimation and parameterization of force fields in molecular dynamics</span></h3>
        <div>
          
          <span id="sect6200"/><span class="c_heading_indent">5.4.1 Machine learning techniques on a classical processor. </span>
          <span>The use of molecular dynamics (MD) simulations to unravel the motion of molecules dates back to 1977 by McCammon <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit553"><sup><span class="sup_ref">553</span></sup></a> Their study simulated dynamics of folded protein for ≈10 ps which unraveled the fluid-like nature of the interior region of protein for the first time. Since then the field of molecular dynamics has seen various incarnations from increasing the simulations system size to million-atoms<a title="Select to navigate to references" href="#cit554"><sup><span class="sup_ref">554</span></sup></a> to simulating it for a longer time scale using parallel computing.<a title="Select to navigate to references" href="#cit555"><sup><span class="sup_ref">555</span></sup></a> While performing MD simulations Newton's laws of motions are numerically integrated at each time step, which requires a set of initial conditions (position and velocity of each particle) and a comprehensive understanding of atomic forces acting on the system. The best way to obtain these forces is <span class="italic">via</span> first-principles, <span class="italic">i.e.</span>, solving the Schrödinger equation for a particular configuration of the nuclei. Unfortunately getting an exact analytical solution for the Schrödinger equation (SE) is a herculean task for most of the chemical species. Thus some levels of approximations are considered while solving the exact SE. In this approach, the size of the problem increases exponentially as a function of degrees of freedom (DOFs) and thus increasing the computation cost. For example advance <span class="italic">ab initio</span> electronic structure calculations, such as coupled cluster singles-doubles (CCSD), its perturbative triples variant CCSD(T) scales as <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t289_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t289.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t289.gif"/></a> where <span class="italic">n</span> is the number of basis functions used.</span>
          <p class="otherpara">Thus <span class="italic">ab initio</span> calculations are known for their precision but they are computationally expensive restricting their application to smaller systems in the gas phase or solid-state periodic materials. To model larger systems we have to use a higher level of approximation and use empirical force field (FF) methods. Their computational efficiency allows the simulation of systems containing millions of atoms<a title="Select to navigate to reference" href="#cit554"><sup><span class="sup_ref">554,556</span></sup></a> and exploration of much longer simulation time scales (100 ms).<a title="Select to navigate to references" href="#cit557"><sup><span class="sup_ref">557</span></sup></a> An FF is an analytical expression that denotes interatomic potential energy as a function of the coordinates of the system (for a particular configuration) and set of parameters. Depending on the intricacy of the underlying system different FFs are employed and today's scientific literature provides a myriad choices. But a standard expression for an FF resembles like<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn125"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t290_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t290.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t290.gif"/></a></td><td class="rightEqn">(125)</td></tr></table></p>
          <p class="otherpara">The first four terms are intramolecular contributions (the first term corresponds to bond stretching followed by bending, dihedral rotation, and improper torsion). The last two terms correspond to van der Waals (12-6 Lennard-Jones potential) and coulombic interactions. The parameters in FF (<span class="italic">k</span><small><sub>bo</sub></small>, <span class="italic">r</span><small><sub>eq</sub></small>, <span class="italic">k</span><small><sub>an</sub></small> and <span class="italic">θ</span><small><sub>eq</sub></small>) are usually obtained <span class="italic">via ab initio</span> or semi-empirical calculations or by fitting to experimental data such as X-ray and electron diffraction, NMR, infrared, and Raman spectroscopy. A general review of the force fields for molecular dynamics can be found in ref. <a title="Select to navigate to references" href="#cit558">558</a> and <a title="Select to navigate to references" href="#cit559">559</a>. The final aim of FF is to express all the quantum mechanical information in classical terms, splitting up the total electronic energy of the system into well-divided atom–atom contributions (coulombic, dispersion, <span class="italic">etc.</span>). However, it is an arduous task to split up the total electronic energy even after using precise quantum mechanical calculations. Thus while determining inter-molecular forces we need to consider crucial physical approximations which limit the accuracy. So depending on the system one chooses a certain level of approximation and uses the input data to optimize parameters which makes this approach empirical. Basic steps to form a new force field involve accessing the system to select a functional form for the system's energy. After this, we need to choose the data set which determines the parameters in the function defined earlier. Earlier people used to use experimental data from X-ray or neutron diffraction (for equilibrium bond lengths) and different spectroscopic techniques (for force constants). But in most of the cases, the experimental data used to be insufficient or inaccurate, thus nowadays <span class="italic">ab initio</span> data are preferred. Next, we optimize these parameters, in general; there exists colinearity between them <span class="italic">i.e.</span> these parameters are coupled (changing one would change another), so the optimization is done iteratively. The last step involves validation where we calculate different properties of the system which are not involved in the parametrization.</p>
          <p class="otherpara">Thus the underlying assumptions behind an FF eventually limit the accuracy of the physical insights gained from them. Since conventional FFs do not explicitly model multi-body interactions, polarizations and bond breakings during a chemical reaction make their predictions highly inaccurate. Although there are specially developed FFs (AMOEBA, ReaxFF, RMDFF, ARMD)<a title="Select to navigate to reference" href="#cit560"><sup><span class="sup_ref">560–563</span></sup></a> that include these effects at a certain computational cost, in most of these cases there exists ambiguity regarding the necessity of inclusion of these effects. Mixed Quantum Mechanics/Molecular Mechanics (QM/MM)<a title="Select to navigate to references" href="#cit564"><sup><span class="sup_ref">564</span></sup></a> becomes a handy tool while dealing with huge systems (bio-molecules). As the name suggests it employees quantum mechanical treatment for a subset of the problem (reactive region) and the rest of the problem (environment) is being treated classically. This way the QM/MM approach includes certain quantum correlations in bigger systems improving its accuracy (compared to FF). Although it may seem that the QM/MM approach is the most optimal way to simulate huge problems but one needs to consider huge “reactive region” to get converged results which eventually increases the computational cost.</p>
          <p class="otherpara">Machine Learning Force Fields (ML FFs) combine the accuracy of <span class="italic">ab initio</span> methods with the efficiency of classical FFs and resolve the accuracy/efficiency dilemma. ML approaches evade solving equations and rely on recognizing the underlying pattern in the data and learning the functional correspondence between the input and the output data with unparalleled accuracy and efficiency.<a title="Select to navigate to reference" href="#cit507"><sup><span class="sup_ref">507,565</span></sup></a> Unlike conventional FFs ML FFs do not require predetermined ideas about the bonding pattern assumptions. This distinctive feature makes ML approaches admirable in the chemical space and there are huge number of options ML models are available or MLFFs ranging from PhysNEt,<a title="Select to navigate to references" href="#cit566"><sup><span class="sup_ref">566</span></sup></a> sGDML,<a title="Select to navigate to references" href="#cit565"><sup><span class="sup_ref">565</span></sup></a> GAP,<a title="Select to navigate to references" href="#cit567"><sup><span class="sup_ref">567</span></sup></a> SchNet,<a title="Select to navigate to references" href="#cit568"><sup><span class="sup_ref">568</span></sup></a> HDNN<a title="Select to navigate to references" href="#cit569"><sup><span class="sup_ref">569</span></sup></a> and ANI.<a title="Select to navigate to references" href="#cit570"><sup><span class="sup_ref">570</span></sup></a> However, it is not trivial to extend the classical ML formalism to generate FFs. The reason being exacting standards are imposed on the ML FFs which offers an alternative to the already established benchmark FFs. Additionally, classical ML techniques (Natural Language Processing (NLP), Computer Vision, <span class="italic">etc.</span>) assume huge reference data sets while optimizing thousands of training parameters, whereas it is very difficult to get such extensive data sets in the case of natural sciences, since each data set is generated either from an expensive <span class="italic">ab initio</span> calculation or from rigorous experimental measurement. Thus data efficiency becomes a key factor while developing ML FFs, which is resolved by encoding the physical knowledge or laws directly into the architecture of the ML models.<a title="Select to navigate to reference" href="#cit565"><sup><span class="sup_ref">565,568</span></sup></a> Ref. <a title="Select to navigate to reference" href="#cit571">571–573</a> discuss the construction of potential energy surfaces using different supervised machine learning techniques for complex chemical systems and electronically excited molecular states. A recent review by Unke <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit574"><sup><span class="sup_ref">574</span></sup></a> presents an in depth overview of ML FFs along with the step-by-step guideline for constructing and testing them. The rest of the section focuses on the application of ML to optimize the FF parameters in ReaxFF<a title="Select to navigate to references" href="#cit575"><sup><span class="sup_ref">575</span></sup></a> which eventually leads to more accurate physical insights. We will show specific example<a title="Select to navigate to references" href="#cit576"><sup><span class="sup_ref">576</span></sup></a> where ML is used for parameter optimization.</p>
          <p class="otherpara">Refinement of parameters is essential while employing ReaxFF MD for chemical reactions. To get insights about static properties (energy, force or charge distribution) fitting of FF parameters is done by using specific quantum mechanical (QM) training data. Genetic algorithm<a title="Select to navigate to reference" href="#cit577"><sup><span class="sup_ref">577,578</span></sup></a> and its multi-objective variant<a title="Select to navigate to references" href="#cit579"><sup><span class="sup_ref">579</span></sup></a> have been developed to ease out the parameter optimization using QM data. However, application of ReaxFF for dynamic non-equilibrium chemical reactions (chemical vapour deposition) is not straightforward, as it is unfeasible to gain QM training data set for fitting. In addition, the dynamical properties we would like to predict decide the computational cost and resources needed for parameter fitting. In such situations, ML-based approaches comes to our rescue. Hiroya and Shandan<a title="Select to navigate to references" href="#cit576"><sup><span class="sup_ref">576</span></sup></a> recognized the flexibility that ML-based models offer and used it to efficiently optimize the parameters in ReaxFF. Their approach uses the <span class="italic">k</span> nearest neighbor algorithm to get several local minima and then optimization is done using ML. The most distinctive feature in this approach is that it can combine efficiency from ML-based optimization with accuracy from other optimization techniques (Monte Carlo/genetic) making itself a powerful tool.</p>
          <p class="otherpara">The first step in parameter optimization is creating a reference parameter data set (<span class="italic">P</span><small><sub>be1</sub></small>, <span class="italic">P</span><small><sub>be2</sub></small>,…, <span class="italic">D</span><small><sub><span class="italic">ij</span></sub></small>, <span class="italic">R</span><small><sub>vdw</sub></small>,…, <span class="italic">P</span><small><sub>val2</sub></small>,…) for the given ReaxFF. The ReaxFF parameters encode the information about the system's potential energy's different terms (bonding, lone pairs, van der Waals <span class="italic">etc.</span>). This reference data set is then used for the random generation of <span class="italic">N</span> (≈100) different data sets. While generating random samples from the reference data two different strategies are implemented; in the first strategy, a random change is made in the ReaxFF parameter set which is followed by the second strategy where we exchange parameters between different ReaxFF data sets. During the random sampling process, it is important to find the sweet spot where the sampling process should not result in a huge deviation making the resultant random sample unphysical at the same time the resultant random sample should not be too close to the reference data (the model will be stuck at the local minima) (<a title="Select to navigate to figure" href="#imgfig35">Fig. 35 and 36</a>).</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig35"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f35_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f35.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f35.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 35 </b> <span id="fig35"><span class="graphic_title">A comparison of expressibility of quantum circuit tensor networks (qMPS, qMERA), standard tensor networks (dMPS and dMERA), and global quantum circuits (QC). The expressibility (or variational power) is measured by seeing the relation between relative energy error δ<span class="italic">E versus</span> the number of variational parameters in the ansatz. Figures (a–c) are for Heisenberg model while (d–f) are for Fermi-Hubbard model of lattice size 32. The post-fixes ‘b’ and ‘l’ denote the brick-wall and ladder circuit. (a, b, d and e) The comparison between brick-wall and ladder qMPS acting on different number of qubits (<span class="italic">q</span>), QC and dMPS. (c and f) Comparison between qMERA-b with varying <span class="italic">q</span>, qMPS-b, and dMERA. Reproduced from ref. <a title="Select to navigate to references" href="#cit549">549</a> under Creative Commons Attribution 4.0 International license.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig36"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f36_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f36.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f36.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 36 </b> <span id="fig36"><span class="graphic_title">Schematic representation of random sampling (a) initial parameter references (A, B, and C inside denote the initial parameter set to generate the reference ReaxFF parameter set); (b) definition of a parameter set and explanation about random modification scheme; and (c) entire image of the sampling space using the random modification scheme. The open dashed circles A, B, and C on the right side denote the sampling space in the vicinity of the initial ReaxFF parameter sets A, B, and C. The open dashed circle D denotes a new sampling space, which is different from A, B, and C. Figure adapted with permission from ref. <a title="Select to navigate to references" href="#cit576">576</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">The next step involves score assessment where a score <span class="italic">S</span>(<span class="italic">p</span><small><sub><span class="italic">i</span></sub></small>)<small><sup>ReaxFF</sup></small> is calculated for each training reference data set <span class="italic">p</span><small><sub><span class="italic">i</span></sub></small>.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn126"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t291_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t291.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t291.gif"/></a></td><td class="rightEqn">(126)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn127"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t292_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t292.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t292.gif"/></a></td><td class="rightEqn">(127)</td></tr></table></p>
          <p class="otherpara">Here <span class="italic">N</span><small><sub>QMtype</sub></small> is the number of geometry sets and <span class="italic">N</span><small><sup>QMtype</sup></small><small><sub><span class="italic">j</span></sub></small> corresponds to the number of different structures in the <span class="italic">j</span>th geometry set. These structures constitute a specific physical property (potential energy change as a function of volume change in α-Al<small><sub>2</sub></small>O<small><sub>3</sub></small> crystal).</p>
          <p class="otherpara">After evaluating the score of every training data ML is used for data analysis. There are three major steps: (1) use Random forest regression to extract important features, (2) update initial parameters by the <span class="italic">k</span>-nearest neighbor (<span class="italic">k</span>-NN) algorithm and (3) use grid search to get the optimal parameters. In the first step, random forest regression is employed, where the objective function for minimization can be written in the form of difference between the actual score calculated <span class="italic">via</span> ReaxFF and the estimated score <span class="italic">via</span> ML:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn128"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t293_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t293.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t293.gif"/></a></td><td class="rightEqn">(128)</td></tr></table><table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn129"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t294_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t294.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t294.gif"/></a></td><td class="rightEqn">(129)</td></tr></table></p>
          <p class="otherpara">Here <span class="italic">N</span> is the number of random samples created and <span class="italic">j</span> denotes each structure group. The ML model is build by minimizing <a title="" href="#eqn128">eqn (128) and (129)</a>. At the end of ML training important features are extracted which will be used further during grid search parameter optimization. The second step consists of applying the <span class="italic">k</span>-NN algorithm to divide the training data into <span class="italic">k</span> different groups on the basis of closest distance. For each group scores (<a title="" href="#eqn127">eqn (127)</a>) are calculated and the parameter set corresponding to the minimum score is selected eventually modifying the initial parameter set. The modified initial parameter set after the <span class="italic">k</span>-NN algorithm is not fully optimized, the reason being in a way it is being chosen from the initial data set. Thus grid search parameter optimization is used in the third step which is combined with the extracted parameters from the trained ML model.</p>
          <p class="otherpara">Grid search parameter optimization (GSPO) is defined by three parameters <span class="italic">n</span>, <span class="italic">l</span> and <span class="italic">m</span>. Every geometry set (containing <span class="italic">N</span><small><sub>QMtype</sub></small> various physical properties) has a corresponding ML model with different variations of important features. Depending on the importance of particular feature, groups are formed containing <span class="italic">n</span> elements. In <a title="Select to navigate to figure" href="#imgfig37">Fig. 37</a> we take <span class="italic">n</span> = 4, the parameters corresponding to model A are sorted according to their importance (sensitivity). Since <span class="italic">n</span> = 4, first four important parameters are clubbed together forming group A; the next set of 4 will be named group B so on so forth. This process is repeated with all the ML models. Since each ML model has varying feature importance levels the groups will contain different parameters. GSPO is then carried out on individual groups by splitting it into <span class="italic">m</span> grid points. As each group contains <span class="italic">n</span> parameters the total number of grid points becomes <span class="italic">m</span><small><sup><span class="italic">n</span></sup></small>. Getting scores <a title="" href="#eqn126">eqn (126)</a> for every grid point for comparison is a laborious task. ML alleviates this issue by selecting <span class="italic">l</span> test parameter sets from the haystack of <span class="italic">m</span><small><sup><span class="italic">n</span></sup></small> different parameter sets. At every group, the scores of <span class="italic">l</span> test parameters are compared with the initial sets of parameters obtained from <span class="italic">k</span>-NN. If the score of a particular parameter set (in <span class="italic">l</span>) is less than the score of the initial sets of parameters the parameters are updated. However, it is still a time-consuming task to repeat the process for all the groups (A, B,…, Z). Since the score does not change much when the group is not sensitive enough. A new cutoff term <span class="italic">n</span><small><sub>layer</sub></small> is introduced which determines when to stop computing score for a particular group and move on to the next. For instance, if <span class="italic">n</span><small><sub>layer</sub></small> = 2 GSPO is carried out on group A (most important) and group B (next most important). After this GSPO moves on to ML model B.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig37"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f37_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f37.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f37.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 37 </b> <span id="fig37"><span class="graphic_title">Illustration showing entire the machine learning scheme. Figure adapted from ref. <a title="Select to navigate to references" href="#cit576">576</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">To summarize, while optimizing the parameters of ReaxFF broadly, there are three major steps involved: (1) use random forest regression to construct ML models and extract important features, (2) locate probable local minima using the <span class="italic">k</span>-nearest neighbour algorithm and (3) use grid search optimization to optimize the parameters using information from developed ML models. It is important to note that different optimized parameters sets will predict the potential energy decently (in agreement with QM), although their estimation for different physical properties will diverge. The reason is an unavoidable uncertainty is carried out corresponding to a particular physical property during the parameter estimation. Hiroya and Shandan<a title="Select to navigate to references" href="#cit576"><sup><span class="sup_ref">576</span></sup></a> used this approach on a pilot test in which the optimized parameters of ReaxFF were further used to simulate chemical vapour deposition (CVD) of an α-Al<small><sub>2</sub></small>O<small><sub>3</sub></small> crystal. The optimized parameters decently predicted the crystal structure of α-Al<small><sub>2</sub></small>O<small><sub>3</sub></small> even at high temperatures (2000 K). Stochastic behavior or the random forest algorithm used in different ML approaches results in hundreds of error evaluations during training for complex training jobs, where each error evaluation involves minimizing the energy for many molecules in the training data. Recently Mehmet <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit580"><sup><span class="sup_ref">580</span></sup></a> proposed a novel method that employs an automatic differentiation technique from the JAX library developed by Google to calculate the gradients of the loss function. It is impressive that the efficiency of the gradient-based local optimizer is independent of the initial approximation made for ReaxFF. Given reasonable computing resources, ML-assisted parameter optimization techniques are powerful tools that can be used to simulate wide spectra of reactive MD simulations.</p>
          <p class="otherpara">In Non-Adiabatic (NA) Molecular Dynamics (MD) simulations Born–Oppenheimer (BO) approximation breaks down and the electronic and nuclear degrees of freedom cannot be treated independently. These simulations perform a pivotal role while understanding the dynamics of the excited states. Recently, supervised machine learning techniques have been employed which interpolate NA Hamiltonian along the classical path approximated NA-MD trajectories<a title="Select to navigate to reference" href="#cit581"><sup><span class="sup_ref">581–583</span></sup></a> eventually speeding up NA MD simulations.<a title="Select to navigate to reference" href="#cit45"><sup><span class="sup_ref">45,584–586</span></sup></a> NA MD simulations act as a robust resource especially while predicting macroscopic observables such as quantum yield without knowing the mechanism for the larger systems involving strong couplings in which it is difficult to choose a reaction coordinate.<a title="Select to navigate to reference" href="#cit587"><sup><span class="sup_ref">587–589</span></sup></a> But they come with a cost of expensive <span class="italic">ab initio</span> calculations of geometry-dependent forces and energies or the different states. In such situations, ML techniques come to the rescue by predicting band gaps and NA coupling using a small fragment of <span class="italic">ab initio</span> training data.<a title="Select to navigate to reference" href="#cit45"><sup><span class="sup_ref">45,585,586,590,591</span></sup></a> To predict the physical properties of materials, unsupervised ML techniques<a title="Select to navigate to reference" href="#cit592"><sup><span class="sup_ref">592–595</span></sup></a> have been employed on the trajectories of NA MD simulations while explaining the dominant structural factors.<a title="Select to navigate to reference" href="#cit596"><sup><span class="sup_ref">596–598</span></sup></a> Many times Mutual Information (MI) is used to identify unanticipated correlations between many important features. Results from MI are easier to interpret and it is supported by information-theoretic bound which makes it not sensitive to the size of the data set.<a title="Select to navigate to reference" href="#cit598"><sup><span class="sup_ref">598,599</span></sup></a> These properties make it popular for its application in the chemical regime. For instance, a model of metal halide perovskites (MHPs) based on unsupervised MI unveiled the importance of geometric features as compared to the atomic velocities while predicting non-adiabatic Coupling (NAC).<a title="Select to navigate to references" href="#cit596"><sup><span class="sup_ref">596</span></sup></a></p>
          <p class="otherpara">In a recent study by How <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit600"><sup><span class="sup_ref">600</span></sup></a> supervised and unsupervised ML techniques have been used for feature selection, prediction of non-adiabatic couplings, and excitation energies of NA MD simulations of CsPbI<small><sub>3</sub></small> metal halide perovskites (MHPs). MHPs have high optical absorption, low cost of manufacturing and long carrier diffusion<a title="Select to navigate to reference" href="#cit601"><sup><span class="sup_ref">601–603</span></sup></a> which make them an ideal candidate for their use in optoelectronics and solar energy harvesting materials. In order to improve the design of MHPs it is important to develop a computationally efficient and a systematic NA MD which utilizes the theory as well as simulations to predict the physical properties of MHPs. How <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit600"><sup><span class="sup_ref">600</span></sup></a> fill this knowledge gap by employing MI on the NA MD trajectory data set of CsPbI<small><sub>3</sub></small> perovskite and extracting the most important features that determine the NA Hamiltonian. The ML model is then validated by checking the performance of the extracted important features to predict the band gap and NAC. Their model showed surprising and counterintuitive results suggesting that the NA Hamiltonian can be predicted by using a single most important feature of the chemical environment information of any of the three elements in CsPbI<small><sub>3</sub></small>. This eventually leads to a drastic reduction in the dimensionality of the original 360-feature ML model developed from ML force fields to merely 12 featured ML models which can produce high-quality NA-MD simulation results which are further confirmed by the present theoretical knowledge about the electronic properties of CsPbI<small><sub>3</sub></small>. This dimensionality reduction technique helps alleviating the high computational cost of <span class="italic">ab initio</span> NA-MD and to extend NA-MD simulations to larger, more complex systems and longer time scales.</p>
        </div>
        <div>
          
          <span id="sect6390"/><span class="c_heading_indent">5.4.2 Quantum-computing enhanced machine learning techniques. </span>
          <span>Previously variational quantum algorithms (VQAs) have been applied for simulation of small systems<a title="Select to navigate to references" href="#cit604"><sup><span class="sup_ref">604</span></sup></a> with strongly bounded intramolecular forces.<a title="Select to navigate to references" href="#cit605"><sup><span class="sup_ref">605</span></sup></a> Most of these approaches relies on complete electronic basis set of the hamiltionian. Smaller coherence times needed in VQAs makes them ideal fit for the current generation Noisy Intermediate Scale Quantum (NISQ) processors devices. However it is difficult to employ them for simulation of weak intermolecular interactions as it requires consideration of core electrons (for dispersive forces) leading to a bigger orbital sets eventually requiring large number for qubits. Recently Anderson <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit606"><sup><span class="sup_ref">606</span></sup></a> resolved this issue by developing a VQA compatible coarse grained model that scales linearly with the system size. Inspired by the maximally coarse grained method<a title="Select to navigate to references" href="#cit607"><sup><span class="sup_ref">607</span></sup></a> their model represents the polarisable part of the molecular charge distribution as a quantum harmonic oscillator in which the parameters are finely tuned empirically to emulate reference polarisability values corresponding to a real molecule. The model contains zero point multipolar fluctuations by definition and thus dispersion interactions exist inherently. Additionally it does not define force laws between atoms and molecules (interaction potentials) which are predicted by using coarse grained electronic structures. Thus the model combines properties from empirical approach and first principle <span class="italic">ab initio</span> approach. Another advantage that this approach has is the number of unique quantum circuits required to measure the given Hamiltonian after considering all possible dipole interactions scales linearly <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t295_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t295.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t295.gif"/></a> with the number of quantum oscillators compared to <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t296_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t296.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t296.gif"/></a> scaling when we consider an orbital based VQE Hamiltonian with <span class="italic">n</span> orbitals.<a title="Select to navigate to references" href="#cit608"><sup><span class="sup_ref">608</span></sup></a></span>
          <p class="otherpara">Anderson <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit606"><sup><span class="sup_ref">606</span></sup></a> showed the solubility of the proposed model by presenting a proof of principle example by calculating London dispersion energies for interacting I<small><sub>2</sub></small> dimers on IBM quantum processor. While relaxing the harmonic approximation, classical methods heavily rely on path integral and Monte Carlo techniques for efficient sampling of two point correlators for Gaussian states created by harmonic Hamiltonians.<a title="Select to navigate to references" href="#cit609"><sup><span class="sup_ref">609</span></sup></a> Since sampling process for non-Gaussian ground states of anharmonic potentials is very expensive anharmonic electron-nuclear potentials remain unexplored using classical computational techniques. The VQA base approach shows a quantum advantage as it suffers negligible experimental overhead while performing relaxation of the harmonic approximation. Thus the VQA based approach provides a road-map that includes anharmonicity and higher order terms for simulation of realistic systems which are inaccessible for current classical methods. In conclusion quantum machine learning provides an efficient and accurate way to model realistic systems and may show a quantum advantage in the future.</p>
          <p class="otherpara">A detailed theoretical description of the nonadiabatic (NA) process is limited by intricate coupling between nuclear and electronic degrees of freedom (DOF) eventually leading towards adverse scaling of classical computational resources as the system size grows. Although in principle Quantum Computers can simulate real-time quantum dynamics within polynomial time and memory resource complexity, quantum algorithms have not been extensively investigated for their application towards the simulation of NA processes. A recent study by Ollitrault <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit610"><sup><span class="sup_ref">610</span></sup></a> proposed a quantum algorithm for simulation of a rapid NA chemical process which scales linearly as the system size. Specifically, they propagated the nuclear wave packet across <span class="italic">κ</span> diabatic surfaces having nonlinear couplings (Marcus model). The algorithm requires three quantum registers. First quantization formalism is used for DOF, so the space and momentum are discretized and encoded in the position quantum register. The population transfer between <span class="italic">κ</span> diabatic potentials is encoded in ancilla registers and the nonlinear coupling in coupling register. The encoding scheme is efficient in terms of number of qubits required as it scales logarithmically with the precision. This majestic memory compression while storing the propagated wave function denotes an exponential quantum advantage as compared to its classical counterparts (<a title="Select to navigate to figure" href="#imgfig38">Fig. 38</a>).</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig38"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f38_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f38.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f38.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 38 </b> <span id="fig38"><span class="graphic_title">(a) Graphical description of the Marcus model; and (b) quantum circuit representation for the time evolution of the wave packet. The blocks represent the evolution operators shown in the form of quantum gates. CQFT, <span class="italic">V</span><small><sub><span class="italic">i</span></sub></small>, <span class="italic">K</span> and <span class="italic">C</span> correspond to centred quantum Fourier transform (used to switch from position to momentum space), <span class="italic">i</span>th potential, and Kinetic and coupling terms, respectively. Figure adapted from ref. <a title="Select to navigate to references" href="#cit610">610</a>.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">The algorithm was applied to simulate NA dynamics of a wave-packet on a simple two one-dimensional harmonic potential which is shifted in energy by a variable offset. In total 18 qubits were required to simulate this simple model. The characteristic feature of decline in the population transfer of the Marcus model in the inverted region was clearly observed in the simulation, showing an excellent agreement with the exact propagation. Although the simulation was not entirely implemented on the quantum processor the reason is the huge circuit depth of the quantum circuit corresponding to the dynamics parts which requires higher coherence time not attainable by the current generation of quantum processors. But the first part of the algorithm which prepares an initial Gaussian wave packet<a title="Select to navigate to references" href="#cit610"><sup><span class="sup_ref">610</span></sup></a> was implemented on an IBM Q device. The extension of this algorithm to higher dimensions is straightforward and a <span class="italic">d</span> dimensional polynomial potential energy surface (PES) can be encoded with <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t297_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t297.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t297.gif"/></a> (<span class="italic">N</span> is the number of discrete grid points). Current classical algorithms<a title="Select to navigate to reference" href="#cit611"><sup><span class="sup_ref">611,612</span></sup></a> are limited to simulations of molecular systems which are characterized by up to ten vibrational modes. Hence a quantum processor offering approximately 165 qubits with sufficient coherence time would alleviate this hurdle thus providing an immense quantum advantage while understanding fast chemical processes involving exciton formation, inter-system crossings, and charge separation.</p>
          <p class="otherpara">Although Quantum Machine Learning (QML) shows quantum advantage in electronic structure calculation, its application towards force field generation remains unexplored. A recent study by Kiss <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit613"><sup><span class="sup_ref">613</span></sup></a> learns a neural network potential energy surface and generates a molecular force field <span class="italic">via</span> systematic application of parametrized Quantum Neural Network (QNN) techniques on the data set generated from classical <span class="italic">ab initio</span> techniques. The proposed QNN model was applied on single molecules and the results show the competitive performance of the QNN model with respect to its classical counterparts. Additionally, the results suggest that a properly designed QNN model exhibits a larger effective dimension resulting in fast and stable training capabilities. This potentially hints towards a possible quantum advantage of QML's application in force field generation.</p>
        </div>
      
      
        
        <h3 id="sect6435"><span class="b_heading">5.5 Drug-discovery pipeline and pharmaceutical applications</span></h3>
        <span>To deploy machine learning or deep learning algorithms in computer-aided drug discovery (CADD)<a title="Select to navigate to reference" href="#cit615"><sup><span class="sup_ref">615,616</span></sup></a> it is important to access large molecular databases wherein such structural information about the target (receptor) and/or drug (ligand) candidates may be found. Such databases are screened to generate prospective hits based on either the binding affinity with the target above a threshold for structure based protocols or by looking at chemical similarities with previously known bio-active candidates for ligand-based protocols. For the second category, chemical databases like PubChem,<a title="Select to navigate to reference" href="#cit617"><sup><span class="sup_ref">617,618</span></sup></a> ChEMBL,<a title="Select to navigate to references" href="#cit619"><sup><span class="sup_ref">619</span></sup></a> Drug-Bank,<a title="Select to navigate to references" href="#cit620"><sup><span class="sup_ref">620</span></sup></a> and DUD-E<a title="Select to navigate to references" href="#cit621"><sup><span class="sup_ref">621</span></sup></a> may be good choices. DUD-E contains many decoys (ligands/drug candidates which show similarity in properties but are not topologically similar in structure) which may be useful for the testing and validation of the trained models. For the first category wherein physical information about the target protein is necessary, databases like UnitProt,<a title="Select to navigate to references" href="#cit622"><sup><span class="sup_ref">622</span></sup></a> PDB,<a title="Select to navigate to reference" href="#cit623"><sup><span class="sup_ref">623–625</span></sup></a> and PDBbind<a title="Select to navigate to references" href="#cit626"><sup><span class="sup_ref">626</span></sup></a> may be a useful resource. Combination databases, BindingDB,<a title="Select to navigate to references" href="#cit627"><sup><span class="sup_ref">627</span></sup></a> which contain experimental data for several ligands and targets together have also been employed extensively too. Most of these databases do encode the structural/physico-chemical attributes of the candidate molecule and/or the target into a computer-inputable format which are either numerically describable or string-based. These are called features or very simply molecular descriptors and often depending on the choice of the user other molecular descriptors can be generated using the accessed structural information with libraries like RDKit, <span class="italic">etc.</span> For the ligand/drug, generic features like number of atoms, molecular weight, number of isomers, <span class="italic">etc.</span> are often called 0D features as they do not describe the specific nature of the connectivity of atoms within the molecule and remain oblivious to conformational changes. 1D features like SMILES,<a title="Select to navigate to reference" href="#cit628"><sup><span class="sup_ref">628–630</span></sup></a> SELFIES,<a title="Select to navigate to reference" href="#cit631"><sup><span class="sup_ref">631,632</span></sup></a> and SMARTS<a title="Select to navigate to reference" href="#cit633"><sup><span class="sup_ref">633,634</span></sup></a> which encode the connectivity pattern within the molecule using strings are quite commonly used. On the other hand, numeric features are based on fingerprints which usually represent the molecule as a binary vector with entry 1 (0) corresponding to the presence (absence) of certain prototypical substructures/functional groups. These are further divided into many categories like circular fingerprints like ECFP<a title="Select to navigate to references" href="#cit635"><sup><span class="sup_ref">635</span></sup></a> which are extremely popular as they are quickly generated, Morgan fingerprints,<a title="Select to navigate to references" href="#cit636"><sup><span class="sup_ref">636</span></sup></a> Molecular ACCess System (MACCS),<a title="Select to navigate to references" href="#cit637"><sup><span class="sup_ref">637</span></sup></a> Tree based fingerprints,<a title="Select to navigate to references" href="#cit638"><sup><span class="sup_ref">638</span></sup></a> and Atom pairs<a title="Select to navigate to reference" href="#cit639"><sup><span class="sup_ref">639,640</span></sup></a> to name a few. Graph based 2D descriptors<a title="Select to navigate to reference" href="#cit641"><sup><span class="sup_ref">641,642</span></sup></a> are also commonly used with the atoms in the molecule represented as vertices and the bonds between them as connectivity pattern. Adjacency matrix derived from such a graph can act as a molecular descriptor. <a title="Select to navigate to figure" href="#imgfig39">Fig. 39(b)</a> shows an example for representing a given molecule in the commonly used 1D and 2D formats. However, these 1D and 2D descriptors even though widely used are often less sensitive to stereochemistry within the molecule which may be important for evaluating binding proclivity with the target. 3D descriptors are useful for this purpose as detailed structural information like dihedral angles are important for this encoding.<a title="Select to navigate to reference" href="#cit643"><sup><span class="sup_ref">643,644</span></sup></a> Higher dimensional encoding with information like specific conformational state, interaction with the solvent or the background residues of the target may also be used to enhance the predictive capacity of the model.<a title="Select to navigate to reference" href="#cit643"><sup><span class="sup_ref">643,644</span></sup></a></span>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig39"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f39_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f39.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f39.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 39 </b> <span id="fig39"><span class="graphic_title">(a) The schematic overview of the key steps in computer-aided drug discovery (CADD). (b) Encoding the prospective drug/molecule into various representative formats<a title="Select to navigate to references" href="#cit614"><sup><span class="sup_ref">614</span></sup></a> for machine learning algorithms to act on. Reprinted from – a review on compound-protein interaction prediction methods: data, format, representation and model, 19, S. Lim, Y. Lu, C. Y. Cho, I. Sung, J. Kim, Y. Kim, S. Park and S. Kim, <span class="italic">Comput. Struct. Biotechnol. J.</span>, 2021, <span class="bold">19</span>, 1541–1556, Copyright (2021), with permission from Elsevier.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <div>
          
          <span id="sect6467"/><span class="c_heading_indent">5.5.1 Structure-based drug designing protocols using classical machine learning techniques. </span>
          <span>We first see the performance of machine learning techniques on the structure-based drug designing protocols (see <a title="Select to navigate to figure" href="#imgfig39">Fig. 39(a)</a>). The primary goal for such studies is to determine whether the prospective candidate molecule can bind effectively (and thereafter evaluate the binding pose and compute the binding affinity) to the target receptor given that the structural information about the target protein is available from prior experimental (like X-ray, NMR, <span class="italic">etc.</span>) or theoretical studies.<a title="Select to navigate to reference" href="#cit615"><sup><span class="sup_ref">615,644</span></sup></a> Even though experimental assurances are the most trusted means of evaluating such an interaction, yet simulation of the process gives an insight into understanding the interaction between a prospective candidate and the desired target relegating the need for direct labor and money intensive experimental verification at a later stage before clinical trial thereby leading to efficient screening. Recently, convolutional neural-network (CNN) (basic theoretical formalism discussed in Section 3.3.2) and recurrent neural network based models (RNN) (Basic theoretical formalism discussed in Section 3.3.3) are being used to investigate the process. In a celebrated study by Ragoza <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit339"><sup><span class="sup_ref">339</span></sup></a> 3D conformational images of the protein and molecule (ligand/drug) are taken and trained with a CNN to identify which poses are suitable for binding and which are not. The SAR-NRC-HiQ dataset was used<a title="Select to navigate to references" href="#cit645"><sup><span class="sup_ref">645</span></sup></a> containing 466 ligand-bound receptors (proteins). Two training sets are generated from it by re-docking using Smina<a title="Select to navigate to references" href="#cit646"><sup><span class="sup_ref">646</span></sup></a> and labelled using the Auto Dock Vina scoring function.<a title="Select to navigate to references" href="#cit647"><sup><span class="sup_ref">647</span></sup></a> These re-docked 3D structures are discretized into grid near the binding-site with a length of 24 Å on either axes and 0.5 Å resolution. The ligand and protein atoms within each such grid point were differentiated. This served as input to the CNN. The CNN model used had an architecture of five 3 × 3 × 3 hidden layers with ReLU activation and additional max pooling layers. The model was trained using Caffe Deep Learning framework<a title="Select to navigate to references" href="#cit648"><sup><span class="sup_ref">648</span></sup></a> minimizing multi-dimensional logistic loss function using gradient descent as the training algorithm. CNN outperformed AutoDock Vina scoring in pose-prediction ability, <span class="italic">i.e.</span>, grouping which poses afford a good binding affinity. The superior performance of the model was upheld for virtual screening too. Compounded datasets by combining training examples from both the tasks were used and the CNN model based training was found to be as effective as their separate counterparts. Even though the CNN was not trained on mutated protein datasets for binding affinity, yet it was able to register the amino acid residues within the protein critical for binding which afforded an easily interpretable visualization of the features that the network is learning. The work envisioned developing protocols to perform tasks like pose-ranking, binding affinity prediction, and virtual screening using a highly multi-task network trained on a much larger dataset which can ameliorate its performance even more.</span>
          <p class="otherpara">In a similar study Yang-Bin Wang <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit649"><sup><span class="sup_ref">649</span></sup></a> developed a computational model with memory based on LSTM to construct a framework for drug–target interaction. Information about the drug–target pairs was obtained using Kegg,<a title="Select to navigate to references" href="#cit650"><sup><span class="sup_ref">650</span></sup></a> DrugBank<a title="Select to navigate to references" href="#cit651"><sup><span class="sup_ref">651</span></sup></a> and Super Target databases.<a title="Select to navigate to references" href="#cit652"><sup><span class="sup_ref">652</span></sup></a> Four classes of targets were considered, <span class="italic">i.e.</span>, enzymes, ion-channels, GPCRs and nuclear receptors. From the dataset curated from the above bases, the drug–target interaction pairs with known affinities are set as positive examples while the remaining are treated as negative examples. The structural features of the protein was described using Position Specific Scoring Matrix (PSSM).<a title="Select to navigate to references" href="#cit649"><sup><span class="sup_ref">649</span></sup></a> For a protein consisting of <span class="italic">N</span> amino acids, PSSM is an <span class="italic">N</span> × 20 matrix with the (<span class="italic">i</span>,<span class="italic">j</span>)th element of the PSSM denoting the probability of mutating the ith amino acid in the sequence with the native amino acid from the list of 20. The PSSM was constructed using PSI BLAST.<a title="Select to navigate to references" href="#cit653"><sup><span class="sup_ref">653</span></sup></a> Legendre moments using the elements of PSSM was then constructed to remove redundancy in features. At the end 961 features were obtained for each protein sequence. The structural features of the drug were encoded molecular fingerprints. PubChem database was used for this purpose which defines 881 sub-structural features. As a result each drug/ligand was represented by an 881 dimensional Boolean vector denoting the presence or absence of these tagged molecular substructures. A total of 1842 dimensional vectors (881 + 961) for the molecule and the receptor target was reduced to 400 size feature vectors using sparse principal component analysis (SPCA). This combined feature vector was fed into the classifier. Multiple LSTM layers were stacked to get a deep LSTM setup, <span class="italic">i.e.</span>, 4 hidden layers with 36 neurons were used. Overfitting was compensated by using dropout of randomly chosen neurons during the training process. Different performance metrics were used like ACC, true positive rate or even the standard AUC as defined before.<a title="Select to navigate to references" href="#cit654"><sup><span class="sup_ref">654</span></sup></a> Both hyperbolic tangent and logistic sigmoid were used as the activation function depending on the case (see Section 3.3.1). The output layer being a classifier uses a softmax. The method attained great accuracy across all performance metrics for all the 4 classes of drug–target chosen in comparison to traditional machine learning techniques or even multi-layer perceptrons. The multi-layer perceptron they trained had the same number of hidden units as the LSTM network being used for a fair comparison. The method performed reasonably well even with a small training sample size like 180 as was available for the nuclear-receptor family. In another recent study by Zheng <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit655"><sup><span class="sup_ref">655</span></sup></a> a deep learning algorithm is developed with both CNN and LSTM. The target/receptor proteins are processed into a fixed length feature vector using a dynamic attentive CNN. 16–32 filters and 30 residual blocks were used in the construction of the dynamic attentive CNN.<a title="Select to navigate to references" href="#cit656"><sup><span class="sup_ref">656</span></sup></a> The drug candidate was represented as a 2D matrix similar to ref. <a title="Select to navigate to references" href="#cit657">657</a>. This is processed using a self-attentional LSTM network<a title="Select to navigate to references" href="#cit655"><sup><span class="sup_ref">655</span></sup></a> known as BiLSTM which extracts features. The number of hidden layers in the BiLSTM network was 64. Three kind of databases were used for numerical experiments. The first is DUD-E.<a title="Select to navigate to references" href="#cit658"><sup><span class="sup_ref">658</span></sup></a> This dataset has 102 target receptors across 8 protein families. Each receptor has 224 active drug molecules and 10<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>000 inactive ones. The final dataset curated from this database had 22<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>645 active drug–protein interaction pair examples and 1<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>407<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>145 negative ones. The second database used was Human<a title="Select to navigate to references" href="#cit659"><sup><span class="sup_ref">659</span></sup></a> containing 1998 unique proteins and 6675 interactions. The third database used is Binding DB<a title="Select to navigate to references" href="#cit660"><sup><span class="sup_ref">660</span></sup></a> which contains experimental results of binding affinities. The dataset curated from this had 39<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>747 positive examples and 31<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>218 negative binding interaction pairs. The two feature vector of the target and the protein are combined and fed into a classifier which generated a probabilitty vector from it using sigmoid activation. This probability vector was then minimized against the data label of positive and negative interaction pairs using cross-entropy with regularization. The metric used to evaluate the final performance of the model is area under the ROC (Receiver-Operating Characteristic)<a title="Select to navigate to references" href="#cit661"><sup><span class="sup_ref">661</span></sup></a> curve. This metric is often abbreviated as AUC in the literature. Receiver-Operating Characteristic (ROC) curve enrichment metric (RE) is also used for the DUD-E dataset. For the Human dataset, the method achieved 98.7% for AUC, outperforming all traditional methods like Random Forests, Support-Vector Machines, <span class="italic">etc.</span> For DUD-E dataset, the method outperformed Vina<a title="Select to navigate to references" href="#cit647"><sup><span class="sup_ref">647</span></sup></a> and AtomNet<a title="Select to navigate to references" href="#cit662"><sup><span class="sup_ref">662</span></sup></a> to name a few. On BindingDB with seen and unseen drug–target pairs too, the model outperformed all traditional competitive algorithms. Visual demonstration of which amino acid residues of the target and what structural features/moieties in the drug are important for interaction was also provided.</p>
        </div>
        <div>
          
          <span id="sect6508"/><span class="c_heading_indent">5.5.2 Ligand-based drug designing protocols using classical machine learning techniques. </span>
          <span>Next we move onto to ligand-based drug designing protocols (see <a title="Select to navigate to figure" href="#imgfig39">Fig. 39(a)</a>). The objective of these methods is to analyze what kind of prospective drug/ligand candidates obtained by screening compound libraries share similar structural features with previously known drug candidates (used as reference) against the given target. In this paradigm one operates under the premise that ligands with such features will have similar bio-activity too against the said target. These methods are therefore useful when direct structural information of the target is not available<a title="Select to navigate to reference" href="#cit615"><sup><span class="sup_ref">615,644</span></sup></a> but the bio-activity of some reference compounds against the target is known from previous domain knowledge. One most commonly employed technique in this category is constructing quantitative relationship between structure and activity (QSAR). QSAR is the analytical quantification to the degree to which structural related molecules will share isomorphic bio-activity and hence allows prediction of the behavior of the newly screened compounds against the specified target. This facilitates the identification of what structural features are responsible for the activity and hence provides insight into the rational synthesis of drugs in the future.</span>
          <p class="otherpara">The first report for the use of deep-learning models in QSAR prediction after the publically accessible Merck challenge was due to Dahl <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit663"><sup><span class="sup_ref">663</span></sup></a> QSAR studies mainly focus on understanding what chemical composition and structural features of the prospective molecule of choice (ligand) might have the desired pharmacological activities against a chosen target (receptor). The study by Dahl <span class="italic">et al.</span> focused mainly on the effectiveness of multi-tasking while designing architectures for neural-networks (basic theoretical formalism discussed in Section 3.3). Multi-tasking refers to the ability of the network design wherein different outputs of interest are simultaneously retrievable. For instance, in the aforesaid study a single-task network would be training using molecular descriptors obtained from compounds within a single assay (data-set) as input features and using the activity of the said molecule as a performance label for comparing the output. However, this model of training requires huge data assemblies from single assays alone which may not always be available. To circumvent this issue, the authors of the aforesaid study combined data from multiple assays using an architecture in the final output layer wherein individual neurons are dedicated to each assay. The same molecule may have appeared in different assays with different activity labels. For any given such molecule, the input feature vector is the molecular descriptor. The output at each of the neurons in the final layer are the activity/inactivity classification values learnt by the network for each assay. This output is then compared against the recorded activity label obtained from the corresponding assay for back-propagation. The study used data from 19 such assays from PubChem database (see Table 1 in ref. <a title="Select to navigate to references" href="#cit663">663</a>) with each assay containing 10<small><sup>4</sup></small>–10<small><sup>5</sup></small> compounds. The molecular descriptors used were generated using Dragon software<a title="Select to navigate to references" href="#cit664"><sup><span class="sup_ref">664</span></sup></a> as a feature vector of length 3764 for each compound. Although the model is a binary classification study, the performance metric used is AUC as defined before. The network was trained using Stochastic-Gradient Descent algorithm with momentum and the problem of over-fitting due to many tunable parameters was eschewed using drop-out.<a title="Select to navigate to references" href="#cit205"><sup><span class="sup_ref">205</span></sup></a> It was seen that the deep-learning network used outperformed traditional machine learning models in 14 out of the 19 assays. Among these 14, multi-task networks outperformed single-task models in 12 of the assays. These favorable results were retained by grouping similar molecules across different assays into a customized composite data-set. The depth of the neural network/addition of more hidden layers did not always produce improvement in the performance of the multi-task network which the authors attribute to the smallness of the data within each assay. All the deep-learning models used handled reasonably well correlated features in the input feature vector. In a related study Ramsundar <span class="italic">et al</span>.<a title="Select to navigate to references" href="#cit665"><sup><span class="sup_ref">665</span></sup></a> resolved certain questions about the efficacy of multi-task neural networks in virtual screening of candidate molecules against several targets using extremely large data-sets. 259 assays were used and divided into 4 groups – PCBA, DUD-E,<a title="Select to navigate to references" href="#cit658"><sup><span class="sup_ref">658</span></sup></a> MUV, and Tox21. Together all these databases had 1.6 M compounds. The validation scheme used is AUC as before and the feature vector of the studied compounds were ECPF4 fingerprints.<a title="Select to navigate to references" href="#cit635"><sup><span class="sup_ref">635</span></sup></a> All such collection of fingerprints for the molecule were hashed into a single bit vector. The trained network as before is a multi-task classifier with each neuron at the output layer having a softmax activation<a title="Select to navigate to references" href="#cit666"><sup><span class="sup_ref">666</span></sup></a> corresponding to each assay. The study found that such multi-task NN performed better than several ML models and the performance metric can be improved with increasing number of tasks and/or increasing the volume of data per task. Certain data-sets in the study showed better performance than others which was attributed to the shared set of compounds among such databases. However the biological class of the target receptor did not affect the performance metric too much. The study concluded by saying that extensive data-sharing among proprietary databases needs to happen to benchmark the performance of such models with bigger data-sets. Another study which thoroughly benchmarked the performance of deep neural networks against a commonly used machine learning model like Random Forest (RF) was due to Ma <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit667"><sup><span class="sup_ref">667</span></sup></a> The report used 15 Kaggle data-sets for training and another 15 for validation too. Each such data-set had around 10<small><sup>4</sup></small>–10<small><sup>5</sup></small> candidates as molecular designs for drugs each labelled with response activity against designated target(s). The type of descriptors used for each of these molecules included combination of atom pairs and global donor–acceptor pairs<a title="Select to navigate to references" href="#cit668"><sup><span class="sup_ref">668</span></sup></a> as input feature vectors. The report used the squared Pearson correlation coefficient (<span class="italic">R</span><small><sup>2</sup></small>)<a title="Select to navigate to references" href="#cit669"><sup><span class="sup_ref">669</span></sup></a> between observed and predicted activities in the testing set as the preferred metric of performance. The study showed that there was a mean improvement in <span class="italic">R</span><small><sup>2</sup></small> of 0.043 when using deep-neural network as opposed to RF against arbitrarily selected parameters. 4 of the data-set showed dramatic improvement in favor of deep-neural network whereas one favored RF. When refined parameter set was used instead of arbitrarily chosen ones, the trend is retained with an expectedly higher mean improvement of 0.051. Increasing the number of hidden layers and also number of neurons in each such hidden layer displayed a <span class="italic">R</span><small><sup>2</sup></small> in favor of deep neural network. Changing the activation function from sigmoid to ReLU<a title="Select to navigate to references" href="#cit666"><sup><span class="sup_ref">666</span></sup></a> also favored the deep network model for 8 data-sets. <span class="italic">R</span><small><sup>2</sup></small> was found to also favor networks when it is not pre-trained.</p>
        </div>
        <div>
          
          <span id="sect6543"/><span class="c_heading_indent">5.5.3 Machine learning and drug-induced toxicity. </span>
          <span>Another area wherein machine learning algorithms are important is identifying if a particular drug when administered in a biological medium would be toxic or not. Such adverse effects due to an administered drug may lead to serious health complications culminating in the eventual withdrawal of the drug during development/testing or even post-marketing, thereby leading to wastage of resources. Many such studies has been initiated like in ref. <a title="Select to navigate to references" href="#cit670">670</a>. The earlier investigations primarily used machine learning methods. This can be exemplified from the work of Rodgers <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit671"><sup><span class="sup_ref">671</span></sup></a> which created a model using the <span class="italic">k</span>-nearest neighbor (kNN) (basic theoretical formalism discussed in Section 3.2.4) for identifying whether a drug candidate from Human Liver Adverse Effects Database (HLAED) belongs to two categories-hepatotoxic in humans or not based on labelled markers from five liver enzymes. With a dataset of over 400 compounds, the algorithm was successful in classifying with a sensitivity of ≥70% and a specificity ≥90%. The model was extended to test unseen cases in World Drug Index (WDI) database and Prestwick Chemical Library (PCL) database with a good success ratio. To do so, a compound similarity metric based on Euclidean norm between molecular candidates was used and an applicability domain threshold was constructed using the metric. Predictions for candidates with similarity scores outside the applicability domain threshold were considered unreliable. Chemical features like aromatic hydroxyl units in drugs like Methyldopa<a title="Select to navigate to references" href="#cit672"><sup><span class="sup_ref">672</span></sup></a> or pyrimidyl units in drugs like Trimethoprim<a title="Select to navigate to references" href="#cit673"><sup><span class="sup_ref">673</span></sup></a> were identified to play a key role in the assignment of high hepatotoxic activity of the respective drugs as they are prone to oxidation and can form hapten adducts with cellular proteins. In yet another study<a title="Select to navigate to references" href="#cit316"><sup><span class="sup_ref">316</span></sup></a> a classification task among molecular candidates was designed using Support-Vector Machines (Basic theoretical formalism discussed in Section 3.2.7) with the Gaussian Radial Basis Function (RBF) as the kernel to group molecules into active and inactive categories with respect to susceptibility to induce phospholipidosis (PLD). Phospholipidosis refers to intracellular accummulation of phospholipids induced by drug candidates when they bind to polar phospholipids in the lysosome.<a title="Select to navigate to references" href="#cit674"><sup><span class="sup_ref">674</span></sup></a> The model was trained by curating dataset from three databases: National Institutes of Health Chemical Genomics Center (NCGC) Pharmaceutical Collections (NPC),<a title="Select to navigate to references" href="#cit675"><sup><span class="sup_ref">675</span></sup></a> the Library of Pharmacologically Active Compounds (LOPAC) and Tocris Biosciences collection, and the target cell used was HepG2.<a title="Select to navigate to references" href="#cit676"><sup><span class="sup_ref">676</span></sup></a> The model developed was found to accomplish the selection task with high sensitivity and specificity as seen from the AUC metric. The training was found to be sensitive to the nature of molecular descriptors used and also to the size of the dataset. Certain simple chemical attributes like size of hydrophillic moieties are often used as indicators to characterize if a drug can induce PLD. Such features even though showed correlation with the identified active compounds in some cases did not agree on some others. On the contrary, features like the presence of positively charged nitrogen center correlated well across the entire dataset. Identification of such features may be important to chemist for avoiding or replacing such structural moieties during the early developmental stage of the drug.</span>
          <p class="otherpara">Deep learning models have also been deployed for the said purpose. In a recent one using artificial neural network, toxicity due to epoxide formation is thoroughly investigated.<a title="Select to navigate to references" href="#cit341"><sup><span class="sup_ref">341</span></sup></a> The study identified among a given set of drugs/ligands and targets which drug is susceptible to be epoxidized with natural oxidants in the biological medium by oxidants like cytochrome P450. Formation of such epoxidized metabolities can be harmful for the body as has been explicitly noted in the case of an anti-epileptic drug like carbamazepine<a title="Select to navigate to references" href="#cit677"><sup><span class="sup_ref">677</span></sup></a> which after epoxidation binds to nucleophillic sites within a protein forming a hapten adduct thereby triggering immune response.<a title="Select to navigate to references" href="#cit678"><sup><span class="sup_ref">678</span></sup></a> The product of such reactions need not always be an epoxide as the study suggests from previous reports<a title="Select to navigate to references" href="#cit679"><sup><span class="sup_ref">679</span></sup></a> for drugs like <span class="italic">N</span>-desmethyl triflubazam wherein a DNA adduct is formed post a transient epoxidation. The neural-network model used in the study not only decides if a given drug is epoxidizable but also focuses on identifying if the site of epoxidation (SOE) is a double bond or an aromatic ring. It further delineates such sites from site of hydroxylation (SOH) which shares some key structural features with SOEs and can also potentially lead to harmful oxidation products. The Accelrys Metabolite Database (AMD) was used from which a dataset of 389 molecules were curated having toxicity labels. These molecules had 411 aromatic SOEs, 168 double bond SOEs and 20 even single bond SOEs. Non-epoxidizable molecules were also included in the set thereafter to afford a proper distinction. To describe each bond within the specific molecule, 214 molecular descriptors/features were identified – 89 each for the left and right atom sharing the bond, 13 specific bond descriptor and 23 overall molecular descriptors. The neural network used had 1 input and 2 output layers. The top output layer was for the molecular epoxidation score whereas the last one for the specific bond epoxidation score. The training data involved using labelled binary vector within the molecule with the designated SOE marked as 1. Cross-entropy minimization was used as the cost function. The final metric of performance as before was AUC. The model outperformed logistic regression and other competitive algorithms in all departments like identification of SOEs, differentiation of SOEs and SOHs. The model could correctly identify SOE in carbamazepine which is absent in substitutes like oxcarbazepine with similar functionality, in the furan ring of furosemide<a title="Select to navigate to references" href="#cit680"><sup><span class="sup_ref">680</span></sup></a> and in severely hepatotoxic sudoxicam <span class="italic">vs.</span> its less problematic cousin like meloxicam.<a title="Select to navigate to references" href="#cit680"><sup><span class="sup_ref">680</span></sup></a> More such examples can be found in specialized topical reviews like ref.<a title="Select to navigate to references" href="#cit670">670</a>.</p>
        </div>
        <div>
          
          <span id="sect6567"/><span class="c_heading_indent">5.5.4 Power of quantum computers and quantum-computing enhanced machine learning techniques. </span>
          <span>As discussed in other domains, quantum computing enhanced machine learning techniques are also beginning to gain attention in the overall drug production pipeline. An early review<a title="Select to navigate to references" href="#cit681"><sup><span class="sup_ref">681</span></sup></a> identified the efficacy of quantum computers in the drug discovery process by noting the key areas wherein quantum computers can impact. The study reported that for structure based drug designing protocols, quantum computers may help in understanding the structure of target protein sequence better. It claimed that using both gate model of quantum computing and quantum annealers, simple problems like the Miyazawa–Jernigan model were investigated for understanding the dynamics of protein folding for smaller peptides.<a title="Select to navigate to references" href="#cit682"><sup><span class="sup_ref">682</span></sup></a> Unfortunately such model problems may not accurately assess the real situation in all cases especially for complicated situations like the presence of several protein conformations with minimal free energy difference thereby rendering them accessible <span class="italic">via</span> thermal fluctuations or how the native 3D conformation of the protein is sustained due to its interaction with the surrounding media. In most cases for biologically significant proteins, crystallized 3D structure is not available in the database due to sheer size of the protein and/or lack of solubility <span class="italic">etc.</span><a title="Select to navigate to references" href="#cit615"><sup><span class="sup_ref">615</span></sup></a> As a result structure-based designing protocols are often thwarted. Better computational models for predicting the protein structure is therefore necessary and can influence the drug-discovery pipeline immensely. Banchi <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit683"><sup><span class="sup_ref">683</span></sup></a> used Gaussian Boson sampling to identify the highest affinity binding poses of a given ligand with the active centre of the target. The primary workhorse of the protocol is to map the active centre and the ligand onto a pharmacophoric feature space of few descriptors. Each such descriptor corresponded to a vertex in a graph and the edges defined the Euclidean distance between the corresponding structural motif in the lowest energy 3D geometry. The resultant encoded graphs were then used to construct a binding-configuration graph wherein the structural features of the ligand and the target that are compatible to bind are represented by weighted vertices. The maximum-weighted clique (a closed sub-graph) within the binding-configuration will be the preferred binding pose. Such configurations are identified with high-probability using a Gaussian Boson sampler with the input state of photons being in the squeezed states and identifying the detector wherein the photon appears at the output. Such detectors correspond to vertices on the binding-configuration graph. If the combination so-attained at the output is not a clique then the authors have defined greedy shrinking of vertices or expansion of vertices by probing the local environment to modify the search space for next iteration. Recently, an efficient hybrid-variational algorithm amenable to the NISQ architecture has also been proposed which using an <span class="italic">N</span>-sequence amino acid can construct a sample of the lower energy 3D-conformations.<a title="Select to navigate to references" href="#cit684"><sup><span class="sup_ref">684</span></sup></a> Previous reports tackling the same problem were either inefficient or offered problem specific solutions.<a title="Select to navigate to references" href="#cit685"><sup><span class="sup_ref">685</span></sup></a> The work represented the given sequence by stacking monomeric units on a tetrahedral lattice. 4 qubits were assigned for each of the 4 different directions the lattice could grow from a given monomer. New ancillary qubits were also used to define the interactions between l nearest neighboring (l-NN) monomeric units. A graph Hamiltonian was constructed with these interactions and the self-energy terms of each residue. Overlapping positions of the amino acids were avoided by including penalty terms corresponding to such assignments. The ground state of this Hamiltonian is the most stable conformation. To solve the problem, a variational circuit was constructed parameterized by tunable rotation angles of both the single-qubit and entangling gates involved. The various bit-strings obtained from the measurement of the circuit encoded the 3D structural arrangement of the amino acid residues in the tetrahedral lattice and the energy distribution would highlight the relative stability of these various arrangements/conformations. The circuit was optimized variationally to selectively enhance the chances of sampling the lower energy conformations corresponding to the tail of the aforesaid energy distribution. The number of qubits in the method scales as <span class="italic">O</span>(<span class="italic">N</span><small><sup>2</sup></small>) and the number of terms in the Hamiltonian is <span class="italic">O</span>(<span class="italic">N</span><small><sup>4</sup></small>). The model was tested on a 10 amino acid peptide Angiotensin using 22 qubits and also on a designed 7 amino acid neuropeptide using 9 qubits on IBMQ processors. In the former the probability of collectively sampling all the lower energy conformations was reported to be 89.5% which augmented further with increase in the number of measurements.</span>
          <p class="otherpara">However, recently, a study from Google's DeepMind (UK),<a title="Select to navigate to references" href="#cit686"><sup><span class="sup_ref">686</span></sup></a> has made enormous strides in predicting the 3D structure of a peptide from just a given sequence of amino acids solving this 50 year old grand challenge. Even though the algorithm uses a neural-network architecture trainable on a classical computer (and hence is not a quantum-computing enhanced algorithm), yet the performance of the method is so good that it deserves a special discussion. This novel algorithm has won the CASP14 challenge which involves a blind assessment of the efficacy of structure determination from amino acid sequence for proteins/peptides whose structure has been recently solved through explicit experimentation yet has not been publicly released in common databases. The neural network design is broken down into three components. The first component in the neural network architecture takes as input a 1D sequence of amino acid residues and searches multiple databases to generate multiple sequence alignments (MSAs) which are essentially sequence of previously identified amino acids closely resembling the target one and understanding the evolutionary history of the mutation of these MSAs. This is important to derive information about structural proximity between amino acid pairs which shows correlated mutation. This component then generates a template 3D structure also called pair representation as an initial hypothesis that is to be modified at later stages. The next functional unit is called the Evoformer and is the heart of the design. This takes as input both the pair representation and the MSA and subsequently refines the representation of each self-iteratively using the architecture of transformer.<a title="Select to navigate to references" href="#cit459"><sup><span class="sup_ref">459</span></sup></a> The third block takes the refined MSA and the pair representation and generates a 3D structure which is essentially cartesian co-ordinates of the individual atoms, <span class="italic">i.e.</span>, the native 3D conformation of the protein/peptide. This process is repeated several times by feeding back the 3D structure into the Evoformer block until convergence. The final prediction seems to have surpassed all previously known methods with an accuracy of 0.95 Å root-mean square error from the target structure for 95% of the residues. This will definitely be a landmark study and for years to come one has to investigate the efficacy of the method for antibodies, synthetic peptide sequences for which evolutionary data to generate the initial MSA will be scarce. This method will positively impact understanding protein–protein interactions and also bring in new insight into diseases like Alzheimer's and Parkinson's. For structure based drug-discovery since at least one method exist now which can determine the 3D conformation efficiently thereby massively speeding up the product development pipeline, quantum computers can now help in understanding the drug–protein affinity and molecular docking mechanisms. The review by Cao <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit681"><sup><span class="sup_ref">681</span></sup></a> already identifies this possibility by noting that with algorithms like quantum phase estimation on fault-tolerant devices and variational eigensolvers for near term devices, we are capable of computing the potential energy surfaces of larger molecular systems efficiently and hence force-field calculations as is required for understanding molecular docking will also be greatly impacted.</p>
          <p class="otherpara">For QSAR studies too, benchmarking the performance of quantum computer against classical processors has been documented recently in the work of Batra <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit317"><sup><span class="sup_ref">317</span></sup></a> The authors have used several molecular databases to identify prospective ligands for diseases like <span class="italic">M. tubercolosis</span>, Krabbe disease, SARS-CoV-2 in Vero cells, plague and hERG. With the curated data from the compound datasets feature vectors were constructed and used for binary classification of the compound in active or inactive using kernel-based SVM techniques (see Section 3.2.7 for basic theoretical formalism). The authors used several techniques to reduce the size of the feature vector such that the classification can be performed on a NISQ device (ibmq_rochester was used) using Qiskit. It was seen that for most datasets comparable accuracy on a quantum computer was attained too using the feature reduction techniques employed by the authors. A hybrid quantum-classical approach was also used for high-throughput virtual screening data screening with good accuracy and slightly faster run time for processing the data on a quantum computer than on a classical computer. Representative data from the study are displayed in <a title="Select to navigate to figure" href="#imgfig40">Fig. 40</a>.</p>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig40"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f40_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f40.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f40.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 40 </b> <span id="fig40"><span class="graphic_title">(a) Classification of ligands into active and inactive ones based on performance against SARS-CoV 2 in the space of the two most dominant features after application of two methods of feature extraction as in ref. <a title="Select to navigate to references" href="#cit317">317;</a> and (b) classification of ligands into active and inactive ones based on performance based on performance against <span class="italic">M. tubercolosis</span> in the space of the two most dominant features.<a title="Select to navigate to references" href="#cit317"><sup><span class="sup_ref">317</span></sup></a> (c) The run time required for screening training datasets of varying sizes on a quantum computer (QC) and a classical computer (CC). The training time shows sublinear scaling on QC displaying an advantage.<a title="Select to navigate to references" href="#cit317"><sup><span class="sup_ref">317</span></sup></a> Reprinted (adapted) with permission from K. Batra, K. M. Zorn, D. H. Foil, E. Minerali, V. O. Gawriljuk, T. R. Lane and S. Ekins, <span class="italic">J. Chem. Inf. Model.</span>, 2021, <span class="bold">61</span>, 6. Copyright 2021 American Chemical Society.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">Beyond the precincts of academic research, even the interest of industrial players on quantum-enabled technologies seems to be escalating rapidly. The report by Zinner <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> has identified that 17 out of 21 established pharmaceutical companies and 38 start-ups are directly working on enhancing and ameliorating the technical challenges in the drug discovery pipeline using quantum computers. 75% of such companies so far have been identified to be geographically in Europe and North America. The cumulative funding received by all the start-ups as per the report<a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> is €311 million with the top five funded start-ups being Cambridge Quantum Computing, Zapata, 1QBit, Quantum Biosystems and SeeQC. Most of the activity is directed towards virtual screening for ligand-based drug designing protocol and subsequent lead optimization.</p>
          <p class="otherpara">The reports from Langione <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit688"><sup><span class="sup_ref">688</span></sup></a> and Evers <span class="italic">et al.</span> from McKinsey<a title="Select to navigate to references" href="#cit689"><sup><span class="sup_ref">689</span></sup></a> also systematically delineate what pharmaceutical industries should do to prepare themselves so as to attain a favorable position in order to leverage the quantum revolution. Both the reports agree that bio-pharmaceutical companies should start now to reap the benefits of early movers advantage. In fact ref. <a title="Select to navigate to references" href="#cit688">688</a> mentions that it might be possible that tech-giants equipped with quantum computers with higher number of qubits and better noise-tolerance might enter the race of <span class="italic">in silico</span> drug discovery in the future relegating the task of post-design synthesis, clinical trials and commercialization to pharmaceutical companies. This can lead to a situation wherein companies may race to patent the best molecule that are responsive to a particular disease. However pharmaceutical companies are at an advantage here due to years of experience with computational drug-designing protocols. So strictly they do not have to change the inherent model or the business goal they already follow. They will likely be using a more capable device like a quantum computer to attain that goal. In order to avoid such undue competition, pharmaceutical companies should start now by assessing and answering a few key questions about the probable impact quantum computers are likely to have on the workflow and the specific product design the respective company is targeting. This can happen by understanding what are the key areas where development can be sought in the product design model the company is following and more importantly if those areas fall into the category of problems that can be solved efficiently on a quantum computer. The key directions to think would be would a quantum-computer enabled business provide an unprecedented value to their supply-chain or an undue advantage to their competitors. If the analysis is positive, one needs to then think of the time scale of such developmental changes and whether the resources the company has access to would be enough to sustain entry into the domain. Some of above resource overheads can be solved through appropriate memberships through consortium like QuPharm that categorically specializes in the designated area. QuPharm, as an organization, was developed by many member pharmaceutical companies for specifically understanding and optimizing the benefits the quantum revolution can have on the pharmaceutical industry. The consortium is known to have a collaboration with Quantum Economic Development Consortium (QED-C).<a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> Hardware needs can be sorted through collaborations with end-to-end technological providers like Google, IBM, Honeywell, Rigetti, Xanadu, <span class="italic">etc.</span> each of which have quantum computers of varying architecture and even on different platforms and have even promised to develop larger scale ones in the near future. For example Amgen, a pharmaceutical company has declared collaboration with both Amazon Braket and IBMQ.<a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> Boehringer, another pharmaceutical company has established collaboration with Google QuantumAI to develop algorithms for molecular dynamics simulation.<a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> New software solutions are necessary to interface with the quantum hardware for which commercially available varieties like Qiskit (IBM), OpenFermion (Google), and tKet (Cambridge Quantum Computing) can be leveraged. Companies can also initiate partnerships with firms like ProteinQure, GTN, Rahko, Qulab <span class="italic">etc.</span> which are developing softwares to specifically cater to advancing quantum-computing algorithms for drug discovery. Classical computing algorithms have also benefited from the ideas that were synthesized to initiate a rapid discovery of quantum algorithms over the last few years. Companies like Qubit Pharmaceuticals, Turbine <span class="italic">etc.</span> are developing such approaches and combining them with machine learning. Such collaborations with global and local companies with specialized expertise can result in engineering custom solutions to tackle specific problems during the drug discovery pipeline. Pharmaceutical companies can thus immensely benefit from such collaborative ventures.</p>
          <p class="otherpara">Partnerships can be built with other pharmaceutical companies or start-ups too to share expertise and develop mutually beneficial quantum computing based drug-development protocols. The report by Zinner <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> has identified 17 pharmaceutical companies with publically disclosed collaborations with at least 12 start-ups. All the pharmaceutical companies are members of QuPharm, NEASQC <span class="italic">etc.</span> Among the big pharma corporations Merck (Germany) has invested €4 million on start-ups like SeeQC and has disclosed active collaboration with Rahko, HQC.<a title="Select to navigate to reference" href="#cit690"><sup><span class="sup_ref">690,691</span></sup></a> Merck (USA) has made financial investment in Zapata.<a title="Select to navigate to references" href="#cit692"><sup><span class="sup_ref">692</span></sup></a> Non-equity partnerships like that by Amgen with QSimulate to develop advanced molecular simulation tools or like that by AstraZeneca with ProteinQure to design amino-acid sequences have also been seen.<a title="Select to navigate to references" href="#cit693"><sup><span class="sup_ref">693</span></sup></a> Some new collaborations are also announced which depicts not only the prospective promise associated with the technology but also the seriousness of the industry players. For example, Cambridge Quantum Computing has announced a collaboration with CrownBio and JSR LifeSciences about using quantum machine learning algorithms to identify multi-cancer genes biomarkers<a title="Select to navigate to references" href="#cit694"><sup><span class="sup_ref">694</span></sup></a> which can positively impact bioinformatics research. Apart from this, it would also be beneficial for pharmaceutical companies to collaborate with scientists in academia. Adopting a strategy that allows effective co-operation among all involved parties internal and/or external beyond the traditional organizational structure will accelerate growth and foster efficient and fast sharing of resources and knowledge which can otherwise be harder to access due to institutional barriers. The reports<a title="Select to navigate to reference" href="#cit688"><sup><span class="sup_ref">688,689</span></sup></a> identify that recruitment of skilled technicians and professionals with training in developing algorithms on a quantum computer is necessary for pharmaceutical companies to enhance quantum-enabled research. The report by Zinner <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit687"><sup><span class="sup_ref">687</span></sup></a> conducted a thorough search across 21 pharmaceutical companies with a combined revenue of €800 billion in 2019 and reported that only about 50 employees were found designated to quantum-enabled technology. This is partly because quantum computing is an emerging technology and hence such a skillset may not be readily available among the usually hired talent pool of the pharmaceutical industries.<a title="Select to navigate to references" href="#cit688"><sup><span class="sup_ref">688</span></sup></a> Companies like IBM which hold outreach programs and workshops in quantum computing interfaces like Qiskit can be a useful resource. The other alternative might be looking into developing specialized programs internally to train scientists and engineers once hired.</p>
        </div>
      
    
    
      
      <h2 id="sect6635"><span class="a_heading">6 Insight into learning mechanisms</span></h2>
      <span>Despite the enormous success of machine learning coming from network based models with large number of tunable parameters, little progress has been made towards understanding the generalization capabilities displayed by them.<a title="Select to navigate to references" href="#cit695"><sup><span class="sup_ref">695</span></sup></a> The choice of hyperparameters in these models have been based on trial and error with no analytical guidance, despite them showing enormous potential in analyzing data sets. Physics on the other hand has provided us with white box models of the universe around us that provide us with tools to predict and examine observed data. Intuition from statistical mechanics has helped provide understanding with respect to the learning limits of some network models. Seminal contributions in this regards include methods from spin glass theory, that have been used to extensively study associative memory of Hopfield networks<a title="Select to navigate to references" href="#cit696"><sup><span class="sup_ref">696</span></sup></a> and Valiants theory of learnable models that introduced statistical learning into the then existing logic based AI.<a title="Select to navigate to references" href="#cit697"><sup><span class="sup_ref">697</span></sup></a> Another major contribution comes from Gardner's usage of replica trick to calculate volume in the parameter space for feed forward neural networks in the case of both supervised and unsupervised models.<a title="Select to navigate to reference" href="#cit698"><sup><span class="sup_ref">698,699</span></sup></a> The problem of learning was also shown to exhibit phase transitions in reference to generalization and training efficiency.<a title="Select to navigate to references" href="#cit700"><sup><span class="sup_ref">700</span></sup></a> A typical example of one such parameter is the ratio of input size to the number of model parameters. A diminishing value usually results in overfitting while a large one allows for successful generalization. The reader is encouraged to refer ref. <a title="Select to navigate to reference" href="#cit701">701–703</a> for some of the early studies that made use of statistical physics to understand multi layered network learning.</span>
      <p class="otherpara">We will see that some self averaging statistical properties in large random systems with microscopic heterogeneity give rise to macroscopic order that does not depend on the microscopic details. Learning these governing dynamics can play an important role in tuning the performance of machine learning techniques. One of the tools that provides an analytical handle in analyzing these details is the replica method.<a title="Select to navigate to references" href="#cit704"><sup><span class="sup_ref">704</span></sup></a> Replica methods have been used to explore the teacher–student model to provide information theoretic best estimates of the latent variables that teacher uses in generating the data matrix handed over to the student.<a title="Select to navigate to references" href="#cit705"><sup><span class="sup_ref">705</span></sup></a> This problem can be specialized to the case of providing a low rank matrix decomposition matrix of underlying input data. Interest on statistical methods has stayed dormant since 1990, due to the limited tractability of algorithms used in learning. It was sparked again with the contribution of Decelle<a title="Select to navigate to references" href="#cit706"><sup><span class="sup_ref">706</span></sup></a> to use spin glass theory to study stochastic block model that played a major role in understanding community detection in sparse networks. They observed second order phase transitions in the models that separated regions of efficient clustering when solved using belief propagation algorithms.<a title="Select to navigate to references" href="#cit707"><sup><span class="sup_ref">707</span></sup></a> For a comprehensive list of physics inspired research in the machine learning community, refer ref. <a title="Select to navigate to references" href="#cit708">708</a>.</p>
      <p class="otherpara">Generative models are suitable for feature extraction apart from doing domain sampling. Within every layer of one such network, one could imagine some form of feature extraction being made to provide for a compact representation, which might later be used by generative models to learn the distribution of classifiers to do prediction. We point towards this feature extraction as the central idea for relating machine learning to renormalization group. We investigate to see if concepts like criticality and fixed points have something to reveal about the nature in which learning happens in the framework of deep learning and machine learning in general. <a title="Select to navigate to figure" href="#imgfig41">Fig. 41</a> provides a schematic sketch of the methods that have been primarily explored in studying network models. In here we shall restrict our attention to cavity methods, Renormalization group and Replica methods. Refer ref. <a title="Select to navigate to references" href="#cit709">709</a> for a thorough exploration of message passing algorithms.</p>
      <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig41"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f41_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f41.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f41.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 41 </b> <span id="fig41"><span class="graphic_title">A schematic representation of the techniques borrowed from statistical physics that have been used to study model learning of networks in machine learning.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
      <p class="otherpara">A somewhat non rigorous argument for the remarkable working of these machine learning algorithm comes from noting the following two observations. Consider a vector of input size <span class="italic">n</span> taking <span class="italic">v</span> values and thus can span a space of <span class="italic">v</span><small><sup><span class="italic">n</span></sup></small> possible configurations. Despite this being exponentially a large space for image datasets, we have managed to build relatively small networks that learn to identify the features of the image accurately. This is to be attributed to the realization that the class of relevant images comes from a relatively small subspace that is efficiently learnt by the neural network with relatively fewer nodes that scale as <span class="italic">nv</span> instead.<a title="Select to navigate to references" href="#cit710"><sup><span class="sup_ref">710</span></sup></a> This is much similar to how low energy states of interest of Hamiltonian mapping to small subspace of the Hilbert space. This simplification comes from the Hamiltonian having a low polynomial order, locality and symmetry built into it. Secondly, despite the vastness of all possible inputs that can be generated, most input data can be thought of coming from a Markovian process that identifies at each stage a select set of parameters. Thus the work of a deep learning machine would be to reverse some form of heirarchial markovian generative process using some minimal statistic function (A function <span class="italic">f</span> is minimal statistic if for some random variables <span class="italic">y</span> and <span class="italic">x</span> we have <span class="italic">P</span>(<span class="italic">y</span>|<span class="italic">x</span>) = <span class="italic">P</span>(<span class="italic">y</span>|<span class="italic">T</span>(<span class="italic">x</span>))) that retains the features of the probability distribution by preserving mutual Information.</p>
      <p class="otherpara">The methods in the following subsection describe its working in the context of Ising model, so we start by describing one briefly. Ising is a model for representing classical spins or magnets arranged in a 2d lattice whose interaction with one another is quantified by the strength of the coupling. Each spin takes a binary configuration (+1, −1) of choosing to align up or down. At low temperatures spins prefer aligning in the same direction forming a magnet. At high temperatures the thermal fluctuations kill any order within the system causing them to arrange chaotically with no net observable magnetic field. A general Ising Hamiltonian is given by,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn130"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t298_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t298.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t298.gif"/></a></td><td class="rightEqn">(130)</td></tr></table>where 〈<span class="italic">ij</span>〉 indicates the sum over nearest neighbour pairs. The probability of any given configuration is determined by the Boltzmann distribution with inverse temperature of the system scaling the governing Hamiltonian. Expectation values of observables correspond to averages computed using the distribution. Given any observable <span class="italic">O</span> the expectation value at a given inverse temperature <span class="italic">β</span> is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn131"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t299_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t299.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t299.gif"/></a></td><td class="rightEqn">(131)</td></tr></table></p>
      
        
        <h3 id="sect6684"><span class="b_heading">6.1 Replica method</span></h3>
        <span>Replica method is a way of computing self averaging expectation values of observables <span class="italic">O</span>(<span class="italic">x</span>) such that <span class="italic">x</span> is a minimizer of <span class="italic">H</span>(<span class="italic">x</span>,<span class="italic">D</span>) where <span class="italic">D</span> is the distribution of some input space. A typical example of <span class="italic">H</span> would be the cost function of a learning problem and <span class="italic">D</span> would be the dataset used for training in the problem. <a title="Select to navigate to figure" href="#imgfig42">Fig. 42</a> provides a schematic representation of the use of replica method. Here we shall explore it in the context of Ising Hamiltonians. Consider the Ising model of <span class="italic">N</span> spins, given by the following Hamiltonian:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn132"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t300_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t300.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t300.gif"/></a></td><td class="rightEqn">(132)</td></tr></table>where the connectivity matrix entries <span class="italic">J</span><small><sub><span class="italic">ij</span></sub></small> has been sampled independently from a Gaussian distribution with zero mean and variance 1/<span class="italic">N</span>. The spins take values from +1, −1. In a bath of inverse temperature <span class="italic">β</span> this results in an equilibrium distribution that is governed by the Gibbs distribution given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn133"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t301_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t301.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t301.gif"/></a></td><td class="rightEqn">(133)</td></tr></table>where <span class="italic">Z</span> is the partition function. We would like to analyze the structure of low energy patterns that is independent of the realization of the couplings <span class="italic">J</span><small><sub><span class="italic">ij</span></sub></small> in the large <span class="italic">N</span> limit. Properties of disordered system can be learnt by studying self averaging properties (have zero relative variance when averaged over multiple realizations). Steady states emerge in the large <span class="italic">N</span> limit as a result of diverging free energy barriers causing time average activity patterns that no longer look like Gibbs averaging, due to broken ergodicity.</span>
        <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig42"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f42_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f42.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f42.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 42 </b> <span id="fig42"><span class="graphic_title">A schematic representation showing the use of replica replica trick and replica ansatz to compute the expectation value of self averaging observables.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
        <p class="otherpara">To study the patterns encoded within the Gibbs distribution, we start with computing the free energy average over all the realizations of <span class="italic">J</span>. This involves computing expectations of a logarithm which can be simplified using the following Replica trick:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn134"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t302_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t302.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t302.gif"/></a></td><td class="rightEqn">(134)</td></tr></table>Evaluating 〈<span class="italic">Z</span><small><sup><span class="italic">n</span></sup></small>〉 is much simpler as this can be expressed as an average over replicated neuronal activity, <span class="italic">i.e.</span>,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn135"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t303_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t303.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t303.gif"/></a></td><td class="rightEqn">(135)</td></tr></table>where <span class="italic">s</span><small><sup><span class="italic">a</span></sup></small> denotes the set of replicated spins over which the averaging is done. The Gaussian integrals can be easily evaluated resulting in<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn136"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t304_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t304.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t304.gif"/></a></td><td class="rightEqn">(136)</td></tr></table>where the overlap matrix <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t305_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t305.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t305.gif"/></a>. Minimizing the free energy amounts to retaining patterns with maximal overlap. The non zero averaging can be interpreted as certain patterns being promoted within replicas and enforced across different realizations of <span class="italic">J</span>. The minimization of free energy gives a self consistent equation for <span class="italic">Q</span>,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn137"/><span id="eqn137"><span class="italic">Q</span><small><sub><span class="italic">ab</span></sub></small> = 〈<span class="italic">s</span><small><sup><span class="italic">a</span></sup></small><span class="italic">s</span><small><sup><span class="italic">b</span></sup></small>〉</span></td><td class="rightEqn">(137)</td></tr></table>where 〈·〉 refers to averaging over the distribution <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t306_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t306.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t306.gif"/></a> where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t307_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t307.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t307.gif"/></a>. The Hamiltonian <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0048_0303.gif" alt="[H with combining tilde]"/></span> is symmetric with respect to permutation of indices and thus <span class="italic">Q</span><small><sub><span class="italic">ab</span></sub></small> = <span class="italic">q</span> for all <span class="italic">a</span> ≠ <span class="italic">b</span>. Minimizing the free energy with respect to the variable <span class="italic">q</span> gives a self consistent equation<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn138"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t308_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t308.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t308.gif"/></a></td><td class="rightEqn">(138)</td></tr></table>where <span class="italic">z</span> is a random variable with Gaussian distribution of mean zero and variance 1/<span class="italic">N</span>. For <span class="italic">β</span> &lt; 1, <span class="italic">i.e.</span>, high temperature, <span class="italic">q</span> = 0 is the only solution, representing a paramagnetic phase while for <span class="italic">β</span> &gt; 1, <span class="italic">i.e.</span>, low temperatures we have a continuously raising <span class="italic">q</span> value from zero, suggesting a phase transition. However, the replica symmetric saddle point solution for <span class="italic">Q</span><small><sub><span class="italic">ab</span></sub></small> derived for the Ising Hamiltonian is unstable and is thus inconsistent with physical predictions.<a title="Select to navigate to references" href="#cit711"><sup><span class="sup_ref">711</span></sup></a></p>
      
      
        
        <h3 id="sect6776"><span class="b_heading">6.2 Cavity method</span></h3>
        <span>The cavity method<a title="Select to navigate to references" href="#cit712"><sup><span class="sup_ref">712</span></sup></a> provides for an alternative analysis to the results derived from Replica method. Consider the Ising Hamiltonian over <span class="italic">N</span> neurons reordered in the following fashion:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn139"/><span id="eqn139"><span class="italic">H</span>(<span class="italic">s</span>,<span class="italic">J</span>) = −<span class="italic">s</span><small><sub>1</sub></small><span class="italic">h</span><small><sub>1</sub></small> + <span class="italic">H</span><small><sub>−1</sub></small></span></td><td class="rightEqn">(139)</td></tr></table>where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t309_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t309.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t309.gif"/></a> is the local field at site 1 and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t310_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t310.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t310.gif"/></a> is the remaining Hamiltonian that defines the interaction over other spins. The distribution of <span class="italic">h</span><small><sub>1</sub></small> in the system of the remaining <span class="italic">N</span> − 1 neurons is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn140"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t311_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t311.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t311.gif"/></a></td><td class="rightEqn">(140)</td></tr></table>The joint distribution of <span class="italic">h</span><small><sub>1</sub></small>, <span class="italic">s</span><small><sub>1</sub></small> is thus given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn141"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t312_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t312.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t312.gif"/></a></td><td class="rightEqn">(141)</td></tr></table>Since the cavity field in this method decouples with the remaining neurons, we can approximate the distribution for <span class="italic">h</span><small><sub>1</sub></small> with a Gaussian of mean <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t313_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t313.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t313.gif"/></a> and variance <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t314_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t314.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t314.gif"/></a>. The variance has inbuilt into it an approximation of vanishing correlations 〈<span class="italic">s</span><small><sub><span class="italic">i</span></sub></small><span class="italic">s</span><small><sub><span class="italic">j</span></sub></small>〉<small><sub>−1</sub></small> that is equivalent to the single energy well approximation made in the Replica solution. Under this approximation we can write<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn142"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t315_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t315.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t315.gif"/></a></td><td class="rightEqn">(142)</td></tr></table></span>
        <p class="otherpara">We expect <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t316_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t316.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t316.gif"/></a> to be self averaging and have a Gaussian distribution (as <span class="italic">J</span><small><sub><span class="italic">ik</span></sub></small> is uncorrelated with 〈<span class="italic">s</span><small><sub><span class="italic">k</span></sub></small>〉<small><sub>−<span class="italic">i</span></sub></small>) with a 0 mean and variance <span class="italic">q</span> over random realizations of <span class="italic">J</span><small><sub><span class="italic">ik</span></sub></small> in the large <span class="italic">N</span> limit. Replacing the averaging over the neurons with an average over the Gaussian distribution we get<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn143"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t317_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t317.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t317.gif"/></a></td><td class="rightEqn">(143)</td></tr></table></p>
        <p class="otherpara">Since all the neurons are equivalent, neuron 1 replaced with any other neuron in <a title="" href="#eqn142">eqn (142)</a>. The mean activity of neuron <span class="italic">i</span> is thus given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn144"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t318_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t318.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t318.gif"/></a></td><td class="rightEqn">(144)</td></tr></table></p>
        <p class="otherpara">We can average over the above expression to write a self consistency condition on <span class="italic">q</span>. We thus get<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn145"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t319_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t319.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t319.gif"/></a></td><td class="rightEqn">(145)</td></tr></table>Substituting a generalized version of <a title="" href="#eqn143">eqn (143)</a> for each neuron <span class="italic">i</span> in the above equation we derive 138, obtained from the replica method.</p>
      
      
        
        <h3 id="sect6854"><span class="b_heading">6.3 Renormalization group and RBM</span></h3>
        <span>Renormalization group (RG)<a title="Select to navigate to references" href="#cit713"><sup><span class="sup_ref">713</span></sup></a> is based on the idea that physics describing long range interactions can be obtained by coarse graining degrees of freedom at the short scale. Under this scheme small scale fluctuations get averaged out iteratively and certain relevant features becomes increasingly more prominent. This helps in building effective low energy physics, starting from microscopic description of the system. Despite its exactness and wide usage within the fields of quantum field theory and condensed matter, any form of exact RG computations in large systems is limited by computational power. RG was introduced within the context of Quantum Electrodynamics (QED)<a title="Select to navigate to references" href="#cit714"><sup><span class="sup_ref">714</span></sup></a> and played a crucial role in addressing the problem of infinities. A proper physical understanding was given by Kadanoff within condensed matter systems while proposing the idea of block spin renormalization group.<a title="Select to navigate to references" href="#cit715"><sup><span class="sup_ref">715</span></sup></a> This formed the ground for the later seminal work of Kenneth Wilson in producing the scaling laws of correlations near the critical point.<a title="Select to navigate to references" href="#cit716"><sup><span class="sup_ref">716</span></sup></a></span>
        <p class="otherpara">RG can be analytically studied for 1d Ising model, as decimation does not produce additional interaction terms, leaving the hierarchy of effective Hamiltonians tractable. Consider the following Ising spin Hamiltonian with <span class="italic">N</span> spins whose interaction is given by<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn146"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t320_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t320.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t320.gif"/></a></td><td class="rightEqn">(146)</td></tr></table>where <span class="italic">i</span> runs over all the spins of the current description. Here <span class="italic">J</span><small><sub>0</sub></small> is the strength of the uniform coupling and no external magnetic field. We study how the couplings transforms by doing a decimation over the odd spins (summing over the degrees of freedom labelled odd). This results in the following Hamiltonian that only depends on the remaining <span class="italic">N</span>/2 spins with no new interaction terms generated:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn147"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t321_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t321.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t321.gif"/></a></td><td class="rightEqn">(147)</td></tr></table>where <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t322_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t322.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t322.gif"/></a>. This can be repeated recursively <span class="italic">k</span> times giving rise to <span class="italic">H</span><small><sub><span class="italic">k</span></sub></small> that depends on the remaining <span class="italic">N</span>/2<small><sup><span class="italic">k</span></sup></small> spins.</p>
        <p class="otherpara">For the 2d Ising model doing renormalization using spin decimation is not feasible as this produces higher order interactions that are not tractable. Approximations of higher order interactions have been introduced to allow for analytical extensions.<a title="Select to navigate to references" href="#cit717"><sup><span class="sup_ref">717</span></sup></a> At the critical temperature the system exhibits conformal symmetry and this fixes the 2 point and higher point correlations along with the scaling dimensions. To verify that an observable <span class="italic">A</span>′ defined over the renormalized degrees of freedom remains invariant under renormalization, we will compute the expectation value over the initial probability distribution. Let <span class="italic">p</span> refer to the probability distribution generated by the initial Hamiltonian <span class="italic">H</span> over spins <span class="italic">σ</span> and <span class="italic">p</span>′ be the probability distribution generated by the renormalized Hamiltonian <span class="italic">H</span>′ over spins <span class="italic">σ</span>′. Thus,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn148"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t323_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t323.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t323.gif"/></a></td><td class="rightEqn">(148)</td></tr></table></p>
        <p class="otherpara">Note that this is only true for observables that are defined on the coarse grained degrees and does not work for those defined on the observables that are defined on the microscopic degrees as these correlations are washed out during renormalization. In the remainder of this section we shall talk about methods of generating RG flows using RBM on a uniform 2d Ising Hamiltonian. Any indication of RBM generating flows that approach criticality like RG should be indicated through correlators that follow the behavior of conformal fields.</p>
        <p class="otherpara">RG flows can be well described in the space of parameters that weighs different operators that make up the Hamiltonian. As one coarse grains the Hamiltonian from UV (microscopic description) to an IR (macroscopic description) prescription, we observe that certain parameter weights flow to zero (monotonically decrease). These are termed as irrelevant operators as they play no role in the flow. Operators which regulate the flow with monotonically increasing weights are relevant operators. Within the space of all possible Hamiltonians lies a critical surface where the theory respects conformal symmetry with length scales that run to infinity. When the RG flow meets such a surface it results in a fixed point referred to as critical point. Critical points are usually associated with phase transitions. For example, the critical temperature of uniform 2d Ising with no external magnetic field is given by <span class="italic">T</span><small><sub>c</sub></small> = 2.269 and marks the demarcation between low temperature ferromagnetic and high temperature paramagnetic phases. We shall describe three different methods of generating RBM flows using: (a) learned weights (b) variational RG and (c) real space mututal information.</p>
        <div>
          
          <span id="sect6898"/><span class="c_heading_indent">6.3.1 From learned weights. </span>
          <span>In this method flows are generated through a Markov Chain of alternatively sampling the hidden and visible layer starting from a given distribution of initial configurations <span class="italic">q</span><small><sub>0</sub></small>(<span class="italic">v</span>) that correspond to a Hamiltonian with parameters <span class="italic">λ</span><small><sub>0</sub></small>. The RBM then generates a flow as follows:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn149"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t324_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t324.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t324.gif"/></a></td><td class="rightEqn">(149)</td></tr></table></span>
          <p class="otherpara">This produces a new distribution <span class="italic">q</span><small><sub>1</sub></small>(<span class="italic">v</span>) that corresponds to a flow within the probability distribution and can be seen as the distribution generated by some Hamiltonian of the same statistical model with parameters <span class="italic">λ</span><small><sub>1</sub></small>. This would correspond to a flow within the parameter space (<span class="italic">λ</span><small><sub>0</sub></small> → <span class="italic">λ</span><small><sub>1</sub></small>). We would like to verify if such an transformation on the parameter space actually corresponds to an RG flow. We do that by implicitly computing correlation functions of certain operators and comparing against known results from RG.</p>
          <p class="otherpara">The number of nodes in the successive layers is kept the same as the same RBM is used to produce the flow, unlike RG with reducing degrees of freedom as one flows. Its observed that the RBM flow generated in this method approaches the critical point, despite the RG flow moving away from unstable points. Despite these differences the authors still manage to provide accurate predictions of the cirtical exponents in ref. <a title="Select to navigate to references" href="#cit15">15</a> and <a title="Select to navigate to references" href="#cit16">16</a>. At the critical temperature the Ising model enjoys conformal symmetry, giving rise to operators whose correlations scale by well known power laws. The authors of the paper have used <span class="italic">s</span><small><sub><span class="italic">ij</span></sub></small> = <span class="italic">σ</span><small><sub><span class="italic">ij</span></sub></small> − <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e0d2.gif" alt="[small sigma, Greek, macron]"/></span> that has a weight of <span class="italic">Δ</span><small><sub><span class="italic">s</span></sub></small> = 1/8 and <span class="italic">ε</span> = <span class="italic">s</span><small><sub><span class="italic">ij</span></sub></small>(<span class="italic">s</span><small><sub><span class="italic">i</span>+1,<span class="italic">j</span></sub></small> + <span class="italic">s</span><small><sub><span class="italic">i</span>−1,<span class="italic">j</span></sub></small> + <span class="italic">s</span><small><sub><span class="italic">i</span>,<span class="italic">j</span>+1</sub></small> + <span class="italic">s</span><small><sub><span class="italic">i</span>,<span class="italic">j</span>−1</sub></small>) − <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_e0c6.gif" alt="[small epsilon, Greek, macron]"/></span> that has a weight of <span class="italic">Δ</span><small><sub><span class="italic">ε</span></sub></small> = 1. The former operator <span class="italic">s</span><small><sub><span class="italic">ij</span></sub></small> acts as a estimator of reproducing long range correlations, as it dies off faster, while <span class="italic">ε</span><small><sub><span class="italic">ij</span></sub></small> acts as a estimator for being able to reproduce short correlations when testing on the RBM.</p>
          <p class="otherpara">Monte Carlo is used to generate 20<img class="charmap" src="https://www.rsc.org/images/entities/char_2009.gif" alt="[thin space (1/6-em)]"/>000 samples of 10 × 10 square lattice Ising configurations according Boltzmann distribution at each temperature over 0 to 6 with increments of 0.1. A neural network is trained over these samples with supervised learning to predict the temperature. The RBM is then used to generate flows for temperatures close to the critical temperature. Samples collected from flow lengths of greater than 26 allows for predicting the proportionality constant <span class="italic">A</span>/<span class="italic">T</span><small><sub>c</sub></small> and scaling dimension <span class="italic">Δ</span><small><sub><span class="italic">m</span></sub></small> with a very high accuracy by fitting against<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn150"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t325_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t325.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t325.gif"/></a></td><td class="rightEqn">(150)</td></tr></table></p>
          <p class="otherpara">Fitting the average magnetization, <span class="italic">m</span> with temperature <span class="italic">T</span> from 2 different flows, <a title="" href="#eqn150">eqn (150)</a> helps compute the critical exponent <span class="italic">Δ</span><small><sub><span class="italic">m</span></sub></small>. We could rather compute the scaling dimension <span class="italic">Δ</span><small><sub><span class="italic">s</span></sub></small> and <span class="italic">Δ</span><small><sub><span class="italic">ε</span></sub></small> from a single flow at different temperatures. This then allows us to interpolate and predict the dimension for the critical temperature. <span class="italic">Δ</span><small><sub><span class="italic">s</span></sub></small> is reproduced with a very high precision, indicating that the RBM flow preserves long range correlation, while high errors in predicting <span class="italic">δ</span><small><sub><span class="italic">ε</span></sub></small> shows that short range correlations are usually lost.</p>
        </div>
        <div>
          
          <span id="sect6992"/><span class="c_heading_indent">6.3.2 Variational RG. </span>
          <span>Here the hidden layer of an RBM is used to construct the output of a single step of a variational RG.<a title="Select to navigate to references" href="#cit718"><sup><span class="sup_ref">718</span></sup></a> This is unlike the previous method where the number of spins were kept fixed with every iteration. To generate a flow several RBMs are stacked with each one using the output from the previous RBM hidden layers. The correlation pattern between the visible and hidden nodes are studied to check for any RG like connection. The quantity 〈<span class="italic">v</span><small><sub><span class="italic">i</span></sub></small><span class="italic">h</span><small><sub><span class="italic">j</span></sub></small>〉 as defined below is computed on the block spin renormalization procedure.<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn151"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t326_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t326.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t326.gif"/></a></td><td class="rightEqn">(151)</td></tr></table>where <span class="italic">v</span><small><sub><span class="italic">i</span></sub></small> is a node within the visible layer, <span class="italic">h</span><small><sub><span class="italic">j</span></sub></small> a node in the hidden layer, <span class="italic">a</span> indexes the samples against which the correlation has been computed and <span class="italic">N</span> refers to the total number of samples. This is then used to compute the following correlation function:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn152"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t327_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t327.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t327.gif"/></a></td><td class="rightEqn">(152)</td></tr></table></span>
          <p class="otherpara">The above correlation is plotted with against |<span class="italic">i</span> − <span class="italic">j</span>| for renormalization over lattices of different sizes at the critical temperature for a RBM trained on data from a 2d Ising Hamiltonian with nearest neighbour interaction. A fall in correlation with the separation is noticed for large lattices and no pattern is obtained for small Ising lattice keeping the decimation ratio fixed. The RBM thus has managed to preserve the correlations with nearest neighbours showing some reminiscent behaviour of RG under some circumstance.</p>
        </div>
        <div>
          
          <span id="sect7020"/><span class="c_heading_indent">6.3.3 Real space mutual information. </span>
          <span>An alternative representation<a title="Select to navigate to references" href="#cit719"><sup><span class="sup_ref">719</span></sup></a> of block spin renormalization can be defined to capture relevant features by using information theoretic expressions. Lets consider a spin system, where a subset of spins <span class="italic">V</span> (visible) are to be effectively represented using spins <span class="italic">H</span> (hidden) such that the replacement retains the maximal mutual information with remaining spins (environment) <span class="italic">E</span> of the system. Refer ref. <a title="Select to navigate to references" href="#cit720">720</a> for a detailed study about mutual information in the context of RG flow. Thus we would like to maximize<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn153"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t328_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t328.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t328.gif"/></a></td><td class="rightEqn">(153)</td></tr></table>where <span class="italic">p</span>(<span class="italic">e</span>) is the probability distribution over the environment and <span class="italic">p</span>(<span class="italic">h</span>) is the probability distribution over the hidden layer. The choice of maximization is motivated from the fact that the coarse grained effective Hamiltonian be compact and short ranged (see ESI in ref. <a title="Select to navigate to references" href="#cit719">719</a>). We construct <span class="italic">p</span>(<span class="italic">h</span>) by marginalizing the joint probability distribution over an RBM that provides for <span class="italic">p</span>(<span class="italic">v</span>,<span class="italic">h</span>). The samples for learning <span class="italic">p</span>(<span class="italic">v</span>,<span class="italic">h</span>), <span class="italic">p</span>(<span class="italic">e</span>) can come from a Markov Chain with 2 RBMs employed to learn these distributions. The updates to the RBM that learns the distribution <span class="italic">p</span>(<span class="italic">e</span>,<span class="italic">h</span>) comes from minimizing −<span class="italic">I</span>(<span class="italic">p</span>(<span class="italic">e</span>)|<span class="italic">p</span>(<span class="italic">h</span>)). Note that this process needs to be repeated on every iteration of the renormalization procedure.</span>
          <p class="otherpara">This procedure reproduces the Kadanoff renormalization when tested on a 2d lattice with 4 visible spins. Samples are generated from a square lattice of size 128 × 128. The effective temperature can be computed against a neural network trained with samples at different temperatures or as described in the earlier section or by plotting the temperature against the mutual information. The procedure reveals a clear separation in the phase transition while predicting the critical temperature with a very high accuracy.</p>
        </div>
      
      
        
        <h3 id="sect7056"><span class="b_heading">6.4 Learnability of quantum neural networks</span></h3>
        <span>The examples discussed in previous sections demonstrate the power of neural networks with regards to generalization, for problems related to classification and generative modelling. We also seen how some of these classically inspired models can be understood from the standpoint of classical physics. Here we would like to address the question of learnability of quantum models with regards to expressibility, trainability and generalization for Quantum Neural Networks (QNN). The working of QNN involves 2 components:</span>
        <p class="otherpara">• Feature encoding layer: feature extraction is performed on raw classical data to extract relevant features for the model to train on. This might for example include denoising, data compression, privacy filtration. Unlike classical ML, for QNN we need to have an efficient method of encoding the feature output as part of quantum states. One might go for qubit encoding (phase encoding), basis encoding (initialize starting state in the computation basis state) or amplitude encoding (using QRAM).</p>
        <p class="otherpara">• A function: in neural networks a generic non linear function this is implemented using layers of fully connected nodes, that seem to resemble multiple layers of RBM stacked over. In the case of QNN, a general unitary constructed from paramaterized quantum circuits is used. The ansatz that defines the unitary could be inspired by the problem at hand or could be a very general multilayered hardware efficient one.</p>
        <p class="otherpara">Following this an optimizer is enrolled to compute gradients on the cost function of interest. The gradients of these operators may not directly correspond to another unitary operator, in which one needs to re-express them as a sum of terms with term corresponding to an efficient computation of expectation value of some observable for the corresponding unitary that is being the output. An example of this would be to use parameters shift rule to re-express gradients as differences.</p>
        <div>
          
          <span id="sect7063"/><span class="c_heading_indent">6.4.1 Expressibility of QNN. </span>
          <span>Just like in DNN where the depth adds to the networks capacity to fit the training data, we would like to quantify the circuits ability to generate states from the Hilbert space. When very little is known about the system that one is dealing with, one might choose to work with generic random ansatz that is agnostic to the system built from hardware efficient elementary gates as layers with repeating elements. The expressibility of such an ansatz<a title="Select to navigate to references" href="#cit721"><sup><span class="sup_ref">721</span></sup></a> can be expressed in terms of the deviation from the the Haar<a title="Select to navigate to references" href="#cit722"><sup><span class="sup_ref">722</span></sup></a> integral<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn154"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t329_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t329.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t329.gif"/></a></td><td class="rightEqn">(154)</td></tr></table>where ‖·‖ refers to the Hilbert Schmidt norm and <span class="italic">t</span> the moment up to which one would like to approximate. The above definition forms the basis for verifying if a given circuit is a <span class="italic">t</span>-design<a title="Select to navigate to references" href="#cit723"><sup><span class="sup_ref">723</span></sup></a> approximation and quantifies the extent to which the ansatz can sample the hilbert space uniformly. Hubregtsen <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit724"><sup><span class="sup_ref">724</span></sup></a> showed that this correlates to the classification accuracy of the circuits on MNIST dataset. We would like to next point out that despite expressibility being a good thing to achieve better approximations, the trainability of such ansatz is prone barren plateaus.</span>
        </div>
        <div>
          
          <span id="sect7077"/><span class="c_heading_indent">6.4.2 Trainability of QNN. </span>
          <span>Let <span class="italic">L</span>(<span class="italic">θ</span>,<span class="italic">z</span>) represent the loss function we would like to optimize to build the learning model, where <span class="italic">θ</span> represent the parameters to be optimized and <a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t330_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t330.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t330.gif"/></a>. Here <span class="italic"><img class="charmap" src="https://www.rsc.org/images/entities/i_char_0078_20d1.gif" alt="[x with combining right harpoon above (vector)]"/></span><small><sub><span class="italic">j</span></sub></small> represents the input vector and <span class="italic">y</span><small><sub><span class="italic">j</span></sub></small> represents the label assigned to it. Thus the optimization procedure solves the following empirical minimization problem:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn155"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t331_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t331.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t331.gif"/></a></td><td class="rightEqn">(155)</td></tr></table>where <span class="italic">ỹ</span><small><sub><span class="italic">j</span></sub></small> represents the label predicted by the classifier and <span class="italic">λ</span>‖<span class="italic">θ</span>‖<small><sup>2</sup></small> is a regularization term added to prevent over-fitting. Some of the major sources for errors include noisy quantum gates, decoherence of qubits (ex:depolorizing noise), errors in measurement and errors coming from finite measurement statistics. Having defined a loss function, one can then define the following metrics,<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn156"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t332_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t332.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t332.gif"/></a></td><td class="rightEqn">(156)</td></tr></table>where <span class="italic">θ</span><small><sup><span class="italic">T</span></sup></small> denotes the parameters in the training iteration <span class="italic">T</span> and the averaging is done over randomness in the noisy quantum gates and multiple measurements. Here <span class="italic">R</span><small><sub>1</sub></small> quantifies the rate of convergence to a stationary point and <span class="italic">R</span><small><sub>2</sub></small> quantifies the rate of convergence and excess error in the loss function. Yuxuan <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit725"><sup><span class="sup_ref">725</span></sup></a> showed that <span class="italic">R</span><small><sub>1</sub></small> and <span class="italic">R</span><small><sub>2</sub></small> (for <span class="italic">λ</span> ∈ [0,(1/3π)] ∪ [1/π,∞]) satisfy the following bounds:<table><td class="leftEqn"> </td><tr><td class="eqn"><span id="eqn157"/><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-t333_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-t333.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-t333.gif"/></a></td><td class="rightEqn">(157)</td></tr></table>where <span class="italic">D</span> is the number of parameters, <span class="italic">T</span> the number of iterations to be executed, <span class="italic">K</span> number of measurements made, <span class="italic">B</span> batch size used for computing gradients, <span class="italic">p</span> is the gate noise and <span class="italic">L</span> is the circuit depth. One key result in establishing these bounds was to show that the empirically estimated gradients <span class="italic">via</span> measurements is biased. A multiplicative bias that depends on (1 − <span class="italic">p</span>)<small><sup><span class="italic">L</span></sup></small> and an additive bias that comes from a distribution that depends on the labels, <span class="italic">K</span> and (1 − <span class="italic">p</span>)<small><sup><span class="italic">L</span></sup></small>. Functionally this marks another distinction between DNN and QNN. The noise models explicitly added to DNN as are bias free and help with the convergence, where as the intrinsic noise that come from gate and measurement errors, results in a bias that degrades learning. The bounds on <span class="italic">R</span><small><sub>1</sub></small> and <span class="italic">R</span><small><sub>2</sub></small> indicate that increasing <span class="italic">K</span>, <span class="italic">B</span> and reducing <span class="italic">p</span>, <span class="italic">d</span> and <span class="italic">L</span> can result in better trainability of the quantum circuit model. We notice that the exponential powering of the noise by the circuit depth <span class="italic">L</span>, indicates that training deep circuits will be infeasible in the NISQ era.</span>
        </div>
        <div>
          
          <span id="sect7146"/><span class="c_heading_indent">6.4.3 Generalizability of QNN. </span>
          <span>Generalizability is an important aspect of an ML model that caters to the ability of that model to generalize a given task. One way to speak about the generalizability of a model is by looking if it capable of transfering the knowledge learnt from a task to perform a similar task with just a little additional training as opposed to training the model from scratch for the second task. In the work by Andrea Mari <span class="italic">et al.</span>,<a title="Select to navigate to references" href="#cit208"><sup><span class="sup_ref">208</span></sup></a> it was shown that a QML model is indeed capable of performing transfer learning. The generic transfer learning approach can be summarized as considering a network trained on a dataset for a particular task, using only the first few layers of this network as a feature extraction network and appending a new network to it that can be trained on a new dataset for a related new task. One can consider the first network to either be classical or quantum and subsequently the second appendable network to also be either classical or quantum, resulting in four possible combinations. The classical-classical network is a common framework, while in this work, the authors provide relevant examples for the other three cases corresponding to classical-quantum, quantum-classical, and quantum–quantum networks, thereby providing evidence that QML models can be generalized for tasks using transfer learning. Generalizability is also the ability for the model to perform well when new data are shown having trained on a given set of data. There have been studies that show the performance of QML models on the testing set for their respective models.<a title="Select to navigate to reference" href="#cit348"><sup><span class="sup_ref">348,726</span></sup></a> However, a general framework to study the generalization abilities of QML models was introduced in ref. <a title="Select to navigate to references" href="#cit727">727</a>. In this work, the authors establish a quantitative metric on the generalization of QML models for classification tasks with an error bound based on the Rényi mutual information between the quantum state space and the classical input space, thereafter showing that overfitting does not occur if the number of training pairs considered is greater than base 2 exponentiation of the mutual information.</span>
        </div>
        <div>
          
          <span id="sect7154"/><span class="c_heading_indent">6.4.4 Barren plateaus in training variational circuits. </span>
          <span>Barren plateaus are characterized by vanishing variance of sample gradients, causing the optimizer to perform random walks in regions on diminishing gradients, with a very low probability of leaving them. McClean<a title="Select to navigate to references" href="#cit728"><sup><span class="sup_ref">728</span></sup></a> in 2018 first showed that on an average the variance of the gradient is exponentially vanishing in the number of qubits for any <span class="italic">t</span>-design circuit leading to barren plateaus anytime the gradient vanishes. <a title="Select to navigate to figure" href="#imgfig43">Fig. 43</a> shows the rapidly falling variance of sample gradients with increasing number of qubits.</span>
          <br/><div class="image_table"><table><tr><td colspan="3" class="imgHolder" id="imgfig43"><a href="/image/article/2022/CS/d2cs00203e/d2cs00203e-f43_hi-res.gif" title="Select to open image in new window" onclick="open(this.href, &#34;_blank&#34;, &#34;toolbar=1,scrollbars=yes,resizable=1&#34;); return false;"><img alt="image file: d2cs00203e-f43.tif" src="/image/article/2022/CS/d2cs00203e/d2cs00203e-f43.gif"/></a></td></tr><tr><td class="pushTitleRight"> </td><td class="image_title"><b>Fig. 43 </b> <span id="fig43"><span class="graphic_title">The plot shows sample variance of the gradient of a two-local Pauli term plotted as a function of the number of qubits on a semi-log plot. Reprinted from ref. <a title="Select to navigate to references" href="#cit728">728</a> with permission under Creative Commons Attribution 4.0 International License.</span></span></td><td class="pushTitleLeft"> </td></tr></table></div>
          <p class="otherpara">As polynomially deep (in terms of the number of qubits) ansatz built out of 2 qubit unitaries as form 2-design circuits,<a title="Select to navigate to references" href="#cit729"><sup><span class="sup_ref">729</span></sup></a> one is likely to encounter barren plateaus while training circuits with sufficiently large number of qubits. Barren plateaus can also arise from the use of global cost functions.<a title="Select to navigate to references" href="#cit730"><sup><span class="sup_ref">730</span></sup></a> Examples of this include computing ground state energies of highly non local Hamiltonians and preparing a given density matrix with high fidelity. Lorocca <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit731"><sup><span class="sup_ref">731</span></sup></a> shows that for controllable systems (roughly refers to ansatz with highly expressibility) the gradients decay by the dimension of the symmetry subspace to which the initial state prepared by the circuit belongs to (in the lack of any symmetry subspaces, it will be the dimension of the Hilbert space). However, one can tailor the ansatz to remain in the uncontrollable regime and sacrifice on expressibility to achieve approximately good results. Another unavoidable source of barren plateaus is the presence of gate noise in NISQ devices. Samson <span class="italic">et al.</span><a title="Select to navigate to references" href="#cit732"><sup><span class="sup_ref">732</span></sup></a> show rigorously that for a noisy circuit the gradients vanish exponentially in the number of qubits with increasing depth.</p>
        </div>
      
    
    
      
      <h2 id="sect7171"><span class="a_heading">7 Conclusions</span></h2>
      <span>In this review, we have explored some of the popular algorithms in machine learning that are used frequently for many physico-chemical applications. We discussed in detail not only the vanilla protocol implementable on a classical computer but also the quantum computing enhanced variants wherever applicable. Equipped with this underlying theoretical framework, we thereafter ventured to investigate five distinct domains of applications which includes tomographic state-reconstruction, state-classification methodologies, electronic structure and property prediction, paramaterizing force-fields for molecular dynamics and even drug discovery pipeline. Such an organizational paradigm places equal emphasis on the methodologies and the applications unlike in most other reviews, and is expected to be beneficial to new entrants in the field especially when supplemented with domain-specific examples as is the case in this review. Last but not the least, we offered an insight into the learning mechanisms, using tools from statistical physics and computer science, that have been used by researchers in recent years to understand the operational mechanism behind the training and feature-extracting ability of the deep-learning algorithms. In particular in the context of Ising Hamiltonians, the Replica and Cavity method provides for calculating observable expectation values that correspond to the least free energy (cost function). We followed this with a discussion on renormalization group searching for connections within deep learning and presented some methods that have been used in exploring the same. The kind of insight we believe reduces the obscurity of these models and the common reluctance associated with the fact that the learning dynamics of these protocols are unreliant on hard-coded physical principles or domain intuition.</span>
      <p class="otherpara">The applications explicated in this review cover a wide spectrum. As discussed even though the quantum-computing enhanced protocols are beginning to be duly recognized, we anticipate that we are still at a nascent stage scratching just the surface. For example, for modeling the electronic structure of molecules and materials there already exists a huge variety of methods ranging from learning the functional form of density functionals, approximating wavefunction, to learning the atomic environment descriptors to predict the atom types and properties which have shown great accuracy. Applying these ML methods with the help of quantum computers can further augment our capabilities especially when solving the electronic Schrodinger equation of large and strongly correlated systems are concerned. One can use the variational quantum generator (VQG) based hybrid quantum-classical architecture developed by Romero and Guzik<a title="Select to navigate to references" href="#cit733"><sup><span class="sup_ref">733</span></sup></a> in order to generate continuous classical probability distributions to perform chemical analysis, quantum state preparation, <span class="italic">etc.</span> The critical point and the critical exponents for a quantum phase transition can be determined using a QML approach through the finite-size scaling (FSS) analysis.<a title="Select to navigate to references" href="#cit734"><sup><span class="sup_ref">734</span></sup></a> Therefore, the development of QML approaches has a huge role to play in the coming years in the domain of electronic structure, materials, and property prediction methods. Similar statements can also be extended to computation of force-fields wherein classical ML techniques even though successful have only efficiently modelled small systems. For the drug discovery pipeline, as has been indicated in Section 5.5, key players in both industry and academia are recognizing the potential of quantum computers through investments and collaborative ventures. In the context of interpretability, analytically understanding how generalization of some of the models presented work while increasing the number of parameters with respect to the size of input space and the number of learning samples is still open. We would like to have a handle over the limit points of a given deep learning model and perturb to understand the neighborhood space much akin to having a conformal field theory that describes the critical surface and is used to explore the space of quantum field theories. Studies have been conducted in the context of 2d Ising model along the lines of analyzing information flow for RG and deep neural networks.<a title="Select to navigate to references" href="#cit735"><sup><span class="sup_ref">735</span></sup></a> Convergence towards critical points in these models is in stark contrast with it being an unstable point in the RG flow. It is important that further studies be conducted in models that have self organized criticality to probe if there exists a definitive relation and if the fixed points have anything to tell us about how choices are to be made in the models that we study, with respect to hyperparameters, cost function, optimizer choice and learning ability.</p>
      <p class="otherpara">Since machine learning tasks are data-intensive efficient protocols for loading entries from a high-dimensional classical vector onto a quantum state without smarter preprocessing for feature extraction continues to be a significant challenge. The early domain of QML algorithms included HHL<a title="Select to navigate to references" href="#cit178"><sup><span class="sup_ref">178</span></sup></a> for PCA and clustering, required the assumption about the oracular presence of qRAM to enable efficient encoding of data. While the development of qRAMs is still an ongoing field of research, recent results claims that the exponential speedup in a subset of such algorithms is only due to the assumption that the encoding of data is efficient.<a title="Select to navigate to references" href="#cit736"><sup><span class="sup_ref">736</span></sup></a> Quantum inspired classical algorithms<a title="Select to navigate to references" href="#cit737"><sup><span class="sup_ref">737</span></sup></a> that manipulate <span class="italic">l</span><small><sup>2</sup></small> norm sampling distributions provide an exponential speedups in the case of recommendation systems imply the lack of provability concerning the quantum speedups of certain early QML algorithms. Another primary concern for the development of any quantum algorithms even beyond ML applications is the inherent presence of noise manifested from shorter coherence times of qubits and greater gate-infidelities especially of multi-qubit operations. The fundamental research related to the development of better qubits, improving gate fidelities in unitary operations, and improving the qubit connectivity is very much an active field of investigation among hardware engineers and physicists. New reports have been demonstrated with protected qubits resilient against certain kind of hardware noises.<a title="Select to navigate to references" href="#cit738"><sup><span class="sup_ref">738</span></sup></a> Fault-tolerant quantum computation wherein logical qubits are protected using more physical qubits like in stabilizer codes<a title="Select to navigate to references" href="#cit739"><sup><span class="sup_ref">739</span></sup></a> or qubit configurations based on topological properties of the underlying interactions<a title="Select to navigate to reference" href="#cit81"><sup><span class="sup_ref">81,740</span></sup></a> have been proposed and is actively under development. First-ever such operation has been recently demonstrated on a trapped-ion platform.<a title="Select to navigate to references" href="#cit741"><sup><span class="sup_ref">741</span></sup></a> The process of such error correction can itself suffer from noise which can be mitigated by the quantum fault-tolerant threshold theorem<a title="Select to navigate to references" href="#cit742"><sup><span class="sup_ref">742</span></sup></a> provided noise levels are low. Partial suppression of bit and phase-flip errors have also been demonstrated.<a title="Select to navigate to references" href="#cit743"><sup><span class="sup_ref">743</span></sup></a> On the algorithmic side, algorithms that utilize the specific problem structure smartly have also been proposed.<a title="Select to navigate to references" href="#cit744"><sup><span class="sup_ref">744</span></sup></a> One also needs to thoroughly understand the noise resilience of some of the existing methods and investigate how much of hardware noise can be tolerated before the results are corrupted beyond a certain threshold and the proclaimed quantum advantages are lost. Proper certification schemes and figures of merit for benchmarking such algorithms are beginning to gain attention.<a title="Select to navigate to references" href="#cit745"><sup><span class="sup_ref">745</span></sup></a> With the increased activity on developing quantum ML algorithms underway, creating a provision for generalizability of these models is an important consideration and this aspect has been already discussed in Section 6.4.3. Some of the key open questions in this area would be a proper theoretical demonstration of asymptotic universality (as was discussed in Section 4) for the function class which quantum models can learn in the presence of trainable unitaries of finite circuit depth<a title="Select to navigate to references" href="#cit309"><sup><span class="sup_ref">309</span></sup></a> thereby relaxing the assumptions used thereof. Another interesting question would be proposing real-life applications tailored to take advantages of the universality in such function classes such that quantum benefits over classical learning can be seen. Resource dependence of such algorithms from the perspective of foundational aspects of quantum mechanics is also an interesting avenue for research. With regards to the trainability of ML models one of the major menaces to tackle is the presence of barren plateaus (see Section 6.4.4) in exploring high dimensional feature spaces to find optimal parameters that minimize the cost function. Much of the questions concerning how the possibility of such exponentially vanishing gradients needs to be handled and mitigated are essentially open to further investigation.</p>
      <p class="otherpara">One must also note that there are other applications which have not been discussed in this review at all. Perhaps the most important one from the point of view of chemistry is modelling chemical reactions and computer aided rational design of molecules and synthetic strategies. In this technique one considers retro-synthetic pathways arising from a given product until a set of precursors which are commercially available or synthetically known in literature is obtained. Such pathways are scored on efficacy based on number of reactions involved, intermediates, reaction conditions, <span class="italic">etc.</span> Two different kinds of strategies are known in this regard. The first involves retrosynthetic disconnection based on domain knowledge or commonly used chemistry-inspired rules followed by subsequent ranking of the precursor steps. This can suffer for unknown or rare reactions where such intuition may not be available. The second category uses simple translation of the molecular descriptors of the reactants into products as is used in machine-induced linguistic translations. Promising results have been obtained for either category using ML/DL algorithms.<a title="Select to navigate to reference" href="#cit746"><sup><span class="sup_ref">746–750</span></sup></a> For further information, the reader may consult already existing topical reviews like.<a title="Select to navigate to reference" href="#cit41"><sup><span class="sup_ref">41,751</span></sup></a> To the best of our knowledge, the role of quantum computing in this area has not been explored. Another area which is very important is understanding non-unitary dynamical evolution of quantum systems and the role of coupling to the environment and the emergence of decoherence.<a title="Select to navigate to reference" href="#cit752"><sup><span class="sup_ref">752,753</span></sup></a> Such open system dynamics have also begun to receive attention from the point of view of machine learning wherein the density matrix of the state is encoded as within an efficiently constructible ansatz. In a recent report<a title="Select to navigate to references" href="#cit128"><sup><span class="sup_ref">128</span></sup></a> Kernel-Ridge Regression (see Section 3.2.2) has been used to faithfully recover long-time dynamical averages of the spin-boson model when linearly coupled to a harmonic bath characterized by the Drude–Lorentz spectral density. Hierarchical equation of motion approach (HEOM) was used to train the model using short-time trajectories but the results when extrapolated beyond the training time intervals using Gaussian kernels leads to unprecedented accuracy. LSTM networks (see Section 3.3.3) have been used to model dynamical evolution of density operators for a coupled two-level system vibronically coupled to a harmonic bath.<a title="Select to navigate to references" href="#cit754"><sup><span class="sup_ref">754</span></sup></a> The population difference between the two levels and the real and imaginary part of the coherence was used as time series data for training at shorter times from the numerically exact multi-layer multi-configurational Time Dependent Hartree method (ML-MCTDH). Remarkable accuracy was seen being preserved even in the long-time limit. A similar result was also obtained with CNN<a title="Select to navigate to references" href="#cit755"><sup><span class="sup_ref">755</span></sup></a> (see Section 3.3.2) where input training data was the density matrix elements at various time steps and the prediction of the network through successive series of convolutions and max-pooling yielded accurate values of averages of the system operators (like the Pauli-<span class="italic">z</span> or <span class="italic">σ</span><small><sub><span class="italic">z</span></sub></small>(<span class="italic">t</span>)). For further elaboration on other such methods, the interested reader is referred to ref. <a title="Select to navigate to reference" href="#cit756">756–759</a>.</p>
      <p class="otherpara">Yet another promising area which is left untouched here is the use of physics-inspired machine learning algorithms which even though is beginning to gain attention in problems of physical or technological interest<a title="Select to navigate to reference" href="#cit760"><sup><span class="sup_ref">760–764</span></sup></a> but has been sparsely adopted in chemistry.<a title="Select to navigate to references" href="#cit765"><sup><span class="sup_ref">765</span></sup></a> Reader may consult a recent review for further discussion.<a title="Select to navigate to references" href="#cit766"><sup><span class="sup_ref">766</span></sup></a> We thus see that the road ahead is ripe with possibilities that can be explored in future especially for the quantum-computing based ML variants. Hopefully with better error mitigating strategies<a title="Select to navigate to references" href="#cit767"><sup><span class="sup_ref">767</span></sup></a> and large scale devices with over 1000 qubits being promised in recent future by tech-giants,<a title="Select to navigate to references" href="#cit768"><sup><span class="sup_ref">768</span></sup></a> this burgeoning field will pick up momentum with enhanced capabilities to conduct many pioneering investigations.</p>
    
    
      <h2 id="sect7215"><span class="a_heading">Conflicts of interest</span></h2>
      <span>There are no conflicts of interest to declare.</span>
    
  
    <h2 id="sect7219"><span class="a_heading">Acknowledgements</span></h2>
      <span>The authors acknowledge funding by the U.S. Department of Energy (Office of Basic Energy Sciences) under Award No. DE-SC0019215, the National Science Foundation under Award No. 1955907 and the U.S. Department of Energy, Office of Science, National Quantum Information Science Research Centers, Quantum Science Center.</span>
    
    <span id="sect7221"><h2 id="sect7218"><span class="a_heading">Notes and references</span></h2></span><ol type="1">
      <li><span id="cit1">H. Li, <span class="italic">Natl. Sci. Rev.</span>, 2017, <span class="bold">5</span>(1), 24–26 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nsr/nwx110" title="DOI Link to resource 10.1093/nsr/nwx110">CrossRef</a>.</span></li>
      <li><span id="cit2">
          A. Torfi, R. A. Shirvani, Y. Keneshloo, N. Tavaf and E. A. Fox, Natural Language Processing Advancements By Deep Learning: A Survey, 2021.</span></li>
      <li><span id="cit3">
          T. P. Nagarhalli, V. Vaze and N. Rana, <span class="italic">2021 Third International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)</span>,  2021, pp. 1529–1534 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2021%20Third%20International%20Conference%20on%20Intelligent%20Communication%20Technologies%20and%20Virtual%20Mobile%20Networks%20(ICICV)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit4">
          Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao and K. Macherey, <span class="italic">et al.</span>, 2016, arXiv preprint arXiv:1609.08144.</span></li>
      <li><span id="cit5">A. Lopez, <span class="italic">ACM Comput. Surv.</span>, 2008, <span class="bold">40</span>, 1–49 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/1380584.1380586" title="DOI Link to resource 10.1145/1380584.1380586">CrossRef</a>.</span></li>
      <li><span id="cit6">
          C. E. Tuncali, G. Fainekos, H. Ito and J. Kapinski, <span class="italic">2018 IEEE Intelligent Vehicles Symposium (IV)</span>,  2018, pp. 1555–1562 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2018%20IEEE%20Intelligent%20Vehicles%20Symposium%20(IV)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit7">J. Janai, F. Güney, A. Behl and A. Geiger, 
            <span class="italic">et al.</span>
          , <span class="italic">Found. Trends Comput. Graph. Vis.</span>, 2020, <span class="bold">12</span>, 1–308 <a target="_blank" class="DOILink" href="https://doi.org/10.1561/0600000079" title="DOI Link to resource 10.1561/0600000079">CrossRef</a>.</span></li>
      <li><span id="cit8">M. Daily, S. Medasani, R. Behringer and M. Trivedi, <span class="italic">Computer</span>, 2017, <span class="bold">50</span>, 18–23 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Computer%5BJour%5D%20AND%2050%5Bvolume%5D%20AND%2018%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit9">K. Siau and W. Wang, <span class="italic">Cut. Bus. Technol. J.</span>, 2018, <span class="bold">31</span>, 47–53 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Cut.%20Bus.%20Technol.%20J.%5BJour%5D%20AND%2031%5Bvolume%5D%20AND%2047%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit10">H. A. Pierson and M. S. Gashler, <span class="italic">Adv. Robotics</span>, 2017, <span class="bold">31</span>, 821–835 <a target="_blank" class="DOILink" href="https://doi.org/10.1080/01691864.2017.1365009" title="DOI Link to resource 10.1080/01691864.2017.1365009">CrossRef</a>.</span></li>
      <li><span id="cit11">
          K. He, X. Zhang, S. Ren and J. Sun, Proceedings of the IEEE international conference on computer vision, 2015, pp. 1026–1034.</span></li>
      <li><span id="cit12">
          K. He, X. Zhang, S. Ren and J. Sun, Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.</span></li>
      <li><span id="cit13">
          M. Wu and L. Chen, <span class="italic">2015 Chinese Automation Congress (CAC)</span>,  2015, pp. 542–546 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2015%20Chinese%20Automation%20Congress%20(CAC)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202015%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit14">
          P. Covington, J. Adams and E. Sargin, Proceedings of the 10th ACM conference on recommender systems, 2016, pp. 191–198.</span></li>
      <li><span id="cit15">M. Pazzani and D. Billsus, <span class="italic">Mach. Learn.</span>, 1997, <span class="bold">27</span>, 313–331 <a target="_blank" class="DOILink" href="https://doi.org/10.1023/A:1007369909943" title="DOI Link to resource 10.1023/A:1007369909943">CrossRef</a>.</span></li>
      <li><span id="cit16">E. G. Dada, J. S. Bassi, H. Chiroma, A. O. Adetunmbi and O. E. Ajibuwa, 
            <span class="italic">et al.</span>
          , <span class="italic">Heliyon</span>, 2019, <span class="bold">5</span>, e01802 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.heliyon.2019.e01802" title="DOI Link to resource 10.1016/j.heliyon.2019.e01802">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31211254%5Buid%5D" title="PubMed Link to resource 31211254">PubMed</a>.</span></li>
      <li><span id="cit17">T. S. Guzella and W. M. Caminhas, <span class="italic">Exp. Syst. Appl.</span>, 2009, <span class="bold">36</span>, 10206–10222 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.eswa.2009.02.037" title="DOI Link to resource 10.1016/j.eswa.2009.02.037">CrossRef</a>.</span></li>
      <li><span id="cit18">T. Lengauer, O. Sander, S. Sierra, A. Thielen and R. Kaiser, <span class="italic">Nat. Biotechnol.</span>, 2007, <span class="bold">25</span>, 1407–1410 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nbt1371" title="DOI Link to resource 10.1038/nbt1371">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXhsVSjurrJ" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18066037%5Buid%5D" title="PubMed Link to resource 18066037">PubMed</a>.</span></li>
      <li><span id="cit19">P. Larranaga, B. Calvo, R. Santana, C. Bielza, J. Galdiano, I. Inza, J. A. Lozano, R. Armananzas, G. Santafé and A. Pérez, 
            <span class="italic">et al.</span>
          , <span class="italic">Briefings Bioinf.</span>, 2006, <span class="bold">7</span>, 86–112 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/bib/bbk007" title="DOI Link to resource 10.1093/bib/bbk007">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXhs1Whtb4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=16761367%5Buid%5D" title="PubMed Link to resource 16761367">PubMed</a>.</span></li>
      <li><span id="cit20">B. J. Erickson, P. Korfiatis, Z. Akkus and T. L. Kline, <span class="italic">Radiographics</span>, 2017, <span class="bold">37</span>, 505–515 <a target="_blank" class="DOILink" href="https://doi.org/10.1148/rg.2017160130" title="DOI Link to resource 10.1148/rg.2017160130">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28212054%5Buid%5D" title="PubMed Link to resource 28212054">PubMed</a>.</span></li>
      <li><span id="cit21">S. Sakhavi, C. Guan and S. Yan, <span class="italic">IEEE Trans. Neural Networks Learn. Syst.</span>, 2018, <span class="bold">29</span>, 5619–5629 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Neural%20Networks%20Learn.%20Syst.%5BJour%5D%20AND%2029%5Bvolume%5D%20AND%205619%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit22">J. Grimmer, M. E. Roberts and B. M. Stewart, <span class="italic">Annu. Rev. Political Sci.</span>, 2021, <span class="bold">24</span>, 395–419 <a target="_blank" class="DOILink" href="https://doi.org/10.1146/annurev-polisci-053119-015921" title="DOI Link to resource 10.1146/annurev-polisci-053119-015921">CrossRef</a>.</span></li>
      <li><span id="cit23">J. B. Heaton, N. G. Polson and J. H. Witte, <span class="italic">Appl. Stoch. Models Bus. Ind.</span>, 2017, <span class="bold">33</span>, 3–12 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/asmb.2209" title="DOI Link to resource 10.1002/asmb.2209">CrossRef</a>.</span></li>
      <li><span id="cit24">K. Bansak, J. Ferwerda, J. Hainmueller, A. Dillon, D. Hangartner, D. Lawrence and J. Weinstein, <span class="italic">Science</span>, 2018, <span class="bold">359</span>, 325–329 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/science.aao4408" title="DOI Link to resource 10.1126/science.aao4408">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhtFKntr8%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29348237%5Buid%5D" title="PubMed Link to resource 29348237">PubMed</a>.</span></li>
      <li><span id="cit25">L. Van Der Maaten, E. Postma and J. Van den Herik, 
            <span class="italic">et al.</span>
          , <span class="italic">J. Mach. Learn. Res.</span>, 2009, <span class="bold">10</span>, 13 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%2010%5Bvolume%5D%20AND%2013%5Bpage%5D%20and%202009%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit26">G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto and L. Zdeborová, <span class="italic">Rev. Modern Phys.</span>, 2019, <span class="bold">91</span>, 045002 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.91.045002" title="DOI Link to resource 10.1103/RevModPhys.91.045002">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXjtFOnur4%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit27">N. M. Ball and R. J. Brunner, <span class="italic">Int. J. Mod. Phys. D</span>, 2010, <span class="bold">19</span>, 1049–1106 <a target="_blank" class="DOILink" href="https://doi.org/10.1142/S0218271810017160" title="DOI Link to resource 10.1142/S0218271810017160">CrossRef</a>.</span></li>
      <li><span id="cit28">
          Ž. Ivezić, A. J. Connolly, J. T. VanderPlas and A. Gray, <span class="italic">Statistics, data mining, and machine learning in astronomy</span>, Princeton University Press,  2014 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Statistics,%20data%20mining,%20and%20machine%20learning%20in%20astronomy%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit29">A. Radovic, M. Williams, D. Rousseau, M. Kagan, D. Bonacorsi, A. Himmel, A. Aurisano, K. Terao and T. Wongjirad, <span class="italic">Nature</span>, 2018, <span class="bold">560</span>, 41–48 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-018-0361-2" title="DOI Link to resource 10.1038/s41586-018-0361-2">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhsVensrfM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30068955%5Buid%5D" title="PubMed Link to resource 30068955">PubMed</a>.</span></li>
      <li><span id="cit30">P. B. Wigley, P. J. Everitt, A. van den Hengel, J. W. Bastian, M. A. Sooriyabandara, G. D. McDonald, K. S. Hardman, C. D. Quinlivan, P. Manju and C. C. Kuhn, 
            <span class="italic">et al.</span>
          , <span class="italic">Sci. Rep.</span>, 2016, <span class="bold">6</span>, 1–6 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41598-016-0001-8" title="DOI Link to resource 10.1038/s41598-016-0001-8">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28442746%5Buid%5D" title="PubMed Link to resource 28442746">PubMed</a>.</span></li>
      <li><span id="cit31">J. Zhou, B. Huang, Z. Yan and J.-C. G. Bünzli, <span class="italic">Light: Sci. Appl.</span>, 2019, <span class="bold">8</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41377-018-0109-7" title="DOI Link to resource 10.1038/s41377-018-0109-7">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhslCktrnP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30622704%5Buid%5D" title="PubMed Link to resource 30622704">PubMed</a>.</span></li>
      <li><span id="cit32">A. Chattopadhyay, E. Nabizadeh and P. Hassanzadeh, <span class="italic">J. Adv. Modeling Earth Syst.</span>, 2020, <span class="bold">12</span>, e2019MS001958 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Adv.%20Modeling%20Earth%20Syst.%5BJour%5D%20AND%2012%5Bvolume%5D%20AND%20e2019MS001958%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit33">X. Ren, X. Li, K. Ren, J. Song, Z. Xu, K. Deng and X. Wang, <span class="italic">Big Data Res.</span>, 2021, <span class="bold">23</span>, 100178 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.bdr.2020.100178" title="DOI Link to resource 10.1016/j.bdr.2020.100178">CrossRef</a>.</span></li>
      <li><span id="cit34">T. A. Monson, D. W. Armitage and L. J. Hlusko, <span class="italic">PaleoBios</span>, 2018, <span class="bold">35</span>, 1–20 <a target="_blank" class="DOILink" href="https://doi.org/10.5070/P9351040776" title="DOI Link to resource 10.5070/P9351040776">CrossRef</a>.</span></li>
      <li><span id="cit35">J. P. Spradley, B. J. Glazer and R. F. Kay, <span class="italic">Palaeogeogr., Palaeoclimatol., Palaeoecol.</span>, 2019, <span class="bold">518</span>, 155–171 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.palaeo.2019.01.014" title="DOI Link to resource 10.1016/j.palaeo.2019.01.014">CrossRef</a>.</span></li>
      <li><span id="cit36">I. C. Romero, S. Kong, C. C. Fowlkes, C. Jaramillo, M. A. Urban, F. Oboh-Ikuenobe, C. D’Apolito and S. W. Punyasena, <span class="italic">Proc. Natl. Acad. Sci. U. S. A.</span>, 2020, <span class="bold">117</span>, 28496–28505 <a target="_blank" class="DOILink" href="https://doi.org/10.1073/pnas.2007324117" title="DOI Link to resource 10.1073/pnas.2007324117">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitlWnu7vO" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33097671%5Buid%5D" title="PubMed Link to resource 33097671">PubMed</a>.</span></li>
      <li><span id="cit37">G. R. Schleder, A. C. Padilha, C. M. Acosta, M. Costa and A. Fazzio, <span class="italic">J. Phys.: Mater.</span>, 2019, <span class="bold">2</span>, 032001 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitFGhtrjJ" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit38">J. Behler, <span class="italic">J. Chem. Phys.</span>, 2016, <span class="bold">145</span>, 170901 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4966192" title="DOI Link to resource 10.1063/1.4966192">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27825224%5Buid%5D" title="PubMed Link to resource 27825224">PubMed</a>.</span></li>
      <li><span id="cit39">O. A. von Lilienfeld and K. Burke, <span class="italic">Nat. Commun.</span>, 2020, <span class="bold">11</span>, 1–4 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-019-13993-7" title="DOI Link to resource 10.1038/s41467-019-13993-7">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31911652%5Buid%5D" title="PubMed Link to resource 31911652">PubMed</a>.</span></li>
      <li><span id="cit40">Y. Liu, C. Niu, Z. Wang, Y. Gan, Y. Zhu, S. Sun and T. Shen, <span class="italic">J. Mater. Sci. Technol.</span>, 2020, <span class="bold">57</span>, 113–122 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.jmst.2020.01.067" title="DOI Link to resource 10.1016/j.jmst.2020.01.067">CrossRef</a>.</span></li>
      <li><span id="cit41">F. Strieth-Kalthoff, F. Sandfort, M. H. Segler and F. Glorius, <span class="italic">Chem. Soc. Rev.</span>, 2020, <span class="bold">49</span>, 6154–6168 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C9CS00786E&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C9CS00786E">RSC</a>.</span></li>
      <li><span id="cit42">C. W. Coley, W. H. Green and K. F. Jensen, <span class="italic">Acc. Chem. Res.</span>, 2018, <span class="bold">51</span>, 1281–1289 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.accounts.8b00087" title="DOI Link to resource 10.1021/acs.accounts.8b00087">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXosFKhsb0%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29715002%5Buid%5D" title="PubMed Link to resource 29715002">PubMed</a>.</span></li>
      <li><span id="cit43">Z. Fu, X. Li, Z. Wang, Z. Li, X. Liu, X. Wu, J. Zhao, X. Ding, X. Wan and F. Zhong, 
            <span class="italic">et al.</span>
          , <span class="italic">Org. Chem. Front.</span>, 2020, <span class="bold">7</span>, 2269–2277 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=D0QO00544D&amp;newsite=1" title="Link to RSC resource DOI:10.1039/D0QO00544D">RSC</a>.</span></li>
      <li><span id="cit44">D. Fooshee, A. Mood, E. Gutman, M. Tavakoli, G. Urban, F. Liu, N. Huynh, D. Van Vranken and P. Baldi, <span class="italic">Mol. Syst. Des. Eng.</span>, 2018, <span class="bold">3</span>, 442–452 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C7ME00107J&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C7ME00107J">RSC</a>.</span></li>
      <li><span id="cit45">D. Hu, Y. Xie, X. Li, L. Li and Z. Lan, <span class="italic">J. Phys. Chem. Lett.</span>, 2018, <span class="bold">9</span>, 2725–2732 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.8b00684" title="DOI Link to resource 10.1021/acs.jpclett.8b00684">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXptVequ7g%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29732893%5Buid%5D" title="PubMed Link to resource 29732893">PubMed</a>.</span></li>
      <li><span id="cit46">S. Amabilino, L. A. Bratholm, S. J. Bennie, A. C. Vaucher, M. Reiher and D. R. Glowacki, <span class="italic">J. Phys. Chem. A</span>, 2019, <span class="bold">123</span>, 4486–4499 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpca.9b01006" title="DOI Link to resource 10.1021/acs.jpca.9b01006">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXlsFemsL0%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30892040%5Buid%5D" title="PubMed Link to resource 30892040">PubMed</a>.</span></li>
      <li><span id="cit47">A. Kumar, S. Loharch, S. Kumar, R. P. Ringe and R. Parkesh, <span class="italic">Comput. Struct. Biotechnol. J.</span>, 2021, <span class="bold">19</span>, 424–438 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.csbj.2020.12.028" title="DOI Link to resource 10.1016/j.csbj.2020.12.028">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXht1akur4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33391634%5Buid%5D" title="PubMed Link to resource 33391634">PubMed</a>.</span></li>
      <li><span id="cit48">
          J. A. Keith, V. Vassilev-Galindo, B. Cheng, S. Chmiela, M. Gastegger, K.-R. Müller and A. Tkatchenko, 2021, arXiv preprint arXiv:2102.06321.</span></li>
      <li><span id="cit49">
          M. A. Nielsen and I. L. Chuang, <span class="italic">Quantum Computation and Quantum Information: 10th Anniversary Edition</span>, Cambridge University Press, USA, 10th edn,  2011 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Quantum%20Computation%20and%20Quantum%20Information:%2010th%20Anniversary%20Edition%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit50">C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres and W. K. Wootters, <span class="italic">Phys. Rev. Lett.</span>, 1993, <span class="bold">70</span>, 1895 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.70.1895" title="DOI Link to resource 10.1103/PhysRevLett.70.1895">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=10053414%5Buid%5D" title="PubMed Link to resource 10053414">PubMed</a>.</span></li>
      <li><span id="cit51">A. Harrow, P. Hayden and D. Leung, <span class="italic">Phys. Rev. Lett.</span>, 2004, <span class="bold">92</span>, 187901 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.92.187901" title="DOI Link to resource 10.1103/PhysRevLett.92.187901">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15169533%5Buid%5D" title="PubMed Link to resource 15169533">PubMed</a>.</span></li>
      <li><span id="cit52">D. S. Abrams and S. Lloyd, <span class="italic">Phys. Rev. Lett.</span>, 1999, <span class="bold">83</span>, 5162–5165 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.83.5162" title="DOI Link to resource 10.1103/PhysRevLett.83.5162">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK1MXnvVKltb4%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit53">P. W. Shor, <span class="italic">SIAM Rev.</span>, 1999, <span class="bold">41</span>, 303–332 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/S0036144598347011" title="DOI Link to resource 10.1137/S0036144598347011">CrossRef</a>.</span></li>
      <li><span id="cit54">
          L. K. Grover, Proceedings of the twenty-eighth annual ACM symposium on Theory of computing, 1996, pp. 212–219.</span></li>
      <li><span id="cit55">J. Biamonte, P. Wittek, N. Pancotti, P. Rebentrost, N. Wiebe and S. Lloyd, <span class="italic">Nature</span>, 2017, <span class="bold">549</span>, 195–202 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nature23474" title="DOI Link to resource 10.1038/nature23474">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhsV2isLjI" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28905917%5Buid%5D" title="PubMed Link to resource 28905917">PubMed</a>.</span></li>
      <li><span id="cit56">
          N. Wiebe, A. Kapoor and K. M. Svore, 2014, arXiv preprint arXiv:1412.3489.</span></li>
      <li><span id="cit57">
          E. Alpaydin, <span class="italic">Introduction to Machine Learning</span>, The MIT Press, 2nd edn,  2010 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Introduction%20to%20Machine%20Learning%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202010%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit58">
          I. Goodfellow, Y. Bengio and A. Courville, <span class="italic">Deep Learning</span>, MIT Press,  2016 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Deep%20Learning%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202016%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit59">
          M. Kubat, <span class="italic">An Introduction to Machine Learning</span>,  2017 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=An%20Introduction%20to%20Machine%20Learning%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit60">O. A. Von Lilienfeld, <span class="italic">Angew. Chem., Int. Ed.</span>, 2018, <span class="bold">57</span>, 4164–4169 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/anie.201709686" title="DOI Link to resource 10.1002/anie.201709686">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXks1Kltb0%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29216413%5Buid%5D" title="PubMed Link to resource 29216413">PubMed</a>.</span></li>
      <li><span id="cit61">
          B. Huang, N. O. Symonds and O. A. V. Lilienfeld, <span class="italic">Handbook of Materials Modeling</span>,  2018, pp. 1–27 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Handbook%20of%20Materials%20Modeling%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit62">H. Häffner, C. F. Roos and R. Blatt, <span class="italic">Phys. Rep.</span>, 2008, <span class="bold">469</span>, 155–203 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.physrep.2008.09.003" title="DOI Link to resource 10.1016/j.physrep.2008.09.003">CrossRef</a>.</span></li>
      <li><span id="cit63">C. D. Bruzewicz, J. Chiaverini, R. McConnell and J. M. Sage, <span class="italic">Appl. Phys. Rev.</span>, 2019, <span class="bold">6</span>, 021314 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Appl.%20Phys.%20Rev.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%20021314%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit64">M. Kjaergaard, M. E. Schwartz, J. Braumüller, P. Krantz, J. I.-J. Wang, S. Gustavsson and W. D. Oliver, <span class="italic">Ann. Rev. Condens. Matter Phys.</span>, 2020, <span class="bold">11</span>, 369–395 <a target="_blank" class="DOILink" href="https://doi.org/10.1146/annurev-conmatphys-031119-050605" title="DOI Link to resource 10.1146/annurev-conmatphys-031119-050605">CrossRef</a>.</span></li>
      <li><span id="cit65">M. Saffman, T. G. Walker and K. Mølmer, <span class="italic">Rev. Mod. Phys.</span>, 2010, <span class="bold">82</span>, 2313 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.82.2313" title="DOI Link to resource 10.1103/RevModPhys.82.2313">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3cXht1Ciur%252FL" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit66">M. Saffman, <span class="italic">J. Phys. B: At., Mol. Opt. Phys.</span>, 2016, <span class="bold">49</span>, 202001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0953-4075/49/20/202001" title="DOI Link to resource 10.1088/0953-4075/49/20/202001">CrossRef</a>.</span></li>
      <li><span id="cit67">Q. Wei, Y. Cao, S. Kais, B. Friedrich and D. R. Herschbach, <span class="italic">ChemPhysChem</span>, 2016, <span class="bold">17</span>(22), 3714–3722 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/cphc.201600781" title="DOI Link to resource 10.1002/cphc.201600781">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27767247%5Buid%5D" title="PubMed Link to resource 27767247">PubMed</a>.</span></li>
      <li><span id="cit68">M. Karra, K. Sharma, B. Friedrich, S. Kais and D. R. Herschbach, <span class="italic">J. Chem. Phys.</span>, 2016, <span class="bold">144</span>(9), 094301 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4942928" title="DOI Link to resource 10.1063/1.4942928">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26957163%5Buid%5D" title="PubMed Link to resource 26957163">PubMed</a>.</span></li>
      <li><span id="cit69">J. Zhu, S. Kais, Q. Wei, D. Herschbach and B. Friedrich, <span class="italic">J. Chem. Phys.</span>, 2013, <span class="bold">138</span>, 024104 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4774058" title="DOI Link to resource 10.1063/1.4774058">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=23320665%5Buid%5D" title="PubMed Link to resource 23320665">PubMed</a>.</span></li>
      <li><span id="cit70">Q. Wei, S. Kais, B. Friedrich and D. Herschbach, <span class="italic">J. Chem. Phys.</span>, 2011, <span class="bold">134</span>, 124107 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.3567486" title="DOI Link to resource 10.1063/1.3567486">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=21456645%5Buid%5D" title="PubMed Link to resource 21456645">PubMed</a>.</span></li>
      <li><span id="cit71">Q. Wei, S. Kais, B. Friedrich and D. Herschbach, <span class="italic">J. Chem. Phys.</span>, 2011, <span class="bold">135</span>, 154102 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.3649949" title="DOI Link to resource 10.1063/1.3649949">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22029292%5Buid%5D" title="PubMed Link to resource 22029292">PubMed</a>.</span></li>
      <li><span id="cit72">
          J. Watrous, <span class="italic">The theory of quantum information</span>, Cambridge University Press,  2018 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=The%20theory%20of%20quantum%20information%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit73">
          J. Preskill, Lecture Notes for Ph219/CS219, http://theory.caltech.edu/~preskill/ph229/, Accessed: 2021-10-22.</span></li>
      <li><span id="cit74">S.-H. Lin, R. Dilip, A. G. Green, A. Smith and F. Pollmann, <span class="italic">PRX Quantum</span>, 2021, <span class="bold">2</span>, 010342 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PRXQuantum.2.010342" title="DOI Link to resource 10.1103/PRXQuantum.2.010342">CrossRef</a>.</span></li>
      <li><span id="cit75">K. Yeter-Aydeniz, E. Moschandreou and G. Siopsis, <span class="italic">Phys. Rev. A</span>, 2022, <span class="bold">105</span>, 012412 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.105.012412" title="DOI Link to resource 10.1103/PhysRevA.105.012412">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38Xjt12hsLk%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit76">C. Yi, <span class="italic">Phys. Rev. A</span>, 2021, <span class="bold">104</span>, 052603 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.104.052603" title="DOI Link to resource 10.1103/PhysRevA.104.052603">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXislagsbbF" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit77">S. Endo, J. Sun, Y. Li, S. C. Benjamin and X. Yuan, <span class="italic">Phys. Rev. Lett.</span>, 2020, <span class="bold">125</span>, 010501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.125.010501" title="DOI Link to resource 10.1103/PhysRevLett.125.010501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhsFKisrjK" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32678631%5Buid%5D" title="PubMed Link to resource 32678631">PubMed</a>.</span></li>
      <li><span id="cit78">D. Deutsch, <span class="italic">Proc. R. Soc. London, Ser. A</span>, 1985, <span class="bold">400</span>, 97–117 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proc.%20R.%20Soc.%20London,%20Ser.%20A%5BJour%5D%20AND%20400%5Bvolume%5D%20AND%2097%5Bpage%5D%20and%201985%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit79">P. Benioff, <span class="italic">J. Stat. Phys.</span>, 1980, <span class="bold">22</span>, 563–591 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/BF01011339" title="DOI Link to resource 10.1007/BF01011339">CrossRef</a>.</span></li>
      <li><span id="cit80">X. Hu and S. D. Sarma, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2002, <span class="bold">66</span>, 012312 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.66.012312" title="DOI Link to resource 10.1103/PhysRevA.66.012312">CrossRef</a>.</span></li>
      <li><span id="cit81">B. M. Terhal, <span class="italic">Rev. Mod. Phys.</span>, 2015, <span class="bold">87</span>, 307–346 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.87.307" title="DOI Link to resource 10.1103/RevModPhys.87.307">CrossRef</a>.</span></li>
      <li><span id="cit82">A. Kitaev, <span class="italic">Ann. Phys.</span>, 2003, <span class="bold">303</span>, 2–30 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3sXisl2hsg%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit83">E. Knill and R. Laflamme, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1997, <span class="bold">55</span>, 900 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.55.900" title="DOI Link to resource 10.1103/PhysRevA.55.900">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK2sXhtVGru7g%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit84">J. Preskill, <span class="italic">Quantum</span>, 2018, <span class="bold">2</span>, 79 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2018-08-06-79" title="DOI Link to resource 10.22331/q-2018-08-06-79">CrossRef</a>.</span></li>
      <li><span id="cit85">K. Bharti, A. Cervera-Lierta, T. H. Kyaw, T. Haug, S. Alperin-Lea, A. Anand, M. Degroote, H. Heimonen, J. S. Kottmann and T. Menke, 
            <span class="italic">et al.</span>
          , <span class="italic">Rev. Mod. Phys.</span>, 2022, <span class="bold">94</span>, 015004 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.94.015004" title="DOI Link to resource 10.1103/RevModPhys.94.015004">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38XhtVOnsLrE" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit86">G. A. Quantum and Collaborators, F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends, S. Boixo, M. Broughton and B. B. Buckley, 
            <span class="italic">et al.</span>
          , <span class="italic">Science</span>, 2020, <span class="bold">369</span>, 1084–1089 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/science.abb9811" title="DOI Link to resource 10.1126/science.abb9811">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32855334%5Buid%5D" title="PubMed Link to resource 32855334">PubMed</a>.</span></li>
      <li><span id="cit87">Y. Cao, J. Romero, J. P. Olson, M. Degroote, P. D. Johnson, M. Kieferová, I. D. Kivlichan, T. Menke, B. Peropadre and N. P. Sawaya, 
            <span class="italic">et al.</span>
          , <span class="italic">Chem. Rev.</span>, 2019, <span class="bold">119</span>, 10856–10915 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.8b00803" title="DOI Link to resource 10.1021/acs.chemrev.8b00803">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhs1Krtb7K" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31469277%5Buid%5D" title="PubMed Link to resource 31469277">PubMed</a>.</span></li>
      <li><span id="cit88">K. Head-Marsden, J. Flick, C. J. Ciccarino and P. Narang, <span class="italic">Chem. Rev.</span>, 2020, <span class="bold">121</span>, 3061–3120 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.0c00620" title="DOI Link to resource 10.1021/acs.chemrev.0c00620">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33326218%5Buid%5D" title="PubMed Link to resource 33326218">PubMed</a>.</span></li>
      <li><span id="cit89">X. Yuan, S. Endo, Q. Zhao, Y. Li and S. C. Benjamin, <span class="italic">Quantum</span>, 2019, <span class="bold">3</span>, 191 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2019-10-07-191" title="DOI Link to resource 10.22331/q-2019-10-07-191">CrossRef</a>.</span></li>
      <li><span id="cit90">M. Cerezo, A. Arrasmith, R. Babbush, S. C. Benjamin, S. Endo, K. Fujii, J. R. McClean, K. Mitarai, X. Yuan and L. Cincio, 
            <span class="italic">et al.</span>
          , <span class="italic">Nat. Rev. Phys.</span>, 2021, <span class="bold">3</span>, 625–644 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s42254-021-00348-9" title="DOI Link to resource 10.1038/s42254-021-00348-9">CrossRef</a>.</span></li>
      <li><span id="cit91">B. Bauer, S. Bravyi, M. Motta and G. K.-L. Chan, <span class="italic">Chem. Rev.</span>, 2020, <span class="bold">120</span>, 12685–12717 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.9b00829" title="DOI Link to resource 10.1021/acs.chemrev.9b00829">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitFagtL3O" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33090772%5Buid%5D" title="PubMed Link to resource 33090772">PubMed</a>.</span></li>
      <li><span id="cit92">E. F. Dumitrescu, A. J. McCaskey, G. Hagen, G. R. Jansen, T. D. Morris, T. Papenbrock, R. C. Pooser, D. J. Dean and P. Lougovski, <span class="italic">Phys. Rev. Lett.</span>, 2018, <span class="bold">120</span>, 210501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.120.210501" title="DOI Link to resource 10.1103/PhysRevLett.120.210501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltVygtrs%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29883142%5Buid%5D" title="PubMed Link to resource 29883142">PubMed</a>.</span></li>
      <li><span id="cit93">S. L. Wu, J. Chan, W. Guan, S. Sun, A. Wang, C. Zhou, M. Livny, F. Carminati, A. Di Meglio and A. C. Li, 
            <span class="italic">et al.</span>
          , <span class="italic">J. Phys. G: Nucl. Part. Phys.</span>, 2021, <span class="bold">48</span>, 125003 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1361-6471/ac1391" title="DOI Link to resource 10.1088/1361-6471/ac1391">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38XlvFWrtLY%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit94">W. Guan, G. Perdue, A. Pesah, M. Schuld, K. Terashi, S. Vallecorsa and J.-R. Vlimant, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2021, <span class="bold">2</span>, 011003 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%202%5Bvolume%5D%20AND%20011003%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit95">H.-P. Cheng, E. Deumens, J. K. Freericks, C. Li and B. A. Sanders, <span class="italic">Front. Chem.</span>, 2020, 1066 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Front.%20Chem.%5BJour%5D%20AND%20%5Bvolume%5D%20AND%201066%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit96">R. Orus, S. Mugel and E. Lizaso, <span class="italic">Rev. Phys.</span>, 2019, <span class="bold">4</span>, 100028 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.revip.2019.100028" title="DOI Link to resource 10.1016/j.revip.2019.100028">CrossRef</a>.</span></li>
      <li><span id="cit97">
          S. Lloyd and S. L. Braunstein, in <span class="italic">Quantum Computation Over Continuous Variables</span>, ed. S. L. Braunstein and A. K. Pati, Springer Netherlands, Dordrecht,  2003, pp. 9–17 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Quantum%20Computation%20Over%20Continuous%20Variables%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit98">A. Aspuru-Guzik, A. D. Dutoi, P. J. Love and M. Head-Gordon, <span class="italic">Science</span>, 2005, <span class="bold">309</span>, 1704–1707 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/science.1113479" title="DOI Link to resource 10.1126/science.1113479">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2MXpvFCisrg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=16151006%5Buid%5D" title="PubMed Link to resource 16151006">PubMed</a>.</span></li>
      <li><span id="cit99">T. Albash and D. A. Lidar, <span class="italic">Rev. Mod. Phys.</span>, 2018, <span class="bold">90</span>, 015002 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.90.015002" title="DOI Link to resource 10.1103/RevModPhys.90.015002">CrossRef</a>.</span></li>
      <li><span id="cit100">D-Wave System Documentation, https://docs.dwavesys.com/docs/latest/doc_getting_started.html, Accessed: 2021-10-22.</span></li>
      <li><span id="cit101">P. Hauke, H. G. Katzgraber, W. Lechner, H. Nishimori and W. D. Oliver, <span class="italic">Rep. Progress Phys.</span>, 2020, <span class="bold">83</span>, 054401 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1361-6633/ab85b8" title="DOI Link to resource 10.1088/1361-6633/ab85b8">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitFOltLjN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32235066%5Buid%5D" title="PubMed Link to resource 32235066">PubMed</a>.</span></li>
      <li><span id="cit102">
          H. N. Djidjev, G. Chapuis, G. Hahn and G. Rizk, <span class="italic">Efficient Combinatorial Optimization Using Quantum Annealing</span>,  2018 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Efficient%20Combinatorial%20Optimization%20Using%20Quantum%20Annealing%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit103">R. Y. Li, R. Di Felice, R. Rohs and D. A. Lidar, <span class="italic">npj Quantum Inform.</span>, 2018, <span class="bold">4</span>, 1–10 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-017-0051-1" title="DOI Link to resource 10.1038/s41534-017-0051-1">CrossRef</a>.</span></li>
      <li><span id="cit104">F. Neukart, G. Compostella, C. Seidel, D. Von Dollen, S. Yarkoni and B. Parney, <span class="italic">Front. ICT</span>, 2017, <span class="bold">4</span>, 29 <a target="_blank" class="DOILink" href="https://doi.org/10.3389/fict.2017.00029" title="DOI Link to resource 10.3389/fict.2017.00029">CrossRef</a>.</span></li>
      <li><span id="cit105">
          R. K. Nath, H. Thapliyal and T. S. Humble, 2021, arXiv preprint arXiv:2106.02964.</span></li>
      <li><span id="cit106">
          S. Ruder, CoRR, 2016, https://arxiv.org/abs/1609.04747.</span></li>
      <li><span id="cit107">
          T. M. Breuel, CoRR, 2015, https://arxiv.org/abs/1508.02788.</span></li>
      <li><span id="cit108">Q. A. Wang, <span class="italic">J. Phys. A: Math. Theor.</span>, 2008, <span class="bold">41</span>, 065004 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1751-8113/41/6/065004" title="DOI Link to resource 10.1088/1751-8113/41/6/065004">CrossRef</a>.</span></li>
      <li><span id="cit109">
          Y. Ouali, C. Hudelot and M. Tami, CoRR, 2020, https://arxiv.org/abs/2006.05278.</span></li>
      <li><span id="cit110">
          V. K. Garg and A. Kalai, CoRR, 2017, https://arxiv.org/abs/1709.05262.</span></li>
      <li><span id="cit111">
          W. B. Powell, CoRR, 2019, https://arxiv.org/abs/1912.03513.</span></li>
      <li><span id="cit112">V. Francois-Lavet, P. Henderson, R. Islam, M. G. Bellemare and J. Pineau, <span class="italic">Found. Trends Mach. Learn.</span>, 2018, <span class="bold">11</span>, 219–354 <a target="_blank" class="DOILink" href="https://doi.org/10.1561/2200000071" title="DOI Link to resource 10.1561/2200000071">CrossRef</a>.</span></li>
      <li><span id="cit113">
          B. Scholkopf and A. J. Smola, <span class="italic">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</span>, MIT Press, Cambridge, MA, USA,  2001 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Learning%20with%20Kernels:%20Support%20Vector%20Machines,%20Regularization,%20Optimization,%20and%20Beyond%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202001%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit114">M. G. Genton, <span class="italic">J. Mach. Learn. Res.</span>, 2001, <span class="bold">2</span>, 299–312 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%202%5Bvolume%5D%20AND%20299%5Bpage%5D%20and%202001%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit115">
          T. Azim and S. Ahmed, <span class="italic">Composing Fisher Kernels from Deep Neural Models</span>, Springer,  2018, pp. 1–7 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Composing%20Fisher%20Kernels%20from%20Deep%20Neural%20Models%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit116">
          M. Schuld and F. Petruccione, <span class="italic">Supervised learning with quantum computers</span>, Springer,  2018, vol. 17 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Supervised%20learning%20with%20quantum%20computers%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit117">M. Schuld and N. Killoran, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">122</span>, 040504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.122.040504" title="DOI Link to resource 10.1103/PhysRevLett.122.040504">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXnvFSqtL8%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30768345%5Buid%5D" title="PubMed Link to resource 30768345">PubMed</a>.</span></li>
      <li><span id="cit118">
          B. Ghojogh, A. Ghodsi, F. Karray and M. Crowley, 2021, arXiv preprint arXiv:2106.08443.</span></li>
      <li><span id="cit119">
          S. L. Brunton and J. N. Kutz, <span class="italic">Data-driven science and engineering: Machine learning, dynamical systems, and control</span>, Cambridge University Press,  2019 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Data-driven%20science%20and%20engineering:%20Machine%20learning,%20dynamical%20systems,%20and%20control%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit120">D. W. Marquardt and R. D. Snee, <span class="italic">Amer. Statist.</span>, 1975, <span class="bold">29</span>, 3–20 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Amer.%20Statist.%5BJour%5D%20AND%2029%5Bvolume%5D%20AND%203%5Bpage%5D%20and%201975%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit121">G. C. McDonald, <span class="italic">Wiley Interdiscip. Rev.: Comput. Mol. Sci.</span>, 2009, <span class="bold">1</span>, 93–100 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/wics.14" title="DOI Link to resource 10.1002/wics.14">CrossRef</a>.</span></li>
      <li><span id="cit122">A. E. Hoerl, R. W. Kannard and K. F. Baldwin, <span class="italic">Commun. Stat. Theory Methods</span>, 1975, <span class="bold">4</span>, 105–123 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Commun.%20Stat.%20Theory%20Methods%5BJour%5D%20AND%204%5Bvolume%5D%20AND%20105%5Bpage%5D%20and%201975%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit123">E. Ostertagová, <span class="italic">Proc. Eng.</span>, 2012, <span class="bold">48</span>, 500–506 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.proeng.2012.09.545" title="DOI Link to resource 10.1016/j.proeng.2012.09.545">CrossRef</a>.</span></li>
      <li><span id="cit124">in <span class="italic">Polynomial Regression</span>, ed. J. O. Rawlings, S. G. Pantula and D. A. Dickey, Springer New York, New York, NY,  1998, pp. 235–268 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Polynomial%20Regression%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201998%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit125">
          V. Vovk, <span class="italic">Empirical inference</span>, Springer,  2013, pp. 105–116 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Empirical%20inference%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202013%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit126">K. Vu, J. C. Snyder, L. Li, M. Rupp, B. F. Chen, T. Khelif, K.-R. Müller and K. Burke, <span class="italic">Int. J. Quantum Chem.</span>, 2015, <span class="bold">115</span>, 1115–1128 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/qua.24939" title="DOI Link to resource 10.1002/qua.24939">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXovVCqsrg%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit127">
          C. Saunders, A. Gammerman and V. Vovk, <span class="italic">Ridge regression learning algorithm in dual variables</span>,  1998.</span></li>
      <li><span id="cit128">A. Ullah and P. O. Dral, <span class="italic">New J. Phys.</span>, 2021, <span class="bold">23</span>, 113019 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/ac3261" title="DOI Link to resource 10.1088/1367-2630/ac3261">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXislehsbvP" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit129">J. Westermayr, F. A. Faber, A. S. Christensen, O. A. von Lilienfeld and P. Marquetand, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2020, <span class="bold">1</span>, 025009 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%20025009%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit130">N. Wiebe, D. Braun and S. Lloyd, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">109</span>, 050505 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.109.050505" title="DOI Link to resource 10.1103/PhysRevLett.109.050505">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=23006156%5Buid%5D" title="PubMed Link to resource 23006156">PubMed</a>.</span></li>
      <li><span id="cit131">A. W. Harrow, A. Hassidim and S. Lloyd, <span class="italic">Phys. Rev. Lett.</span>, 2009, <span class="bold">103</span>, 150502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.103.150502" title="DOI Link to resource 10.1103/PhysRevLett.103.150502">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=19905613%5Buid%5D" title="PubMed Link to resource 19905613">PubMed</a>.</span></li>
      <li><span id="cit132">Y. Lee, J. Joo and S. Lee, <span class="italic">Sci. Rep.</span>, 2019, <span class="bold">9</span>, 1–12 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41598-018-37186-2" title="DOI Link to resource 10.1038/s41598-018-37186-2">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30626917%5Buid%5D" title="PubMed Link to resource 30626917">PubMed</a>.</span></li>
      <li><span id="cit133">Y. Liu and S. Zhang, <span class="italic">Theor. Comput. Sci.</span>, 2017, <span class="bold">657</span>, 38–47 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.tcs.2016.05.044" title="DOI Link to resource 10.1016/j.tcs.2016.05.044">CrossRef</a>.</span></li>
      <li><span id="cit134">G. Wang, <span class="italic">Phys. Rev. A</span>, 2017, <span class="bold">96</span>, 012335 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.96.012335" title="DOI Link to resource 10.1103/PhysRevA.96.012335">CrossRef</a>.</span></li>
      <li><span id="cit135">J. Pan, Y. Cao, X. Yao, Z. Li, C. Ju, H. Chen, X. Peng, S. Kais and J. Du, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2014, <span class="bold">89</span>, 022313 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.89.022313" title="DOI Link to resource 10.1103/PhysRevA.89.022313">CrossRef</a>.</span></li>
      <li><span id="cit136">Y. Zheng, C. Song, M.-C. Chen, B. Xia, W. Liu, Q. Guo, L. Zhang, D. Xu, H. Deng, K. Huang, Y. Wu, Z. Yan, D. Zheng, L. Lu, J.-W. Pan, H. Wang, C.-Y. Lu and X. Zhu, <span class="italic">Phys. Rev. Lett.</span>, 2017, <span class="bold">118</span>, 210504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.118.210504" title="DOI Link to resource 10.1103/PhysRevLett.118.210504">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28598660%5Buid%5D" title="PubMed Link to resource 28598660">PubMed</a>.</span></li>
      <li><span id="cit137">S. Slussarenko and G. J. Pryde, <span class="italic">Appl. Phys. Rev.</span>, 2019, <span class="bold">6</span>, 041303 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Appl.%20Phys.%20Rev.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%20041303%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit138">M. Schuld, I. Sinayskiy and F. Petruccione, <span class="italic">Phys. Rev. A</span>, 2016, <span class="bold">94</span>, 022342 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.94.022342" title="DOI Link to resource 10.1103/PhysRevA.94.022342">CrossRef</a>.</span></li>
      <li><span id="cit139">Y. Subas, R. D. Somma and D. Orsucci, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">122</span>, 060504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.122.060504" title="DOI Link to resource 10.1103/PhysRevLett.122.060504">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30822089%5Buid%5D" title="PubMed Link to resource 30822089">PubMed</a>.</span></li>
      <li><span id="cit140">
          C. Bravo-Prieto, R. LaRose, M. Cerezo, Y. Subasi, L. Cincio and P. J. Coles, 2019, arXiv preprint arXiv:1909.05820.</span></li>
      <li><span id="cit141">C.-H. Yu, F. Gao and Q.-Y. Wen, <span class="italic">IEEE Trans. Knowledge Data Eng.</span>, 2019, <span class="bold">33</span>, 858–866 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Knowledge%20Data%20Eng.%5BJour%5D%20AND%2033%5Bvolume%5D%20AND%20858%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit142">
          D. Berrar, <span class="italic">Encyclopedia of Bioinformatics and Computational Biology</span>, Academic Press, Oxford,  2019, pp. 542–545 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Encyclopedia%20of%20Bioinformatics%20and%20Computational%20Biology%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit143">I. T. Jolliffe and J. Cadima, <span class="italic">Philos. Trans. R. Soc., A</span>, 2016, <span class="bold">374</span>, 20150202 <a target="_blank" class="DOILink" href="https://doi.org/10.1098/rsta.2015.0202" title="DOI Link to resource 10.1098/rsta.2015.0202">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26953178%5Buid%5D" title="PubMed Link to resource 26953178">PubMed</a>.</span></li>
      <li><span id="cit144">
          I. T. Jolliffe, <span class="italic">Principal component analysis for special types of data</span>, Springer,  2002 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Principal%20component%20analysis%20for%20special%20types%20of%20data%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202002%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit145">
          B. Schölkopf, A. Smola and K.-R. Müller, International conference on artificial neural networks, 1997, pp. 583–588.</span></li>
      <li><span id="cit146">
          H. Q. Minh, P. Niyogi and Y. Yao, International Conference on Computational Learning Theory, 2006, pp. 154–168.</span></li>
      <li><span id="cit147">C. Campbell, <span class="italic">Neurocomputing</span>, 2002, <span class="bold">48</span>, 63–84 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/S0925-2312(01)00643-9" title="DOI Link to resource 10.1016/S0925-2312(01)00643-9">CrossRef</a>.</span></li>
      <li><span id="cit148">
          Q. Wang, 2012, arXiv preprint arXiv:1207.3538.</span></li>
      <li><span id="cit149">S. Lloyd, M. Mohseni and P. Rebentrost, <span class="italic">Nat. Phys.</span>, 2014, <span class="bold">10</span>, 631–633 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2010%5Bvolume%5D%20AND%20631%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit150">Z. Li, Z. Chai, Y. Guo, W. Ji, M. Wang, F. Shi, Y. Wang, S. Lloyd and J. Du, <span class="italic">Sci. Adv.</span>, 2021, <span class="bold">7</span>, eabg2589 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Sci.%20Adv.%5BJour%5D%20AND%207%5Bvolume%5D%20AND%20eabg2589%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit151">Y. Li, R.-G. Zhou, R. Xu, W. Hu and P. Fan, <span class="italic">Quantum Sci. Technol.</span>, 2020, <span class="bold">6</span>, 014001 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Quantum%20Sci.%20Technol.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%20014001%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit152">T. Cover and P. Hart, <span class="italic">IEEE Trans. Inform. Theory</span>, 1967, <span class="bold">13</span>, 21–27 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Inform.%20Theory%5BJour%5D%20AND%2013%5Bvolume%5D%20AND%2021%5Bpage%5D%20and%201967%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit153">Y. Ruan, X. Xue, H. Liu, J. Tan and X. Li, <span class="italic">Int. J. Theor. Phys.</span>, 2017, <span class="bold">56</span>, 3496–3507 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10773-017-3514-4" title="DOI Link to resource 10.1007/s10773-017-3514-4">CrossRef</a>.</span></li>
      <li><span id="cit154">J. Gou, Z. Yi, L. Du and T. Xiong, <span class="italic">Comput. J.</span>, 2012, <span class="bold">55</span>, 1058–1071 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/comjnl/bxr131" title="DOI Link to resource 10.1093/comjnl/bxr131">CrossRef</a>.</span></li>
      <li><span id="cit155">Z. Geler, V. Kurbalija, M. Radovanović and M. Ivanović, <span class="italic">Knowledge Inform. Syst.</span>, 2016, <span class="bold">48</span>, 331–378 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10115-015-0881-0" title="DOI Link to resource 10.1007/s10115-015-0881-0">CrossRef</a>.</span></li>
      <li><span id="cit156">
          S. Lloyd, M. Mohseni and P. Rebentrost, 2013, arXiv preprint arXiv:1307.0411.</span></li>
      <li><span id="cit157">
          S. Aaronson, <span class="italic">Proceedings of the forty-second ACM symposium on Theory of computing</span>,  2010, pp. 141–150 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proceedings%20of%20the%20forty-second%20ACM%20symposium%20on%20Theory%20of%20computing%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202010%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit158">J. Wiśniewska and M. Sawerwain, <span class="italic">Vietnam J. Comput. Sci.</span>, 2018, <span class="bold">5</span>, 197–204 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s40595-018-0115-y" title="DOI Link to resource 10.1007/s40595-018-0115-y">CrossRef</a>.</span></li>
      <li><span id="cit159">Y. Wang, R. Wang, D. Li, D. Adu-Gyamfi, K. Tian and Y. Zhu, <span class="italic">Int. J. Theor. Phys.</span>, 2019, <span class="bold">58</span>, 2331–2340 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10773-019-04124-5" title="DOI Link to resource 10.1007/s10773-019-04124-5">CrossRef</a>.</span></li>
      <li><span id="cit160">S. K. Murthy, <span class="italic">Data Min. Knowl. Discov.</span>, 1998, <span class="bold">2</span>, 345–389 <a target="_blank" class="DOILink" href="https://doi.org/10.1023/A:1009744630224" title="DOI Link to resource 10.1023/A:1009744630224">CrossRef</a>.</span></li>
      <li><span id="cit161">
          S. B. Kotsiantis, I. Zaharakis, P. Pintelas, <span class="italic">et al.</span>, <span class="italic">Emerging artificial intelligence applications in computer engineering</span>,  2007, vol. 160, pp. 3–24 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Emerging%20artificial%20intelligence%20applications%20in%20computer%20engineering%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202007%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit162">E. B. Hunt, J. Marin and P. J. Stone, <span class="italic">Experiments in induction</span>, 1966 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Experiments%20in%20induction%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201966%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit163">
          L. Breiman, J. H. Friedman, R. A. Olshen and C. J. Stone, <span class="italic">Classification and regression trees</span>, Routledge,  2017 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Classification%20and%20regression%20trees%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit164">E. Farhi and S. Gutmann, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1998, <span class="bold">58</span>, 915 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.58.915" title="DOI Link to resource 10.1103/PhysRevA.58.915">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK1cXkslentbc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit165">S. Lu and S. L. Braunstein, <span class="italic">Quantum Inf. Process.</span>, 2014, <span class="bold">13</span>, 757–770 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s11128-013-0687-5" title="DOI Link to resource 10.1007/s11128-013-0687-5">CrossRef</a>.</span></li>
      <li><span id="cit166">
          K. Khadiev, I. Mannapov and L. Safina, 2019, arXiv preprint arXiv:1907.06840.</span></li>
      <li><span id="cit167">X. Wu, V. Kumar, J. R. Quinlan, J. Ghosh, Q. Yang, H. Motoda, G. J. McLachlan, A. Ng, B. Liu and S. Y. Philip, 
            <span class="italic">et al.</span>
          , <span class="italic">Knowledge Inform. Syst.</span>, 2008, <span class="bold">14</span>, 1–37 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10115-007-0114-2" title="DOI Link to resource 10.1007/s10115-007-0114-2">CrossRef</a>.</span></li>
      <li><span id="cit168">
          F. V. Jensen and T. D. Nielsen, <span class="italic">Bayesian networks and decision graphs</span>, Springer,  2007, vol. 2 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Bayesian%20networks%20and%20decision%20graphs%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202007%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit169">
          D. Heckerman, <span class="italic">Innovations Bayesian Networks</span>,  2008, pp. 33–82 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Innovations%20Bayesian%20Networks%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202008%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit170">R. R. Tucci, <span class="italic">Int. J. Mod. Phys. B</span>, 1995, <span class="bold">9</span>, 295–337 <a target="_blank" class="DOILink" href="https://doi.org/10.1142/S0217979295000148" title="DOI Link to resource 10.1142/S0217979295000148">CrossRef</a>.</span></li>
      <li><span id="cit171">M. S. Leifer and D. Poulin, <span class="italic">Ann. Phys.</span>, 2008, <span class="bold">323</span>, 1899–1946 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1cXntFKlsLo%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit172">C. Moreira and A. Wichert, <span class="italic">Front. Psychol.</span>, 2016, <span class="bold">7</span>, 11 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Front.%20Psychol.%5BJour%5D%20AND%207%5Bvolume%5D%20AND%2011%5Bpage%5D%20and%202016%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit173">G. H. Low, T. J. Yoder and I. L. Chuang, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2014, <span class="bold">89</span>, 062315 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.89.062315" title="DOI Link to resource 10.1103/PhysRevA.89.062315">CrossRef</a>.</span></li>
      <li><span id="cit174">S. E. Borujeni, S. Nannapaneni, N. H. Nguyen, E. C. Behrman and J. E. Steck, <span class="italic">Exp. Syst. Appl.</span>, 2021, <span class="bold">176</span>, 114768 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.eswa.2021.114768" title="DOI Link to resource 10.1016/j.eswa.2021.114768">CrossRef</a>.</span></li>
      <li><span id="cit175">
          V. Vapnik, <span class="italic">The nature of statistical learning theory</span>, Springer science &amp; business media,  2013 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=The%20nature%20of%20statistical%20learning%20theory%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202013%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit176">Z. Li, X. Liu, N. Xu and J. Du, <span class="italic">Phys. Rev. Lett.</span>, 2015, <span class="bold">114</span>, 140504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.114.140504" title="DOI Link to resource 10.1103/PhysRevLett.114.140504">CrossRef</a>.</span></li>
      <li><span id="cit177">P. Rebentrost, M. Mohseni and S. Lloyd, <span class="italic">Phys. Rev. Lett.</span>, 2014, <span class="bold">113</span>, 130503 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.113.130503" title="DOI Link to resource 10.1103/PhysRevLett.113.130503">CrossRef</a>.</span></li>
      <li><span id="cit178">A. W. Harrow, A. Hassidim and S. Lloyd, <span class="italic">Phys. Rev. Lett.</span>, 2009, <span class="bold">103</span>, 150502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.103.150502" title="DOI Link to resource 10.1103/PhysRevLett.103.150502">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=19905613%5Buid%5D" title="PubMed Link to resource 19905613">PubMed</a>.</span></li>
      <li><span id="cit179">M. Schuld and N. Killoran, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">122</span>, 040504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.122.040504" title="DOI Link to resource 10.1103/PhysRevLett.122.040504">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXnvFSqtL8%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit180">Y. Liu, S. Arunachalam and K. Temme, <span class="italic">Nat. Phys.</span>, 2021, <span class="bold">17</span>, 1013–1017 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2017%5Bvolume%5D%20AND%201013%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit181">
          M. Otten, I. R. Goumiri, B. W. Priest, G. F. Chapline and M. D. Schneider, 2020, arXiv preprint arXiv:2004.11280.</span></li>
      <li><span id="cit182">
          C. K. Williams and C. E. Rasmussen, <span class="italic">Gaussian processes for machine learning</span>, MIT Press, Cambridge, MA,  2006, vol. 2 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Gaussian%20processes%20for%20machine%20learning%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202006%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit183">V. L. Deringer, A. P. Bartók, N. Bernstein, D. M. Wilkins, M. Ceriotti and G. Csányi, <span class="italic">Chem. Rev.</span>, 2021, <span class="bold">121</span>, 10073–10141 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.1c00022" title="DOI Link to resource 10.1021/acs.chemrev.1c00022">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhslyhs7rN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34398616%5Buid%5D" title="PubMed Link to resource 34398616">PubMed</a>.</span></li>
      <li><span id="cit184">
          C. M. Bishop, <span class="italic">Pattern Recognition and Machine Learning (Information Science and Statistics)</span>, Springer-Verlag, Berlin, Heidelberg,  2006 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Pattern%20Recognition%20and%20Machine%20Learning%20(Information%20Science%20and%20Statistics)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202006%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit185">Y. Freund, R. Schapire and N. Abe, A short introduction to boosting, <span class="italic">J. Jpn. Soc. Artif. Intell.</span>, 1999, <span class="bold">14</span>(771–780), 1612 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Jpn.%20Soc.%20Artif.%20Intell.%5BJour%5D%20AND%2014%5Bvolume%5D%20AND%201612%5Bpage%5D%20and%201999%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit186">
          A. J. Al-Mahasneh, S. G. Anavatti and M. A. Garratt, <span class="italic">2017 International Conference on Advanced Mechatronics, Intelligent Manufacture, and Industrial Automation (ICAMIMIA)</span>,  2017, pp. 1–6 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2017%20International%20Conference%20on%20Advanced%20Mechatronics,%20Intelligent%20Manufacture,%20and%20Industrial%20Automation%20(ICAMIMIA)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit187">F. Rosenblatt, <span class="italic">Psychological Rev.</span>, 1958, <span class="bold">65</span>, 386 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADyaG1M%252FjtFCmtw%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit188">S. Haykin and N. Network, <span class="italic">Neural Networks</span>, 2004, <span class="bold">2</span>, 41 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Neural%20Networks%5BJour%5D%20AND%202%5Bvolume%5D%20AND%2041%5Bpage%5D%20and%202004%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit189">
          M. M. Lau and K. H. Lim, <span class="italic">2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES)</span>,  2018, pp. 686–690 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2018%20IEEE-EMBS%20Conference%20on%20Biomedical%20Engineering%20and%20Sciences%20(IECBES)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit190">S. Sharma and S. Sharma, <span class="italic">Towards Data Sci.</span>, 2017, <span class="bold">6</span>, 310–316 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Towards%20Data%20Sci.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%20310%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit191">
          A. L. Maas, A. Y. Hannun, A. Y. Ng, <span class="italic">et al.</span>, Proc. ICML, 2013, p. 3.</span></li>
      <li><span id="cit192">S. Jankowski, A. Lozowski and J. M. Zurada, <span class="italic">IEEE Trans. Neural Networks</span>, 1996, <span class="bold">7</span>, 1491–1496 <a target="_blank" class="DOILink" href="https://doi.org/10.1109/72.548176" title="DOI Link to resource 10.1109/72.548176">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1c7gsFCrsQ%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit193">G. Tanaka and K. Aihara, <span class="italic">IEEE Trans. Neural Networks</span>, 2009, <span class="bold">20</span>, 1463–1473 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Neural%20Networks%5BJour%5D%20AND%2020%5Bvolume%5D%20AND%201463%5Bpage%5D%20and%202009%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit194">B. Karlik and A. V. Olgac, <span class="italic">Int. J. Artificial Intell. Exp. Syst.</span>, 2011, <span class="bold">1</span>, 111–122 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Int.%20J.%20Artificial%20Intell.%20Exp.%20Syst.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%20111%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit195">
          F. Agostinelli, M. Hoffman, P. Sadowski and P. Baldi, 2014, arXiv preprint arXiv:1412.6830.</span></li>
      <li><span id="cit196">P.-T. De Boer, D. P. Kroese, S. Mannor and R. Y. Rubinstein, <span class="italic">Ann. Oper. Res.</span>, 2005, <span class="bold">134</span>, 19–67 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10479-005-5724-z" title="DOI Link to resource 10.1007/s10479-005-5724-z">CrossRef</a>.</span></li>
      <li><span id="cit197">D. E. Rumelhart, G. E. Hinton and R. J. Williams, <span class="italic">Nature</span>, 1986, <span class="bold">323</span>, 533–536 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/323533a0" title="DOI Link to resource 10.1038/323533a0">CrossRef</a>.</span></li>
      <li><span id="cit198">Y. LeCun, Y. Bengio and G. Hinton, <span class="italic">Nature</span>, 2015, <span class="bold">521</span>, 436–444 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nature14539" title="DOI Link to resource 10.1038/nature14539">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXht1WlurzP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26017442%5Buid%5D" title="PubMed Link to resource 26017442">PubMed</a>.</span></li>
      <li><span id="cit199">
          S. Ruder, 2016, arXiv preprint arXiv:1609.04747.</span></li>
      <li><span id="cit200">J. Duchi, E. Hazan and Y. Singer, <span class="italic">J. Mach. Learn. Res.</span>, 2011, <span class="bold">12</span>, 2121–2159 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%2012%5Bvolume%5D%20AND%202121%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit201">
          D. P. Kingma and J. Ba, 2014, arXiv preprint arXiv:1412.6980.</span></li>
      <li><span id="cit202">T. Dozat, Incorporating Nesterov Momentum into Adam, <span class="italic">ICLR Workshop</span>, 2016 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=ICLR%20Workshop%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202016%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit203">
          L. Wan, M. Zeiler, S. Zhang, Y. Le Cun and R. Fergus, <span class="italic">International conference on machine learning</span>,  2013, pp. 1058–1066 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=International%20conference%20on%20machine%20learning%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202013%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit204">F. Girosi, M. Jones and T. Poggio, <span class="italic">Neural Comput.</span>, 1995, <span class="bold">7</span>, 219–269 <a target="_blank" class="DOILink" href="https://doi.org/10.1162/neco.1995.7.2.219" title="DOI Link to resource 10.1162/neco.1995.7.2.219">CrossRef</a>.</span></li>
      <li><span id="cit205">N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever and R. Salakhutdinov, <span class="italic">J. Mach. Learn. Res.</span>, 2014, <span class="bold">15</span>, 1929–1958 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%2015%5Bvolume%5D%20AND%201929%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit206">R. Xia and S. Kais, <span class="italic">Entropy</span>, 2020, <span class="bold">22</span>, 828 <a target="_blank" class="DOILink" href="https://doi.org/10.3390/e22080828" title="DOI Link to resource 10.3390/e22080828">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitVCjtLjJ" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit207">J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush and H. Neven, <span class="italic">Nat. Commun.</span>, 2018, <span class="bold">9</span>, 1–6 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-018-07090-4" title="DOI Link to resource 10.1038/s41467-018-07090-4">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXit1Chtb7I" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit208">A. Mari, T. R. Bromley, J. Izaac, M. Schuld and N. Killoran, <span class="italic">Quantum</span>, 2020, <span class="bold">4</span>, 340 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2020-10-09-340" title="DOI Link to resource 10.22331/q-2020-10-09-340">CrossRef</a>.</span></li>
      <li><span id="cit209">J. Li and S. Kais, <span class="italic">New J. Phys.</span>, 2021, <span class="bold">23</span>(10), 103022 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/ac2cb4" title="DOI Link to resource 10.1088/1367-2630/ac2cb4">CrossRef</a>.</span></li>
      <li><span id="cit210">N. Wiebe and V. Kliuchnikov, <span class="italic">New J. Phys.</span>, 2013, <span class="bold">15</span>, 093041 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/15/9/093041" title="DOI Link to resource 10.1088/1367-2630/15/9/093041">CrossRef</a>.</span></li>
      <li><span id="cit211">
          A. Paetznick and K. M. Svore, 2013, arXiv preprint arXiv:1311.1074.</span></li>
      <li><span id="cit212">
          Y. Cao, G. G. Guerreschi and A. Aspuru-Guzik, 2017, arXiv preprint arXiv:1711.11240.</span></li>
      <li><span id="cit213">
          A. Daskin, <span class="italic">2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC)</span>,  2018, pp. 2887–2891 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2018%20IEEE%20International%20Conference%20on%20Systems,%20Man,%20and%20Cybernetics%20(SMC)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit214">M. Schuld, I. Sinayskiy and F. Petruccione, <span class="italic">Phys. Lett. A</span>, 2015, <span class="bold">379</span>, 660–663 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.physleta.2014.11.061" title="DOI Link to resource 10.1016/j.physleta.2014.11.061">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXntVWrsA%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit215">I. Cong, S. Choi and M. D. Lukin, <span class="italic">Nat. Phys.</span>, 2019, <span class="bold">15</span>, 1273–1278 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2015%5Bvolume%5D%20AND%201273%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit216">W. Rawat and Z. Wang, <span class="italic">Neural Comput.</span>, 2017, <span class="bold">29</span>, 2352–2449 <a target="_blank" class="DOILink" href="https://doi.org/10.1162/neco_a_00990" title="DOI Link to resource 10.1162/neco_a_00990">CrossRef</a>.</span></li>
      <li><span id="cit217">A. Voulodimos, N. Doulamis, A. Doulamis and E. Protopapadakis, <span class="italic">Comput. Intell. Neurosci.</span>, 2018, <span class="bold">2018</span>, 13 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Comput.%20Intell.%20Neurosci.%5BJour%5D%20AND%202018%5Bvolume%5D%20AND%2013%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit218">
          S. Albawi, T. A. Mohammed and S. Al-Zawi, <span class="italic">2017 International Conference on Engineering and Technology (ICET)</span>,  2017, pp. 1–6 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2017%20International%20Conference%20on%20Engineering%20and%20Technology%20(ICET)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit219">
          Y. LeCun, P. Haffner, L. Bottou and Y. Bengio, <span class="italic">Shape, contour and grouping in computer vision</span>, Springer,  1999, pp. 319–345 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Shape,%20contour%20and%20grouping%20in%20computer%20vision%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201999%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit220">A. Dhillon and G. K. Verma, <span class="italic">Progress Artificial Intell.</span>, 2020, <span class="bold">9</span>, 85–112 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s13748-019-00203-0" title="DOI Link to resource 10.1007/s13748-019-00203-0">CrossRef</a>.</span></li>
      <li><span id="cit221">
          N. Aloysius and M. Geetha, <span class="italic">2017 International Conference on Communication and Signal Processing (ICCSP)</span>,  2017, pp. 0588–0592 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2017%20International%20Conference%20on%20Communication%20and%20Signal%20Processing%20(ICCSP)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit222">
          C. Kong and S. Lucey, 2017, arXiv preprint arXiv:1712.02502.</span></li>
      <li><span id="cit223">L. Alzubaidi, J. Zhang, A. J. Humaidi, A. Al-Dujaili, Y. Duan, O. Al-Shamma, J. Santamara, M. A. Fadhel, M. Al-Amidie and L. Farhan, <span class="italic">J. Big Data</span>, 2021, <span class="bold">8</span>, 1–74 <a target="_blank" class="DOILink" href="https://doi.org/10.1186/s40537-020-00387-6" title="DOI Link to resource 10.1186/s40537-020-00387-6">CrossRef</a>.</span></li>
      <li><span id="cit224">
          D. Yu, H. Wang, P. Chen and Z. Wei, <span class="italic">International conference on rough sets and knowledge technology</span>,  2014, pp. 364–375 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=International%20conference%20on%20rough%20sets%20and%20knowledge%20technology%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit225">Z. Li, S.-H. Wang, R.-R. Fan, G. Cao, Y.-D. Zhang and T. Guo, <span class="italic">Int. J. Imaging Syst. Technol.</span>, 2019, <span class="bold">29</span>, 577–583 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/ima.22337" title="DOI Link to resource 10.1002/ima.22337">CrossRef</a>.</span></li>
      <li><span id="cit226">
          W. Yin, K. Kann, M. Yu and H. Schütze, 2017, arXiv preprint arXiv:1702.01923.</span></li>
      <li><span id="cit227">
          S. Selvin, R. Vinayakumar, E. Gopalakrishnan, V. K. Menon and K. Soman, 2017 international conference on advances in computing, communications and informatics (icacci), 2017, pp. 1643–1647.</span></li>
      <li><span id="cit228">
          K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk and Y. Bengio, 2014, arXiv preprint arXiv:1406.1078.</span></li>
      <li><span id="cit229">
          Y. Qin, D. Song, H. Chen, W. Cheng, G. Jiang and G. Cottrell, 2017, arXiv preprint arXiv:1704.02971.</span></li>
      <li><span id="cit230">Y. Yu, X. Si, C. Hu and J. Zhang, <span class="italic">Neural Comput.</span>, 2019, <span class="bold">31</span>, 1235–1270 <a target="_blank" class="DOILink" href="https://doi.org/10.1162/neco_a_01199" title="DOI Link to resource 10.1162/neco_a_01199">CrossRef</a>.</span></li>
      <li><span id="cit231">Y. Takaki, K. Mitarai, M. Negoro, K. Fujii and M. Kitagawa, <span class="italic">Phys. Rev. A</span>, 2021, <span class="bold">103</span>, 052414 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.103.052414" title="DOI Link to resource 10.1103/PhysRevA.103.052414">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXht1ymtbbE" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit232">A. Sherstinsky, <span class="italic">Phys. D</span>, 2020, <span class="bold">404</span>, 132306 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.physd.2019.132306" title="DOI Link to resource 10.1016/j.physd.2019.132306">CrossRef</a>.</span></li>
      <li><span id="cit233">
          S. Yang, X. Yu and Y. Zhou, <span class="italic">2020 International workshop on electronic communication and artificial intelligence (IWECAI)</span>,  2020, pp. 98–101 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2020%20International%20workshop%20on%20electronic%20communication%20and%20artificial%20intelligence%20(IWECAI)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit234">
          R. Dey and F. M. Salem, <span class="italic">2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS)</span>,  2017, pp. 1597–1600 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=2017%20IEEE%2060th%20international%20midwest%20symposium%20on%20circuits%20and%20systems%20(MWSCAS)%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit235">
          J. Chung, C. Gulcehre, K. Cho and Y. Bengio, 2014, arXiv preprint arXiv:1412.3555.</span></li>
      <li><span id="cit236">S. Hochreiter and J. Schmidhuber, <span class="italic">Neural Comput.</span>, 1997, <span class="bold">9</span>, 1735–1780 <a target="_blank" class="DOILink" href="https://doi.org/10.1162/neco.1997.9.8.1735" title="DOI Link to resource 10.1162/neco.1997.9.8.1735">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADyaK1c%252FhvVahsQ%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=9377276%5Buid%5D" title="PubMed Link to resource 9377276">PubMed</a>.</span></li>
      <li><span id="cit237">
          S. Y.-C. Chen, S. Yoo and Y.-L. L. Fang, 2020, arXiv preprint arXiv:2009.01783.</span></li>
      <li><span id="cit238">
          A. Makhzani and B. Frey, K-sparse autoencoders, arXiv:1312.5663,  2013.</span></li>
      <li><span id="cit239">P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio and P.-A. Manzagol, <span class="italic">J. Mach. Learn. Res.</span>, 2010, <span class="bold">11</span>, 3371–3408 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%2011%5Bvolume%5D%20AND%203371%5Bpage%5D%20and%202010%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit240">R. Salakhutdinov and G. Hinton, <span class="italic">Int. J. Approx. Reason.</span>, 2009, <span class="bold">50</span>, 969–978 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.ijar.2008.11.006" title="DOI Link to resource 10.1016/j.ijar.2008.11.006">CrossRef</a>.</span></li>
      <li><span id="cit241">M. Ribeiro, A. E. Lazzaretti and H. S. Lopes, <span class="italic">Pattern Recognition Lett.</span>, 2018, <span class="bold">105</span>, 13–22 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.patrec.2017.07.016" title="DOI Link to resource 10.1016/j.patrec.2017.07.016">CrossRef</a>.</span></li>
      <li><span id="cit242">J. Romero, J. P. Olson and A. Aspuru-Guzik, <span class="italic">Quantum Sci. Technol.</span>, 2017, <span class="bold">2</span>, 045001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/aa8072" title="DOI Link to resource 10.1088/2058-9565/aa8072">CrossRef</a>.</span></li>
      <li><span id="cit243">V. Shende, I. L. Markov and S. S. Bullock, Minimal universal two-qubit controlled-not-based circuits, <span class="italic">Phys. Rev. A</span>, 2004, <span class="bold">69</span>(6), 062321 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.69.062321" title="DOI Link to resource 10.1103/PhysRevA.69.062321">CrossRef</a>.</span></li>
      <li><span id="cit244">L. Lamata, U. Alvarez-Rodriguez, J. D. Martín-Guerrero, M. Sanz and E. Solano, <span class="italic">Quantum Sci. Technol.</span>, 2018, <span class="bold">4</span>, 014007 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/aae22b" title="DOI Link to resource 10.1088/2058-9565/aae22b">CrossRef</a>.</span></li>
      <li><span id="cit245">K. H. Wan, O. Dahlsten, H. Kristjánsson, R. Gardner and M. S. Kim, <span class="italic">npj Quantum Inform.</span>, 2017, <span class="bold">3</span>, 36 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-017-0032-4" title="DOI Link to resource 10.1038/s41534-017-0032-4">CrossRef</a>.</span></li>
      <li><span id="cit246">
          N. Dilokthanakul, P. A. M. Mediano, M. Garnelo, M. C. H. Lee, H. Salimbeni, K. Arulkumaran and M. Shanahan,  2016, arXiv preprint arXiv:1611.02648.</span></li>
      <li><span id="cit247">W. Xu, H. Sun, C. Deng and Y. Tan, <span class="italic">Proc. AAAI Conf. Artificial Intell.</span>, 2017, <span class="bold">31</span>, 3358 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proc.%20AAAI%20Conf.%20Artificial%20Intell.%5BJour%5D%20AND%2031%5Bvolume%5D%20AND%203358%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit248">P.-L. Dallaire-Demers and N. Killoran, <span class="italic">Phys. Rev. A</span>, 2018, <span class="bold">98</span>, 012324 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.98.012324" title="DOI Link to resource 10.1103/PhysRevA.98.012324">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXlsV2ht7Y%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit249">A. Khoshaman, W. Vinci, B. Denis, E. Andriyash, H. Sadeghi and M. H. Amin, <span class="italic">Quantum Sci. Technol.</span>, 2018, <span class="bold">4</span>, 014001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/aada1f" title="DOI Link to resource 10.1088/2058-9565/aada1f">CrossRef</a>.</span></li>
      <li><span id="cit250">M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy and R. Melko, <span class="italic">Phys. Rev. X</span>, 2018, <span class="bold">8</span>, 021050 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSgurc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit251">I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville and Y. Bengio, Generative Adversarial Nets, <span class="italic">Advances in Neural Information Processing Systems 27</span>, 2014 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Advances%20in%20Neural%20Information%20Processing%20Systems%2027%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit252">J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu and T. S. Huang, Generative image inpainting with contextual attention, <span class="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, 2018 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proceedings%20of%20the%20IEEE%20conference%20on%20computer%20vision%20and%20pattern%20recognition%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a> , p. 5505.</span></li>
      <li><span id="cit253">K. Schawinski, C. Zhang, H. Zhang, L. Fowler and G. K. Santhanam, <span class="italic">Mon. Not. R. Astron. Soc.: Lett.</span>, 2017, slx008 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/mnrasl/slx008" title="DOI Link to resource 10.1093/mnrasl/slx008">CrossRef</a>.</span></li>
      <li><span id="cit254">X. Wang, K. Yu, S. Wu, J. Gu, Y. Liu, C. Dong, C. C. Loy, Y. Qiao and X. Tang, <span class="italic">Esrgan: Enhanced super-resolution generative adversarial networks</span>, 2018 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Esrgan:%20Enhanced%20super-resolution%20generative%20adversarial%20networks%5BJour%5D%20AND%20%5Bvolume%5D%20AND%202018%5Bpage%5D%20and%20%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a> , arXiv:1809.00219.</span></li>
      <li><span id="cit255">
          B. Li, V. François-Lavet, T. Doan and J. Pineau,  2021, arXiv preprint arXiv:2102.07097.</span></li>
      <li><span id="cit256">S. Lloyd and C. Weedbrook, <span class="italic">Phys. Rev. Lett.</span>, 2018, <span class="bold">121</span>, 040502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.121.040502" title="DOI Link to resource 10.1103/PhysRevLett.121.040502">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSjs7g%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit257">M. Schuld, V. Bergholm, C. Gogolin, J. Izaac and N. Killoran, <span class="italic">Phys. Rev. A</span>, 2019, <span class="bold">99</span>, 032331 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.99.032331" title="DOI Link to resource 10.1103/PhysRevA.99.032331">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXnsFGqu7o%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit258">
          J. Biamonte and V. Bergholm, 2017, arXiv preprint arXiv:1708.00006.</span></li>
      <li><span id="cit259">J. C. Bridgeman and C. T. Chubb, <span class="italic">J. Phys. A: Math. Theor.</span>, 2017, <span class="bold">50</span>(22), 223001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1751-8121/aa6dc3" title="DOI Link to resource 10.1088/1751-8121/aa6dc3">CrossRef</a>.</span></li>
      <li><span id="cit260">G. Vidal, <span class="italic">Phys. Rev. Lett.</span>, 2004, <span class="bold">93</span>, 040502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.93.040502" title="DOI Link to resource 10.1103/PhysRevLett.93.040502">CrossRef</a>.</span></li>
      <li><span id="cit261">G. Vidal, <span class="italic">Phys. Rev. Lett.</span>, 2003, <span class="bold">91</span>, 147902 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.91.147902" title="DOI Link to resource 10.1103/PhysRevLett.91.147902">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=14611555%5Buid%5D" title="PubMed Link to resource 14611555">PubMed</a>.</span></li>
      <li><span id="cit262">J. Eisert, M. Cramer and M. B. Plenio, <span class="italic">Rev. Mod. Phys.</span>, 2010, <span class="bold">82</span>, 277 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.82.277" title="DOI Link to resource 10.1103/RevModPhys.82.277">CrossRef</a>.</span></li>
      <li><span id="cit263">M. B. Hastings, <span class="italic">J. Stat. Mech.: Theory Exp.</span>, 2007, <span class="bold">2007</span>, P08024 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Stat.%20Mech.:%20Theory%20Exp.%5BJour%5D%20AND%202007%5Bvolume%5D%20AND%20P08024%5Bpage%5D%20and%202007%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit264">R. Orús, <span class="italic">Ann. Phys.</span>, 2014, <span class="bold">349</span>, 117–158 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Ann.%20Phys.%5BJour%5D%20AND%20349%5Bvolume%5D%20AND%20117%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit265">
          G. Vidal, 2009, arXiv preprint arXiv:0912.1651.</span></li>
      <li><span id="cit266">U. Schollwöck, <span class="italic">Ann. Phys.</span>, 2011, <span class="bold">326</span>, 96–192 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Ann.%20Phys.%5BJour%5D%20AND%20326%5Bvolume%5D%20AND%2096%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit267">V. Oseledets and E. E. Tyrtyshnikov, <span class="italic">SIAM J. Sci. Comput.</span>, 2009, <span class="bold">31</span>, 3744–3759 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/090748330" title="DOI Link to resource 10.1137/090748330">CrossRef</a>.</span></li>
      <li><span id="cit268">I. V. Oseledets, <span class="italic">SIAM J. Sci. Comput.</span>, 2011, <span class="bold">33</span>, 2295–2317 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/090752286" title="DOI Link to resource 10.1137/090752286">CrossRef</a>.</span></li>
      <li><span id="cit269">Y.-Y. Shi, L.-M. Duan and G. Vidal, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2006, <span class="bold">74</span>, 022320 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.74.022320" title="DOI Link to resource 10.1103/PhysRevA.74.022320">CrossRef</a>.</span></li>
      <li><span id="cit270">D. Nagaj, E. Farhi, J. Goldstone, P. Shor and I. Sylvester, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 2008, <span class="bold">77</span>, 214431 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.77.214431" title="DOI Link to resource 10.1103/PhysRevB.77.214431">CrossRef</a>.</span></li>
      <li><span id="cit271">B. Friedman, <span class="italic">J. Phys.: Condens. Matter</span>, 1997, <span class="bold">9</span>, 9021 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0953-8984/9/42/016" title="DOI Link to resource 10.1088/0953-8984/9/42/016">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK2sXntlOqs70%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit272">N. Nakatani and G. K.-L. Chan, <span class="italic">J. Chem. Phys.</span>, 2013, <span class="bold">138</span>, 134113 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4798639" title="DOI Link to resource 10.1063/1.4798639">CrossRef</a>.</span></li>
      <li><span id="cit273">
          D. Castellana and D. Bacciu, Learning from Non-Binary Constituency Trees via Tensor Decomposition, 2020, https://arxiv.org/abs/2011.00860.</span></li>
      <li><span id="cit274">M. E. Fisher, <span class="italic">Rev. Mod. Phys.</span>, 1998, <span class="bold">70</span>, 653 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.70.653" title="DOI Link to resource 10.1103/RevModPhys.70.653">CrossRef</a>.</span></li>
      <li><span id="cit275">L. Tagliacozzo, G. Evenbly and G. Vidal, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 2009, <span class="bold">80</span>, 235127 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.80.235127" title="DOI Link to resource 10.1103/PhysRevB.80.235127">CrossRef</a>.</span></li>
      <li><span id="cit276">H. R. Larsson, <span class="italic">J. Chem. Phys.</span>, 2019, <span class="bold">151</span>, 204102 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5130390" title="DOI Link to resource 10.1063/1.5130390">CrossRef</a>.</span></li>
      <li><span id="cit277">Y. Kurashige, <span class="italic">J. Chem. Phys.</span>, 2018, <span class="bold">149</span>, 194114 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5051498" title="DOI Link to resource 10.1063/1.5051498">CrossRef</a>.</span></li>
      <li><span id="cit278">M. Kliesch, D. Gross and J. Eisert, <span class="italic">Phys. Rev. Lett.</span>, 2014, <span class="bold">113</span>, 160503 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.113.160503" title="DOI Link to resource 10.1103/PhysRevLett.113.160503">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BC2M3kvFSqsA%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=25361243%5Buid%5D" title="PubMed Link to resource 25361243">PubMed</a>.</span></li>
      <li><span id="cit279">R. Raussendorf and H. J. Briegel, <span class="italic">Phys. Rev. Lett.</span>, 2001, <span class="bold">86</span>, 5188 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.86.5188" title="DOI Link to resource 10.1103/PhysRevLett.86.5188">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3MXjvFWmsL4%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit280">F. Verstraete, M. M. Wolf, D. Perez-Garcia and J. I. Cirac, <span class="italic">Phys. Rev. Lett.</span>, 2006, <span class="bold">96</span>, 220601 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.96.220601" title="DOI Link to resource 10.1103/PhysRevLett.96.220601">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD28zovValtQ%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit281">A. Y. Kitaev, <span class="italic">Ann. Phys.</span>, 2003, <span class="bold">303</span>, 2–30 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3sXisl2hsg%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit282">M. Schwarz, O. Buerschaper and J. Eisert, <span class="italic">Phys. Rev. A</span>, 2017, <span class="bold">95</span>, 060102 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.95.060102" title="DOI Link to resource 10.1103/PhysRevA.95.060102">CrossRef</a>.</span></li>
      <li><span id="cit283">J. Jordan, R. Orús, G. Vidal, F. Verstraete and J. I. Cirac, <span class="italic">Phys. Rev. Lett.</span>, 2008, <span class="bold">101</span>(25), 250602 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.101.250602" title="DOI Link to resource 10.1103/PhysRevLett.101.250602">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1M%252Fht1SktQ%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=19113687%5Buid%5D" title="PubMed Link to resource 19113687">PubMed</a>.</span></li>
      <li><span id="cit284">Z.-C. Gu, M. Levin and X.-G. Wen, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 2008, <span class="bold">78</span>, 205116 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.78.205116" title="DOI Link to resource 10.1103/PhysRevB.78.205116">CrossRef</a>.</span></li>
      <li><span id="cit285">M. Schwarz, K. Temme and F. Verstraete, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">108</span>, 110502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.108.110502" title="DOI Link to resource 10.1103/PhysRevLett.108.110502">CrossRef</a>.</span></li>
      <li><span id="cit286">M. Schwarz, K. Temme, F. Verstraete, D. Perez-Garcia and T. S. Cubitt, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2013, <span class="bold">88</span>, 032321 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.88.032321" title="DOI Link to resource 10.1103/PhysRevA.88.032321">CrossRef</a>.</span></li>
      <li><span id="cit287">P. Corboz, P. Czarnik, G. Kapteijns and L. Tagliacozzo, <span class="italic">Phys. Rev. X</span>, 2018, <span class="bold">8</span>(3), 031031 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Phys.%20Rev.%20X%5BJour%5D%20AND%208%5Bvolume%5D%20AND%20031031%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit288">
          A. Milsted and G. Vidal, 2018, arXiv preprint arXiv:1812.00529.</span></li>
      <li><span id="cit289">G. Vidal, <span class="italic">Phys. Rev. Lett.</span>, 2008, <span class="bold">101</span>, 110501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.101.110501" title="DOI Link to resource 10.1103/PhysRevLett.101.110501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1cnlslSntw%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18851269%5Buid%5D" title="PubMed Link to resource 18851269">PubMed</a>.</span></li>
      <li><span id="cit290">G. Evenbly and G. Vidal, <span class="italic">Phys. Rev. Lett.</span>, 2009, <span class="bold">102</span>(18), 180406 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.102.180406" title="DOI Link to resource 10.1103/PhysRevLett.102.180406">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1MvhtlantA%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=19518850%5Buid%5D" title="PubMed Link to resource 19518850">PubMed</a>.</span></li>
      <li><span id="cit291">S. R. White, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 1993, <span class="bold">48</span>, 10345 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.48.10345" title="DOI Link to resource 10.1103/PhysRevB.48.10345">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK2cXhsFWquw%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit292">J. Kempe, A. Kitaev and O. Regev, <span class="italic">SIAM J. Comput.</span>, 2006, <span class="bold">35</span>, 1070–1097 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/S0097539704445226" title="DOI Link to resource 10.1137/S0097539704445226">CrossRef</a>.</span></li>
      <li><span id="cit293">
          A. Kawaguchi, K. Shimizu, Y. Tokura and N. Imoto, 2004, arXiv preprint quant-ph/0411205.</span></li>
      <li><span id="cit294">A. Dang, C. D. Hill and L. C. Hollenberg, <span class="italic">Quantum</span>, 2019, <span class="bold">3</span>, 116 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2019-01-25-116" title="DOI Link to resource 10.22331/q-2019-01-25-116">CrossRef</a>.</span></li>
      <li><span id="cit295">E. Dumitrescu, <span class="italic">Phys. Rev. A</span>, 2017, <span class="bold">96</span>, 062322 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.96.062322" title="DOI Link to resource 10.1103/PhysRevA.96.062322">CrossRef</a>.</span></li>
      <li><span id="cit296">
          C. Huang, F. Zhang, M. Newman, J. Cai, X. Gao, Z. Tian, J. Wu, H. Xu, H. Yu, B. Yuan, <span class="italic">et al.</span>, 2020, arXiv preprint arXiv:2005.06787.</span></li>
      <li><span id="cit297">F. Arute, K. Arya, R. Babbush, D. Bacon, J. C. Bardin, R. Barends, R. Biswas, S. Boixo, F. G. Brandao and D. A. Buell, 
            <span class="italic">et al.</span>
          , <span class="italic">Nature</span>, 2019, <span class="bold">574</span>, 505–510 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-019-1666-5" title="DOI Link to resource 10.1038/s41586-019-1666-5">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXitVagsb3I" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31645734%5Buid%5D" title="PubMed Link to resource 31645734">PubMed</a>.</span></li>
      <li><span id="cit298">C. Huang, F. Zhang, M. Newman, X. Ni, D. Ding, J. Cai, X. Gao, T. Wang, F. Wu and G. Zhang, 
            <span class="italic">et al.</span>
          , <span class="italic">Nat. Comput. Sci.</span>, 2021, <span class="bold">1</span>, 578–587 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s43588-021-00119-7" title="DOI Link to resource 10.1038/s43588-021-00119-7">CrossRef</a>.</span></li>
      <li><span id="cit299">I. L. Markov and Y. Shi, <span class="italic">SIAM J. Comput.</span>, 2008, <span class="bold">38</span>, 963–981 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/050644756" title="DOI Link to resource 10.1137/050644756">CrossRef</a>.</span></li>
      <li><span id="cit300">J.-G. Liu, Y.-H. Zhang, Y. Wan and L. Wang, <span class="italic">Phys. Rev. Res.</span>, 2019, <span class="bold">1</span>, 023025 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.1.023025" title="DOI Link to resource 10.1103/PhysRevResearch.1.023025">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhvVOhtrjP" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit301">S.-J. Ran, <span class="italic">Phys. Rev. A</span>, 2020, <span class="bold">101</span>(3), 032310 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.101.032310" title="DOI Link to resource 10.1103/PhysRevA.101.032310">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXptVyjsbg%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit302">W. Huggins, P. Patil, B. Mitchell, K. B. Whaley and E. M. Stoudenmire, <span class="italic">Quantum Sci. Technol.</span>, 2019, <span class="bold">4</span>, 024001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/aaea94" title="DOI Link to resource 10.1088/2058-9565/aaea94">CrossRef</a>.</span></li>
      <li><span id="cit303">A. Kardashin, A. Uvarov and J. Biamonte, <span class="italic">Front. Phys.</span>, 2021, 644 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Front.%20Phys.%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20644%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit304">
          I. H. Kim and B. Swingle, 2017, arXiv preprint arXiv:1711.07500.</span></li>
      <li><span id="cit305">X. Yuan, J. Sun, J. Liu, Q. Zhao and Y. Zhou, <span class="italic">Phys. Rev. Lett.</span>, 2021, <span class="bold">127</span>, 040501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.127.040501" title="DOI Link to resource 10.1103/PhysRevLett.127.040501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhslWnsrnF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34355945%5Buid%5D" title="PubMed Link to resource 34355945">PubMed</a>.</span></li>
      <li><span id="cit306">K. Mitarai, M. Negoro, M. Kitagawa and K. Fujii, <span class="italic">Phys. Rev. A</span>, 2018, <span class="bold">98</span>, 032309 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.98.032309" title="DOI Link to resource 10.1103/PhysRevA.98.032309">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXlsFyht7k%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit307">P. Arrighi, <span class="italic">Natural Comput.</span>, 2019, <span class="bold">18</span>, 885–899 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s11047-019-09762-6" title="DOI Link to resource 10.1007/s11047-019-09762-6">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvVWkur3J" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit308">A. Pérez-Salinas, A. Cervera-Lierta, E. Gil-Fuster and J. I. Latorre, <span class="italic">Quantum</span>, 2020, <span class="bold">4</span>, 226 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2020-02-06-226" title="DOI Link to resource 10.22331/q-2020-02-06-226">CrossRef</a>.</span></li>
      <li><span id="cit309">M. Schuld, R. Sweke and J. J. Meyer, <span class="italic">Phys. Rev. A</span>, 2021, <span class="bold">103</span>, 032430 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.103.032430" title="DOI Link to resource 10.1103/PhysRevA.103.032430">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXovVCrtrw%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit310">H. Chen, L. Wossnig, S. Severini, H. Neven and M. Mohseni, <span class="italic">Quantum Mach. Intell.</span>, 2021, <span class="bold">3</span>, 1–11 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s42484-020-00025-7" title="DOI Link to resource 10.1007/s42484-020-00025-7">CrossRef</a>.</span></li>
      <li><span id="cit311">S. Sim, P. D. Johnson and A. Aspuru-Guzik, <span class="italic">Adv. Quantum Technol.</span>, 2019, <span class="bold">2</span>, 1900070 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/qute.201900070" title="DOI Link to resource 10.1002/qute.201900070">CrossRef</a>.</span></li>
      <li><span id="cit312">J. C. Snyder, M. Rupp, K. Hansen, K.-R. Müller and K. Burke, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">108</span>, 253002 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.108.253002" title="DOI Link to resource 10.1103/PhysRevLett.108.253002">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=23004593%5Buid%5D" title="PubMed Link to resource 23004593">PubMed</a>.</span></li>
      <li><span id="cit313">M. Rupp, A. Tkatchenko, K.-R. Müller and O. A. von Lilienfeld, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">108</span>, 058301 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.108.058301" title="DOI Link to resource 10.1103/PhysRevLett.108.058301">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22400967%5Buid%5D" title="PubMed Link to resource 22400967">PubMed</a>.</span></li>
      <li><span id="cit314">A. S. Christensen, L. A. Bratholm, F. A. Faber and O. Anatole von Lilienfeld, <span class="italic">J. Chem. Phys.</span>, 2020, <span class="bold">152</span>, 044107 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5126701" title="DOI Link to resource 10.1063/1.5126701">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitVOlsLY%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32007071%5Buid%5D" title="PubMed Link to resource 32007071">PubMed</a>.</span></li>
      <li><span id="cit315">M.-W. Huang, C.-W. Chen, W.-C. Lin, S.-W. Ke and C.-F. Tsai, <span class="italic">PLoS One</span>, 2017, <span class="bold">12</span>, e0161501 <a target="_blank" class="DOILink" href="https://doi.org/10.1371/journal.pone.0161501" title="DOI Link to resource 10.1371/journal.pone.0161501">CrossRef</a>.</span></li>
      <li><span id="cit316">H. Sun, S. Shahane, M. Xia, C. P. Austin and R. Huang, <span class="italic">J. Chem. Inf. Model.</span>, 2012, <span class="bold">52</span>, 1798–1805 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci3001875" title="DOI Link to resource 10.1021/ci3001875">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC38XptVWmsL4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22725677%5Buid%5D" title="PubMed Link to resource 22725677">PubMed</a>.</span></li>
      <li><span id="cit317">K. Batra, K. M. Zorn, D. H. Foil, E. Minerali, V. O. Gawriljuk, T. R. Lane and S. Ekins, <span class="italic">J. Chem. Inf. Model.</span>, 2021, <span class="bold">61</span>(6), 2641–2647 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jcim.1c00166" title="DOI Link to resource 10.1021/acs.jcim.1c00166">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhtFChtrzM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34032436%5Buid%5D" title="PubMed Link to resource 34032436">PubMed</a>.</span></li>
      <li><span id="cit318">
          L. H. Li, D.-B. Zhang and Z. Wang, 2021, arXiv preprint arXiv:2108.11114.</span></li>
      <li><span id="cit319">
          S. D. Bartlett, B. C. Sanders, S. L. Braunstein and K. Nemoto, in <span class="italic">Efficient Classical Simulation of Continuous Variable Quantum Information Processes</span>, ed. S. L. Braunstein and A. K. Pati, Springer Netherlands, Dordrecht,  2003, pp. 47–55 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Efficient%20Classical%20Simulation%20of%20Continuous%20Variable%20Quantum%20Information%20Processes%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit320">S. L. Braunstein and P. van Loock, <span class="italic">Rev. Mod. Phys.</span>, 2005, <span class="bold">77</span>, 513–577 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.77.513" title="DOI Link to resource 10.1103/RevModPhys.77.513">CrossRef</a>.</span></li>
      <li><span id="cit321">S. D. Bartlett and B. C. Sanders, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2002, <span class="bold">65</span>, 042304 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.65.042304" title="DOI Link to resource 10.1103/PhysRevA.65.042304">CrossRef</a>.</span></li>
      <li><span id="cit322">T. Douce, D. Markham, E. Kashefi, E. Diamanti, T. Coudreau, P. Milman, P. van Loock and G. Ferrini, <span class="italic">Phys. Rev. Lett.</span>, 2017, <span class="bold">118</span>, 070503 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.118.070503" title="DOI Link to resource 10.1103/PhysRevLett.118.070503">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BC1czisFChtw%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit323">F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss and V. Dubourg, 
            <span class="italic">et al.</span>
          , <span class="italic">J. Mach. Learn. Res.</span>, 2011, <span class="bold">12</span>, 2825–2830 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Mach.%20Learn.%20Res.%5BJour%5D%20AND%2012%5Bvolume%5D%20AND%202825%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit324">V. Havlček, A. D. Córcoles, K. Temme, A. W. Harrow, A. Kandala, J. M. Chow and J. M. Gambetta, <span class="italic">Nature</span>, 2019, <span class="bold">567</span>, 209–212 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-019-0980-2" title="DOI Link to resource 10.1038/s41586-019-0980-2">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30867609%5Buid%5D" title="PubMed Link to resource 30867609">PubMed</a>.</span></li>
      <li><span id="cit325">T. Kusumoto, K. Mitarai, K. Fujii, M. Kitagawa and M. Negoro, <span class="italic">npj Quantum Inform.</span>, 2021, <span class="bold">7</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-020-00339-1" title="DOI Link to resource 10.1038/s41534-020-00339-1">CrossRef</a>.</span></li>
      <li><span id="cit326">K. Bartkiewicz, C. Gneiting, A. Černoch, K. Jiráková, K. Lemr and F. Nori, <span class="italic">Sci. Rep.</span>, 2020, <span class="bold">10</span>, 1–9 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41598-019-56847-4" title="DOI Link to resource 10.1038/s41598-019-56847-4">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31913322%5Buid%5D" title="PubMed Link to resource 31913322">PubMed</a>.</span></li>
      <li><span id="cit327">
          M. Guo and Y. Weng, Where can quantum kernel methods make a big difference?, 2022, https://openreview.net/forum?id=NoE4RfaOOa.</span></li>
      <li><span id="cit328">M. Matsumoto and T. Nishimura, <span class="italic">ACM Trans. Model. Comput. Simul.</span>, 1998, <span class="bold">8</span>, 3–30 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/272991.272995" title="DOI Link to resource 10.1145/272991.272995">CrossRef</a>.</span></li>
      <li><span id="cit329">X. Wang, Y. Du, Y. Luo and D. Tao, <span class="italic">Quantum</span>, 2021, <span class="bold">5</span>, 531 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2021-08-30-531" title="DOI Link to resource 10.22331/q-2021-08-30-531">CrossRef</a>.</span></li>
      <li><span id="cit330">
          S. S. Vedaie, M. Noori, J. S. Oberoi, B. C. Sanders and E. Zahedinejad, 2020, arXiv preprint arXiv:2011.09694.</span></li>
      <li><span id="cit331">C. Blank, D. K. Park, J.-K. K. Rhee and F. Petruccione, <span class="italic">npj Quantum Inform.</span>, 2020, <span class="bold">6</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-019-0235-y" title="DOI Link to resource 10.1038/s41534-019-0235-y">CrossRef</a>.</span></li>
      <li><span id="cit332">
          S. Lloyd, M. Schuld, A. Ijaz, J. Izaac and N. Killoran, 2020, arXiv preprint arXiv:2001.03622.</span></li>
      <li><span id="cit333">E. Peters, J. Caldeira, A. Ho, S. Leichenauer, M. Mohseni, H. Neven, P. Spentzouris, D. Strain and G. N. Perdue, <span class="italic">npj Quantum Inform.</span>, 2021, <span class="bold">7</span>, 1–5 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-020-00339-1" title="DOI Link to resource 10.1038/s41534-020-00339-1">CrossRef</a>.</span></li>
      <li><span id="cit334">X. Yang, M. Kahnt, D. Brückner, A. Schropp, Y. Fam, J. Becher, J.-D. Grunwaldt, T. L. Sheppard and C. G. Schroer, <span class="italic">J. Synchrotron Radiat.</span>, 2020, <span class="bold">27</span>, 486–493 <a target="_blank" class="DOILink" href="https://doi.org/10.1107/S1600577520000831" title="DOI Link to resource 10.1107/S1600577520000831">CrossRef</a>.</span></li>
      <li><span id="cit335">Z. Liu, T. Bicer, R. Kettimuthu, D. Gursoy, F. De Carlo and I. Foster, <span class="italic">JOSA A</span>, 2020, <span class="bold">37</span>, 422–434 <a target="_blank" class="DOILink" href="https://doi.org/10.1364/JOSAA.375595" title="DOI Link to resource 10.1364/JOSAA.375595">CrossRef</a>.</span></li>
      <li><span id="cit336">J. Carrasquilla and R. G. Melko, <span class="italic">Nat. Phys.</span>, 2017, <span class="bold">13</span>, 431–434 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2013%5Bvolume%5D%20AND%20431%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit337">J. Gao, L.-F. Qiao, Z.-Q. Jiao, Y.-C. Ma, C.-Q. Hu, R.-J. Ren, A.-L. Yang, H. Tang, M.-H. Yung and X.-M. Jin, <span class="italic">Phys. Rev. Lett.</span>, 2018, <span class="bold">120</span>, 240501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.120.240501" title="DOI Link to resource 10.1103/PhysRevLett.120.240501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXlslOnsro%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit338">S. Lohani, B. T. Kirby, M. Brodsky, O. Danaci and R. T. Glasser, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2020, <span class="bold">1</span>, 035007 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%20035007%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit339">M. Ragoza, J. Hochuli, E. Idrobo, J. Sunseri and D. R. Koes, <span class="italic">J. Chem. Inf. Model.</span>, 2017, <span class="bold">57</span>, 942–957 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jcim.6b00740" title="DOI Link to resource 10.1021/acs.jcim.6b00740">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXlsVems7Y%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28368587%5Buid%5D" title="PubMed Link to resource 28368587">PubMed</a>.</span></li>
      <li><span id="cit340">K. Yao, J. E. Herr, D. W. Toth, R. Mckintyre and J. Parkhill, <span class="italic">Chem. Sci.</span>, 2018, <span class="bold">9</span>, 2261–2269 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C7SC04934J&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C7SC04934J">RSC</a>.</span></li>
      <li><span id="cit341">T. B. Hughes, G. P. Miller and S. J. Swamidass, <span class="italic">ACS Cent. Sci.</span>, 2015, <span class="bold">1</span>, 168–180 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acscentsci.5b00131" title="DOI Link to resource 10.1021/acscentsci.5b00131">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhtVars7jL" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit342">C. Caetano, J. Reis Jr, J. Amorim, M. R. Lemes and A. D. Pino Jr, <span class="italic">Int. J. Quantum Chem.</span>, 2011, <span class="bold">111</span>, 2732–2740 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/qua.22572" title="DOI Link to resource 10.1002/qua.22572">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3MXmsVamtLc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit343">K. T. Schütt, M. Gastegger, A. Tkatchenko, K.-R. Müller and R. J. Maurer, <span class="italic">Nat. Commun.</span>, 2019, <span class="bold">10</span>, 1–10 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-018-07882-8" title="DOI Link to resource 10.1038/s41467-018-07882-8">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30602773%5Buid%5D" title="PubMed Link to resource 30602773">PubMed</a>.</span></li>
      <li><span id="cit344">R. Galvelis and Y. Sugita, <span class="italic">J. Chem. Theory Comput.</span>, 2017, <span class="bold">13</span>, 2489–2500 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.7b00188" title="DOI Link to resource 10.1021/acs.jctc.7b00188">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXmsFCks7s%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28437616%5Buid%5D" title="PubMed Link to resource 28437616">PubMed</a>.</span></li>
      <li><span id="cit345">J. Zeng, L. Cao, M. Xu, T. Zhu and J. Z. Zhang, <span class="italic">Nat. Commun.</span>, 2020, <span class="bold">11</span>, 1–9 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-019-13993-7" title="DOI Link to resource 10.1038/s41467-019-13993-7">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31911652%5Buid%5D" title="PubMed Link to resource 31911652">PubMed</a>.</span></li>
      <li><span id="cit346">G. Carleo and M. Troyer, <span class="italic">Science</span>, 2017, <span class="bold">355</span>, 602–606 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/science.aag2302" title="DOI Link to resource 10.1126/science.aag2302">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXit1Okur0%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28183973%5Buid%5D" title="PubMed Link to resource 28183973">PubMed</a>.</span></li>
      <li><span id="cit347">M. Sajjan, S. H. Sureshbabu and S. Kais, <span class="italic">J. Am. Chem. Soc.</span>, 2021, <span class="bold">143</span>(44), 18426–18445 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jacs.1c06246" title="DOI Link to resource 10.1021/jacs.1c06246">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXitlajurrP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34705449%5Buid%5D" title="PubMed Link to resource 34705449">PubMed</a>.</span></li>
      <li><span id="cit348">A. Abbas, D. Sutter, C. Zoufal, A. Lucchi, A. Figalli and S. Woerner, <span class="italic">Nat. Comput. Sci.</span>, 2021, <span class="bold">1</span>, 403–409 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s43588-021-00084-1" title="DOI Link to resource 10.1038/s43588-021-00084-1">CrossRef</a>.</span></li>
      <li><span id="cit349">F. Kunstner, P. Hennig and L. Balles, <span class="italic">Adv. Neural Inform. Process. Syst.</span>, 2019, <span class="bold">32</span>, 1 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Adv.%20Neural%20Inform.%20Process.%20Syst.%5BJour%5D%20AND%2032%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit350">
          R. Karakida, S. Akaho and S.-I. Amari, The 22nd International Conference on Artificial Intelligence and Statistics, 2019, pp. 1032–1041.</span></li>
      <li><span id="cit351">
          O. Berezniuk, A. Figalli, R. Ghigliazza and K. Musaelian, 2020, arXiv preprint arXiv:2001.10872.</span></li>
      <li><span id="cit352">K. Sharma, M. Cerezo, Z. Holmes, L. Cincio, A. Sornborger and P. J. Coles, <span class="italic">Phys. Rev. Lett.</span>, 2022, <span class="bold">128</span>, 070501 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.128.070501" title="DOI Link to resource 10.1103/PhysRevLett.128.070501">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38Xmslemsbg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=35244415%5Buid%5D" title="PubMed Link to resource 35244415">PubMed</a>.</span></li>
      <li><span id="cit353">
          S. P. Adam, S.-A. N. Alexandropoulos, P. M. Pardalos and M. N. Vrahatis, in <span class="italic">No Free Lunch Theorem: A Review</span>, ed. I. C. Demetriou and P. M. Pardalos, Springer International Publishing, Cham,  2019, pp. 57–82 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=No%20Free%20Lunch%20Theorem:%20A%20Review%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit354">
          M. M. Wolf, Mathematical Foundations of Supervised Learning, 2018, https://www-m5.ma.tum.de/Allgemeines/MA4801_2016S.</span></li>
      <li><span id="cit355">
          K. Poland, K. Beer and T. J. Osborne,  2020, arXiv preprint arXiv:2003.14103.</span></li>
      <li><span id="cit356">C. H. Bennett and S. J. Wiesner, <span class="italic">Phys. Rev. Lett.</span>, 1992, <span class="bold">69</span>, 2881–2884 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.69.2881" title="DOI Link to resource 10.1103/PhysRevLett.69.2881">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=10046665%5Buid%5D" title="PubMed Link to resource 10046665">PubMed</a>.</span></li>
      <li><span id="cit357">A. Harrow, P. Hayden and D. Leung, <span class="italic">Phys. Rev. Lett.</span>, 2004, <span class="bold">92</span>, 187901 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.92.187901" title="DOI Link to resource 10.1103/PhysRevLett.92.187901">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15169533%5Buid%5D" title="PubMed Link to resource 15169533">PubMed</a>.</span></li>
      <li><span id="cit358">C. H. Bennett, G. Brassard, C. Crépeau, R. Jozsa, A. Peres and W. K. Wootters, <span class="italic">Phys. Rev. Lett.</span>, 1993, <span class="bold">70</span>, 1895–1899 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.70.1895" title="DOI Link to resource 10.1103/PhysRevLett.70.1895">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=10053414%5Buid%5D" title="PubMed Link to resource 10053414">PubMed</a>.</span></li>
      <li><span id="cit359">Y.-H. Luo, H.-S. Zhong, M. Erhard, X.-L. Wang, L.-C. Peng, M. Krenn, X. Jiang, L. Li, N.-L. Liu, C.-Y. Lu, A. Zeilinger and J.-W. Pan, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">123</span>, 070505 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.123.070505" title="DOI Link to resource 10.1103/PhysRevLett.123.070505">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvF2js7rF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31491117%5Buid%5D" title="PubMed Link to resource 31491117">PubMed</a>.</span></li>
      <li><span id="cit360">A. Pesah, M. Cerezo, S. Wang, T. Volkoff, A. T. Sornborger and P. J. Coles, <span class="italic">Phys. Rev. X</span>, 2021, <span class="bold">11</span>, 041011 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38XhtV2ru78%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit361">I. MacCormack, C. Delaney, A. Galda, N. Aggarwal and P. Narang, <span class="italic">Phys. Rev. Res.</span>, 2022, <span class="bold">4</span>, 013117 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.4.013117" title="DOI Link to resource 10.1103/PhysRevResearch.4.013117">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38XpsFOjt70%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit362">H.-Y. Lee, N. Kawashima and Y. B. Kim, <span class="italic">Phys. Rev. Res.</span>, 2020, <span class="bold">2</span>, 033318 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.2.033318" title="DOI Link to resource 10.1103/PhysRevResearch.2.033318">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitVyjsLjI" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit363">S. C. Kuhn and M. Richter, <span class="italic">Phys. Rev. B</span>, 2020, <span class="bold">101</span>, 075302 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.101.075302" title="DOI Link to resource 10.1103/PhysRevB.101.075302">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXms1ajsLk%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit364">K. Gunst, F. Verstraete, S. Wouters, O. Legeza and D. Van Neck, <span class="italic">J. Chem. Theory Comput.</span>, 2018, <span class="bold">14</span>, 2026–2033 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.8b00098" title="DOI Link to resource 10.1021/acs.jctc.8b00098">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXjtlOltLc%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29481743%5Buid%5D" title="PubMed Link to resource 29481743">PubMed</a>.</span></li>
      <li><span id="cit365">
          Y. LeCun, C. Cortes and C. Burges, MNIST handwritten digit database,  2010.</span></li>
      <li><span id="cit366">
          J. C. Spall, Proceedings of the 31st conference on Winter simulation: Simulation-a bridge to the future-Volume 1, 1999, pp. 101–109.</span></li>
      <li><span id="cit367">I. V. Oseledets and E. E. Tyrtyshnikov, <span class="italic">SIAM J. Sci. Comput.</span>, 2009, <span class="bold">31</span>, 3744–3759 <a target="_blank" class="DOILink" href="https://doi.org/10.1137/090748330" title="DOI Link to resource 10.1137/090748330">CrossRef</a>.</span></li>
      <li><span id="cit368">
          S. Kais, <span class="italic">Quantum Information and Computation for Chemistry</span>, Wiley and Sons: Hoboken, NJ,  2014, vol. 154 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Quantum%20Information%20and%20Computation%20for%20Chemistry%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit369">
          J. Altepeter, E. Jeffrey and P. Kwiat, <span class="italic">Photonic State Tomography</span>, Academic Press,  2005, vol. 52, pp. 105–159 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Photonic%20State%20Tomography%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202005%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit370">D. F. V. James, P. G. Kwiat, W. J. Munro and A. G. White, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2001, <span class="bold">64</span>, 052312 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.64.052312" title="DOI Link to resource 10.1103/PhysRevA.64.052312">CrossRef</a>.</span></li>
      <li><span id="cit371">K. Banaszek, M. Cramer and D. Gross, <span class="italic">New J. Phys.</span>, 2013, <span class="bold">15</span>, 125020 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/15/12/125020" title="DOI Link to resource 10.1088/1367-2630/15/12/125020">CrossRef</a>.</span></li>
      <li><span id="cit372">C. Song, K. Xu, W. Liu, C.-P. Yang, S.-B. Zheng, H. Deng, Q. Xie, K. Huang, Q. Guo, L. Zhang, P. Zhang, D. Xu, D. Zheng, X. Zhu, H. Wang, Y.-A. Chen, C.-Y. Lu, S. Han and J.-W. Pan, <span class="italic">Phys. Rev. Lett.</span>, 2017, <span class="bold">119</span>, 180511 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.119.180511" title="DOI Link to resource 10.1103/PhysRevLett.119.180511">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29219550%5Buid%5D" title="PubMed Link to resource 29219550">PubMed</a>.</span></li>
      <li><span id="cit373">G. Torlai, G. Mazzola, J. Carrasquilla, M. Troyer, R. Melko and G. Carleo, <span class="italic">Nat. Phys.</span>, 2018, <span class="bold">14</span>, 447–450 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2014%5Bvolume%5D%20AND%20447%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit374">J. Carrasquilla, G. Torlai, R. G. Melko and L. Aolita, <span class="italic">Nat. Mach. Intell.</span>, 2019, <span class="bold">1</span>, 155–161 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Mach.%20Intell.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%20155%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit375">A. M. Palmieri, E. Kovlakov, F. Bianchi, D. Yudin, S. Straupe, J. D. Biamonte and S. Kulik, <span class="italic">npj Quantum Inform.</span>, 2020, <span class="bold">6</span>, 20 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=npj%20Quantum%20Inform.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%2020%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit376">T. Xin, S. Lu, N. Cao, G. Anikeeva, D. Lu, J. Li, G. Long and B. Zeng, <span class="italic">npj Quantum Inform.</span>, 2019, <span class="bold">5</span>, 1–8 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=npj%20Quantum%20Inform.%5BJour%5D%20AND%205%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit377">J. Cotler and F. Wilczek, <span class="italic">Phys. Rev. Lett.</span>, 2020, <span class="bold">124</span>, 100401 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.124.100401" title="DOI Link to resource 10.1103/PhysRevLett.124.100401">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXmslyjtb4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32216420%5Buid%5D" title="PubMed Link to resource 32216420">PubMed</a>.</span></li>
      <li><span id="cit378">
          S. Aaronson, <span class="italic">Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing</span>, New York, NY, USA,  2018, pp. 325–338 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proceedings%20of%20the%2050th%20Annual%20ACM%20SIGACT%20Symposium%20on%20Theory%20of%20Computing%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit379">H.-Y. Huang, R. Kueng and J. Preskill, <span class="italic">Nat. Phys.</span>, 2020, <span class="bold">16</span>, 1050–1057 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2016%5Bvolume%5D%20AND%201050%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit380">E. T. Jaynes, <span class="italic">Phys. Rev.</span>, 1957, <span class="bold">108</span>, 171–190 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRev.108.171" title="DOI Link to resource 10.1103/PhysRev.108.171">CrossRef</a>.</span></li>
      <li><span id="cit381">E. H. Wichmann, <span class="italic">J. Math. Phys.</span>, 1963, <span class="bold">4</span>, 884–896 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.1704014" title="DOI Link to resource 10.1063/1.1704014">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaF2sXktlKnsr4%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit382">
          A. Katz, <span class="italic">Principles of Statistical Mechanics: The Information Theory Approach</span>, W. H. Freeman,  1967 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Principles%20of%20Statistical%20Mechanics:%20The%20Information%20Theory%20Approach%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201967%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit383">Z. Hradil, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1997, <span class="bold">55</span>, R1561–R1564 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.55.R1561" title="DOI Link to resource 10.1103/PhysRevA.55.R1561">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK2sXhvVWhtbc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit384">Y. S. Teo, H. Zhu, B.-G. Englert, J. Řeháček and Z. Hradil, <span class="italic">Phys. Rev. Lett.</span>, 2011, <span class="bold">107</span>, 020404 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.107.020404" title="DOI Link to resource 10.1103/PhysRevLett.107.020404">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=21797584%5Buid%5D" title="PubMed Link to resource 21797584">PubMed</a>.</span></li>
      <li><span id="cit385">Y. S. Teo, B. Stoklasa, B.-G. Englert, J. Řeháček and Z. Hradil, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2012, <span class="bold">85</span>, 042317 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.85.042317" title="DOI Link to resource 10.1103/PhysRevA.85.042317">CrossRef</a>.</span></li>
      <li><span id="cit386">R. Blume-Kohout, <span class="italic">Phys. Rev. Lett.</span>, 2010, <span class="bold">105</span>, 200504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.105.200504" title="DOI Link to resource 10.1103/PhysRevLett.105.200504">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=21231215%5Buid%5D" title="PubMed Link to resource 21231215">PubMed</a>.</span></li>
      <li><span id="cit387">J. A. Smolin, J. M. Gambetta and G. Smith, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">108</span>, 070502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.108.070502" title="DOI Link to resource 10.1103/PhysRevLett.108.070502">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22401185%5Buid%5D" title="PubMed Link to resource 22401185">PubMed</a>.</span></li>
      <li><span id="cit388">T. Baumgratz, A. Nüßeler, M. Cramer and M. B. Plenio, <span class="italic">New J. Phys.</span>, 2013, <span class="bold">15</span>, 125004 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/15/12/125004" title="DOI Link to resource 10.1088/1367-2630/15/12/125004">CrossRef</a>.</span></li>
      <li><span id="cit389">R. Blume-Kohout, <span class="italic">New J. Phys.</span>, 2010, <span class="bold">12</span>, 043034 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/12/4/043034" title="DOI Link to resource 10.1088/1367-2630/12/4/043034">CrossRef</a>.</span></li>
      <li><span id="cit390">F. Huszár and N. M. Houlsby, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2012, <span class="bold">85</span>, 052120 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.85.052120" title="DOI Link to resource 10.1103/PhysRevA.85.052120">CrossRef</a>.</span></li>
      <li><span id="cit391">J. M. Lukens, K. J. Law, A. Jasra and P. Lougovski, <span class="italic">New J. Phys.</span>, 2020, <span class="bold">22</span>, 063038 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/ab8efa" title="DOI Link to resource 10.1088/1367-2630/ab8efa">CrossRef</a>.</span></li>
      <li><span id="cit392">
          J. M. Lukens, K. J. Law and R. S. Bennink, 2020, arXiv preprint arXiv:2012.08997.</span></li>
      <li><span id="cit393">R. Gupta, R. Xia, R. D. Levine and S. Kais, <span class="italic">PRX Quantum</span>, 2021, <span class="bold">2</span>, 010318 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PRXQuantum.2.010318" title="DOI Link to resource 10.1103/PRXQuantum.2.010318">CrossRef</a>.</span></li>
      <li><span id="cit394">R. Gupta, R. D. Levine and S. Kais, <span class="italic">J. Phys. Chem. A</span>, 2021, 7588–7594 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpca.1c05884" title="DOI Link to resource 10.1021/acs.jpca.1c05884">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhvVWmsbnP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34410718%5Buid%5D" title="PubMed Link to resource 34410718">PubMed</a>.</span></li>
      <li><span id="cit395">
          C. Helstrom, <span class="italic">Mathematics in Science and Engineering</span>, Academic Press, New York,  1976, vol. 123, pp. 1572–9613 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mathematics%20in%20Science%20and%20Engineering%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201976%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit396">S. Gudder, 
            <span class="italic">et al.</span>
          , <span class="italic">Bull. (New Series) Am. Math. Soc.</span>, 1985, <span class="bold">13</span>, 80–85 <a target="_blank" class="DOILink" href="https://doi.org/10.1090/S0273-0979-1985-15378-9" title="DOI Link to resource 10.1090/S0273-0979-1985-15378-9">CrossRef</a>.</span></li>
      <li><span id="cit397">
          A. Peres, <span class="italic"><span class="italic">Quantum Theory: Concepts and Methods</span></span>, Kluwer Academic Publishers, Boston,  1993 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Quantum%20Theory:%20Concepts%20and%20Methods%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201993%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit398">M. Sasaki and A. Carlini, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2002, <span class="bold">66</span>, 022303 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.66.022303" title="DOI Link to resource 10.1103/PhysRevA.66.022303">CrossRef</a>.</span></li>
      <li><span id="cit399">
          R. Chrisley, <span class="italic">New directions in cognitive science: Proceedings of the international symposium</span>, Saariselka,  1995 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=New%20directions%20in%20cognitive%20science:%20Proceedings%20of%20the%20international%20symposium%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201995%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit400">
          E. C. Behrman, J. Niemel, J. E. Steck and S. R. Skinner, Proceedings of the 4th Workshop on Physics of Computation, 1996, pp. 22–24.</span></li>
      <li><span id="cit401">
          E. C. Behrman, J. E. Steck and S. R. Skinner, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No. 99CH36339), 1999, pp. 874–877.</span></li>
      <li><span id="cit402">M. Benedetti, D. Garcia-Pintos, O. Perdomo, V. Leyton-Ortega, Y. Nam and A. Perdomo-Ortiz, <span class="italic">npj Quantum Inform.</span>, 2019, <span class="bold">5</span>, 1–9 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-018-0113-z" title="DOI Link to resource 10.1038/s41534-018-0113-z">CrossRef</a>.</span></li>
      <li><span id="cit403">S. Cheng, J. Chen and L. Wang, <span class="italic">Entropy</span>, 2018, <span class="bold">20</span>, 583 <a target="_blank" class="DOILink" href="https://doi.org/10.3390/e20080583" title="DOI Link to resource 10.3390/e20080583">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33265672%5Buid%5D" title="PubMed Link to resource 33265672">PubMed</a>.</span></li>
      <li><span id="cit404">
          E. M. Stoudenmire and D. J. Schwab, 2016, arXiv preprint arXiv:1605.05775.</span></li>
      <li><span id="cit405">Z.-Y. Han, J. Wang, H. Fan, L. Wang and P. Zhang, <span class="italic">Phys. Rev. X</span>, 2018, <span class="bold">8</span>, 031012 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSmtLs%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit406">X. Gao, Z.-Y. Zhang and L.-M. Duan, <span class="italic">Sci. Adv.</span>, 2018, <span class="bold">4</span>, eaat9004 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/sciadv.aat9004" title="DOI Link to resource 10.1126/sciadv.aat9004">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BB3cngvFKnsg%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30539141%5Buid%5D" title="PubMed Link to resource 30539141">PubMed</a>.</span></li>
      <li><span id="cit407">J. M. Arrazola, T. R. Bromley, J. Izaac, C. R. Myers, K. Brádler and N. Killoran, <span class="italic">Quantum Sci. Technol.</span>, 2019, <span class="bold">4</span>, 024004 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/aaf59e" title="DOI Link to resource 10.1088/2058-9565/aaf59e">CrossRef</a>.</span></li>
      <li><span id="cit408">N. Killoran, T. R. Bromley, J. M. Arrazola, M. Schuld, N. Quesada and S. Lloyd, <span class="italic">Phys. Rev. Res.</span>, 2019, <span class="bold">1</span>, 033063 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.1.033063" title="DOI Link to resource 10.1103/PhysRevResearch.1.033063">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhvVOhtbjL" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit409">N. Killoran, J. Izaac, N. Quesada, V. Bergholm, M. Amy and C. Weedbrook, <span class="italic">Quantum</span>, 2019, <span class="bold">3</span>, 129 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2019-03-11-129" title="DOI Link to resource 10.22331/q-2019-03-11-129">CrossRef</a>.</span></li>
      <li><span id="cit410">X.-M. Zhang, Z. Wei, R. Asad, X.-C. Yang and X. Wang, <span class="italic">npj Quantum Inform.</span>, 2019, <span class="bold">5</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-018-0113-z" title="DOI Link to resource 10.1038/s41534-018-0113-z">CrossRef</a>.</span></li>
      <li><span id="cit411">M. Benedetti, E. Lloyd, S. Sack and M. Fiorentini, <span class="italic">Quantum Sci. Technol.</span>, 2019, <span class="bold">4</span>, 043001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/ab4eb5" title="DOI Link to resource 10.1088/2058-9565/ab4eb5">CrossRef</a>.</span></li>
      <li><span id="cit412">J. R. McClean, J. Romero, R. Babbush and A. Aspuru-Guzik, <span class="italic">New J. Phys.</span>, 2016, <span class="bold">18</span>, 023023 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/1367-2630/18/2/023023" title="DOI Link to resource 10.1088/1367-2630/18/2/023023">CrossRef</a>.</span></li>
      <li><span id="cit413">
          H.-Y. Huang, K. Bharti and P. Rebentrost, 2019, arXiv preprint arXiv:1909.07344.</span></li>
      <li><span id="cit414">R. LaRose, A. Tikku, É. O’Neel-Judy, L. Cincio and P. J. Coles, <span class="italic">npj Quantum Inform.</span>, 2019, <span class="bold">5</span>, 1–10 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-018-0113-z" title="DOI Link to resource 10.1038/s41534-018-0113-z">CrossRef</a>.</span></li>
      <li><span id="cit415">
          M. Cerezo, K. Sharma, A. Arrasmith and P. J. Coles, 2020, arXiv preprint arXiv:2004.01372.</span></li>
      <li><span id="cit416">X. Wang, Z. Song and Y. Wang, <span class="italic">Quantum</span>, 2021, <span class="bold">5</span>, 483 <a target="_blank" class="DOILink" href="https://doi.org/10.22331/q-2021-06-29-483" title="DOI Link to resource 10.22331/q-2021-06-29-483">CrossRef</a>.</span></li>
      <li><span id="cit417">
          Y. Wang, G. Li and X. Wang, 2021, arXiv preprint arXiv:2103.01061.</span></li>
      <li><span id="cit418">
          G. Li, Z. Song and X. Wang, Proceedings of the AAAI Conference on Artificial Intelligence, 2021, pp. 8357–8365.</span></li>
      <li><span id="cit419">R. Chen, Z. Song, X. Zhao and X. Wang, <span class="italic">Quantum Sci. Technol.</span>, 2021, <span class="bold">7</span>, 015019 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/ac38ba" title="DOI Link to resource 10.1088/2058-9565/ac38ba">CrossRef</a>.</span></li>
      <li><span id="cit420">Y. Wang, G. Li and X. Wang, <span class="italic">Phys. Rev. Appl.</span>, 2021, <span class="bold">16</span>, 054035 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevApplied.16.054035" title="DOI Link to resource 10.1103/PhysRevApplied.16.054035">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXislansrfI" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit421">D. Aharonov, I. Arad and T. Vidick, <span class="italic">ACM Sigact News</span>, 2013, <span class="bold">44</span>, 47–79 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/2491533.2491549" title="DOI Link to resource 10.1145/2491533.2491549">CrossRef</a>.</span></li>
      <li><span id="cit422">A. M. Childs, D. Maslov, Y. Nam, N. J. Ross and Y. Su, <span class="italic">Proc. Natl. Acad. Sci. U. S. A.</span>, 2018, <span class="bold">115</span>, 9456–9461 <a target="_blank" class="DOILink" href="https://doi.org/10.1073/pnas.1801723115" title="DOI Link to resource 10.1073/pnas.1801723115">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXitVKhu7rP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30190433%5Buid%5D" title="PubMed Link to resource 30190433">PubMed</a>.</span></li>
      <li><span id="cit423">R. D. Somma, S. Boixo, H. Barnum and E. Knill, <span class="italic">Phys. Rev. Lett.</span>, 2008, <span class="bold">101</span>, 130504 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.101.130504" title="DOI Link to resource 10.1103/PhysRevLett.101.130504">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1cnlslajsA%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18851429%5Buid%5D" title="PubMed Link to resource 18851429">PubMed</a>.</span></li>
      <li><span id="cit424">
          A. Gheorghiu and M. J. Hoban, 2020, arXiv preprint arXiv:2002.12814.</span></li>
      <li><span id="cit425">H. Buhrman, R. Cleve, J. Watrous and R. De Wolf, <span class="italic">Phys. Rev. Lett.</span>, 2001, <span class="bold">87</span>, 167902 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.87.167902" title="DOI Link to resource 10.1103/PhysRevLett.87.167902">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD3MnjslGksA%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=11690244%5Buid%5D" title="PubMed Link to resource 11690244">PubMed</a>.</span></li>
      <li><span id="cit426">
          D. Gottesman and I. Chuang, 2001, arXiv preprint quant-ph/0105032.</span></li>
      <li><span id="cit427">
          H. Neven, V. S. Denchev, G. Rose and W. G. Macready, 2008, arXiv preprint arXiv:0811.0416.</span></li>
      <li><span id="cit428">K. L. Pudenz and D. A. Lidar, <span class="italic">Quantum Inf. Process.</span>, 2013, <span class="bold">12</span>, 2027–2070 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s11128-012-0506-4" title="DOI Link to resource 10.1007/s11128-012-0506-4">CrossRef</a>.</span></li>
      <li><span id="cit429">M. W. Johnson, M. H. Amin, S. Gildert, T. Lanting, F. Hamze, N. Dickson, R. Harris, A. J. Berkley, J. Johansson and P. Bunyk, 
            <span class="italic">et al.</span>
          , <span class="italic">Nature</span>, 2011, <span class="bold">473</span>, 194–198 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nature10012" title="DOI Link to resource 10.1038/nature10012">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3MXlvFGmurw%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=21562559%5Buid%5D" title="PubMed Link to resource 21562559">PubMed</a>.</span></li>
      <li><span id="cit430">
          S. H. Adachi and M. P. Henderson, 2015, arXiv preprint arXiv:1510.06356.</span></li>
      <li><span id="cit431">M. Benedetti, J. Realpe-Gómez, R. Biswas and A. Perdomo-Ortiz, <span class="italic">Phys. Rev. X</span>, 2017, <span class="bold">7</span>, 041052 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Phys.%20Rev.%20X%5BJour%5D%20AND%207%5Bvolume%5D%20AND%20041052%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit432">
          N. Wiebe, A. Kapoor, C. Granade and K. M. Svore, 2015, arXiv preprint arXiv:1507.02642.</span></li>
      <li><span id="cit433">M. H. Amin, E. Andriyash, J. Rolfe, B. Kulchytskyy and R. Melko, <span class="italic">Phys. Rev. X</span>, 2018, <span class="bold">8</span>, 021050 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSgurc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit434">M. Kieferová and N. Wiebe, <span class="italic">Phys. Rev. A</span>, 2017, <span class="bold">96</span>, 062327 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.96.062327" title="DOI Link to resource 10.1103/PhysRevA.96.062327">CrossRef</a>.</span></li>
      <li><span id="cit435">
          Q. Xu and S. Xu, 2018, arXiv preprint arXiv:1811.06654.</span></li>
      <li><span id="cit436">C. H. Baldwin, I. H. Deutsch and A. Kalev, <span class="italic">Phys. Rev. A</span>, 2016, <span class="bold">93</span>, 052105 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.93.052105" title="DOI Link to resource 10.1103/PhysRevA.93.052105">CrossRef</a>.</span></li>
      <li><span id="cit437">N. Linden, S. Popescu and W. Wootters, <span class="italic">Phys. Rev. Lett.</span>, 2002, <span class="bold">89</span>, 207901 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.89.207901" title="DOI Link to resource 10.1103/PhysRevLett.89.207901">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD38notleltg%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=12443507%5Buid%5D" title="PubMed Link to resource 12443507">PubMed</a>.</span></li>
      <li><span id="cit438">N. Linden and W. Wootters, <span class="italic">Phys. Rev. Lett.</span>, 2002, <span class="bold">89</span>, 277906 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.89.277906" title="DOI Link to resource 10.1103/PhysRevLett.89.277906">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD3s%252FgvVejsA%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=12513246%5Buid%5D" title="PubMed Link to resource 12513246">PubMed</a>.</span></li>
      <li><span id="cit439">J. Chen, Z. Ji, B. Zeng and D. Zhou, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2012, <span class="bold">86</span>, 022339 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.86.022339" title="DOI Link to resource 10.1103/PhysRevA.86.022339">CrossRef</a>.</span></li>
      <li><span id="cit440">J. Chen, H. Dawkins, Z. Ji, N. Johnston, D. Kribs, F. Shultz and B. Zeng, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2013, <span class="bold">88</span>, 012109 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.88.012109" title="DOI Link to resource 10.1103/PhysRevA.88.012109">CrossRef</a>.</span></li>
      <li><span id="cit441">B. Qi, Z. Hou, L. Li, D. Dong, G. Xiang and G. Guo, <span class="italic">Sci. Rep.</span>, 2013, <span class="bold">3</span>, 1–6 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Sci.%20Rep.%5BJour%5D%20AND%203%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202013%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit442">
          D. F. James, P. G. Kwiat, W. J. Munro and A. G. White, <span class="italic">Asymptotic Theory of Quantum Statistical Inference: Selected Papers</span>, World Scientific,  2005, pp. 509–538 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Asymptotic%20Theory%20of%20Quantum%20Statistical%20Inference:%20Selected%20Papers%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202005%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit443">T. Opatrny, D.-G. Welsch and W. Vogel, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1997, <span class="bold">56</span>, 1788 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.56.1788" title="DOI Link to resource 10.1103/PhysRevA.56.1788">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaK2sXlvVemsrc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit444">D. G. Fischer, S. H. Kienle and M. Freyberger, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2000, <span class="bold">61</span>, 032306 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.61.032306" title="DOI Link to resource 10.1103/PhysRevA.61.032306">CrossRef</a>.</span></li>
      <li><span id="cit445">G. I. Struchalin, I. A. Pogorelov, S. S. Straupe, K. S. Kravtsov, I. V. Radchenko and S. P. Kulik, <span class="italic">Phys. Rev. A</span>, 2016, <span class="bold">93</span>, 012103 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.93.012103" title="DOI Link to resource 10.1103/PhysRevA.93.012103">CrossRef</a>.</span></li>
      <li><span id="cit446">Y. Quek, S. Fort and H. K. Ng, <span class="italic">npj Quantum Inform.</span>, 2021, <span class="bold">7</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-020-00339-1" title="DOI Link to resource 10.1038/s41534-020-00339-1">CrossRef</a>.</span></li>
      <li><span id="cit447">L. Hu, S.-H. Wu, W. Cai, Y. Ma, X. Mu, Y. Xu, H. Wang, Y. Song, D.-L. Deng and C.-L. Zou, 
            <span class="italic">et al.</span>
          , <span class="italic">Sci. Adv.</span>, 2019, <span class="bold">5</span>, eaav2761 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/sciadv.aav2761" title="DOI Link to resource 10.1126/sciadv.aav2761">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30746476%5Buid%5D" title="PubMed Link to resource 30746476">PubMed</a>.</span></li>
      <li><span id="cit448">B. Qi, Z. Hou, Y. Wang, D. Dong, H.-S. Zhong, L. Li, G.-Y. Xiang, H. M. Wiseman, C.-F. Li and G.-C. Guo, <span class="italic">npj Quantum Inform.</span>, 2017, <span class="bold">3</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-016-0002-2" title="DOI Link to resource 10.1038/s41534-016-0002-2">CrossRef</a>.</span></li>
      <li><span id="cit449">S. Lloyd and C. Weedbrook, <span class="italic">Phys. Rev. Lett.</span>, 2018, <span class="bold">121</span>, 040502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.121.040502" title="DOI Link to resource 10.1103/PhysRevLett.121.040502">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSjs7g%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30095952%5Buid%5D" title="PubMed Link to resource 30095952">PubMed</a>.</span></li>
      <li><span id="cit450">P.-L. Dallaire-Demers and N. Killoran, <span class="italic">Phys. Rev. A</span>, 2018, <span class="bold">98</span>, 012324 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.98.012324" title="DOI Link to resource 10.1103/PhysRevA.98.012324">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXlsV2ht7Y%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit451">
          S. Ahmed, C. S. Muñoz, F. Nori and A. F. Kockum, 2020, arXiv preprint arXiv:2008.03240.</span></li>
      <li><span id="cit452">
          P. Isola, J.-Y. Zhu, T. Zhou and A. A. Efros, Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1125–1134.</span></li>
      <li><span id="cit453">
          T. Karras, S. Laine and T. Aila, 2018, arXiv preprint arXiv:1812.04948.</span></li>
      <li><span id="cit454">
          T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen and T. Aila, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 8110–8119.</span></li>
      <li><span id="cit455">
          Z. Yang, Z. Hu, C. Dyer, E. P. Xing and T. Berg-Kirkpatrick, Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018, pp. 7298–7309.</span></li>
      <li><span id="cit456">
          S. Subramanian, S. Rajeswar, A. Sordoni, A. Trischler, A. Courville and C. Pal, Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018, pp. 7562–7574.</span></li>
      <li><span id="cit457">
          J. Cheng, L. Dong and M. Lapata, 2016, arXiv preprint arXiv:1601.06733.</span></li>
      <li><span id="cit458">
          A. P. Parikh, O. Täckström, D. Das and J. Uszkoreit, 2016, arXiv preprint arXiv:1606.01933.</span></li>
      <li><span id="cit459">A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser and I. Polosukhin, <span class="italic">Adv. Neural Information Process. Syst.</span>, 2017, <span class="bold">30</span>, 1–11 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Adv.%20Neural%20Information%20Process.%20Syst.%5BJour%5D%20AND%2030%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit460">
          Z. Lin, M. Feng, C. N. D. Santos, M. Yu, B. Xiang, B. Zhou and Y. Bengio, 2017, arXiv preprint arXiv:1703.03130.</span></li>
      <li><span id="cit461">
          R. Paulus, C. Xiong and R. Socher, 2017, arXiv preprint arXiv:1705.04304.</span></li>
      <li><span id="cit462">
          D. Bahdanau, K. Cho and Y. Bengio, 2014, arXiv preprint arXiv:1409.0473.</span></li>
      <li><span id="cit463">I. Sutskever, O. Vinyals and Q. V. Le, <span class="italic">Adv. Neural Inform. Process. Syst.</span>, 2014, <span class="bold">27</span>, 1 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Adv.%20Neural%20Inform.%20Process.%20Syst.%5BJour%5D%20AND%2027%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit464">P. Cha, P. Ginsparg, F. Wu, J. Carrasquilla, P. L. McMahon and E.-A. Kim, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2021, <span class="bold">3</span>, 01LT01 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%203%5Bvolume%5D%20AND%2001LT01%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit465">J. Carrasquilla, G. Torlai, R. G. Melko and L. Aolita, <span class="italic">Nat. Mach. Intell.</span>, 2019, <span class="bold">1</span>, 155–161 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s42256-019-0028-1" title="DOI Link to resource 10.1038/s42256-019-0028-1">CrossRef</a>.</span></li>
      <li><span id="cit466">S. Aaronson, <span class="italic">Nat. Phys.</span>, 2015, <span class="bold">11</span>, 291–293 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2011%5Bvolume%5D%20AND%20291%5Bpage%5D%20and%202015%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit467">
          P. C. Sen, M. Hajra and M. Ghosh, <span class="italic">Emerging technology in modelling and graphics</span>, Springer,  2020, pp. 99–111 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Emerging%20technology%20in%20modelling%20and%20graphics%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit468">S. Zhang, C. Zhang and Q. Yang, <span class="italic">Appl. Artificial Intell.</span>, 2003, <span class="bold">17</span>, 375–381 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Appl.%20Artificial%20Intell.%5BJour%5D%20AND%2017%5Bvolume%5D%20AND%20375%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit469">A. Karim, A. Mishra, M. H. Newton and A. Sattar, <span class="italic">ACS Omega</span>, 2019, <span class="bold">4</span>, 1874–1888 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=ACS%20Omega%5BJour%5D%20AND%204%5Bvolume%5D%20AND%201874%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit470">
          Y. Chevaleyre and J.-D. Zucker, Conference of the Canadian Society for Computational Studies of Intelligence, 2001, pp. 204–214.</span></li>
      <li><span id="cit471">G. Skoraczyński, P. Dittwald, B. Miasojedow, S. Szymkuć, E. Gajewska, B. A. Grzybowski and A. Gambin, <span class="italic">Sci. Rep.</span>, 2017, <span class="bold">7</span>, 1–9 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Sci.%20Rep.%5BJour%5D%20AND%207%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit472">S. Heinen, G. F. von Rudorff and O. A. von Lilienfeld, <span class="italic">J. Chem. Phys.</span>, 2021, <span class="bold">155</span>, 064105 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0059742" title="DOI Link to resource 10.1063/5.0059742">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhslykurrO" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34391351%5Buid%5D" title="PubMed Link to resource 34391351">PubMed</a>.</span></li>
      <li><span id="cit473">E. A. Engel, A. Anelli, A. Hofstetter, F. Paruzzo, L. Emsley and M. Ceriotti, <span class="italic">Phys. Chem. Chem. Phys.</span>, 2019, <span class="bold">21</span>, 23385–23400 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C9CP04489B&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C9CP04489B">RSC</a>.</span></li>
      <li><span id="cit474">R. Jinnouchi, J. Lahnsteiner, F. Karsai, G. Kresse and M. Bokdam, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">122</span>, 225701 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.122.225701" title="DOI Link to resource 10.1103/PhysRevLett.122.225701">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhslWlsLnN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31283285%5Buid%5D" title="PubMed Link to resource 31283285">PubMed</a>.</span></li>
      <li><span id="cit475">R. Krems, <span class="italic">Phys. Chem. Chem. Phys.</span>, 2019, <span class="bold">21</span>, 13392–13410 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C9CP01883B&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C9CP01883B">RSC</a>.</span></li>
      <li><span id="cit476">M. Senekane and B. M. Taele, 
            <span class="italic">et al.</span>
          , <span class="italic">Smart Grid Renewable Energy</span>, 2016, <span class="bold">7</span>, 293 <a target="_blank" class="DOILink" href="https://doi.org/10.4236/sgre.2016.712022" title="DOI Link to resource 10.4236/sgre.2016.712022">CrossRef</a>.</span></li>
      <li><span id="cit477">
          J. Heredge, C. Hill, L. Hollenberg and M. Sevior, 2021, arXiv preprint arXiv:2103.12257.</span></li>
      <li><span id="cit478">F. Schindler, N. Regnault and T. Neupert, <span class="italic">Phys. Rev. B</span>, 2017, <span class="bold">95</span>, 245134 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.95.245134" title="DOI Link to resource 10.1103/PhysRevB.95.245134">CrossRef</a>.</span></li>
      <li><span id="cit479">G. Purushothaman and N. B. Karayiannis, <span class="italic">IEEE Trans. Neural Networks</span>, 1997, <span class="bold">8</span>, 679–693 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BD1c%252FpvVeitg%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit480">
          J. Zhou, <span class="italic">Third IEEE Symposium on Bioinformatics and Bioengineering, 2003. Proceedings</span>,  2003, pp. 169–173 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Third%20IEEE%20Symposium%20on%20Bioinformatics%20and%20Bioengineering,%202003.%20Proceedings%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit481">
          V. Dumoulin, I. Goodfellow, A. Courville and Y. Bengio, Proceedings of the AAAI Conference on Artificial Intelligence, 2014.</span></li>
      <li><span id="cit482">V. Gandhi, G. Prasad, D. Coyle, L. Behera and T. M. McGinnity, <span class="italic">IEEE Trans. Neural Networks Learn. Syst.</span>, 2013, <span class="bold">25</span>, 278–288 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Neural%20Networks%20Learn.%20Syst.%5BJour%5D%20AND%2025%5Bvolume%5D%20AND%20278%5Bpage%5D%20and%202013%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit483">Z. Gao, C. Ma, Y. Luo and Z. Liu, <span class="italic">Eng. Appl. Artificial Intell.</span>, 2018, <span class="bold">76</span>, 119–129 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.engappai.2018.08.013" title="DOI Link to resource 10.1016/j.engappai.2018.08.013">CrossRef</a>.</span></li>
      <li><span id="cit484">M. Zidan, A.-H. Abdel-Aty, M. El-shafei, M. Feraig, Y. Al-Sbou, H. Eleuch and M. Abdel-Aty, <span class="italic">Appl. Sci.</span>, 2019, <span class="bold">9</span>, 1277 <a target="_blank" class="DOILink" href="https://doi.org/10.3390/app9071277" title="DOI Link to resource 10.3390/app9071277">CrossRef</a>.</span></li>
      <li><span id="cit485">
          E. Farhi and H. Neven, 2018, arXiv preprint arXiv:1802.06002.</span></li>
      <li><span id="cit486">S. Adhikary, S. Dangwal and D. Bhowmik, <span class="italic">Quantum Inf. Process.</span>, 2020, <span class="bold">19</span>, 1–12 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s11128-019-2494-0" title="DOI Link to resource 10.1007/s11128-019-2494-0">CrossRef</a>.</span></li>
      <li><span id="cit487">M. Schuld, A. Bocharov, K. M. Svore and N. Wiebe, <span class="italic">Phys. Rev. A</span>, 2020, <span class="bold">101</span>, 032308 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.101.032308" title="DOI Link to resource 10.1103/PhysRevA.101.032308">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXptVyjsLo%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit488">D. Liu, G. Bai and C. Gao, <span class="italic">J. Appl. Phys.</span>, 2020, <span class="bold">127</span>, 154101 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0004167" title="DOI Link to resource 10.1063/5.0004167">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXnsVWjsrs%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit489">
          G. Deffrennes, K. Terayama, T. Abe and R. Tamura, 2022, arXiv preprint arXiv:2201.01932.</span></li>
      <li><span id="cit490">A. V. Uvarov, A. S. Kardashin and J. D. Biamonte, <span class="italic">Phys. Rev. A</span>, 2020, <span class="bold">102</span>, 012415 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.102.012415" title="DOI Link to resource 10.1103/PhysRevA.102.012415">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhs1emsb%252FO" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit491">R. J. Bartlett and M. Musiał, <span class="italic">Rev. Mod. Phys.</span>, 2007, <span class="bold">79</span>, 291–352 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.79.291" title="DOI Link to resource 10.1103/RevModPhys.79.291">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXmt1Cqtbw%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit492">
          P. Broecker, F. F. Assaad and S. Trebst, 2017, arXiv preprint arXiv:1707.00663.</span></li>
      <li><span id="cit493">Y. Che, C. Gneiting, T. Liu and F. Nori, <span class="italic">Phys. Rev. B</span>, 2020, <span class="bold">102</span>, 134213 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.102.134213" title="DOI Link to resource 10.1103/PhysRevB.102.134213">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitlegtLjE" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit494">S. Lloyd, <span class="italic">IEEE Trans. Inform. Theory</span>, 1982, <span class="bold">28</span>, 129–137 <a target="_blank" class="DOILink" href="https://doi.org/10.1109/TIT.1982.1056489" title="DOI Link to resource 10.1109/TIT.1982.1056489">CrossRef</a>.</span></li>
      <li><span id="cit495">
          D. J. MacKay and D. J. Mac Kay, <span class="italic">Information theory, inference and learning algorithms</span>, Cambridge University Press,  2003 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Information%20theory,%20inference%20and%20learning%20algorithms%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit496">
          J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick, M. Block, B. Bloom, S. Caldwell, N. Didier, E. S. Fried, S. Hong, <span class="italic">et al.</span>, 2017, arXiv preprint arXiv:1712.05771.</span></li>
      <li><span id="cit497">W. Kohn and L. J. Sham, <span class="italic">Phys. Rev.</span>, 1965, <span class="bold">140</span>, A1133 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRev.140.A1133" title="DOI Link to resource 10.1103/PhysRev.140.A1133">CrossRef</a>.</span></li>
      <li><span id="cit498">J. Schmidt, C. L. Benavides-Riveros and M. A. Marques, <span class="italic">J. Phys. Chem. Lett.</span>, 2019, <span class="bold">10</span>, 6425–6431 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.9b02422" title="DOI Link to resource 10.1021/acs.jpclett.9b02422">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvFWrt7jE" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31596092%5Buid%5D" title="PubMed Link to resource 31596092">PubMed</a>.</span></li>
      <li><span id="cit499">M. Bogojeski, L. Vogt-Maranto, M. E. Tuckerman, K.-R. Müller and K. Burke, <span class="italic">Nat. Commun.</span>, 2020, <span class="bold">11</span>, 1–11 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-019-13993-7" title="DOI Link to resource 10.1038/s41467-019-13993-7">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31911652%5Buid%5D" title="PubMed Link to resource 31911652">PubMed</a>.</span></li>
      <li><span id="cit500">R. Nagai, R. Akashi and O. Sugino, <span class="italic">npj Comput. Mater.</span>, 2020, <span class="bold">6</span>, 1–8 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41524-019-0267-z" title="DOI Link to resource 10.1038/s41524-019-0267-z">CrossRef</a>.</span></li>
      <li><span id="cit501">L. Li, T. E. Baker, S. R. White and K. Burke, 
            <span class="italic">et al.</span>
          , <span class="italic">Phys. Rev. B</span>, 2016, <span class="bold">94</span>, 245129 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.94.245129" title="DOI Link to resource 10.1103/PhysRevB.94.245129">CrossRef</a>.</span></li>
      <li><span id="cit502">P. Borlido, J. Schmidt, A. W. Huran, F. Tran, M. A. Marques and S. Botti, <span class="italic">npj Comput. Mater.</span>, 2020, <span class="bold">6</span>, 1–17 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41524-019-0267-z" title="DOI Link to resource 10.1038/s41524-019-0267-z">CrossRef</a>.</span></li>
      <li><span id="cit503">M. Fritz, M. Fernández-Serra and J. M. Soler, <span class="italic">J. Chem. Phys.</span>, 2016, <span class="bold">144</span>, 224101 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4953081" title="DOI Link to resource 10.1063/1.4953081">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27305990%5Buid%5D" title="PubMed Link to resource 27305990">PubMed</a>.</span></li>
      <li><span id="cit504">Q. Liu, J. Wang, P. Du, L. Hu, X. Zheng and G. Chen, <span class="italic">J. Phys. Chem. A</span>, 2017, <span class="bold">121</span>, 7273–7281 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpca.7b07045" title="DOI Link to resource 10.1021/acs.jpca.7b07045">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhsVelu7nN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28876064%5Buid%5D" title="PubMed Link to resource 28876064">PubMed</a>.</span></li>
      <li><span id="cit505">K. Ryczko, D. A. Strubbe and I. Tamblyn, <span class="italic">Phys. Rev. A</span>, 2019, <span class="bold">100</span>, 022512 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.100.022512" title="DOI Link to resource 10.1103/PhysRevA.100.022512">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvF2itb7F" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit506">A. Stuke, M. Todorović, M. Rupp, C. Kunkel, K. Ghosh, L. Himanen and P. Rinke, <span class="italic">J. Chem. Phys.</span>, 2019, <span class="bold">150</span>, 204121 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5086105" title="DOI Link to resource 10.1063/1.5086105">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31153160%5Buid%5D" title="PubMed Link to resource 31153160">PubMed</a>.</span></li>
      <li><span id="cit507">K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp, M. Scheffler, O. A. Von Lilienfeld, A. Tkatchenko and K.-R. Muller, <span class="italic">J. Chem. Theory Comput.</span>, 2013, <span class="bold">9</span>, 3404–3419 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct400195d" title="DOI Link to resource 10.1021/ct400195d">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3sXhtV2ktLzL" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26584096%5Buid%5D" title="PubMed Link to resource 26584096">PubMed</a>.</span></li>
      <li><span id="cit508">F. A. Faber, A. S. Christensen, B. Huang and O. A. von Lilienfeld, <span class="italic">J. Chem. Phys.</span>, 2018, <span class="bold">148</span>, 241717 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5020710" title="DOI Link to resource 10.1063/1.5020710">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29960351%5Buid%5D" title="PubMed Link to resource 29960351">PubMed</a>.</span></li>
      <li><span id="cit509">
          P. O. Dral, <span class="italic">Chemical Physics and Quantum Chemistry</span>, Academic Press,  2020, vol. 81 of Advances in Quantum Chemistry, pp. 291–324 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Chemical%20Physics%20and%20Quantum%20Chemistry%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit510">R. Ramakrishnan, P. O. Dral, M. Rupp and O. A. von Lilienfeld, <span class="italic">J. Chem. Theory Comput.</span>, 2015, <span class="bold">11</span>, 2087–2096 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.5b00099" title="DOI Link to resource 10.1021/acs.jctc.5b00099">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXmtlams7Y%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26574412%5Buid%5D" title="PubMed Link to resource 26574412">PubMed</a>.</span></li>
      <li><span id="cit511">M. Rupp, A. Tkatchenko, K.-R. Müller and O. A. Von Lilienfeld, <span class="italic">Phys. Rev. Lett.</span>, 2012, <span class="bold">108</span>, 058301 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.108.058301" title="DOI Link to resource 10.1103/PhysRevLett.108.058301">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22400967%5Buid%5D" title="PubMed Link to resource 22400967">PubMed</a>.</span></li>
      <li><span id="cit512">B. Himmetoglu, <span class="italic">J. Chem. Phys.</span>, 2016, <span class="bold">145</span>, 134101 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4964093" title="DOI Link to resource 10.1063/1.4964093">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27782427%5Buid%5D" title="PubMed Link to resource 27782427">PubMed</a>.</span></li>
      <li><span id="cit513">A. Denzel and J. Kästner, <span class="italic">J. Chem. Phys.</span>, 2018, <span class="bold">148</span>, 094114 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5017103" title="DOI Link to resource 10.1063/1.5017103">CrossRef</a>.</span></li>
      <li><span id="cit514">K. Choo, A. Mezzacapo and G. Carleo, <span class="italic">Nat. Commun.</span>, 2020, <span class="bold">11</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-019-13993-7" title="DOI Link to resource 10.1038/s41467-019-13993-7">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31911652%5Buid%5D" title="PubMed Link to resource 31911652">PubMed</a>.</span></li>
      <li><span id="cit515">Y. Cao, J. Romero, J. Olson, M. Degroote, P. D. Johnson, M. Kieferová, I. D. Kivlichan, T. Menke, B. Peropadre, N. P. D. Sawaya, S. Sim, L. Veis and A. Aspuru-Guzik, <span class="italic">Chem. Rev.</span>, 2019, <span class="bold">119</span>(19), 10856–10915 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.8b00803" title="DOI Link to resource 10.1021/acs.chemrev.8b00803">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhs1Krtb7K" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31469277%5Buid%5D" title="PubMed Link to resource 31469277">PubMed</a>.</span></li>
      <li><span id="cit516">H. Saito, <span class="italic">J. Phys. Soc.
Jpn.</span>, 2017, <span class="bold">86</span>, 093001 <a target="_blank" class="DOILink" href="https://doi.org/10.7566/JPSJ.86.093001" title="DOI Link to resource 10.7566/JPSJ.86.093001">CrossRef</a>.</span></li>
      <li><span id="cit517">R. Xia and S. Kais, <span class="italic">Nat. Commun.</span>, 2018, <span class="bold">9</span>, 1–6 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-017-02088-w" title="DOI Link to resource 10.1038/s41467-017-02088-w">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvFGqurbF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29317637%5Buid%5D" title="PubMed Link to resource 29317637">PubMed</a>.</span></li>
      <li><span id="cit518">S. Kanno and T. Tada, <span class="italic">Quantum Sci. Technol.</span>, 2021, <span class="bold">6</span>, 025015 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/2058-9565/abe139" title="DOI Link to resource 10.1088/2058-9565/abe139">CrossRef</a>.</span></li>
      <li><span id="cit519">S. H. Sureshbabu, M. Sajjan, S. Oh and S. Kais, <span class="italic">J. Chem. Inf. Model.</span>, 2021, <span class="bold">61</span>(6), 2667–2674 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jcim.1c00294" title="DOI Link to resource 10.1021/acs.jcim.1c00294">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhtlWltbzF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34133166%5Buid%5D" title="PubMed Link to resource 34133166">PubMed</a>.</span></li>
      <li><span id="cit520">J. P. Coe, <span class="italic">J. Chem. Theory Comput.</span>, 2018, <span class="bold">14</span>, 5739–5749 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.8b00849" title="DOI Link to resource 10.1021/acs.jctc.8b00849">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhvVKltL%252FN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30285426%5Buid%5D" title="PubMed Link to resource 30285426">PubMed</a>.</span></li>
      <li><span id="cit521">J. P. Coe, <span class="italic">J. Chem. Theory Comput.</span>, 2019, <span class="bold">15</span>, 6179–6189 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.9b00828" title="DOI Link to resource 10.1021/acs.jctc.9b00828">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhvFyitbzN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31622101%5Buid%5D" title="PubMed Link to resource 31622101">PubMed</a>.</span></li>
      <li><span id="cit522">C. A. Custódio, É. R. Filletti and V. V. França, <span class="italic">Sci. Rep.</span>, 2019, <span class="bold">9</span>, 1–7 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41598-018-37186-2" title="DOI Link to resource 10.1038/s41598-018-37186-2">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30626917%5Buid%5D" title="PubMed Link to resource 30626917">PubMed</a>.</span></li>
      <li><span id="cit523">J. R. Moreno, G. Carleo and A. Georges, <span class="italic">Phys. Rev. Lett.</span>, 2020, <span class="bold">125</span>, 076402 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.125.076402" title="DOI Link to resource 10.1103/PhysRevLett.125.076402">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhslert73O" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32857556%5Buid%5D" title="PubMed Link to resource 32857556">PubMed</a>.</span></li>
      <li><span id="cit524">K. T. Schütt, H. E. Sauceda, P.-J. Kindermans, A. Tkatchenko and K.-R. Müller, <span class="italic">J. Chem. Phys.</span>, 2018, <span class="bold">148</span>, 241722 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5019779" title="DOI Link to resource 10.1063/1.5019779">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29960322%5Buid%5D" title="PubMed Link to resource 29960322">PubMed</a>.</span></li>
      <li><span id="cit525">J. Hermann, Z. Schätzle and F. Noé, <span class="italic">Nat. Chem.</span>, 2020, <span class="bold">12</span>, 891–897 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41557-020-0544-y" title="DOI Link to resource 10.1038/s41557-020-0544-y">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhvFCht77L" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32968231%5Buid%5D" title="PubMed Link to resource 32968231">PubMed</a>.</span></li>
      <li><span id="cit526">F. A. Faber, A. S. Christensen, B. Huang and O. A. Von Lilienfeld, <span class="italic">J. Chem. Phys.</span>, 2018, <span class="bold">148</span>, 241717 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5020710" title="DOI Link to resource 10.1063/1.5020710">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29960351%5Buid%5D" title="PubMed Link to resource 29960351">PubMed</a>.</span></li>
      <li><span id="cit527">A. S. Christensen, L. A. Bratholm, F. A. Faber and O. Anatole von Lilienfeld, <span class="italic">J. Chem. Phys.</span>, 2020, <span class="bold">152</span>, 044107 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5126701" title="DOI Link to resource 10.1063/1.5126701">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitVOlsLY%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32007071%5Buid%5D" title="PubMed Link to resource 32007071">PubMed</a>.</span></li>
      <li><span id="cit528">B. Huang and O. A. von Lilienfeld, <span class="italic">Nat. Chem.</span>, 2020, <span class="bold">12</span>, 945–951 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41557-020-0527-z" title="DOI Link to resource 10.1038/s41557-020-0527-z">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhvVCltb7N" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32929248%5Buid%5D" title="PubMed Link to resource 32929248">PubMed</a>.</span></li>
      <li><span id="cit529">E. O. Pyzer-Knapp, K. Li and A. Aspuru-Guzik, <span class="italic">Adv. Funct. Mater.</span>, 2015, <span class="bold">25</span>, 6495–6502 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/adfm.201501919" title="DOI Link to resource 10.1002/adfm.201501919">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhsFWrtL3I" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit530">K. Choudhary, M. Bercx, J. Jiang, R. Pachter, D. Lamoen and F. Tavazza, <span class="italic">Chem. Mater.</span>, 2019, <span class="bold">31</span>, 5900–5908 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemmater.9b02166" title="DOI Link to resource 10.1021/acs.chemmater.9b02166">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhtlyisr3E" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32165788%5Buid%5D" title="PubMed Link to resource 32165788">PubMed</a>.</span></li>
      <li><span id="cit531">V. Stanev, C. Oses, A. G. Kusne, E. Rodriguez, J. Paglione, S. Curtarolo and I. Takeuchi, <span class="italic">npj Comput. Mater.</span>, 2018, <span class="bold">4</span>, 1–14 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41524-017-0060-9" title="DOI Link to resource 10.1038/s41524-017-0060-9">CrossRef</a>.</span></li>
      <li><span id="cit532">
          T. D. Barrett, A. Malyshev and A. Lvovsky, 2021, arXiv preprint arXiv:2109.12606.</span></li>
      <li><span id="cit533">P. V. Balachandran, J. Young, T. Lookman and J. M. Rondinelli, <span class="italic">Nat. Commun.</span>, 2017, <span class="bold">8</span>, 1–13 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-016-0009-6" title="DOI Link to resource 10.1038/s41467-016-0009-6">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28232747%5Buid%5D" title="PubMed Link to resource 28232747">PubMed</a>.</span></li>
      <li><span id="cit534">R. Gómez-Bombarelli, J. N. Wei, D. Duvenaud, J. M. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J. Aguilera-Iparraguirre, T. D. Hirzel, R. P. Adams and A. Aspuru-Guzik, <span class="italic">ACS Cent. Sci.</span>, 2018, <span class="bold">4</span>, 268–276 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acscentsci.7b00572" title="DOI Link to resource 10.1021/acscentsci.7b00572">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29532027%5Buid%5D" title="PubMed Link to resource 29532027">PubMed</a>.</span></li>
      <li><span id="cit535">B. Kim, S. Lee and J. Kim, <span class="italic">Sci. Adv.</span>, 2020, <span class="bold">6</span>, eaax9324 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/sciadv.aax9324" title="DOI Link to resource 10.1126/sciadv.aax9324">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitFGgu7%252FE" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31922005%5Buid%5D" title="PubMed Link to resource 31922005">PubMed</a>.</span></li>
      <li><span id="cit536">Z. Zhang, M. Li, K. Flores and R. Mishra, <span class="italic">J. Appl. Phys.</span>, 2020, <span class="bold">128</span>, 105103 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0012323" title="DOI Link to resource 10.1063/5.0012323">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhsl2htL%252FF" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit537">E. Mazhnik and A. R. Oganov, <span class="italic">J. Appl. Phys.</span>, 2020, <span class="bold">128</span>, 075102 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0012055" title="DOI Link to resource 10.1063/5.0012055">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhs1aqtrnP" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit538">
          B. Bruognolo, PhD thesis, LMU,  2017.</span></li>
      <li><span id="cit539">A. Kshetrimayum, C. Balz, B. Lake and J. Eisert, <span class="italic">Ann. Phys.</span>, 2020, <span class="bold">421</span>, 168292 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhsleisb7K" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit540">C. Balz, B. Lake, J. Reuther, H. Luetkens, R. Schönemann, T. Herrmannsdörfer, Y. Singh, A. Nazmul Islam, E. M. Wheeler and J. A. Rodriguez-Rivera, 
            <span class="italic">et al.</span>
          , <span class="italic">Nat. Phys.</span>, 2016, <span class="bold">12</span>, 942–949 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2012%5Bvolume%5D%20AND%20942%5Bpage%5D%20and%202016%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit541">
          J. Biamonte, 2019, arXiv preprint arXiv:1912.10049.</span></li>
      <li><span id="cit542">V. Murg, F. Verstraete, Ö. Legeza and R. M. Noack, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 2010, <span class="bold">82</span>, 205105 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.82.205105" title="DOI Link to resource 10.1103/PhysRevB.82.205105">CrossRef</a>.</span></li>
      <li><span id="cit543">T. Barthel, C. Pineda and J. Eisert, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2009, <span class="bold">80</span>, 042333 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.80.042333" title="DOI Link to resource 10.1103/PhysRevA.80.042333">CrossRef</a>.</span></li>
      <li><span id="cit544">C. Wille, O. Buerschaper and J. Eisert, <span class="italic">Phys. Rev. B</span>, 2017, <span class="bold">95</span>, 245127 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.95.245127" title="DOI Link to resource 10.1103/PhysRevB.95.245127">CrossRef</a>.</span></li>
      <li><span id="cit545">C. Krumnow, L. Veis, Ö. Legeza and J. Eisert, <span class="italic">Phys. Rev. Lett.</span>, 2016, <span class="bold">117</span>, 210402 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.117.210402" title="DOI Link to resource 10.1103/PhysRevLett.117.210402">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADC%252BC2sjivVGqug%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27911544%5Buid%5D" title="PubMed Link to resource 27911544">PubMed</a>.</span></li>
      <li><span id="cit546">C. Gogolin and J. Eisert, <span class="italic">Rep. Progress Phys.</span>, 2016, <span class="bold">79</span>, 056001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0034-4885/79/5/056001" title="DOI Link to resource 10.1088/0034-4885/79/5/056001">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27088565%5Buid%5D" title="PubMed Link to resource 27088565">PubMed</a>.</span></li>
      <li><span id="cit547">A. Kshetrimayum, M. Rizzi, J. Eisert and R. Orús, <span class="italic">Phys. Rev. Lett.</span>, 2019, <span class="bold">122</span>, 070502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.122.070502" title="DOI Link to resource 10.1103/PhysRevLett.122.070502">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXnvFSks7s%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30848636%5Buid%5D" title="PubMed Link to resource 30848636">PubMed</a>.</span></li>
      <li><span id="cit548">A. Kshetrimayum, M. Goihl and J. Eisert, <span class="italic">Phys. Rev. B</span>, 2020, <span class="bold">102</span>, 235132 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.102.235132" title="DOI Link to resource 10.1103/PhysRevB.102.235132">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXivVOlsb0%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit549">R. Haghshenas, J. Gray, A. C. Potter and G. K.-L. Chan, <span class="italic">Phys. Rev. X</span>, 2022, <span class="bold">12</span>(1), 011047 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB38XpsFKlurc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit550">
          J. R. Shewchuk
          
            <span class="italic">et al.</span>
          , <span class="italic">An introduction to the conjugate gradient method without the agonizing pain</span>,  1994 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=An%20introduction%20to%20the%20conjugate%20gradient%20method%20without%20the%20agonizing%20pain%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201994%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit551">
          N. N. Schraudolph, J. Yu and S. Günter, <span class="italic">Artificial intelligence and statistics</span>,  2007, pp. 436–443 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Artificial%20intelligence%20and%20statistics%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202007%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit552">A. Eddins, M. Motta, T. P. Gujarati, S. Bravyi, A. Mezzacapo, C. Hadfield and S. Sheldon, <span class="italic">PRX Quantum</span>, 2022, <span class="bold">3</span>, 010309 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PRXQuantum.3.010309" title="DOI Link to resource 10.1103/PRXQuantum.3.010309">CrossRef</a>.</span></li>
      <li><span id="cit553">J. A. McCammon, B. R. Gelin and M. Karplus, <span class="italic">Nature</span>, 1977, <span class="bold">267</span>, 585–590 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/267585a0" title="DOI Link to resource 10.1038/267585a0">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaE2sXlsVOgsbg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=301613%5Buid%5D" title="PubMed Link to resource 301613">PubMed</a>.</span></li>
      <li><span id="cit554">R. Schulz, B. Lindner, L. Petridis and J. C. Smith, <span class="italic">J. Chem. Theory Comput.</span>, 2009, <span class="bold">5</span>, 2798–2808 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct900292r" title="DOI Link to resource 10.1021/ct900292r">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1MXhtVGgurrJ" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26631792%5Buid%5D" title="PubMed Link to resource 26631792">PubMed</a>.</span></li>
      <li><span id="cit555">D. E. Shaw, M. M. Deneroff, R. O. Dror, J. S. Kuskin, R. H. Larson, J. K. Salmon, C. Young, B. Batson, K. J. Bowers and J. C. Chao, 
            <span class="italic">et al.</span>
          , <span class="italic">Commun. ACM</span>, 2008, <span class="bold">51</span>, 91–97 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/1364782.1364802" title="DOI Link to resource 10.1145/1364782.1364802">CrossRef</a>.</span></li>
      <li><span id="cit556">
          A. D. MacKerell, <span class="italic">Computational Methods for Protein Structure Prediction and Modeling</span>, Springer,  2007, pp. 45–69 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Computational%20Methods%20for%20Protein%20Structure%20Prediction%20and%20Modeling%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202007%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit557">M. I. Zimmerman and G. Bowman, <span class="italic">Biophys. J.</span>, 2021, <span class="bold">120</span>, 299a <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.bpj.2020.11.1909" title="DOI Link to resource 10.1016/j.bpj.2020.11.1909">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXkvFGlt7k%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit558">M. Gonzalez, <span class="italic">Ethématic school of the Societé Franccease of Neutronics</span>, 2011, <span class="bold">12</span>, 169–200 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Ethématic%20school%20of%20the%20Societé%20Franccease%20of%20Neutronics%5BJour%5D%20AND%2012%5Bvolume%5D%20AND%20169%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit559">A. D. MacKerell Jr, <span class="italic">J. Comput. Chem.</span>, 2004, <span class="bold">25</span>, 1584–1604 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/jcc.20082" title="DOI Link to resource 10.1002/jcc.20082">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15264253%5Buid%5D" title="PubMed Link to resource 15264253">PubMed</a>.</span></li>
      <li><span id="cit560">Y. Shi, Z. Xia, J. Zhang, R. Best, C. Wu, J. W. Ponder and P. Ren, <span class="italic">J. Chem. Theory Comput.</span>, 2013, <span class="bold">9</span>, 4046–4063 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct4003702" title="DOI Link to resource 10.1021/ct4003702">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3sXhtFKrsrjN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=24163642%5Buid%5D" title="PubMed Link to resource 24163642">PubMed</a>.</span></li>
      <li><span id="cit561">A. Warshel, M. Kato and A. V. Pisliakov, <span class="italic">J. Chem. Theory Comput.</span>, 2007, <span class="bold">3</span>, 2034–2045 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct700127w" title="DOI Link to resource 10.1021/ct700127w">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXhtVOhtrbE" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26636199%5Buid%5D" title="PubMed Link to resource 26636199">PubMed</a>.</span></li>
      <li><span id="cit562">O. T. Unke, M. Devereux and M. Meuwly, <span class="italic">J. Chem. Phys.</span>, 2017, <span class="bold">147</span>, 161712 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4993424" title="DOI Link to resource 10.1063/1.4993424">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29096479%5Buid%5D" title="PubMed Link to resource 29096479">PubMed</a>.</span></li>
      <li><span id="cit563">T. Nagy, J. Yosa Reyes and M. Meuwly, <span class="italic">J. Chem. Theory Comput.</span>, 2014, <span class="bold">10</span>, 1366–1375 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct400953f" title="DOI Link to resource 10.1021/ct400953f">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXjvV2rsLc%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26580356%5Buid%5D" title="PubMed Link to resource 26580356">PubMed</a>.</span></li>
      <li><span id="cit564">H. M. Senn and W. Thiel, <span class="italic">ANIE</span>, 2009, <span class="bold">48</span>, 1198–1229 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1MXitFOqs7g%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit565">S. Chmiela, H. E. Sauceda, I. Poltavsky, K.-R. Muller and A. Tkatchenko, <span class="italic">Comput. Phys. Commun.</span>, 2019, <span class="bold">240</span>, 38–45 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.cpc.2019.02.007" title="DOI Link to resource 10.1016/j.cpc.2019.02.007">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXktFehsLc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit566">O. T. Unke and M. Meuwly, <span class="italic">J. Chem. Theory Comput.</span>, 2019, <span class="bold">15</span>, 3678–3693 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.9b00181" title="DOI Link to resource 10.1021/acs.jctc.9b00181">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXosF2ms7g%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31042390%5Buid%5D" title="PubMed Link to resource 31042390">PubMed</a>.</span></li>
      <li><span id="cit567">A. P. Bartok, M. C. Payne, R. Kondor and G. Csanyi, <span class="italic">Phys. Rev. Lett.</span>, 2010, <span class="bold">104</span>, 136403 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.104.136403" title="DOI Link to resource 10.1103/PhysRevLett.104.136403">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=20481899%5Buid%5D" title="PubMed Link to resource 20481899">PubMed</a>.</span></li>
      <li><span id="cit568">K. Schutt, P. Kessel, M. Gastegger, K. Nicoli, A. Tkatchenko and K.-R. Muller, <span class="italic">J. Chem. Theory Comput.</span>, 2018, <span class="bold">15</span>, 448–455 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.8b00908" title="DOI Link to resource 10.1021/acs.jctc.8b00908">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30481453%5Buid%5D" title="PubMed Link to resource 30481453">PubMed</a>.</span></li>
      <li><span id="cit569">J. Behler and M. Parrinello, <span class="italic">Phys. Rev. Lett.</span>, 2007, <span class="bold">98</span>, 146401 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.98.146401" title="DOI Link to resource 10.1103/PhysRevLett.98.146401">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=17501293%5Buid%5D" title="PubMed Link to resource 17501293">PubMed</a>.</span></li>
      <li><span id="cit570">J. S. Smith, O. Isayev and A. E. Roitberg, <span class="italic">Chem. Sci.</span>, 2017, <span class="bold">8</span>, 3192–3203 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C6SC05720A&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C6SC05720A">RSC</a>.</span></li>
      <li><span id="cit571">L. Zhang, J. Han, H. Wang, R. Car and E. Weinan, <span class="italic">Phys. Rev. Lett.</span>, 2018, <span class="bold">120</span>, 143001 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.120.143001" title="DOI Link to resource 10.1103/PhysRevLett.120.143001">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXltFSksrg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29694129%5Buid%5D" title="PubMed Link to resource 29694129">PubMed</a>.</span></li>
      <li><span id="cit572">J. Westermayr and P. Marquetand, <span class="italic">Chem. Rev.</span>, 2020, <span class="bold">121</span>, 9873–9926 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.0c00749" title="DOI Link to resource 10.1021/acs.chemrev.0c00749">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33211478%5Buid%5D" title="PubMed Link to resource 33211478">PubMed</a>.</span></li>
      <li><span id="cit573">K. T. Schutt, F. Arbabzadah, S. Chmiela, K. R. Muller and A. Tkatchenko, <span class="italic">Nat. Commun.</span>, 2017, <span class="bold">8</span>, 1–8 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-016-0009-6" title="DOI Link to resource 10.1038/s41467-016-0009-6">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28232747%5Buid%5D" title="PubMed Link to resource 28232747">PubMed</a>.</span></li>
      <li><span id="cit574">O. T. Unke, S. Chmiela, H. E. Sauceda, M. Gastegger, I. Poltavsky, K. T. Schutt, A. Tkatchenko and K.-R. Muller, <span class="italic">Chem. Rev.</span>, 2021, <span class="bold">121</span>(16), 10142–10186 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.0c01111" title="DOI Link to resource 10.1021/acs.chemrev.0c01111">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXmtVOksL0%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33705118%5Buid%5D" title="PubMed Link to resource 33705118">PubMed</a>.</span></li>
      <li><span id="cit575">A. C. Van Duin, S. Dasgupta, F. Lorant and W. A. Goddard, <span class="italic">J. Phys. Chem. A</span>, 2001, <span class="bold">105</span>, 9396–9409 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jp004368u" title="DOI Link to resource 10.1021/jp004368u">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3MXmvFChu78%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit576">H. Nakata and S. Bai, <span class="italic">J. Comput. Chem.</span>, 2019, <span class="bold">40</span>, 2000–2012 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXnt1Wnu7s%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit577">L. Angibaud, L. Briquet, P. Philipp, T. Wirtz and J. Kieffer, <span class="italic">Nucl. Instrum. Methods Phys. Res., Sect. B</span>, 2011, <span class="bold">269</span>, 1559–1563 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.nimb.2010.11.024" title="DOI Link to resource 10.1016/j.nimb.2010.11.024">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3MXntlSnsro%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit578">M. Dittner, J. Muller, H. M. Aktulga and B. Hartke, <span class="italic">J. Comput. Chem.</span>, 2015, <span class="bold">36</span>, 1550–1561 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/jcc.23966" title="DOI Link to resource 10.1002/jcc.23966">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhtVensr3J" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26085201%5Buid%5D" title="PubMed Link to resource 26085201">PubMed</a>.</span></li>
      <li><span id="cit579">A. Jaramillo-Botero, S. Naserifar and W. A. Goddard III, <span class="italic">J. Chem. Theory Comput.</span>, 2014, <span class="bold">10</span>, 1426–1439 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct5001044" title="DOI Link to resource 10.1021/ct5001044">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXksVWjtbw%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26580361%5Buid%5D" title="PubMed Link to resource 26580361">PubMed</a>.</span></li>
      <li><span id="cit580">
          M. C. Kaymak, A. Rahnamoun, K. A. O'Hearn, A. C. van Duin, K. M. Merz Jr and H. M. Aktulga, <span class="italic">ChemRxiv</span>, Cambridge Open Engage, Cambridge,  2022 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=ChemRxiv%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202022%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit581">A. V. Akimov and O. V. Prezhdo, <span class="italic">J. Chem. Theory Comput.</span>, 2013, <span class="bold">9</span>, 4959–4972 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct400641n" title="DOI Link to resource 10.1021/ct400641n">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3sXhs1Wlt7fK" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26583414%5Buid%5D" title="PubMed Link to resource 26583414">PubMed</a>.</span></li>
      <li><span id="cit582">A. V. Akimov and O. V. Prezhdo, <span class="italic">J. Chem. Theory Comput.</span>, 2014, <span class="bold">10</span>, 789–804 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ct400934c" title="DOI Link to resource 10.1021/ct400934c">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXlslemug%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26580053%5Buid%5D" title="PubMed Link to resource 26580053">PubMed</a>.</span></li>
      <li><span id="cit583">P. Nijjar, J. Jankowska and O. V. Prezhdo, <span class="italic">J. Chem. Phys.</span>, 2019, <span class="bold">150</span>, 204124 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.5095810" title="DOI Link to resource 10.1063/1.5095810">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31153168%5Buid%5D" title="PubMed Link to resource 31153168">PubMed</a>.</span></li>
      <li><span id="cit584">P. O. Dral, M. Barbatti and W. Thiel, <span class="italic">J. Phys. Chem. Lett.</span>, 2018, <span class="bold">9</span>, 5660–5663 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.8b02469" title="DOI Link to resource 10.1021/acs.jpclett.8b02469">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhs1ygtrnF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30200766%5Buid%5D" title="PubMed Link to resource 30200766">PubMed</a>.</span></li>
      <li><span id="cit585">B. Wang, W. Chu, A. Tkatchenko and O. V. Prezhdo, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 6070–6077 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c01645" title="DOI Link to resource 10.1021/acs.jpclett.1c01645">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhsVSjs7%252FJ" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34170705%5Buid%5D" title="PubMed Link to resource 34170705">PubMed</a>.</span></li>
      <li><span id="cit586">Z. Zhang, Y. Zhang, J. Wang, J. Xu and R. Long, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 835–842 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.0c03522" title="DOI Link to resource 10.1021/acs.jpclett.0c03522">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXmvFWntg%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33417761%5Buid%5D" title="PubMed Link to resource 33417761">PubMed</a>.</span></li>
      <li><span id="cit587">W. Li, Y. She, A. S. Vasenko and O. V. Prezhdo, <span class="italic">Nanoscale</span>, 2021, <span class="bold">13</span>, 10239–10265 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=D1NR01990B&amp;newsite=1" title="Link to RSC resource DOI:10.1039/D1NR01990B">RSC</a>.</span></li>
      <li><span id="cit588">L. Zhang, W. Chu, C. Zhao, Q. Zheng, O. V. Prezhdo and J. Zhao, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 2191–2198 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c00003" title="DOI Link to resource 10.1021/acs.jpclett.1c00003">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXkvFGgsLo%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33630612%5Buid%5D" title="PubMed Link to resource 33630612">PubMed</a>.</span></li>
      <li><span id="cit589">D. H. Olson, M. G. Sales, J. A. Tomko, T.-F. Lu, O. V. Prezhdo, S. J. McDonnell and P. E. Hopkins, <span class="italic">Appl. Phys. Lett.</span>, 2021, <span class="bold">118</span>, 163503 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0046566" title="DOI Link to resource 10.1063/5.0046566">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXpsVKls7g%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit590">J. Westermayr, M. Gastegger and P. Marquetand, <span class="italic">J. Phys. Chem. Lett.</span>, 2020, <span class="bold">11</span>, 3828–3834 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.0c00527" title="DOI Link to resource 10.1021/acs.jpclett.0c00527">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXns1eksb4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32311258%5Buid%5D" title="PubMed Link to resource 32311258">PubMed</a>.</span></li>
      <li><span id="cit591">E. Posenitskiy, F. Spiegelman and D. Lemoine, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2021, <span class="bold">2</span>, 035039 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%202%5Bvolume%5D%20AND%20035039%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit592">A. Glielmo, B. E. Husic, A. Rodriguez, C. Clementi, F. Noé and A. Laio, <span class="italic">Chem. Rev.</span>, 2021, <span class="bold">121</span>, 9722–9758 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.0c01195" title="DOI Link to resource 10.1021/acs.chemrev.0c01195">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhtVSksbbO" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33945269%5Buid%5D" title="PubMed Link to resource 33945269">PubMed</a>.</span></li>
      <li><span id="cit593">A. M. Virshup, J. Chen and T. J. Martnez, <span class="italic">J. Chem. Phys.</span>, 2012, <span class="bold">137</span>, 22A519 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/1.4742066" title="DOI Link to resource 10.1063/1.4742066">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=23249056%5Buid%5D" title="PubMed Link to resource 23249056">PubMed</a>.</span></li>
      <li><span id="cit594">X. Li, Y. Xie, D. Hu and Z. Lan, <span class="italic">J. Chem. Theory Comput.</span>, 2017, <span class="bold">13</span>, 4611–4623 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jctc.7b00394" title="DOI Link to resource 10.1021/acs.jctc.7b00394">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhsVeks7zJ" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28862858%5Buid%5D" title="PubMed Link to resource 28862858">PubMed</a>.</span></li>
      <li><span id="cit595">J. Peng, Y. Xie, D. Hu and Z. Lan, <span class="italic">J. Chem. Phys.</span>, 2021, <span class="bold">154</span>, 094122 <a target="_blank" class="DOILink" href="https://doi.org/10.1063/5.0039743" title="DOI Link to resource 10.1063/5.0039743">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXmtVSntb8%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33685149%5Buid%5D" title="PubMed Link to resource 33685149">PubMed</a>.</span></li>
      <li><span id="cit596">G. Zhou, W. Chu and O. V. Prezhdo, <span class="italic">ACS Energy Lett.</span>, 2020, <span class="bold">5</span>, 1930–1938 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acsenergylett.0c00899" title="DOI Link to resource 10.1021/acsenergylett.0c00899">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXptlGls7g%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit597">P. Tavadze, G. Avendano Franco, P. Ren, X. Wen, Y. Li and J. P. Lewis, <span class="italic">J. Am. Chem. Soc.</span>, 2018, <span class="bold">140</span>, 285–290 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jacs.7b10030" title="DOI Link to resource 10.1021/jacs.7b10030">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhvFOqsrfK" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29235856%5Buid%5D" title="PubMed Link to resource 29235856">PubMed</a>.</span></li>
      <li><span id="cit598">S. M. Mangan, G. Zhou, W. Chu and O. V. Prezhdo, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 8672–8678 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c02361" title="DOI Link to resource 10.1021/acs.jpclett.1c02361">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhvFaisbnF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34472856%5Buid%5D" title="PubMed Link to resource 34472856">PubMed</a>.</span></li>
      <li><span id="cit599">A. Kraskov, H. Stögbauer and P. Grassberger, <span class="italic">Phys. Rev. E: Stat., Nonlinear, Soft Matter Phys.</span>, 2004, <span class="bold">69</span>, 066138 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevE.69.066138" title="DOI Link to resource 10.1103/PhysRevE.69.066138">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15244698%5Buid%5D" title="PubMed Link to resource 15244698">PubMed</a>.</span></li>
      <li><span id="cit600">W. B. How, B. Wang, W. Chu, A. Tkatchenko and O. V. Prezhdo, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 12026–12032 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c03469" title="DOI Link to resource 10.1021/acs.jpclett.1c03469">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXislantbnM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34902248%5Buid%5D" title="PubMed Link to resource 34902248">PubMed</a>.</span></li>
      <li><span id="cit601">W. Xiang, S. F. Liu and W. Tress, <span class="italic">Energy Environ. Sci.</span>, 2021, <span class="bold">14</span>, 2090–2113 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=D1EE00157D&amp;newsite=1" title="Link to RSC resource DOI:10.1039/D1EE00157D">RSC</a>.</span></li>
      <li><span id="cit602">M. A. Green, A. Ho-Baillie and H. J. Snaith, <span class="italic">Nat. photonics</span>, 2014, <span class="bold">8</span>, 506–514 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nphoton.2014.134" title="DOI Link to resource 10.1038/nphoton.2014.134">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXhtVGqu7zN" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit603">N. Ahn, D.-Y. Son, I.-H. Jang, S. M. Kang, M. Choi and N.-G. Park, <span class="italic">J. Am. Chem. Soc.</span>, 2015, <span class="bold">137</span>, 8696–8699 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jacs.5b04930" title="DOI Link to resource 10.1021/jacs.5b04930">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhtVyitLfJ" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26125203%5Buid%5D" title="PubMed Link to resource 26125203">PubMed</a>.</span></li>
      <li><span id="cit604">T. Bian, D. Murphy, R. Xia, A. Daskin and S. Kais, <span class="italic">Mol. Phys.</span>, 2019, <span class="bold">117</span>, 2069–2082 <a target="_blank" class="DOILink" href="https://doi.org/10.1080/00268976.2019.1580392" title="DOI Link to resource 10.1080/00268976.2019.1580392">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXht1Kjtr3K" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit605">A. Kandala, A. Mezzacapo, K. Temme, M. Takita, M. Brink, J. M. Chow and J. M. Gambetta, <span class="italic">Nature</span>, 2017, <span class="bold">549</span>, 242–246 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nature23879" title="DOI Link to resource 10.1038/nature23879">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhsV2isLjP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28905916%5Buid%5D" title="PubMed Link to resource 28905916">PubMed</a>.</span></li>
      <li><span id="cit606">
          L. W. Anderson, M. Kiffner, P. K. Barkoutsos, I. Tavernelli, J. Crain and D. Jaksch, 2021, arXiv preprint arXiv:2110.00968.</span></li>
      <li><span id="cit607">F. Cipcigan, J. Crain, V. Sokhan and G. Martyna, <span class="italic">Rev. Mod. Phys.</span>, 2019, <span class="bold">91</span>, 025003 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.91.025003" title="DOI Link to resource 10.1103/RevModPhys.91.025003">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXit1WhsLjP" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit608">P. Gokhale, O. Angiuli, Y. Ding, K. Gui, T. Tomesh, M. Suchara, M. Martonosi and F. T. Chong, <span class="italic">IEEE Trans. Quantum Eng.</span>, 2020, <span class="bold">1</span>, 1–24 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Trans.%20Quantum%20Eng.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit609">M. J. Westbroek, P. R. King, D. D. Vvedensky and S. Dürr, <span class="italic">Am. J. Phys.</span>, 2018, <span class="bold">86</span>, 293–304 <a target="_blank" class="DOILink" href="https://doi.org/10.1119/1.5024926" title="DOI Link to resource 10.1119/1.5024926">CrossRef</a>.</span></li>
      <li><span id="cit610">P. J. Ollitrault, G. Mazzola and I. Tavernelli, <span class="italic">Phys. Rev. Lett.</span>, 2020, <span class="bold">125</span>, 260511 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.125.260511" title="DOI Link to resource 10.1103/PhysRevLett.125.260511">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXivVOksbk%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33449795%5Buid%5D" title="PubMed Link to resource 33449795">PubMed</a>.</span></li>
      <li><span id="cit611">G. Capano, M. Chergui, U. Rothlisberger, I. Tavernelli and T. J. Penfold, <span class="italic">J. Phys. Chem. A</span>, 2014, <span class="bold">118</span>, 9861–9869 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jp509728m" title="DOI Link to resource 10.1021/jp509728m">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXhs1Oit7jO" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=25275666%5Buid%5D" title="PubMed Link to resource 25275666">PubMed</a>.</span></li>
      <li><span id="cit612">A. Zhugayevych and S. Tretiak, <span class="italic">Ann. Rev. Phys. Chem.</span>, 2015, <span class="bold">66</span>, 305–330 <a target="_blank" class="DOILink" href="https://doi.org/10.1146/annurev-physchem-040214-121440" title="DOI Link to resource 10.1146/annurev-physchem-040214-121440">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXnvFarsbk%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=25580623%5Buid%5D" title="PubMed Link to resource 25580623">PubMed</a>.</span></li>
      <li><span id="cit613">
          O. Kiss, F. Tacchino, S. Vallecorsa and I. Tavernelli, 2022, arXiv:2203.04666 [physics, physics:quant-ph].</span></li>
      <li><span id="cit614">S. Lim, Y. Lu, C. Y. Cho, I. Sung, J. Kim, Y. Kim, S. Park and S. Kim, <span class="italic">Comput. Struct. Biotechnol. J.</span>, 2021, <span class="bold">19</span>, 1541–1556 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.csbj.2021.03.004" title="DOI Link to resource 10.1016/j.csbj.2021.03.004">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhs1yns7nL" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33841755%5Buid%5D" title="PubMed Link to resource 33841755">PubMed</a>.</span></li>
      <li><span id="cit615">G. Sliwoski, S. Kothiwale, J. Meiler and E. W. Lowe, <span class="italic">Pharmacol. Rev.</span>, 2014, <span class="bold">66</span>, 334–395 <a target="_blank" class="DOILink" href="https://doi.org/10.1124/pr.112.007336" title="DOI Link to resource 10.1124/pr.112.007336">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=24381236%5Buid%5D" title="PubMed Link to resource 24381236">PubMed</a>.</span></li>
      <li><span id="cit616">S. P. Leelananda and S. Lindert, <span class="italic">Beilstein J. Org. Chem.</span>, 2016, <span class="bold">12</span>, 2694–2718 <a target="_blank" class="DOILink" href="https://doi.org/10.3762/bjoc.12.267" title="DOI Link to resource 10.3762/bjoc.12.267">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXitlyrsrs%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28144341%5Buid%5D" title="PubMed Link to resource 28144341">PubMed</a>.</span></li>
      <li><span id="cit617">S. Kim, P. A. Thiessen, E. E. Bolton, J. Chen, G. Fu, A. Gindulyte, L. Han, J. He, S. He and B. A. Shoemaker, 
            <span class="italic">et al.</span>
          , <span class="italic">Nucleic Acids Res.</span>, 2016, <span class="bold">44</span>, D1202–D1213 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkv951" title="DOI Link to resource 10.1093/nar/gkv951">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhtV2gu7bE" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26400175%5Buid%5D" title="PubMed Link to resource 26400175">PubMed</a>.</span></li>
      <li><span id="cit618">S. Kim, J. Chen, T. Cheng, A. Gindulyte, J. He, S. He, Q. Li, B. A. Shoemaker, P. A. Thiessen and B. Yu, 
            <span class="italic">et al.</span>
          , <span class="italic">Nucleic Acids Res.</span>, 2019, <span class="bold">47</span>, D1102–D1109 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gky1033" title="DOI Link to resource 10.1093/nar/gky1033">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30371825%5Buid%5D" title="PubMed Link to resource 30371825">PubMed</a>.</span></li>
      <li><span id="cit619">A. Gaulton, A. Hersey, M. Nowotka, A. P. Bento, J. Chambers, D. Mendez, P. Mutowo, F. Atkinson, L. J. Bellis and E. Cibrián-Uhalte, 
            <span class="italic">et al.</span>
          , <span class="italic">Nucleic Acids Res.</span>, 2017, <span class="bold">45</span>, D945–D954 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkw1074" title="DOI Link to resource 10.1093/nar/gkw1074">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhslWhurs%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=27899562%5Buid%5D" title="PubMed Link to resource 27899562">PubMed</a>.</span></li>
      <li><span id="cit620">D. S. Wishart, Y. D. Feunang, A. C. Guo, E. J. Lo, A. Marcu, J. R. Grant, T. Sajed, D. Johnson, C. Li and Z. Sayeeda, 
            <span class="italic">et al.</span>
          , <span class="italic">Nucleic Acids Res.</span>, 2018, <span class="bold">46</span>, D1074–D1082 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkx1037" title="DOI Link to resource 10.1093/nar/gkx1037">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXitlGisbvI" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29126136%5Buid%5D" title="PubMed Link to resource 29126136">PubMed</a>.</span></li>
      <li><span id="cit621">M. M. Mysinger, M. Carchia, J. J. Irwin and B. K. Shoichet, <span class="italic">J. Med. Chem.</span>, 2012, <span class="bold">55</span>, 6582–6594 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jm300687e" title="DOI Link to resource 10.1021/jm300687e">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC38XovFaku7c%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22716043%5Buid%5D" title="PubMed Link to resource 22716043">PubMed</a>.</span></li>
      <li><span id="cit622">R. Apweiler, A. Bairoch, C. H Wu, W. C. Barker, B. Boeckmann, S. Ferro, E. Gasteiger and H. Huang, 
            <span class="italic">et al.</span>
          , <span class="italic">Nucleic Acids Res.</span>, 2004, <span class="bold">32</span>, D115–D119 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkh131" title="DOI Link to resource 10.1093/nar/gkh131">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3sXhtVSru7vK" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=14681372%5Buid%5D" title="PubMed Link to resource 14681372">PubMed</a>.</span></li>
      <li><span id="cit623">H. M. Berman, J. Westbrook, Z. Feng, G. Gilliland, T. N. Bhat, H. Weissig, I. N. Shindyalov and P. E. Bourne, <span class="italic">Nucleic Acids Res.</span>, 2000, <span class="bold">28</span>, 235–242 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/28.1.235" title="DOI Link to resource 10.1093/nar/28.1.235">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD3cXhvVKjt7w%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=10592235%5Buid%5D" title="PubMed Link to resource 10592235">PubMed</a>.</span></li>
      <li><span id="cit624">H. Berman, K. Henrick, H. Nakamura and J. L. Markley, <span class="italic">Nucleic Acids Res.</span>, 2007, <span class="bold">35</span>, D301–D303 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkl971" title="DOI Link to resource 10.1093/nar/gkl971">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXivFKmtQ%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=17142228%5Buid%5D" title="PubMed Link to resource 17142228">PubMed</a>.</span></li>
      <li><span id="cit625">S. K. Burley, H. M. Berman, G. J. Kleywegt, J. L. Markley, H. Nakamura and S. Velankar, <span class="italic">Methods Mol. Biol.</span>, 2017, <span class="bold">1607</span>, 627–641 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/978-1-4939-7000-1_26" title="DOI Link to resource 10.1007/978-1-4939-7000-1_26">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXhsFOgt7fN" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=28573592%5Buid%5D" title="PubMed Link to resource 28573592">PubMed</a>.</span></li>
      <li><span id="cit626">R. Wang, X. Fang, Y. Lu, C.-Y. Yang and S. Wang, <span class="italic">J. Med. Chem.</span>, 2005, <span class="bold">48</span>, 4111–4119 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jm048957q" title="DOI Link to resource 10.1021/jm048957q">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2MXktlKisbg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15943484%5Buid%5D" title="PubMed Link to resource 15943484">PubMed</a>.</span></li>
      <li><span id="cit627">M. K. Gilson, T. Liu, M. Baitaluk, G. Nicola, L. Hwang and J. Chong, <span class="italic">Nucleic Acids Res.</span>, 2016, <span class="bold">44</span>, D1045–D1053 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkv1072" title="DOI Link to resource 10.1093/nar/gkv1072">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhtV2gu7fM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26481362%5Buid%5D" title="PubMed Link to resource 26481362">PubMed</a>.</span></li>
      <li><span id="cit628">D. Weininger, <span class="italic">J. Chem. Inform. Comput. Sci.</span>, 1988, <span class="bold">28</span>, 31–36 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci00057a005" title="DOI Link to resource 10.1021/ci00057a005">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaL1cXnsVeqsA%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit629">D. Weininger, A. Weininger and J. L. Weininger, <span class="italic">J. Chem. Inform. Comput. Sci.</span>, 1989, <span class="bold">29</span>, 97–101 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci00062a008" title="DOI Link to resource 10.1021/ci00062a008">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaL1MXitFWlt7s%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit630">N. M. O’Boyle, <span class="italic">J. Cheminf.</span>, 2012, <span class="bold">4</span>, 1–14 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Cheminf.%5BJour%5D%20AND%204%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202012%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit631">M. Krenn, F. Häse, A. Nigam, P. Friederich and A. Aspuru-Guzik, <span class="italic">Mach. Learn.: Sci. Technol.</span>, 2020, <span class="bold">1</span>, 045024 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Mach.%20Learn.:%20Sci.%20Technol.%5BJour%5D%20AND%201%5Bvolume%5D%20AND%20045024%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit632">
          M. Krenn, F. Häse, A. Nigam, P. Friederich and A. Aspuru-Guzik, 2019, arXiv preprint arXiv:1905.13741.</span></li>
      <li><span id="cit633">A. Dalke, <span class="italic">Chem. Cent. J.</span>, 2008, <span class="bold">2</span>, 1 <a target="_blank" class="DOILink" href="https://doi.org/10.1186/1752-153X-2-1" title="DOI Link to resource 10.1186/1752-153X-2-1">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18234100%5Buid%5D" title="PubMed Link to resource 18234100">PubMed</a>.</span></li>
      <li><span id="cit634">V. J. Sykora and D. E. Leahy, <span class="italic">J. Chem. Inf. Model.</span>, 2008, <span class="bold">48</span>, 1931–1942 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci800135h" title="DOI Link to resource 10.1021/ci800135h">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1cXhtFelt7rI" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18803371%5Buid%5D" title="PubMed Link to resource 18803371">PubMed</a>.</span></li>
      <li><span id="cit635">D. Rogers and M. Hahn, <span class="italic">J. Chem. Inf. Model.</span>, 2010, <span class="bold">50</span>, 742–754 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci100050t" title="DOI Link to resource 10.1021/ci100050t">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3cXlt1Onsbg%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=20426451%5Buid%5D" title="PubMed Link to resource 20426451">PubMed</a>.</span></li>
      <li><span id="cit636">A. Cereto-Massagué, M. J. Ojeda, C. Valls, M. Mulero, S. Garcia-Vallvé and G. Pujadas, <span class="italic">Methods</span>, 2015, <span class="bold">71</span>, 58–63 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.ymeth.2014.08.005" title="DOI Link to resource 10.1016/j.ymeth.2014.08.005">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=25132639%5Buid%5D" title="PubMed Link to resource 25132639">PubMed</a>.</span></li>
      <li><span id="cit637">J. Duan, S. L. Dixon, J. F. Lowrie and W. Sherman, <span class="italic">J. Mol. Graphics Modell.</span>, 2010, <span class="bold">29</span>, 157–170 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.jmgm.2010.05.008" title="DOI Link to resource 10.1016/j.jmgm.2010.05.008">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3cXhtFamtLrM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=20579912%5Buid%5D" title="PubMed Link to resource 20579912">PubMed</a>.</span></li>
      <li><span id="cit638">J. Hert, P. Willett, D. J. Wilton, P. Acklin, K. Azzaoui, E. Jacoby and A. Schuffenhauer, <span class="italic">J. Chem. Inform. Comput. Sci.</span>, 2004, <span class="bold">44</span>, 1177–1185 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci034231b" title="DOI Link to resource 10.1021/ci034231b">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2cXhsVOis7g%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=15154787%5Buid%5D" title="PubMed Link to resource 15154787">PubMed</a>.</span></li>
      <li><span id="cit639">V. I. Pérez-Nueno, O. Rabal, J. I. Borrell and J. Teixidó, <span class="italic">J. Chem. Inf. Model.</span>, 2009, <span class="bold">49</span>, 1245–1260 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci900043r" title="DOI Link to resource 10.1021/ci900043r">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=19364101%5Buid%5D" title="PubMed Link to resource 19364101">PubMed</a>.</span></li>
      <li><span id="cit640">M. Awale and J.-L. Reymond, <span class="italic">J. Chem. Inf. Model.</span>, 2014, <span class="bold">54</span>, 1892–1907 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci500232g" title="DOI Link to resource 10.1021/ci500232g">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2cXpvFCltr4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=24988038%5Buid%5D" title="PubMed Link to resource 24988038">PubMed</a>.</span></li>
      <li><span id="cit641">
          N. De Cao and T. Kipf, 2018, arXiv preprint arXiv:1805.11973.</span></li>
      <li><span id="cit642">D. Jiang, Z. Wu, C.-Y. Hsieh, G. Chen, B. Liao, Z. Wang, C. Shen, D. Cao, J. Wu and T. Hou, <span class="italic">J. Cheminf.</span>, 2021, <span class="bold">13</span>, 1–23 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Cheminf.%5BJour%5D%20AND%2013%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit643">P. Carracedo-Reboredo, J. Liñares-Blanco, N. Rodrguez-Fernández, F. Cedrón, F. J. Novoa, A. Carballal, V. Maojo, A. Pazos and C. Fernandez-Lozano, <span class="italic">Comput. Struct. Biotechnol. J.</span>, 2021, <span class="bold">19</span>, 4538 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.csbj.2021.08.011" title="DOI Link to resource 10.1016/j.csbj.2021.08.011">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXisF2gs7zM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34471498%5Buid%5D" title="PubMed Link to resource 34471498">PubMed</a>.</span></li>
      <li><span id="cit644">X. Lin, X. Li and X. Lin, <span class="italic">Molecules</span>, 2020, <span class="bold">25</span>, 1375 <a target="_blank" class="DOILink" href="https://doi.org/10.3390/molecules25061375" title="DOI Link to resource 10.3390/molecules25061375">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXovV2ht70%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32197324%5Buid%5D" title="PubMed Link to resource 32197324">PubMed</a>.</span></li>
      <li><span id="cit645">J. B. Dunbar, R. D. Smith, C.-Y. Yang, P. M.-U. Ung, K. W. Lexa, N. A. Khazanov, J. A. Stuckey, S. Wang and H. A. Carlson, <span class="italic">J. Chem. Inf. Model.</span>, 2011, <span class="bold">51</span>, 2036–2046 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci200082t" title="DOI Link to resource 10.1021/ci200082t">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3MXpt1Cjur4%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=21728306%5Buid%5D" title="PubMed Link to resource 21728306">PubMed</a>.</span></li>
      <li><span id="cit646">D. R. Koes, M. P. Baumgartner and C. J. Camacho, <span class="italic">J. Chem. Inf. Model.</span>, 2013, <span class="bold">53</span>(8), 1893–1904 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci300604z" title="DOI Link to resource 10.1021/ci300604z">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC3sXhvVyns78%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=23379370%5Buid%5D" title="PubMed Link to resource 23379370">PubMed</a>.</span></li>
      <li><span id="cit647">O. Trott and A. J. Olson, <span class="italic">J. Comput. Chem.</span>, 2010, <span class="bold">31</span>, 455–461 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1MXhsFGnur3O" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit648">
          Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama and T. Darrell, <span class="italic">Proceedings of the 22nd ACM international conference on Multimedia</span>,  2014, pp. 675–678 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proceedings%20of%20the%2022nd%20ACM%20international%20conference%20on%20Multimedia%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202014%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit649">Y.-B. Wang, Z.-H. You, S. Yang, H.-C. Yi, Z.-H. Chen and K. Zheng, <span class="italic">BMC Med. Inf. Decis. Making</span>, 2020, <span class="bold">20</span>, 1–9 <a target="_blank" class="DOILink" href="https://doi.org/10.1186/s12911-019-1002-x" title="DOI Link to resource 10.1186/s12911-019-1002-x">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXivVKhuw%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31906929%5Buid%5D" title="PubMed Link to resource 31906929">PubMed</a>.</span></li>
      <li><span id="cit650">M. Kanehisa, M. Araki, S. Goto, M. Hattori, M. Hirakawa, M. Itoh, T. Katayama, S. Kawashima, S. Okuda, T. Tokimatsu and Y. Yamanishi, <span class="italic">Nucleic Acids Res.</span>, 2008, <span class="bold">36</span>, D480–D484 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkm882" title="DOI Link to resource 10.1093/nar/gkm882">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1cXhtVSku7k%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18077471%5Buid%5D" title="PubMed Link to resource 18077471">PubMed</a>.</span></li>
      <li><span id="cit651">D. S. Wishart, C. Knox, A. Guo, S. Shrivastava, M. Hassanali, P. Stothard, Z. Chang and J. Woolsey, <span class="italic">Nucleic Acids Res.</span>, 2006, <span class="bold">34</span>, D668–D672 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkj067" title="DOI Link to resource 10.1093/nar/gkj067">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD28XisFOrsw%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=16381955%5Buid%5D" title="PubMed Link to resource 16381955">PubMed</a>.</span></li>
      <li><span id="cit652">S. Günther, M. Kuhn, M. Dunkel, M. Campillos, C. Senger, E. Petsalaki, J. Ahmed, E. G. Urdiales, A. Gewiess, L. J. Jensen, R. Schneider, R. Skoblo, R. B. Russell, P. E. Bourne, P. Bork and R. Preissner, <span class="italic">Nucleic Acids Res.</span>, 2008, <span class="bold">36</span>, D919–D922 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkm862" title="DOI Link to resource 10.1093/nar/gkm862">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=17942422%5Buid%5D" title="PubMed Link to resource 17942422">PubMed</a>.</span></li>
      <li><span id="cit653">Y. Wang, Z.-H. You, X. Li, X. Chen, T. Jiang and J. Zhang, <span class="italic">Int. J. Mol. Sci.</span>, 2017, <span class="bold">18</span>, 1–13 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Int.%20J.%20Mol.%20Sci.%5BJour%5D%20AND%2018%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit654">J. A. Hanley and B. J. McNeil, <span class="italic">Radiology</span>, 1982, <span class="bold">143</span>, 29–36 <a target="_blank" class="DOILink" href="https://doi.org/10.1148/radiology.143.1.7063747" title="DOI Link to resource 10.1148/radiology.143.1.7063747">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADyaL387ltFyksQ%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=7063747%5Buid%5D" title="PubMed Link to resource 7063747">PubMed</a>.</span></li>
      <li><span id="cit655">H. Zhang, Z. Kang, H. Gong, X. Da and J. Wang, 
            <span class="italic">et al.</span>, The digestive system is a potential route of 2019-ncov infection: a bioinformatics analysis based on single-cell transcriptomes, <span class="italic">bioRxiv</span>, 2020<small> DOI:<a class="DOILink" href="https://doi.org/10.1101/2020.01.30.927806" TARGET="_BLANK" title="DOI Link to 10.1101/2020.01.30.927806">10.1101/2020.01.30.927806</a></small>.</span></li>
      <li><span id="cit656">
          K. He, X. Zhang, S. Ren and J. Sun, European conference on computer vision, 2016, pp. 630–645.</span></li>
      <li><span id="cit657">M. Olivecrona, T. Blaschke, O. Engkvist and H. Chen, <span class="italic">J. Cheminf.</span>, 2017, <span class="bold">9</span>, 1–14 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=J.%20Cheminf.%5BJour%5D%20AND%209%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202017%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit658">M. M. Mysinger, M. Carchia, J. J. Irwin and B. K. Shoichet, <span class="italic">J. Med. Chem.</span>, 2012, <span class="bold">55</span>, 6582–6594 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/jm300687e" title="DOI Link to resource 10.1021/jm300687e">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC38XovFaku7c%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22716043%5Buid%5D" title="PubMed Link to resource 22716043">PubMed</a>.</span></li>
      <li><span id="cit659">H. Liu, J. Sun, J. Guan, J. Zheng and S. Zhou, <span class="italic">Bioinformatics</span>, 2015, <span class="bold">31</span>, i221–i229 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/bioinformatics/btv256" title="DOI Link to resource 10.1093/bioinformatics/btv256">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC28Xht1Cit7fF" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26072486%5Buid%5D" title="PubMed Link to resource 26072486">PubMed</a>.</span></li>
      <li><span id="cit660">M. K. Gilson, T. Liu, M. Baitaluk, G. Nicola, L. Hwang and J. Chong, <span class="italic">Nucleic Acids Res.</span>, 2016, <span class="bold">44</span>, D1045–D1053 <a target="_blank" class="DOILink" href="https://doi.org/10.1093/nar/gkv1072" title="DOI Link to resource 10.1093/nar/gkv1072">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhtV2gu7fM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26481362%5Buid%5D" title="PubMed Link to resource 26481362">PubMed</a>.</span></li>
      <li><span id="cit661">K. H. Zou, A. J. O’Malley and L. Mauri, <span class="italic">Circulation</span>, 2007, <span class="bold">115</span>, 654–657 <a target="_blank" class="DOILink" href="https://doi.org/10.1161/CIRCULATIONAHA.105.594929" title="DOI Link to resource 10.1161/CIRCULATIONAHA.105.594929">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=17283280%5Buid%5D" title="PubMed Link to resource 17283280">PubMed</a>.</span></li>
      <li><span id="cit662">
          I. Wallach, M. Dzamba and A. Heifets, CoRR, 2015, pp. 1–9, https://arxiv.org/abs/1510.02855.</span></li>
      <li><span id="cit663">
          G. E. Dahl, N. Jaitly and R. Salakhutdinov, 2014, arXiv preprint arXiv:1406.1231.</span></li>
      <li><span id="cit664">A. Mauri, V. Consonni, M. Pavan, R. Todeschini and M. Chemometrics, <span class="italic">Commun. Math. Comput. Chem.</span>, 2006, <span class="bold">56</span>, 237–248 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2sXhtVWhtbg%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit665">
          B. Ramsundar, S. M. Kearnes, P. F. Riley, D. R. Webster, D. E. Konerding and V. S. Pande, 2015, pp. 1–9, https://arxiv.org/abs/1502.02072.</span></li>
      <li><span id="cit666">
          C. Nwankpa, W. Ijomah, A. Gachagan and S. Marshall, 2018, arXiv preprint arXiv:1811.03378.</span></li>
      <li><span id="cit667">J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl and V. Svetnik, <span class="italic">J. Chem. Inf. Model.</span>, 2015, <span class="bold">55</span>, 263–274 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci500747n" title="DOI Link to resource 10.1021/ci500747n">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhvFGns70%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=25635324%5Buid%5D" title="PubMed Link to resource 25635324">PubMed</a>.</span></li>
      <li><span id="cit668">R. E. Carhart, D. H. Smith and R. Venkataraghavan, <span class="italic">J. Chem. Inf. Comput. Sci.</span>, 1985, <span class="bold">25</span>, 64–73 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/ci00046a002" title="DOI Link to resource 10.1021/ci00046a002">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaL2MXitVehu7o%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit669">D. L. Alexander, A. Tropsha and D. A. Winkler, <span class="italic">J. Chem. Inf. Model.</span>, 2015, <span class="bold">55</span>, 1316–1322 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jcim.5b00206" title="DOI Link to resource 10.1021/acs.jcim.5b00206">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2MXhtVCmtL%252FO" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=26099013%5Buid%5D" title="PubMed Link to resource 26099013">PubMed</a>.</span></li>
      <li><span id="cit670">A. H. Vo, T. R. Van Vleet, R. R. Gupta, M. J. Liguori and M. S. Rao, <span class="italic">Chem. Res. Toxicol.</span>, 2019, <span class="bold">33</span>, 20–37 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Chem.%20Res.%20Toxicol.%5BJour%5D%20AND%2033%5Bvolume%5D%20AND%2020%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit671">A. D. Rodgers, H. Zhu, D. Fourches, I. Rusyn and A. Tropsha, <span class="italic">Chem. Res. Toxicol.</span>, 2010, <span class="bold">23</span>, 724–732 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Chem.%20Res.%20Toxicol.%5BJour%5D%20AND%2023%5Bvolume%5D%20AND%20724%5Bpage%5D%20and%202010%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit672">
          G. L. Plaa and W. R. Hewitt, <span class="italic">Toxicology of the Liver</span>, CRC Press,  1998 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Toxicology%20of%20the%20Liver%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%201998%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit673">K. Z. Guyton, J. A. Thompson and T. W. Kensler, <span class="italic">Chem. Res. Toxicol.</span>, 1993, <span class="bold">6</span>, 731–738 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Chem.%20Res.%20Toxicol.%5BJour%5D%20AND%206%5Bvolume%5D%20AND%20731%5Bpage%5D%20and%201993%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit674">T. Nonoyama and R. Fukuda, <span class="italic">J. Toxicol. Pathol.</span>, 2008, <span class="bold">21</span>, 9–24 <a target="_blank" class="DOILink" href="https://doi.org/10.1293/tox.21.9" title="DOI Link to resource 10.1293/tox.21.9">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1cXltlSlsbs%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit675">R. Huang, N. Southall, Y. Wang, A. Yasgar, P. Shinn, A. Jadhav, D.-T. Nguyen and C. P. Austin, <span class="italic">Sci. Trans. Med.</span>, 2011, <span class="bold">3</span>, 80ps16 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Sci.%20Trans.%20Med.%5BJour%5D%20AND%203%5Bvolume%5D%20AND%2080ps16%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit676">N. Bhandari, D. J. Figueroa, J. W. Lawrence and D. L. Gerhold, <span class="italic">Assay Drug Dev. Technol.</span>, 2008, <span class="bold">6</span>, 407–419 <a target="_blank" class="DOILink" href="https://doi.org/10.1089/adt.2007.119" title="DOI Link to resource 10.1089/adt.2007.119">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD1cXnvFCgsrk%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=18537465%5Buid%5D" title="PubMed Link to resource 18537465">PubMed</a>.</span></li>
      <li><span id="cit677">R. E. Pearce, J. Uetrecht and J. S. Leeder, <span class="italic">Drug Metab. Dispos.</span>, 2005, <span class="bold">33</span>, 1819–1826 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BD2MXht1ygu7vE" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit678">V. L. M. Yip, J. L. Maggs, X. Meng, A. G. Marson, K. B. Park and M. Pirmohamed, <span class="italic">The Lancet</span>, 2014, <span class="bold">383</span>, S114 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/S0140-6736(14)60377-9" title="DOI Link to resource 10.1016/S0140-6736(14)60377-9">CrossRef</a>.</span></li>
      <li><span id="cit679">K. B. Alton, R. M. Grimes, C. J. Shaw, J. E. Patrick and J. L. Mcguire, <span class="italic">Drug Metab. Dispos.</span>, 1975, <span class="bold">3</span>(5), 352–360 <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADyaE28Xht1aktw%253D%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit680">A. F. Stepan, D. P. Walker, J. N. Bauman, D. Price, T. A. Baillie, A. S. Kalgutkar and M. D. Aleo, <span class="italic">Chem. Res. Toxicol.</span>, 2011, <span class="bold">24</span>(9), 1345–1410 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Chem.%20Res.%20Toxicol.%5BJour%5D%20AND%2024%5Bvolume%5D%20AND%201345%5Bpage%5D%20and%202011%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit681">Y. Cao, J. Romero and A. Aspuru-Guzik, <span class="italic">IBM J. Res. Dev.</span>, 2018, <span class="bold">62</span>(6), 1–6 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IBM%20J.%20Res.%20Dev.%5BJour%5D%20AND%2062%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit682">A. Perdomo-Ortiz, N. Dickson, M. Drew-Brook, G. Rose and A. Aspuru-Guzik, <span class="italic">Sci. Rep.</span>, 2012, <span class="bold">2</span>, 1–7 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Sci.%20Rep.%5BJour%5D%20AND%202%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202012%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit683">L. Banchi, M. Fingerhuth, T. Babej, C. Ing and J. M. Arrazola, <span class="italic">Sci. Adv.</span>, 2020, <span class="bold">6</span>, eaax1950 <a target="_blank" class="DOILink" href="https://doi.org/10.1126/sciadv.aax1950" title="DOI Link to resource 10.1126/sciadv.aax1950">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXitFGgtL7I" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32548251%5Buid%5D" title="PubMed Link to resource 32548251">PubMed</a>.</span></li>
      <li><span id="cit684">A. Robert, P. K. Barkoutsos, S. Woerner and I. Tavernelli, <span class="italic">npj Quantum Inform.</span>, 2021, <span class="bold">7</span>, 1–5 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41534-020-00339-1" title="DOI Link to resource 10.1038/s41534-020-00339-1">CrossRef</a>.</span></li>
      <li><span id="cit685">
          T. Babej, C. Ing and M. Fingerhuth, Coarse-grained lattice protein folding on a quantum annealer, arXiv preprint arXiv:1811.00713,  2018.</span></li>
      <li><span id="cit686">J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Ždek and A. Potapenko, 
            <span class="italic">et al.</span>
          , <span class="italic">Nature</span>, 2021, <span class="bold">596</span>, 583–589 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-021-03819-2" title="DOI Link to resource 10.1038/s41586-021-03819-2">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhvVaktrrL" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34265844%5Buid%5D" title="PubMed Link to resource 34265844">PubMed</a>.</span></li>
      <li><span id="cit687">M. Zinner, F. Dahlhausen, P. Boehme, J. Ehlers, L. Bieske and L. Fehring, <span class="italic">Drug Discovery Today</span>, 2021, <span class="bold">26</span>(7), 1680–1688 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.drudis.2021.06.003" title="DOI Link to resource 10.1016/j.drudis.2021.06.003">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhvFGjsbjL" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34119668%5Buid%5D" title="PubMed Link to resource 34119668">PubMed</a>.</span></li>
      <li><span id="cit688">
          M. Langione, F. Bobier, C. Meier, S. Hasenfuss and U. Schulze, <span class="italic">Will Quantum Computing Transform Biopharma R&amp;D</span>? Accessed: 2021-10-12.</span></li>
      <li><span id="cit689">
          M. Evers, A. Heid and E. Ostojic, <span class="italic">Pharma's digital Rx: Quantum computing in drug research and development</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit690">
          <span class="italic">Merck KGaA, Darmstadt, Germany, and HQS Quantum Simulations Cooperate in Quantum Computing</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit691">
          <span class="italic">Rahko announces Merck collaboration</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit692">
          C. Metinko, <span class="italic">Zapata Computing Raises $ 38M As Quantum Computing Nears</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit693">
          L. Siow, <span class="italic">ProteinQure Collaborates with AstraZeneca to Design Novel Peptide Therapeutics</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit694">
          M. Beyer, <span class="italic">CrownBio and JSR Life Sciences Partner with Cambridge Quantum Computing to Leverage Quantum Machine Learning for Novel Cancer Treatment Biomarker Discovery</span>, Accessed: 2021-10-12.</span></li>
      <li><span id="cit695">C. Zhang, S. Bengio, M. Hardt, B. Recht and O. Vinyals, <span class="italic">Commun. ACM</span>, 2021, <span class="bold">64</span>(3), 107–115 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/3446776" title="DOI Link to resource 10.1145/3446776">CrossRef</a>.</span></li>
      <li><span id="cit696">J. J. Hopfield, <span class="italic">Proc. Natl. Acad. Sci. U. S. A.</span>, 1982, <span class="bold">79</span>, 2554–2558 <a target="_blank" class="DOILink" href="https://doi.org/10.1073/pnas.79.8.2554" title="DOI Link to resource 10.1073/pnas.79.8.2554">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ASTN%3A280%3ADyaL383it1WktQ%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=6953413%5Buid%5D" title="PubMed Link to resource 6953413">PubMed</a>.</span></li>
      <li><span id="cit697">L. G. Valiant, <span class="italic">Commun. ACM</span>, 1984, <span class="bold">27</span>, 1134–1142 <a target="_blank" class="DOILink" href="https://doi.org/10.1145/1968.1972" title="DOI Link to resource 10.1145/1968.1972">CrossRef</a>.</span></li>
      <li><span id="cit698">M. V. Tsodyks and M. K. Feigel’man, <span class="italic">Europhys. Lett.</span>, 1988, <span class="bold">6</span>(2), 101 <a target="_blank" class="DOILink" href="https://doi.org/10.1209/0295-5075/6/2/002" title="DOI Link to resource 10.1209/0295-5075/6/2/002">CrossRef</a>.</span></li>
      <li><span id="cit699">E. Gardner, <span class="italic">J. Phys. A: Math. Gen.</span>, 1988, <span class="bold">21</span>(1), 257 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0305-4470/21/1/030" title="DOI Link to resource 10.1088/0305-4470/21/1/030">CrossRef</a>.</span></li>
      <li><span id="cit700">G. Györgyi, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1990, <span class="bold">41</span>, 7097–7100 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.41.7097" title="DOI Link to resource 10.1103/PhysRevA.41.7097">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=9903140%5Buid%5D" title="PubMed Link to resource 9903140">PubMed</a>.</span></li>
      <li><span id="cit701">E. Barkai, D. Hansel and I. Kanter, <span class="italic">Phys. Rev. Lett.</span>, 1990, <span class="bold">65</span>, 2312–2315 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.65.2312" title="DOI Link to resource 10.1103/PhysRevLett.65.2312">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=10042513%5Buid%5D" title="PubMed Link to resource 10042513">PubMed</a>.</span></li>
      <li><span id="cit702">E. Barkai, D. Hansel and H. Sompolinsky, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1992, <span class="bold">45</span>, 4146–4161 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.45.4146" title="DOI Link to resource 10.1103/PhysRevA.45.4146">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=9907466%5Buid%5D" title="PubMed Link to resource 9907466">PubMed</a>.</span></li>
      <li><span id="cit703">A. Engel, H. M. Köhler, F. Tschepke, H. Vollmayr and A. Zippelius, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 1992, <span class="bold">45</span>, 7590–7609 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.45.7590" title="DOI Link to resource 10.1103/PhysRevA.45.7590">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=9906832%5Buid%5D" title="PubMed Link to resource 9906832">PubMed</a>.</span></li>
      <li><span id="cit704">
          F. Morone, F. Caltagirone, E. Harrison and G. Parisi, Replica theory and spin glasses, arXiv preprint arXiv:1409.2722,  2014.</span></li>
      <li><span id="cit705">L. Zdeborová and F. Krzakala, <span class="italic">Adv. Phys.</span>, 2016, <span class="bold">65</span>, 453–552 <a target="_blank" class="DOILink" href="https://doi.org/10.1080/00018732.2016.1211393" title="DOI Link to resource 10.1080/00018732.2016.1211393">CrossRef</a>.</span></li>
      <li><span id="cit706">A. Decelle, F. Krzakala, C. Moore and L. Zdeborová, <span class="italic">Phys. Rev. E: Stat., Nonlinear, Soft Matter Phys.</span>, 2011, <span class="bold">84</span>, 066106 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevE.84.066106" title="DOI Link to resource 10.1103/PhysRevE.84.066106">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22304154%5Buid%5D" title="PubMed Link to resource 22304154">PubMed</a>.</span></li>
      <li><span id="cit707">J. S. Yedidia, W. T. Freeman and Y. Weiss, 
            <span class="italic">et al.</span>
          , <span class="italic">Exploring Artificial Intelligence in the New Millennium</span>, 2003, <span class="bold">8</span>, 0018–9448 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Exploring%20Artificial%20Intelligence%20in%20the%20New%20Millennium%5BJour%5D%20AND%208%5Bvolume%5D%20AND%200018%5Bpage%5D%20and%202003%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit708">G. Carleo, I. Cirac, K. Cranmer, L. Daudet, M. Schuld, N. Tishby, L. Vogt-Maranto and L. Zdeborová, <span class="italic">Rev. Mod. Phys.</span>, 2019, <span class="bold">91</span>, 045002 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.91.045002" title="DOI Link to resource 10.1103/RevModPhys.91.045002">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXjtFOnur4%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit709">
          O. Y. Feng, R. Venkataramanan, C. Rush and R. J. Samworth, <span class="italic">A unifying tutorial on Approximate Message Passing</span>,  2021 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=A%20unifying%20tutorial%20on%20Approximate%20Message%20Passing%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit710">H. W. Lin, M. Tegmark and D. Rolnick, <span class="italic">J. Stat. Phys.</span>, 2017, <span class="bold">168</span>, 1223–1247 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s10955-017-1836-5" title="DOI Link to resource 10.1007/s10955-017-1836-5">CrossRef</a>.</span></li>
      <li><span id="cit711">J. R. L. de Almeida and D. J. Thouless, <span class="italic">J. Phys. A: Math. Gen.</span>, 1978, <span class="bold">11</span>(5), 983–990 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0305-4470/11/5/028" title="DOI Link to resource 10.1088/0305-4470/11/5/028">CrossRef</a>.</span></li>
      <li><span id="cit712">M. Mézard, G. Parisi and M. A. Virasoro, <span class="italic">Europhys. Lett.</span>, 1986, <span class="bold">1</span>, 77–82 <a target="_blank" class="DOILink" href="https://doi.org/10.1209/0295-5075/1/2/006" title="DOI Link to resource 10.1209/0295-5075/1/2/006">CrossRef</a>.</span></li>
      <li><span id="cit713">K. G. Wilson, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 1971, <span class="bold">4</span>, 3174–3183 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.4.3174" title="DOI Link to resource 10.1103/PhysRevB.4.3174">CrossRef</a>.</span></li>
      <li><span id="cit714">M. Gell-Mann and F. E. Low, <span class="italic">Phys. Rev.</span>, 1954, <span class="bold">95</span>, 1300–1312 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRev.95.1300" title="DOI Link to resource 10.1103/PhysRev.95.1300">CrossRef</a>.</span></li>
      <li><span id="cit715">L. P. Kadanoff, <span class="italic">Phys. Phys. Fiz.</span>, 1966, <span class="bold">2</span>, 263–272 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Phys.%20Phys.%20Fiz.%5BJour%5D%20AND%202%5Bvolume%5D%20AND%20263%5Bpage%5D%20and%201966%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit716">K. G. Wilson, <span class="italic">Phys. Rev. B: Condens. Matter Mater. Phys.</span>, 1971, <span class="bold">4</span>, 3174–3183 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevB.4.3174" title="DOI Link to resource 10.1103/PhysRevB.4.3174">CrossRef</a>.</span></li>
      <li><span id="cit717">K. G. Wilson, <span class="italic">Rev. Mod. Phys.</span>, 1975, <span class="bold">47</span>, 773–840 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/RevModPhys.47.773" title="DOI Link to resource 10.1103/RevModPhys.47.773">CrossRef</a>.</span></li>
      <li><span id="cit718">
          P. Mehta and D. J. Schwab, An exact mapping between the variational renormalization group and deep learning, arXiv preprint arXiv:1410.3831,  2014.</span></li>
      <li><span id="cit719">M. Koch-Janusz and Z. Ringel, <span class="italic">Nat. Phys.</span>, 2018, <span class="bold">14</span>, 578–582 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Nat.%20Phys.%5BJour%5D%20AND%2014%5Bvolume%5D%20AND%20578%5Bpage%5D%20and%202018%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit720">S. M. Apenko, <span class="italic">Phys. A</span>, 2012, <span class="bold">391</span>(1–2), 62–77 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.physa.2011.08.014" title="DOI Link to resource 10.1016/j.physa.2011.08.014">CrossRef</a>.</span></li>
      <li><span id="cit721">S. Sim, P. D. Johnson and A. Aspuru-Guzik, <span class="italic">Adv. Quantum Technol.</span>, 2019, <span class="bold">2</span>, 1900070 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/qute.201900070" title="DOI Link to resource 10.1002/qute.201900070">CrossRef</a>.</span></li>
      <li><span id="cit722">B. Collins and P. Ŝniady, <span class="italic">Commun. Math. Phys.</span>, 2006, <span class="bold">264</span>, 773–795 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s00220-006-1554-3" title="DOI Link to resource 10.1007/s00220-006-1554-3">CrossRef</a>.</span></li>
      <li><span id="cit723">
          A. Ambainis and J. Emerson, Quantum t-designs: t-wise independence in the quantum world, 2007.</span></li>
      <li><span id="cit724">T. Hubregtsen, J. Pichlmeier, P. Stecher and K. Bertels, <span class="italic">Quantum Mach. Intell.</span>, 2021, <span class="bold">3</span>, 1–19 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s42484-020-00025-7" title="DOI Link to resource 10.1007/s42484-020-00025-7">CrossRef</a>.</span></li>
      <li><span id="cit725">Y. Du, M.-H. Hsieh, T. Liu, S. You and D. Tao, <span class="italic">PRX Quantum</span>, 2021, <span class="bold">2</span>, 040337 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PRXQuantum.2.040337" title="DOI Link to resource 10.1103/PRXQuantum.2.040337">CrossRef</a>.</span></li>
      <li><span id="cit726">
          H.-C. Cheng, M.-H. Hsieh and P.-C. Yeh, 2015, arXiv preprint arXiv:1501.00559.</span></li>
      <li><span id="cit727">L. Banchi, J. Pereira and S. Pirandola, <span class="italic">PRX Quantum</span>, 2021, <span class="bold">2</span>, 040321 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PRXQuantum.2.040321" title="DOI Link to resource 10.1103/PRXQuantum.2.040321">CrossRef</a>.</span></li>
      <li><span id="cit728">J. R. McClean, S. Boixo, V. N. Smelyanskiy, R. Babbush and H. Neven, <span class="italic">Nat. Commun.</span>, 2018, <span class="bold">9</span>(1), 1–6 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-017-02088-w" title="DOI Link to resource 10.1038/s41467-017-02088-w">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1cXit1Chtb7I" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29317637%5Buid%5D" title="PubMed Link to resource 29317637">PubMed</a>.</span></li>
      <li><span id="cit729">A. W. Harrow and R. A. Low, <span class="italic">Commun. Math. Phys.</span>, 2009, <span class="bold">291</span>, 257–302 <a target="_blank" class="DOILink" href="https://doi.org/10.1007/s00220-009-0873-6" title="DOI Link to resource 10.1007/s00220-009-0873-6">CrossRef</a>.</span></li>
      <li><span id="cit730">M. Cerezo, A. Sone, T. Volkoff, L. Cincio and P. J. Coles, <span class="italic">Nat. Commun.</span>, 2021, <span class="bold">12</span>(1), 1–12 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-020-20314-w" title="DOI Link to resource 10.1038/s41467-020-20314-w">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33397941%5Buid%5D" title="PubMed Link to resource 33397941">PubMed</a>.</span></li>
      <li><span id="cit731">
          M. Larocca, P. Czarnik, K. Sharma, G. Muraleedharan, P. J. Coles and M. Cerezo, 2021, arXiv preprint arXiv:2105.14377.</span></li>
      <li><span id="cit732">S. Wang, E. Fontana, M. Cerezo, K. Sharma, A. Sone, L. Cincio and P. J. Coles, <span class="italic">Nat. Commun.</span>, 2021, <span class="bold">12</span>(1), 1–11 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41467-020-20314-w" title="DOI Link to resource 10.1038/s41467-020-20314-w">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33397941%5Buid%5D" title="PubMed Link to resource 33397941">PubMed</a>.</span></li>
      <li><span id="cit733">J. Romero and A. Aspuru-Guzik, <span class="italic">Adv. Quantum Technol.</span>, 2021, <span class="bold">4</span>, 2000003 <a target="_blank" class="DOILink" href="https://doi.org/10.1002/qute.202000003" title="DOI Link to resource 10.1002/qute.202000003">CrossRef</a>.</span></li>
      <li><span id="cit734">
          B. Khalid, S. H. Sureshbabu, A. Banerjee and S. Kais, 2022, arXiv preprint arXiv:2202.00112.</span></li>
      <li><span id="cit735">
          J. Erdmenger, K. T. Grosvenor and R. Jefferson, <span class="italic">Towards quantifying information flows: relative entropy in deep neural networks and the renormalization group</span>,  2021 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Towards%20quantifying%20information%20flows:%20relative%20entropy%20in%20deep%20neural%20networks%20and%20the%20renormalization%20group%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202021%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit736">E. Tang, <span class="italic">Phys. Rev. Lett.</span>, 2021, <span class="bold">127</span>, 060503 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.127.060503" title="DOI Link to resource 10.1103/PhysRevLett.127.060503">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhvValsr3L" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34420330%5Buid%5D" title="PubMed Link to resource 34420330">PubMed</a>.</span></li>
      <li><span id="cit737">
          E. Tang, <span class="italic">Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing</span>,  2019, pp. 217–228 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Proceedings%20of%20the%2051st%20Annual%20ACM%20SIGACT%20Symposium%20on%20Theory%20of%20Computing%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit738">B. Douçot and L. B. Ioffe, <span class="italic">Rep. Progress Phys.</span>, 2012, <span class="bold">75</span>, 072001 <a target="_blank" class="DOILink" href="https://doi.org/10.1088/0034-4885/75/7/072001" title="DOI Link to resource 10.1088/0034-4885/75/7/072001">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22790777%5Buid%5D" title="PubMed Link to resource 22790777">PubMed</a>.</span></li>
      <li><span id="cit739">A. G. Fowler, M. Mariantoni, J. M. Martinis and A. N. Cleland, <span class="italic">Phys. Rev. A: At., Mol., Opt. Phys.</span>, 2012, <span class="bold">86</span>, 032324 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.86.032324" title="DOI Link to resource 10.1103/PhysRevA.86.032324">CrossRef</a>.</span></li>
      <li><span id="cit740">X.-C. Yao, T.-X. Wang, H.-Z. Chen, W.-B. Gao, A. G. Fowler, R. Raussendorf, Z.-B. Chen, N.-L. Liu, C.-Y. Lu and Y.-J. Deng, 
            <span class="italic">et al.</span>
          , <span class="italic">Nature</span>, 2012, <span class="bold">482</span>, 489–494 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/nature10770" title="DOI Link to resource 10.1038/nature10770">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC38Xis1erur8%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=22358838%5Buid%5D" title="PubMed Link to resource 22358838">PubMed</a>.</span></li>
      <li><span id="cit741">
          L. Egan, D. M. Debroy, C. Noel, A. Risinger, D. Zhu, D. Biswas, M. Newman, M. Li, K. R. Brown and M. Cetina, <span class="italic">et al.</span>, 2020, arXiv preprint arXiv:2009.11482.</span></li>
      <li><span id="cit742">D. Aharonov and M. Ben-Or, <span class="italic">SIAM J. Comput.</span>, 2008, <span class="bold">38</span>(4)<small> DOI:<a class="DOILink" href="https://doi.org/10.1137/S0097539799359385" TARGET="_BLANK" title="DOI Link to 10.1137/S0097539799359385">10.1137/S0097539799359385</a></small>.</span></li>
      <li><span id="cit743">Z. Chen, K. J. Satzinger, J. Atalaya, A. N. Korotkov, A. Dunsworth, D. Sank, C. Quintana, M. McEwen, R. Barends and P. V. Klimov, 
            <span class="italic">et al.</span>
          , <span class="italic">Nature</span>, 2021, <span class="bold">595</span>, 383–387 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-021-03721-x" title="DOI Link to resource 10.1038/s41586-021-03721-x">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34262173%5Buid%5D" title="PubMed Link to resource 34262173">PubMed</a>.</span></li>
      <li><span id="cit744">A. Parra-Rodriguez, P. Lougovski, L. Lamata, E. Solano and M. Sanz, <span class="italic">Phys. Rev. A</span>, 2020, <span class="bold">101</span>, 022305 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevA.101.022305" title="DOI Link to resource 10.1103/PhysRevA.101.022305">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXmsFWntbc%253D" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit745">J. Eisert, D. Hangleiter, N. Walk, I. Roth, D. Markham, R. Parekh, U. Chabaud and E. Kashefi, <span class="italic">Nat. Rev. Phys.</span>, 2020, <span class="bold">2</span>, 382–390 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s42254-020-0186-4" title="DOI Link to resource 10.1038/s42254-020-0186-4">CrossRef</a>.</span></li>
      <li><span id="cit746">J. S. Schreck, C. W. Coley and K. J. Bishop, <span class="italic">ACS Cent. Sci.</span>, 2019, <span class="bold">5</span>, 970–981 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acscentsci.9b00055" title="DOI Link to resource 10.1021/acscentsci.9b00055">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXhtVKiu7jP" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31263756%5Buid%5D" title="PubMed Link to resource 31263756">PubMed</a>.</span></li>
      <li><span id="cit747">J. L. Baylon, N. A. Cilfone, J. R. Gulcher and T. W. Chittenden, <span class="italic">J. Chem. Inf. Model.</span>, 2019, <span class="bold">59</span>, 673–688 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jcim.8b00801" title="DOI Link to resource 10.1021/acs.jcim.8b00801">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC1MXpvFCqug%253D%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=30642173%5Buid%5D" title="PubMed Link to resource 30642173">PubMed</a>.</span></li>
      <li><span id="cit748">
          H. Dai, C. Li, C. W. Coley, B. Dai and L. Song, 2020, arXiv preprint arXiv:2001.01408.</span></li>
      <li><span id="cit749">
          K. Lin, Y. Xu, J. Pei and L. Lai, 2019, arXiv preprint arXiv:1906.02308.</span></li>
      <li><span id="cit750">B. Liu, B. Ramsundar, P. Kawthekar, J. Shi, J. Gomes, Q. Luu Nguyen, S. Ho, J. Sloane, P. Wender and V. Pande, <span class="italic">ACS Cent. Sci.</span>, 2017, <span class="bold">3</span>, 1103–1113 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acscentsci.7b00303" title="DOI Link to resource 10.1021/acscentsci.7b00303">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BC2sXhsVahu7fI" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=29104927%5Buid%5D" title="PubMed Link to resource 29104927">PubMed</a>.</span></li>
      <li><span id="cit751">S. Johansson, A. Thakkar, T. Kogej, E. Bjerrum, S. Genheden, T. Bastys, C. Kannas, A. Schliep, H. Chen and O. Engkvist, <span class="italic">Drug Discov. Today Technol.</span>, 2019, <span class="bold">32–33</span>, 65–72 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.ddtec.2020.06.002" title="DOI Link to resource 10.1016/j.ddtec.2020.06.002">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33386096%5Buid%5D" title="PubMed Link to resource 33386096">PubMed</a>.</span></li>
      <li><span id="cit752">
          H.-P. Breuer, F. Petruccione, <span class="italic">et al.</span>, <span class="italic">The theory of open quantum systems</span>, Oxford University Press on Demand,  2002 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=The%20theory%20of%20open%20quantum%20systems%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202002%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit753">Z. Hu, R. Xia and S. Kais, <span class="italic">Sci. Rep.</span>, 2020, <span class="bold">10</span>(1), 1–9 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41598-019-56847-4" title="DOI Link to resource 10.1038/s41598-019-56847-4">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=31913322%5Buid%5D" title="PubMed Link to resource 31913322">PubMed</a>.</span></li>
      <li><span id="cit754">K. Lin, J. Peng, F. L. Gu and Z. Lan, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 10225–10234 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c02672" title="DOI Link to resource 10.1021/acs.jpclett.1c02672">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXit1aktL%252FM" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34647736%5Buid%5D" title="PubMed Link to resource 34647736">PubMed</a>.</span></li>
      <li><span id="cit755">L. E. Herrera Rodriguez and A. A. Kananenka, <span class="italic">J. Phys. Chem. Lett.</span>, 2021, <span class="bold">12</span>, 2476–2483 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.jpclett.1c00079" title="DOI Link to resource 10.1021/acs.jpclett.1c00079">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXls1Wqs7Y%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=33666085%5Buid%5D" title="PubMed Link to resource 33666085">PubMed</a>.</span></li>
      <li><span id="cit756">P. P. Mazz, D. Zietlow, F. Carollo, S. Andergassen, G. Martius and I. Lesanovsky, <span class="italic">Phys. Rev. Res.</span>, 2021, <span class="bold">3</span>, 023084 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.3.023084" title="DOI Link to resource 10.1103/PhysRevResearch.3.023084">CrossRef</a>.</span></li>
      <li><span id="cit757">I. Luchnikov, S. Vintskevich, D. Grigoriev and S. Filippov, <span class="italic">Phys. Rev. Lett.</span>, 2020, <span class="bold">124</span>, 140502 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevLett.124.140502" title="DOI Link to resource 10.1103/PhysRevLett.124.140502">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXptF2nu74%253D" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=32338970%5Buid%5D" title="PubMed Link to resource 32338970">PubMed</a>.</span></li>
      <li><span id="cit758">F. Häse, C. Kreisbeck and A. Aspuru-Guzik, <span class="italic">Chem. Sci.</span>, 2017, <span class="bold">8</span>, 8419–8426 <a target="_blank" class="RSCLink" href="http://xlink.rsc.org/?doi=C7SC03542J&amp;newsite=1" title="Link to RSC resource DOI:10.1039/C7SC03542J">RSC</a>.</span></li>
      <li><span id="cit759">C. K. Lee, P. Patil, S. Zhang and C. Y. Hsieh, <span class="italic">Phys. Rev. Res.</span>, 2021, <span class="bold">3</span>, 023095 <a target="_blank" class="DOILink" href="https://doi.org/10.1103/PhysRevResearch.3.023095" title="DOI Link to resource 10.1103/PhysRevResearch.3.023095">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhsFSgtLfO" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit760">A. Khan, E. Huerta and A. Das, <span class="italic">Phys. Lett. B</span>, 2020, <span class="bold">808</span>, 135628 <a target="_blank" class="DOILink" href="https://doi.org/10.1016/j.physletb.2020.135628" title="DOI Link to resource 10.1016/j.physletb.2020.135628">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3cXhsVKktLfE" title="Link to resource in CAS">CAS</a>.</span></li>
      <li><span id="cit761">T. Villmann, A. Engelsberger, J. Ravichandran, A. Villmann and M. Kaden, <span class="italic">Neural Comput. Appl.</span>, 2020, 1–10 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Neural%20Comput.%20Appl.%5BJour%5D%20AND%20%5Bvolume%5D%20AND%201%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit762">
          C. Bellinger, R. Coles, M. Crowley and I. Tamblyn, Reinforcement Learning in a Physics-Inspired Semi-Markov Environment, <span class="italic">Canadian Conference on Artificial Intelligence</span>,  2020, pp. 55–56 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=Canadian%20Conference%20on%20Artificial%20Intelligence%5BJour%5D%20AND%20%5Bvolume%5D%20AND%20%5Bpage%5D%20and%202020%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit763">
          M. Trenti, L. Sestini, A. Gianelle, D. Zuliani, T. Felser, D. Lucchesi and S. Montangero, 2020, arXiv preprint arXiv:2004.13747.</span></li>
      <li><span id="cit764">P. Tiwari and M. Melucci, <span class="italic">IEEE Access</span>, 2019, <span class="bold">7</span>, 42354–42372 <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/sites/entrez?orig_db=PubMed&amp;db=pubmed&amp;cmd=Search&amp;term=IEEE%20Access%5BJour%5D%20AND%207%5Bvolume%5D%20AND%2042354%5Bpage%5D%20and%202019%5Bpdat%5D" title="Search PubMed for this citation">Search PubMed</a>.</span></li>
      <li><span id="cit765">F. Musil, A. Grisafi, A. P. Bartók, C. Ortner, G. Csányi and M. Ceriotti, <span class="italic">Chem. Rev.</span>, 2021, <span class="bold">121</span>, 9759–9815 <a target="_blank" class="DOILink" href="https://doi.org/10.1021/acs.chemrev.1c00021" title="DOI Link to resource 10.1021/acs.chemrev.1c00021">CrossRef</a> <a target="_blank" class="COILink" href="/en/content/coiresolver?coi=1%3ACAS%3A528%3ADC%252BB3MXhs1aisL3J" title="Link to resource in CAS">CAS</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34310133%5Buid%5D" title="PubMed Link to resource 34310133">PubMed</a>.</span></li>
      <li><span id="cit766">G. E. Karniadakis, I. G. Kevrekidis, L. Lu, P. Perdikaris, S. Wang and L. Yang, <span class="italic">Nat. Rev. Phys.</span>, 2021, <span class="bold">3</span>, 422–440 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s42254-021-00314-5" title="DOI Link to resource 10.1038/s42254-021-00314-5">CrossRef</a>.</span></li>
      <li><span id="cit767">Z. Chen, K. Satzinger, J. Atalaya, A. Korotkov, A. Dunsworth, D. Sank, C. Quintana, M. McEwen, R. Barends, P. Klimov, S. Hong, C. Jones, A. Petukhov, D. Kafri, S. Demura, B. Burkett, C. Gidney, A. Fowler, A. Paler and J. Kelly, <span class="italic">Nature</span>, 2021, <span class="bold">595</span>, 383–387 <a target="_blank" class="DOILink" href="https://doi.org/10.1038/s41586-021-03721-x" title="DOI Link to resource 10.1038/s41586-021-03721-x">CrossRef</a> <a target="_blank" class="PMedLink" href="http://www.ncbi.nlm.nih.gov/pubmed/?term=34262173%5Buid%5D" title="PubMed Link to resource 34262173">PubMed</a>.</span></li>
      <li><span id="cit768">IBM's roadmap for scaling quantum technology, https://research.ibm.com/blog/ibm-quantum-roadmap, Accessed: 2021-10-12.</span></li>
    </ol>
    
    
    
    
    
  <hr/><table><tr><td><h3>Footnotes</h3></td></tr><tr><td><span class="sup_ref">† <span id="fn1">These authors contributed equally to this work.</span></span></td></tr><tr><td><span class="sup_ref">‡ <span id="fn2">These authors contributed equally to this work.</span></span></td></tr></table><table><tr><td><hr/></td></tr><tr><td><b>This journal is © The Royal Society of Chemistry 2022</b></td></tr></table></div></div></div></body><script src="/content/scripts/CrossMarkIE.js"> </script><SaxonLicenceTest result="pass" message="Licenced Enterprise Edition [ EE 9.3.0.4 ]"/></html>