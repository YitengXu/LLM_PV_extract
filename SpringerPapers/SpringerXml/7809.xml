<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41598-019-41316-9</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" specific-use="web-only" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41598</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title><abbrev-journal-title abbrev-type="publisher">Sci Rep</abbrev-journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41598-019-41316-9</article-id><article-id pub-id-type="manuscript">41316</article-id><article-id pub-id-type="doi">10.1038/s41598-019-41316-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/705/117</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/925/927/1021</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/4077/4072/4062</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/119/118</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/119</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/139</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/141</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Using a Novel Transfer Learning Method for Designing Thin Film Solar Cells with Enhanced Quantum Efficiencies</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6615-1552</contrib-id><name><surname>Kaya</surname><given-names>Mine</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2835-863X</contrib-id><name><surname>Hajimirza</surname><given-names>Shima</given-names></name><address><email>shima.hm@tamu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs41598019413169_cor2">b</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 4687 2082</institution-id><institution-id institution-id-type="GRID">grid.264756.4</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Texas A&amp;M University, 3123 TAMU</institution></institution-wrap><addr-line content-type="postcode">77843-3123</addr-line><addr-line content-type="city">College Station</addr-line><addr-line content-type="state">TX</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs41598019413169_cor2"><label>b</label><email>shima.hm@tamu.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>3</month><year>2019</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2019</year></pub-date><volume>9</volume><issue seq="30887">1</issue><elocation-id>5034</elocation-id><history><date date-type="registration"><day>6</day><month>3</month><year>2019</year></date><date date-type="received"><day>8</day><month>11</month><year>2018</year></date><date date-type="accepted"><day>1</day><month>3</month><year>2019</year></date><date date-type="online"><day>22</day><month>3</month><year>2019</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2019</copyright-statement><copyright-year>2019</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">In this study a new method for design optimization is proposed that is based on “transfer learning”. The proposed framework improves the accuracy and efficiency of surrogate-based optimization. A surrogate model is an approximation to a costly black-box function that can be used for more efficient search of optimal points. When design specifications change, the objective function changes too. Therefore, there is a need for a new surrogate model. However, the concept of transfer learning can be applied to refit the new surrogate more efficiently. In other words insights from previous experiences can be applied to learning and optimizing the new function. We use the proposed method in a particular problem pertaining to the design of “thin film multilayer solar cells”, where the goal is to maximize the external quantum efficiency of photoelectric conversion. The results show that the accuracy of the surrogate model is improved by 2–3 times using the transfer learning approach, using only half as many training data points as the original model. In addition, by transferring the design knowledge from one particular set of materials to another similar set of materials in the thin film structure, the surrogate-based optimization is improved, and is it obtained with far less computational time.</p></abstract><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Research</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>3</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41598_2019_Article_41316.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Science, Humanities and Social Sciences, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Science, Humanities and Social Sciences, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Science, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Science (multidisciplinary)</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Machine learning has empowered important technological developments in the last decades benefiting many engineering applications. Machine learning algorithms resemble human learning by collecting data for the task in hand and establishing reasonable connections between inputs and outputs. However, the conventional methods of machine learning start learning from scratch for every new task, unlike the way human brain normally functions. The ability of human brain to transfer knowledge among tasks can lend itself to smarter machine learning algorithms. This is officically known as transfer learning which has proven to be a promising concept in data science.</p><p id="Par3">Transfer learning has received attention of data scientists as a methodology for taking advantage of available training data/models from related tasks and applying them to the problem in hand<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The technique has been useful in many engineering applications where learning tasks can take a variety of forms including classification, regression and statistical inference. Example of classification tasks that has benefited from transfer learning include image<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>, web document<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>, brain-computer interface<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>, music<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and emotion<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> classification. Regression transfer has received less attention compared to transfer classification<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Nonetheless, there are several studies on transfer learning in regression problems such as configurable software performance prediction<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, shape model matching in medical applications<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> and visual tracking<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p><p id="Par4">Artificial neural networks (ANN) are one of the regression methods with significantly generalizable learning capabilities<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup>. The advance of computation and parallel processing in training large ANNs have led to the very popular domain of deep learning. The multilayer structure of neural networks provides a suitable framework for knowledge transfer in both regression and classification tasks. Specifically, some of the neurons/layers (<italic>e.g</italic>., the hidden layers) of the structure can be shared among tasks while the remaining neurons/layers (<italic>e.g</italic>., the output layer) determine task specific behaviors. The former layer is generally called the <italic>general</italic> layer which represents similarities between different tasks and the latter is the <italic>specific</italic> layer<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. This flexibility has resulted in many successful implementations of transfer learning in (deep) neural networks for applications such as wind speed prediction<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>, remote sensing<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, text classification<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> and image classification<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p><p id="Par5">Despite the above-mentioned applications, transfer learning in optimization problems has not been evaluated thoroughly except a few fields. There are reports of the use of transfer learning in automatic hyper-parameter tuning problems<sup><xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR26">26</xref></sup> to increase training speed and improve prediction accuracy. Transfer learning is also suitable for the iterative nature of the engineering design where surrogate-based optimization is utilized due to the complexity of the objective function. Li <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> proposed a transfer learning based design space exploration method for microprocessor design. Min <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> investigated the use of transfer learning in aircraft design problems and demonstrated the effectiveness of the proposed algorithms. Gupta <italic>et al</italic>.<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> reviewed the recent progress in transfer learning in optimization problems and categorized them as sequential, multitasking and multiform transfer optimizations. Neural networks are ideal tools for surrogate model building in complex tasks particularly for knowledge transfer, due to excellent prediction performance and the ability to handle high dimensional and highly nonlinear data<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>.</p><p id="Par6">Another area where transfer learning can be useful is the optimization of different but similar black-box functions with high computational cost. This is often the case in many physical or industrial design optimization problems. Suppose one would like to determine the optimal parameters of a time-consuming function. Because the evaluation of the function is intense, the practical approach to optimization is to use past funtcion samples to approximate the function behavior and generate smarter search points. Most heuristic particle based search algorithms (e.g. Genetic algorithm, particle swarm optimization, etc.) and the well-known Bayesian optimization implicitly use a similar surface learning concept. The approximate model is called a <italic>surrogate model</italic>, and this approach to optimization is called <italic>surrogate-based optimization</italic>. The surrogate model is essentially a regression/machine learning tool. Now suppose that one is dealing with optimization of the same physical objective function but in multiple different settings. The settings can be different boundary or initial conditions, different sets of physical characteristics, different environments or materials, or any other practical variation. The functions can naturally be assumed to have similar surface patterns with unique features. As a few concrete examples, imagine designing an airfoil with optimal aerodynamic properties under different settings of speed, altitude, allowable material types, etc. Every design problem is unique, but the objective functions are correlated as they pertain to the same underlying physical function (e.g. sheer stress, drag, etc.). These correlations can be captured and used across various settings. Therefore, once a surrogate model is fit and learned for one objective function, it is expected that the knowledge can be transfered to more accurately or more efficiently fit a surrogate model for another similar function. If the process improves the efficiancy and accuracy of surrogate model fitting, then it is expected that the black-box optimization is improved in general. In other words: more optimal values can be found with less computation time.</p><p id="Par7">An area where the transfer learning surrogate-based optimization can be advantageous is material design problems. High fidelity simulations are computationally costly and there are many material choices for the individual parts, resulting in different settings. In particular, we are interested in a material design problem at nano-scale related to multilayer thin film solar cells. For a fixed set of materials, the objective is to choose the dimensions of the layers. These dimensions affect the photo-electric properties of the solar cell in complex ways that are hard to analytically express or anticipate<sup><xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. Therefore intense computational FDTD simulations must be used for function evalaution, and hence the design is a time consuming global optimization<sup><xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR33">33</xref></sup>. Recently, we have shown that surrogate-based optimization methods can be used to solve optimization problems of this sort, and we have established their efficacy in several thin film design problems<sup><xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR36">36</xref></sup>. Consequently, the computational costs for completing optimizations were significantly reduced compared to traditional search methods. In the present study, we aim to further demonstrate that by using transfer learning both accuracy and speed of optimizations can be additionally improved.</p><p id="Par8">In this study, a novel neural-network transfer learning based optimization framework is proposed. We demonstrate that the proposed framework can expedite and improve the design of multi-layered thin film structure. We assume that at least one optimization has taken place (base case). The aim is to repeat the optimization for structures with different material choices (transfer cases). The knowledge gained from the base case is transferred to the new problem by means of neural network layers. Improvement in the prediction performance due to transfer learning is studied using the out-sample mean squared error metric. The work has therefore two novelties: 1) proposing a neural network transfer learning based optimization framework for solving complex optimization problems and, 2) using the proposed method to design a multi-layer thin film solar cell structure. The organization of this paper is as follows: first the neural network based transfer optimization method is described. Then, the thin film solar cell design is explained in section 3. The training and optimization results are presented and discussed for base case and transfer cases in section 4.</p></sec><sec id="Sec2"><title>Description of the Method</title><p id="Par9">The present method enables knowledge transfer between two optimization problems using transfer learning. The method consists of a base surrogate model to be used in surrogate-based optimization, and a transfer learning framework to share the gained knowledge. Surrogates (metamodels) are regression tools to map the input space <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mi mathvariant="script">X</mml:mi></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{X}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq1.gif"/></alternatives></inline-formula> to the output space <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mi mathvariant="script">Y</mml:mi></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathscr{Y}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq2.gif"/></alternatives></inline-formula> using low fidelity models. The response of the surrogate model can be expressed as:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math display="block" id="Equ1_Math"><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ε</mml:mi><mml:mo>,</mml:mo></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F({\bf{x}})={{\bf{y}}}_{{\bf{t}}}+\varepsilon ,$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ1.gif"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">Y</mml:mi></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{y}}}_{{\bf{t}}}\in {\mathscr{Y}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq3.gif"/></alternatives></inline-formula> is the real output, <italic>F</italic>(<bold><italic>x</italic></bold>) is the objective function approximation at <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>∈</mml:mo><mml:mi mathvariant="script">X</mml:mi></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{x}}\in {\mathscr{X}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq4.gif"/></alternatives></inline-formula> and <italic>ε</italic> is the error between the real and the predicted outputs. <italic>F</italic> is obtained by an iterative training procedure where a training dataset of input-output pairs are fed to the regressor. As a result of the training, coefficients of the predefined metamodel (in this case the weight and biases for multilayer neural networks) are obtained.</p><p id="Par10">Depending on the similarity between the input-output spaces, the knowledge can be transferred from one domain (<italic>source)</italic> to another (<italic>target)</italic>. This transfer can be achieved in many ways depending on the used metamodel. Knowledge transfer using Gaussian Processes for instance can be achieved by learning a joint probability distribution and defining a common response surface<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Knowledge transfer in neural networks was previously recommended via shared layers<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. In this study, we assume that we already have at least one previously trained surrogate model from a previous optimization problem. The knowledge is then transferred from this model to a new optimization problem. We propose to use a transfer learning assisted surrogate based optimization via multilayer neural networks. The aim of the present method is to improve the accuracy of the surrogate model with the same or fewer number of function evaluations. To do so, one hidden layer of the previously trained network can be borrowed as an intermediate layer. The dimensions of the new hidden layer then becomes <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>×</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{1}^{0}\times {R}_{1}^{1}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq5.gif"/></alternatives></inline-formula> where superscripts 0 and 1 refer to the base case and the first transfer learning sequence. Therefore the input space is transformed to another space through the previously gained knowledge. This method is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The dimensions of the input and output spaces can be same or different. In the case of different dimensions, knowledge is transferred between the matching features and the rest is treated as usual. Thus the method reduces to a dimensionality reduction approach and the accuracy of the new predictions is expected to be improved due to the similarity between the subspaces in the two different input spaces.<fig id="Fig1"><label>Figure 1</label><caption xml:lang="en"><p>Schematic of neural network with transfer learning for a single output.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig1_HTML.png"/></fig></p><p id="Par11">The output of a two layer feed forward neural network is calculated from:<disp-formula id="Equ2"><label>2</label><alternatives><mml:math display="block" id="Equ2_Math"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mn>0</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y={F}^{0}({\bf{x}})={f}_{2}^{0}({W}_{2}^{0}{f}_{1}^{0}({W}_{1}^{0}{\bf{x}})),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ2.gif"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{1}^{0}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{2}^{0}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq7.gif"/></alternatives></inline-formula> are the coefficient matrices of the base case NN found from iterative training. <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mspace width="-.25em"/><mml:mo>⋅</mml:mo><mml:mspace width="-.25em"/><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${f}_{j}(\,\cdot \,)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq8.gif"/></alternatives></inline-formula> is the functional operation at the <italic>j</italic><sup>th</sup> layer, such as sigmoid and linear function<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. The knowledge transfer is then accomplished by transferring the hidden layer of the base case to the new case, expressed as:<disp-formula id="Equ3"><label>3</label><alternatives><mml:math display="block" id="Equ3_Math"><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi>F</mml:mi></mml:mrow><mml:mn>1</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y={F}^{1}({\bf{x}})={f}_{2}^{1}({W}_{2}^{1}{f}_{1}^{1}({W}_{1}^{1}{f}_{1}^{0}({W}_{1}^{0}{\bf{x}}))),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ3.gif"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><mml:math id="IEq9_Math"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{1}^{0}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq9.gif"/></alternatives></inline-formula> is transferred from the base case. Training of the new case is done to find <inline-formula id="IEq10"><alternatives><mml:math id="IEq10_Math"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{1}^{1}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq10.gif"/></alternatives></inline-formula> and <inline-formula id="IEq11"><alternatives><mml:math id="IEq11_Math"><mml:msubsup><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${W}_{2}^{1}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq11.gif"/></alternatives></inline-formula>. When another case is to be optimized in the same manner, the same procedure can be repeated or the trained layer of the new case can be transferred. One drawback of the proposed method is the increase in the number of coefficients of the neural network if <inline-formula id="IEq12"><alternatives><mml:math id="IEq12_Math"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>&gt;</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{1}^{0} &gt; {R}_{0}^{1}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq12.gif"/></alternatives></inline-formula> the new number of coefficients to train increases from <inline-formula id="IEq13"><alternatives><mml:math id="IEq13_Math"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{1}^{1}({R}_{0}^{1}+1)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq13.gif"/></alternatives></inline-formula> to <inline-formula id="IEq14"><alternatives><mml:math id="IEq14_Math"><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{1}^{1}({R}_{1}^{0}+1)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq14.gif"/></alternatives></inline-formula>, which may result in overfitting<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p><p id="Par12">The surrogate based optimization procedure starts with the design of experiment (sampling)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. Then, the outputs of the forward problem are evaluated at the sampled points using the simulation tool. The input/output pairs constitute the training set which is then fed to the NN trainer. All of the data is not used at once for training. Rather, it is split into training and validation. The validation error is monitored to estimate the out-sample performance of the model. The neural network is trained using one of the most widely used training algorithms, called the Levenberg-Marquardt (LM) method with Gauss-Newton approximation for Hessian<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. For optimization, simulated annealing<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> is used to optimize the surrogate objective function. The details of these methods can be found in the Supporting Information.</p><p id="Par13">The performance of a predictive model can be quantified considering the validation set. The most common performance metric is the mean squared error defined as:<disp-formula id="Equ4"><label>4</label><alternatives><mml:math display="block" id="Equ4_Math"><mml:mi>M</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$MSE=\frac{1}{{N}_{j}}\sum _{i=1}^{{N}_{j}}{\varepsilon }_{i}^{2},$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ4.gif"/></alternatives></disp-formula>where N is the number of data, <italic>j</italic> = <italic>T</italic>, <italic>V</italic> for training and validation sets respectively. <italic>ε</italic><sub><italic>i</italic></sub> is the error between real and approximate output for <italic>i</italic><sup>th</sup> instance (see equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>)).</p></sec><sec id="Sec3"><title>Multi-Layer Thin Film Solar Cell Optimization</title><p id="Par14">We can use the explained methodology in a design optimization for multi-layer thin film solar cells. For this purpose, a simple multilayer solar cell structure is used consisting of an absorber, an antireflective coating, a back-reflector metal layer and interlayers stacked together as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. A solar cell functions on the photovoltaic principle where an electron-hole pair inside a semiconductor is created due to photon absorption. Completing the electrical circuit, the electron generates photocurrent. The interaction between the incoming light and the multilayer structure of solar cell is explained by the Maxwell’s electromagnetic theory since the characteristic length of thin film solar cells and the operation wavelengths are at the same order of magnitude (0.1−1 μm). Light-matter interaction in the near field region provides unique properties which strongly depend on the dimensions of the thin film structures. Therefore a careful optimization of the thin film geometry is required to maximize the solar cell efficiency.<fig id="Fig2"><label>Figure 2</label><caption xml:lang="en"><p>Multilayer solar cell.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig2_HTML.png"/></fig></p><p id="Par15">In addition to the dimensions, the choice of materials used in the solar cell layers greatly affects the optical and electrical properties. On the other hand, when the material choices are included as a design variable, the optimization problem becomes a mixed-integer programming which is known to be computationally costly. Furthermore, for the present problem where the optimizations are done one by one, the optimization study should be repeated (<italic>m</italic><sub>1</sub>×<italic>m</italic><sub>2</sub>× … ×<italic>m</italic><sub><italic>d</italic></sub>) times for all possible material combinations where <italic>d</italic> is the input space dimension and <italic>m</italic><sub><italic>j</italic></sub> (1 ≤ <italic>j</italic> ≤ <italic>d</italic>) is the number of choices for the <italic>j</italic><sup>th</sup> input. In this case, knowledge transfer between different material combination tasks is worthwhile, as similar geometries with different material combinations can have similar opto-electrical responses. In general, the initial assumption is that source and target domains are similar<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Although, sometimes the false similarity assumption can case negative transfer and hurt the learning<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Therefore the similarity assumption must be monitored and evaluated carefully.</p><p id="Par16">An efficient solar cell must provide desirable optical and electrical properties which can be quantified by the external quantum efficiency (EQE). EQE is defined as the ratio of number of generated electrons to the number of incident photons on the solar cell. Previously, a probabilistic expression for EQE was developed as follows<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>:<disp-formula id="Equ5"><label>5</label><alternatives><mml:math display="block" id="Equ5_Math"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width=".25em"/></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\eta }_{e}=\frac{{N}_{p}}{{N}_{i}}\frac{2{L}_{D}}{{t}_{A}}(1-\exp (-{t}_{A}/2{L}_{D}))\,$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ5.gif"/></alternatives></disp-formula>where <italic>N</italic><sub><italic>p</italic></sub> and <italic>N</italic><sub><italic>i</italic></sub>are the number of photons absorbed and incident respectively, <italic>t</italic><sub><italic>A</italic></sub> is the absorber layer thickness and <italic>L</italic><sub><italic>D</italic></sub> is the diffusion length of the semiconductor material used as the absorber (<italic>L</italic><sub><italic>D</italic></sub> ≈ 100 nm). The proposed EQE model (equation (<xref rid="Equ5" ref-type="disp-formula">5</xref>)) was validated with experiment results<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The EQE of a Ag/ZnO:Al/a-Si/ITO solar cell was measured and absorbed power in the active layer was calculated using FDTD method. The same absorption profile is used to calculate EQE using equation (<xref rid="Equ5" ref-type="disp-formula">5</xref>) for <italic>t</italic><sub><italic>A</italic></sub> = 100 nm and <italic>L</italic><sub><italic>D</italic></sub> = 100 nm. The comparison of experiments and present calculations based on absorptivity is given in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. FDTD simulations are performed using a commercial software by Lumerical Inc.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> Note that the model matches closely with the experiments for most of the relevant spectrum. The details of this probabilistic model can be found in the previous study of the authors<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>.<fig id="Fig3"><label>Figure 3</label><caption xml:lang="en"><p>Comparison of measured and calculated EQE and simulated absorptivity profile for Ag/ZnO:Al/a-Si/ITO solar cell.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig3_HTML.png"/></fig></p><p id="Par17">After doing necessary replacements, EQE becomes:<disp-formula id="Equ6"><label>6</label><alternatives><mml:math display="block" id="Equ6_Math"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>exp</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mspace width=".25em"/><mml:msubsup><mml:mo>∫</mml:mo><mml:mi mathvariant="normal">Λ</mml:mi><mml:mspace width=".25em"/></mml:msubsup><mml:mi>λ</mml:mi><mml:mi>α</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>,</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>λ</mml:mi></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\eta }_{e}({\bf{x}})=\frac{2{L}_{D}}{hc{N}_{i}}\frac{1-\exp (-{t}_{A}/2{L}_{D})}{{t}_{A}}\,{\int }_{{\rm{\Lambda }}}^{\,}\lambda \alpha ({\bf{x}},\lambda )I(\lambda )d\lambda $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ6.gif"/></alternatives></disp-formula>where <italic>λ</italic> is wavelength, Λ is the relevant spectrum, <italic>α</italic>(<bold>x</bold>, <italic>λ</italic>) is absorptivity, <italic>I</italic>(<italic>λ</italic>) is AM1.5 standard spectrum and <inline-formula id="IEq15"><alternatives><mml:math id="IEq15_Math"><mml:mi mathvariant="bold">x</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>L</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:math><tex-math id="IEq15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{x}}={[{t}_{ARC},{t}_{IL1},{t}_{A},{t}_{IL2},{t}_{M}]}^{T}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_IEq15.gif"/></alternatives></inline-formula> is the geometry vector.</p><p id="Par18">The optimization problem then becomes:<disp-formula id="Equa"><alternatives><mml:math display="block" id="Equa_Math"><mml:munder><mml:mrow><mml:mi mathvariant="normal">max</mml:mi></mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:munder><mml:mspace width=".25em"/><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math><tex-math id="Equa_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mathop{{\rm{\max }}}\limits_{{\bf{x}}}\,{\eta }_{e}({\bf{x}}),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equa.gif"/></alternatives></disp-formula><disp-formula id="Equ7"><label>7</label><alternatives><mml:math display="block" id="Equ7_Math"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">L</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="bold">x</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">U</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:math><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{x}}}_{{\bf{L}}} &lt; {\bf{x}} &lt; {{\bf{x}}}_{{\bf{U}}}.$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ7.gif"/></alternatives></disp-formula></p><p id="Par19">The surrogate objective function can be calculated using the surrogate model of absorptivity, <italic>f</italic>(<bold>x</bold>, <italic>λ</italic>) ≈ <italic>α</italic>(<bold>x</bold>, <italic>λ</italic>) instead of EQE, since EQE can be calculated using the spectral absorptivity (see equation (<xref rid="Equ6" ref-type="disp-formula">6</xref>)). Therefore, the computational cost is further reduced by considering the wavelength as an input variable of the surrogate model and calculating EQE accordingly.</p><p id="Par20">Once an optimization study was carried out for a base case, the present method can be used to optimize a solar cell structure with the same geometry but different materials. For example, once we optimize an ITO/ZnO/P3HT:PCBM/MoO<sub>3</sub>/Al solar cell structure as a base case, less effort should be necessary for the optimization of a five layer solar cell consisting of different materials. For this purpose, a base case and transfer cases are selected as follows:<disp-formula id="Equ8"><label>8</label><alternatives><mml:math display="block" id="Equ8_Math"><mml:mtable columnalign="left"><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>Z</mml:mi><mml:mi>n</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mn>3</mml:mn><mml:mi>H</mml:mi><mml:mi>T</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">TL</mml:mi><mml:mo>−</mml:mo><mml:mn mathvariant="bold">1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>T</mml:mi><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>S</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr><mml:mtr columnalign="left"><mml:mtd columnalign="right"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">TL</mml:mi><mml:mo>−</mml:mo><mml:mn mathvariant="bold">2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mo>=</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>E</mml:mi><mml:mi>D</mml:mi><mml:mi>O</mml:mi><mml:mi>T</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>S</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mi>B</mml:mi><mml:mi>T</mml:mi><mml:mo>:</mml:mo><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>B</mml:mi><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>O</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{rcl}{{\bf{x}}}_{{\boldsymbol{S}}} &amp; = &amp; {[{t}_{ITO},{t}_{ZnO},{t}_{P3HT:PCBM},{t}_{Mo{O}_{3}},{t}_{Al}]}^{T}\\ {{\bf{x}}}_{{\boldsymbol{TL}}-{\bf{1}}} &amp; = &amp; {[{t}_{ITO},{t}_{Si{O}_{2}},{t}_{aSi},{t}_{A{l}_{2}{O}_{3}},{t}_{Al}]}^{T}\\ {{\bf{x}}}_{{\boldsymbol{TL}}-{\bf{2}}} &amp; = &amp; {[{t}_{S{i}_{3}{N}_{4}},{t}_{PEDOT:PSS},{t}_{PCDTBT:PCBM},{t}_{A{l}_{2}{O}_{3}},{t}_{Al}]}^{T}\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41598_2019_41316_Article_Equ8.gif"/></alternatives></disp-formula></p><p id="Par21">These materials are widely used in thin film solar cells. TL-1 case was previously designed and optimized by the authors<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup> using surrogate based and direct optimization methods. TL-2 is optimized for the first time.</p><p id="Par22">As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the base case is optimized by using traditional surrogate-based optimization methods. Then the hidden layer of the trained model is transferred to other cases. In the next section, the results are presented for the base case and transfer cases with emphasis on the training and validation performances (mean squared error). We will also discuss computational cost as the required number of simulation iterations.</p></sec><sec id="Sec4" sec-type="results"><title>Results and Discussion</title><sec id="Sec5"><title>Base Case</title><p id="Par23">The training of the base case is done using 1000 data points with 750 of them used as the training set and the rest for validation. The number of neurons in the hidden layer is determined based on the principle of minimum validation error as follows: The in-sample and out-sample errors are recorded as the number of neurons in the hidden layer is increased and the network configuration providing the minimum out-sample error is selected to be used in the optimization. This procedure is repeated 10 times to eliminate the possibility of training algorithm being trapped in local optima. Optimization is also repeated 10 times using all NN models obtained. This results in 10 possible optimal points. These points are run throught the high-fidelity (FDTD) model, and the highest function value is selected accordingly. The number of neurons in the hidden layer for the base case is selected as 12 based on the results in Fig. <xref rid="Fig4" ref-type="fig">4a</xref>. Then the optimization is done using the NN models with 12 neurons using all the generated models. Full-fidelity optimization is also done using the software in order to validate the results (See Table <xref rid="Tab1" ref-type="table">1</xref>). The optimized values are in a good agreement with a maximum 5% error. The evolution of EQE during surrogate-based optimization iterations is presented in Fig. <xref rid="Fig4" ref-type="fig">4b</xref>. Note that the best reported EQE in Table <xref rid="Tab1" ref-type="table">1</xref> is obtained using simulations so discrepancies between this value and that of Fig. <xref rid="Fig4" ref-type="fig">4b</xref> are expected.<fig id="Fig4"><label>Figure 4</label><caption xml:lang="en"><p><bold>(a</bold>) Variation of mean squared error for training and validation data sets with respect to the number of neurons in the hidden layer of NN for base case, (<bold>b</bold>) Evolution of EQE during the optimization for the base case.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig4_HTML.png"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Optimization results for Base, TL-1 and TL-2 cases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Case Name</p></th><th><p><italic>N</italic><sub><italic>sims</italic></sub></p><p>[−]</p></th><th><p><italic>t</italic><sub><italic>ARC</italic></sub></p><p>[nm]</p></th><th><p><italic>t</italic><sub><italic>IL</italic>1</sub></p><p>[nm]</p></th><th><p><italic>t</italic><sub><italic>A</italic></sub></p><p>[nm]</p></th><th><p><italic>t</italic><sub><italic>IL</italic>2</sub></p><p>[nm]</p></th><th><p><italic>t</italic><sub><italic>M</italic></sub></p><p>[nm]</p></th><th><p><italic>EQE</italic></p><p>[−]</p></th></tr></thead><tbody><tr><td><p>Base – NN-based</p></td><td><p>1,000</p></td><td><p>76</p></td><td><p>19</p></td><td><p>79</p></td><td><p>12</p></td><td><p>100</p></td><td><p>0.370</p></td></tr><tr><td><p>Base - Direct</p></td><td><p>6,900</p></td><td><p>77</p></td><td><p>20</p></td><td><p>80</p></td><td><p>10</p></td><td><p>95</p></td><td><p>0.371</p></td></tr><tr><td><p>TL-1 – NN-based – w/TL</p></td><td><p>500</p></td><td><p>31</p></td><td><p>20</p></td><td><p>65</p></td><td><p>20</p></td><td><p>102</p></td><td><p>0.371</p></td></tr><tr><td><p>TL-1 – NN-based – no TL</p></td><td><p>1,000</p></td><td><p>29</p></td><td><p>19</p></td><td><p>65</p></td><td><p>20</p></td><td><p>101</p></td><td><p>0.370</p></td></tr><tr><td><p>TL-1 – Direct</p></td><td><p>9,200</p></td><td><p>30</p></td><td><p>19</p></td><td><p>65</p></td><td><p>20</p></td><td><p>103</p></td><td><p>0.372</p></td></tr><tr><td><p>TL-1 – Reference<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>4,600</p></td><td><p>30</p></td><td><p>16</p></td><td><p>62</p></td><td><p>20</p></td><td><p>50</p></td><td><p>0.361</p></td></tr><tr><td><p>TL-2 – NN-based – w/TL</p></td><td><p>500</p></td><td><p>40</p></td><td><p>5</p></td><td><p>98</p></td><td><p>5</p></td><td><p>95</p></td><td><p>0.355</p></td></tr><tr><td><p>TL-2 – NN-based – no TL</p></td><td><p>1,000</p></td><td><p>38</p></td><td><p>7</p></td><td><p>95</p></td><td><p>5</p></td><td><p>100</p></td><td><p>0.352</p></td></tr><tr><td><p>TL-2 – Direct</p></td><td><p>5,520</p></td><td><p>42</p></td><td><p>5</p></td><td><p>100</p></td><td><p>5</p></td><td><p>97</p></td><td><p>0.360</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec6"><title>Transfer Cases</title><p id="Par24">In order to demonstrate the proposed approach, two material sets different from the base case are considered. These sets are represented by vectors <bold>x</bold><sub><bold>TL</bold>−1</sub> and <bold>x</bold><sub><bold>TL</bold>−2</sub>. First, the same steps as in the base case are followed without the transfer learning framework as a comparison. In these cases, 1000 data points are used where 750 of them are used as the training set and the rest is used for validation. Then training is repeated for the transfer learning cases using equation <xref rid="Equ3" ref-type="disp-formula">3</xref> with 500 new data points where 375 of them are used as the training set and the rest is used for validation. The prediction performances using transfer learning are presented and compared with the traditional method in Fig. <xref rid="Fig5" ref-type="fig">5a,b</xref>.<fig id="Fig5"><label>Figure 5</label><caption xml:lang="en"><p>Results for (<bold>a</bold>) TL-1 (ITO-SiO<sub>2</sub>-aSi-Al<sub>2</sub>O<sub>3</sub>-Al) and (<bold>b</bold>) TL-2 (Si<sub>3</sub>N<sub>4</sub>-PEDOT:PSS-PCDTBT:PCBM-Al<sub>2</sub>O<sub>3</sub>-Al) without the transfer layer (no TL, dashed lines) using 1000 data points and with transfer layer (w/TL, solid lines) using 500 data points.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig5_HTML.png"/></fig></p><p id="Par25">Figure <xref rid="Fig5" ref-type="fig">5</xref> shows the effectiveness of the transfer learning method. The smallest out-sample MSE of no TL case in TL-1 is more than 3 times larger than the largest out-sample MSE w/TL case even though the number of data is half of the no TL case. Furthermore, although the improvement in TL-2 case is not as significant as in TL-1, using the transfer layer reduces error to almost half of the TL-2 (no TL). The reason of this less significant improvement is that the validation error of TL-2 case without transfer layer is similar to that of the base case. On the other hand, the validation error of TL-1 (no TL) case is ~5 times larger than that of the base case. As can be seen from Fig. <xref rid="Fig5" ref-type="fig">5b</xref>, the most significant improvement in validation error is obtained when 3 neurons is used where the largest deviation between errors of TL-2 and base cases is observed. Therefore the relation between the deviation between errors of transfer and base cases suggest that the more accurate the base case is, the more the validation error is reduced. Furthermore, if the base case is less accurate than the transfer cases, prediction performance can even become worse. This is known as negative transfer which is an undesirable phenomenon in transfer learning applications.</p><p id="Par26">The effect of negative transfer on prediction accuracy is illustrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref> by switching the base and TL-1 cases where the hidden layer of TL-1 (ITO-SiO<sub>2</sub>-aSi-Al<sub>2</sub>O<sub>3</sub>-Al) is transferred to the base case (ITO-ZnO-P3HT:PCBM-MoO<sub>3</sub>-Al). As seen from Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the training MSE does not change as expected; however, the validation error significantly increases since the transferred layer is adopted from a less accurate model.<fig id="Fig6"><label>Figure 6</label><caption xml:lang="en"><p>Negative Transfer: Comparison of MSE of no TL Base case (dashed) and w/TL from TL-1 (solid).</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig6_HTML.png"/></fig></p><p id="Par27">In TL-1 case, 12 and 9 neurons are selected for no TL and w/TL respectively for optimization. The results are compared with the previous optimization studies for the same 5-layer a-Si solar cell<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. Similarly, in TL-2 case, 15 and 12 neurons are selected for no TL and w/TL respectively for optimization.</p><p id="Par28">The results obtained using transfer learning are in a good agreement with the direct optimization results for both cases. The optimized geometry in TL-1 case is also very close to the results from the previous study<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. In the other study<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>, a regression-tree based optimizer is used as well as simulated annealing on direct FDTD simulations to find the optimal solution. However, since the objective function in this study<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> is slightly different than the present objective function, a deviation between the results of these two studies is expected. The present study achieved a slightly higher EQE than that the previous result<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. The optimization results are presented in Table <xref rid="Tab1" ref-type="table">1</xref> and evolutions of EQE are presented in Fig. 7.<fig id="Fig7"><label>Figure 7</label><caption xml:lang="en"><p>Evolution of EQE during optimization for (<bold>a</bold>) TL-1, w/out TL, (<bold>b</bold>) TL-1 w/TL, (<bold>c</bold>) TL-2 w/out TL (<bold>b</bold>) TL-2 w/TL.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41598_2019_41316_Fig7_HTML.png"/></fig></p><p id="Par29">Results show that equivalent EQEs can be obtained from an amorphous silicon and an organic P3HT:PCBM solar cell. EQE of PCDTBT:PCBM solar cell is lower than the others because the longer wavelengths where PCDTBT:PCBM can absorb more than a-Si and P3HT:PCBM are ignored in EQE calculation. EQE is calculated between <italic>λ</italic> = 300–750 nm for all cases for consistency.</p></sec></sec><sec id="Sec7" sec-type="conclusion"><title>Conclusion</title><p id="Par30">In this paper, a novel methodology of multilayer neural network based transfer optimization for design problems was presented. The proposed method was applied to a case study where a multilayer thin film solar cell was to be optimized for the best external quantum efficiency. The results showed that the prediction accuracy can be improved using transfer learning. Furthermore, the number of high fidelity function evaluations during surrogate based optimization can be decreased without sacrificing the accuracy.</p></sec></body><back><ack><title>Acknowledgements</title><p>The open access publishing fees for this article have been covered by the Texas A&amp;M University Open Access to Knowledge Fund (OAKFund), supported by the University Libraries and the Office of the Vice President for Research.</p></ack><sec sec-type="author-contribution"><title>Author Contributions</title><p>M.K. provided the results, prepared the figures and wrote the manuscript. S.H. (Corresponding Author) reviewed and revised the results and figures and wrote and reviewed the manuscript.</p></sec><sec sec-type="data-availability"><title>Data Availability</title><p>The datasets generated during the current study are available from the corresponding author on reasonable request.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing Interests</title><p id="Par31">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>A</given-names></name><name><surname>Ong</surname><given-names>Y</given-names></name><name><surname>Feng</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Insights on transfer optimization: Because experience is the best teacher</article-title><source>IEEE Trans. Emerg. Top. Comput. Intell.</source><year>2017</year><volume>2</volume><fpage>1</fpage><lpage>14</lpage></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Quattoni, A., Collins, M. &amp; Darrell, T. Transfer learning for image classification with sparse prototype representations. In <italic>Proceedings of IEEE Conf. Computer Vision and Pattern Recognition</italic>, 1–8 (2008).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Raina, R., Battle, A., Lee, H., Packer, B. &amp; Ng, A. Y. Self-taught learning: Transfer learning from unlabeled data. In <italic>Proceedings of the 24th International Conference on Machine Learning</italic>, 759–766 (2007).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname><given-names>GPC</given-names></name><name><surname>Yu</surname><given-names>JX</given-names></name><name><surname>Lu</surname><given-names>H</given-names></name><name><surname>Yu</surname><given-names>PS</given-names></name></person-group><article-title xml:lang="en">Text classification without negative examples revisit</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2006</year><volume>18</volume><fpage>6</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2006.16</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Dai, W., Yang, Q., Xue, G.-R. &amp; Yu, Y. Boosting for transfer learning. In <italic>Proceedings of the 24th International Conference on Machine Learning</italic>, 193–200 (2007).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanini</surname><given-names>P</given-names></name><name><surname>Congedo</surname><given-names>M</given-names></name><name><surname>Jutten</surname><given-names>C</given-names></name><name><surname>Said</surname><given-names>S</given-names></name><name><surname>Berthoumieu</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Transfer learning: a Riemannian geometry framework with applications to brain-computer interfaces</article-title><source>IEEE Trans. Biomed. Eng.</source><year>2018</year><volume>65</volume><fpage>1107</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1109/TBME.2017.2742541</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waytowich</surname><given-names>NR</given-names></name><name><surname>Lawhern</surname><given-names>VJ</given-names></name><name><surname>Bohannon</surname><given-names>AW</given-names></name><name><surname>Ball</surname><given-names>KR</given-names></name><name><surname>Lance</surname><given-names>BJ</given-names></name></person-group><article-title xml:lang="en">Spectral transfer learning using information geometry for a user-independent brain-computer interface</article-title><source>Front. Neurosci.</source><year>2016</year><volume>10</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.3389/fnins.2016.00430</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Choi, K., Fazekas, G., Sandler, M. &amp; Cho, K. Transfer learning for music classification and regression tasks, <ext-link xlink:href="http://arxiv.org/abs/1703.09179" ext-link-type="uri">http://arxiv.org/abs/1703.09179</ext-link> (2017).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>YP</given-names></name><name><surname>Jung</surname><given-names>TP</given-names></name></person-group><article-title xml:lang="en">Improving EEG-based emotion classification using conditional transfer learning</article-title><source>Front. Hum. Neurosci.</source><year>2017</year><volume>11</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXmsl2isrs%3D</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Pardoe, D. &amp; Stone, P. Boosting for regression transfer. In <italic>Proceedings of the 27th International Conference on Machine Learning</italic>, 863–870 (2010).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Jamshidi, P., Velez, M., Kästner, C., Siegmund, N. &amp; Kawthekar, P. Transfer learning for improving model predictions in highly configurable software. In <italic>Proceedings of IEEE/ACM 12th International Symposium on Software Engineering for Adaptive and Self-Managing Systems</italic>, 31–41 (2017).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lindner</surname><given-names>C</given-names></name><name><surname>Waring</surname><given-names>D</given-names></name><name><surname>Thiruvenkatachari</surname><given-names>B</given-names></name><name><surname>O’Brien</surname><given-names>K</given-names></name><name><surname>Cootes</surname><given-names>TF</given-names></name></person-group><article-title xml:lang="en">Adaptable landmark localisation: Applying model transfer learning to a shape model matching system</article-title><source>In Lecture Notes in Computer Science</source><year>2017</year><volume>10433</volume><fpage>144</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-66182-7_17</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Gao, J., Ling, H., Hu, W. &amp; Xing, J. Transfer Learning Based Visual Tracking with Gaussian Processes Regression. In <italic>Proceedings of European Conference on Computer Vision</italic>, 188–203 (2014).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Hagan, M. T., Demuth, H. B., Beale, M. H. &amp; De Jesus, O. Neural network design. (PWS Publishing Company, 2014).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Were</surname><given-names>K</given-names></name><name><surname>Bui</surname><given-names>DT</given-names></name><name><surname>Dick</surname><given-names>ØB</given-names></name><name><surname>Singh</surname><given-names>BR</given-names></name></person-group><article-title xml:lang="en">A comparative assessment of support vector regression, artificial neural networks, and random forests for predicting and mapping soil organic carbon stocks across an Afromontane landscape</article-title><source>Ecol. Indic.</source><year>2015</year><volume>52</volume><fpage>394</fpage><lpage>403</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXmtFCgsA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.ecolind.2014.12.028</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Iwata, T. &amp; Ghahramani, Z. Improving output uncertainty estimation and generalization in deep learning via neural network Gaussian processes., <ext-link xlink:href="http://arxiv.org/abs/1707.05922" ext-link-type="uri">http://arxiv.org/abs/1707.05922</ext-link> (2017).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Yosinski, J., Clune, J., Bengio, Y. &amp; Lipson, H. How transferable are features in deep neural networks? In <italic>Proceedingg of Advances in Neural Information Processing Systems</italic> 1–9 (2014).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Q</given-names></name><name><surname>Zhang</surname><given-names>R</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Transfer learning for short-term wind speed prediction with deep neural networks</article-title><source>Renew. Energy</source><year>2016</year><volume>85</volume><fpage>83</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.renene.2015.06.034</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qureshi</surname><given-names>AS</given-names></name><name><surname>Khan</surname><given-names>A</given-names></name><name><surname>Zameer</surname><given-names>A</given-names></name><name><surname>Usman</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Wind power prediction using deep neural network based meta regression and transfer learning</article-title><source>Appl. Soft Comput.</source><year>2017</year><volume>58</volume><fpage>742</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1016/j.asoc.2017.05.031</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>F</given-names></name><name><surname>Xia</surname><given-names>GS</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Transferring deep convolutional neural networks for the scene classification of high-resolution remote sensing imagery</article-title><source>Remote Sens.</source><year>2015</year><volume>7</volume><fpage>14680</fpage><lpage>14707</lpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2015RemS....714680H</pub-id><pub-id pub-id-type="doi">10.3390/rs71114680</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Johnson, R. &amp; Zhang, T. Semi-supervised convolutional neural networks for text categorization via region embedding. In <italic>Proceedings of Advances in Neural Information Processing Systems</italic> 1–9 (2015).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Long, M., Cao, Z., Wang, J. &amp; Yu, P. S. Learning multiple tasks with multilinear relationship networks. In <italic>Proceedings of Advances in Neural Information Processing Systems</italic> (2017).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Yogatama, D. &amp; Mann, G. Efficient transfer learning method for automatic hyperparameter tuning. In <italic>Proceedings of the 17th International Conference on Artificial Intelligence and Statistics (AISTATS)</italic>, 1077–1085 (2014).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Joy, T. T., Rana, S., Gupta, S. &amp; Venkatesh, S. Hyperparameter tuning for big data using Bayesian optimisation. In <italic>Proceedings of International Conference on Pattern Recognition</italic> 2574–2579 (2016).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Feurer, M., Springenberg, J. T. &amp; Hutter, F. Initializing Bayesian hyperparameter optimization via meta-learning. In <italic>Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</italic>, 1128–1135 (2015).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Wistuba, M., Schilling, N. &amp; Schmidt-Thieme, L. Learning hyperparameter optimization initializations. In <italic>Proceedings of IEEE International Conference on Data Science and Advanced Analytics</italic> (2015).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Li, D. <italic>et al</italic>. Efficient design space exploration by knowledge transfer. In <italic>Proceedings of International Conference on Hardware/Software Codesign and System Synthesis (CODES</italic> + <italic>ISSS)</italic><bold>1</bold> (2016).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>A</given-names></name><name><surname>Min</surname><given-names>W</given-names></name><name><surname>Sagarna</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Knowledge transfer through machine learning in aircraft design</article-title><source>IEEE Comput. Intell. Mag.</source><year>2017</year><volume>12</volume><fpage>48</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1109/MCI.2016.2627666</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferry</surname><given-names>VE</given-names></name><name><surname>Polman</surname><given-names>A</given-names></name><name><surname>Atwater</surname><given-names>HA</given-names></name></person-group><article-title xml:lang="en">Modeling light trapping in nanostructured solar cells</article-title><source>ACS Nano</source><year>2011</year><volume>5</volume><fpage>10055</fpage><lpage>10064</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXhsVKiu7rJ</pub-id><pub-id pub-id-type="doi">10.1021/nn203906t</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atwater</surname><given-names>HA</given-names></name><name><surname>Polman</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Plasmonics for improved photovoltaic devices</article-title><source>Nat. Mater.</source><year>2010</year><volume>9</volume><fpage>865</fpage><lpage>865</lpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2010NatMa...9..865A</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3cXhtF2msbjN</pub-id><pub-id pub-id-type="doi">10.1038/nmat2866</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hajimirza</surname><given-names>S</given-names></name><name><surname>El Hitti</surname><given-names>G</given-names></name><name><surname>Heltzel</surname><given-names>A</given-names></name><name><surname>Howell</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Specification of micro-nanoscale radiative patterns using inverse analysis for increasing solar panel efficiency</article-title><source>J. Heat Transfer</source><year>2012</year><volume>134</volume><fpage>102702</fpage><pub-id pub-id-type="doi">10.1115/1.4006209</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hajimirza</surname><given-names>S</given-names></name><name><surname>Howell</surname><given-names>JR</given-names></name></person-group><article-title xml:lang="en">Design and analysis of spectrally selective patterned thin-film cells</article-title><source>Int. J. Thermophys.</source><year>2013</year><volume>34</volume><fpage>1930</fpage><lpage>1952</lpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2013IJT....34.1930H</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3sXhtlenurrP</pub-id><pub-id pub-id-type="doi">10.1007/s10765-013-1495-y</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hajimirza</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Expedited quasi-updated gradient based optimization techniques for energy conversion nano-materials</article-title><source>J. Nanoelectron. Optoelectron.</source><year>2015</year><volume>10</volume><fpage>140</fpage><lpage>146</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXht1SlsLnP</pub-id><pub-id pub-id-type="doi">10.1166/jno.2015.1717</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaya</surname><given-names>M</given-names></name><name><surname>Hajimirza</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Surrogate based modeling and optimization of plasmonic thin film organic solar cells</article-title><source>Int. J. Heat Mass Transf.</source><year>2018</year><volume>118</volume><fpage>1128</fpage><lpage>1142</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhvFSkurjJ</pub-id><pub-id pub-id-type="doi">10.1016/j.ijheatmasstransfer.2017.11.044</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaya</surname><given-names>M</given-names></name><name><surname>Hajimirza</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Application of artificial neural network for accelerated optimization of ultra thin organic solar cells</article-title><source>Sol. Energy</source><year>2018</year><volume>165</volume><fpage>159</fpage><lpage>166</lpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018SoEn..165..159K</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXkslCmtrs%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.solener.2018.02.062</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaya</surname><given-names>M</given-names></name><name><surname>Hajimirza</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Rapid optimization of external quantum efficiency of thin film solar cells using surrogate modeling of absorptivity</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018NatSR...8.8170K</pub-id><pub-id pub-id-type="doi">10.1038/s41598-018-26469-3</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacKay</surname><given-names>DJC</given-names></name></person-group><article-title xml:lang="en">A Practical Bayesian framework for backpropagation networks</article-title><source>Neural Comput.</source><year>1992</year><volume>4</volume><fpage>448</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1162/neco.1992.4.3.448</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Queipo</surname><given-names>NV</given-names></name><etal/></person-group><article-title xml:lang="en">Surrogate-based analysis and optimization</article-title><source>Prog. Aerosp. Sci.</source><year>2005</year><volume>41</volume><fpage>1</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.paerosci.2005.02.001</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirkpatrick</surname><given-names>S</given-names></name><name><surname>Gelatt</surname><given-names>CD</given-names></name><name><surname>Vecchi</surname><given-names>MP</given-names></name></person-group><article-title xml:lang="en">Optimization by simulated annealing</article-title><source>Science</source><year>1983</year><volume>220</volume><fpage>671</fpage><lpage>680</lpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">1983Sci...220..671K</pub-id><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">702485</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DC%2BC3cvktFWjtw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.220.4598.671</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>SJ</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name></person-group><article-title xml:lang="en">A Survey on transfer learning</article-title><source>IEEE Trans. Knowl. Data Eng.</source><year>2009</year><volume>22</volume><fpage>1345</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1109/TKDE.2009.191</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Lumerical, Inc. Available at: <ext-link xlink:href="http://www.lumerical.com" ext-link-type="uri">www.lumerical.com</ext-link>.</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Hajimirza, S. A novel machine-learning aided optimization technique for material design: application in thin film solar cells. In <italic>Proceedings of the ASME HT/FEDSM/ICNMM Summer Heat Transfer Conference</italic> (2016).</mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><p><bold>Publisher’s note:</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Science, Humanities and Social Sciences, multidisciplinary</facet-value><facet-value count="1">Science, multidisciplinary</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">Scientific Reports</facet-value></facet><facet name="year"><facet-value count="1">2019</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
