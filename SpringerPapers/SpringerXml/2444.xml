<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1007/s12541-021-00600-3</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="review-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">12541</journal-id><journal-id journal-id-type="doi">10.1007/12541.2005-4602</journal-id><journal-title-group><journal-title>International Journal of Precision Engineering and Manufacturing</journal-title><abbrev-journal-title abbrev-type="publisher">Int. J. Precis. Eng. Manuf.</abbrev-journal-title></journal-title-group><issn pub-type="ppub">2234-7593</issn><issn pub-type="epub">2005-4602</issn><publisher><publisher-name>Korean Society for Precision Engineering</publisher-name><publisher-loc>Seoul</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s12541-021-00600-3</article-id><article-id pub-id-type="manuscript">600</article-id><article-id pub-id-type="doi">10.1007/s12541-021-00600-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Recent Advances of Artificial Intelligence in Manufacturing Industrial Sectors: A Review</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><name><surname>Kim</surname><given-names>Sung Wook</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec id="FPar10"><title>Sung Wook Kim</title><p>received a B.S. degree in Mechanical Engineering from Hanyang University, Seoul, South Korea, in 2016. He then received his M.S. degree in Mechanical Engineering from Pohang University of Science and Technology, Pohang, South Korea, in 2018. He is now a Ph.D. candidate at the Industrial AI Lab. of Pohang University of Science and Technology. His research interests include industrial artificial intelligence with mechanical systems, and deep learning for smart manufacturing.<fig id="Figa" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/12541_2021_600_Figa_HTML.jpg" position="anchor" id="MO42"/></fig></p></sec></bio></contrib><contrib contrib-type="author" id="Au2"><name><surname>Kong</surname><given-names>Jun Ho</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><bio><sec id="FPar11"><title>Jun Ho Kong</title><p>received his B.S. degree in the school of Mechanical Engineering from Dalian University of Technology, China, in 2019. He is currently M.S. student at Sungkyunkwan University, Korea. His research interests include metal 3D printing, high precision printing, 4D printing, prognostics and health management (PHM) and artificial intelligence.<fig id="Figb" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/12541_2021_600_Figb_HTML.jpg" position="anchor" id="MO43"/></fig></p></sec></bio></contrib><contrib contrib-type="author" corresp="yes" id="Au3"><name><surname>Lee</surname><given-names>Sang Won</given-names></name><address><email>sangwonl@skku.edu</email></address><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="corresp" rid="IDs12541021006003_cor3">c</xref><bio><sec id="FPar12"><title>Sang Won Lee</title><p>received his B.S. and M.S. degrees in the department of Mechanical Design and Production Engineering from Seoul National University, Korea, in 1995 and 1997. He received Ph.D. in the department of Mechanical Engineering from University of Michigan in 2004. Dr. Lee joined the school of Mechanical Engineering at Sungkyunkwan University in 2006 and is currently a professor. His research interests include smart factory, environmentally-friendly mechanical machining, additive manufacturing, and data-driven design.<fig id="Figc" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/12541_2021_600_Figc_HTML.jpg" position="anchor" id="MO44"/></fig></p></sec></bio></contrib><contrib contrib-type="author" corresp="yes" id="Au4"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1034-1410</contrib-id><name><surname>Lee</surname><given-names>Seungchul</given-names></name><address><email>seunglee@postech.ac.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="corresp" rid="IDs12541021006003_cor4">d</xref><bio><sec id="FPar13"><title>Seungchul Lee</title><p>is an associate professor at the department of Mechanical Engineering at POSTECH, Korea. His research focuses on industrial AI for smart manufacturing, materials science, and medical applications. He received the B.S. (2001) and Ph.D. (2010) in Mechanical Engineering from Seoul National University, Korea, and from the University of Michigan at Ann Arbor, USA, respectively.<fig id="Figd" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/12541_2021_600_Figd_HTML.jpg" position="anchor" id="MO45"/></fig></p></sec></bio></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.49100.3c</institution-id><institution-id institution-id-type="ISNI">0000 0001 0742 4007</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Pohang University of Science and Technology</institution></institution-wrap><addr-line content-type="street">77 Cheongam-ro</addr-line><addr-line content-type="city">Pohang</addr-line><country country="KR">Republic of Korea</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.264381.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2181 989X</institution-id><institution content-type="org-division">Department of Mechanical Engineering, Graduate School</institution><institution content-type="org-name">Sungkyunkwan University</institution></institution-wrap><addr-line content-type="street">2066, Seobu-ro, Jangan-gu</addr-line><addr-line content-type="city">Suwon</addr-line><country country="KR">Republic of Korea</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.264381.a</institution-id><institution-id institution-id-type="ISNI">0000 0001 2181 989X</institution-id><institution content-type="org-division">School of Mechanical Engineering</institution><institution content-type="org-name">Sungkyunkwan University</institution></institution-wrap><addr-line content-type="street">2066, Seobu-ro, Jangan-gu</addr-line><addr-line content-type="city">Suwon</addr-line><country country="KR">Republic of Korea</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.49100.3c</institution-id><institution-id institution-id-type="ISNI">0000 0001 0742 4007</institution-id><institution content-type="org-division">Graduate School of Artificial Intelligence</institution><institution content-type="org-name">Pohang University of Science and Technology</institution></institution-wrap><addr-line content-type="street">77 Cheongam-ro</addr-line><addr-line content-type="city">Pohang</addr-line><country country="KR">Republic of Korea</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.15444.30</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 5454</institution-id><institution content-type="org-division">Institute of Convergence Research and Education in Advanced Technology</institution><institution content-type="org-name">Yonsei University</institution></institution-wrap><addr-line content-type="street">50 Yonsei-ro</addr-line><addr-line content-type="city">Seoul</addr-line><country country="KR">Republic of Korea</country></aff></contrib-group><author-notes><corresp id="IDs12541021006003_cor3"><label>c</label><email>sangwonl@skku.edu</email></corresp><corresp id="IDs12541021006003_cor4"><label>d</label><email>seunglee@postech.ac.kr</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>3</day><month>11</month><year>2021</year></pub-date><pub-date date-type="pub" publication-format="print"><month>1</month><year>2022</year></pub-date><volume>23</volume><issue seq="10">1</issue><fpage>111</fpage><lpage>129</lpage><history><date date-type="registration"><day>14</day><month>10</month><year>2021</year></date><date date-type="received"><day>10</day><month>5</month><year>2021</year></date><date date-type="rev-recd"><day>9</day><month>10</month><year>2021</year></date><date date-type="accepted"><day>14</day><month>10</month><year>2021</year></date><date date-type="online"><day>3</day><month>11</month><year>2021</year></date></history><permissions><copyright-statement content-type="compact">Â© The Author(s) 2021</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">The recent advances in artificial intelligence have already begun to penetrate our daily lives. Even though the development is still in its infancy, it has been shown that it can outperform human beings even in terms of intelligence (e.g., AlphaGo by DeepMind), implying a massive potential for its broader application in various industrial sectors. In particular, the growing public interest in industry 4.0, which focuses on revolutionizing the traditional manufacturing scene, has stimulated a deeper investigation of its possible applications in the related industries. Since it has several limitations that hinder its direct usage, research on the convergence of artificial intelligence with other engineering fields, including precision engineering and manufacturing, is ongoing. This overview looks to summarize some of the important achievements made using artificial intelligence in some of the most influential and lucrative manufacturing industries in hopes of transforming the manufacturing sites.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Artificial intelligence</kwd><kwd>Deep learning</kwd><kwd>Fault detection and diagnosis</kwd><kwd>Condition monitoring</kwd><kwd>Manufacturing process</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>National Research Foundation of Korea</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/501100003725</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">2020R1A2C1009744</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Seungchul</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Korea Institute for Advancement of Technology</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/501100003661</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">P0008691</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Seungchul</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Korea Electric Power Corporation</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/501100010193</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">IDPP</award-id><principal-award-recipient><name><surname>Lee</surname><given-names>Seungchul</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Korean Society for Precision Engineering, co-published with Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>12</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Korean Society for Precision Engineering</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>14</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/12541_2021_Article_600.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-day</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-day</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>ReviewPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Industrial and Production Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta><notes notes-type="Misc"><p>This paper is an invited paper (Invited Review).</p></notes></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Artificial intelligence (AI) is now at the forefront in the pursuit of industry 4.0. Over the past several years, the accumulation of big data via Internet-of-Things (IoT) technology has led to the rapid growth of information retrieval and analysis techniques such as AI. Such advancement in ways to deal with a large amount of data is about to revolutionize many manufacturing industry sectors, and it is the driving force behind the foundation of smart factories where everything is conducted intelligently and in an automated fashion during every cycle of the manufacturing process.</p><p id="Par3">Industrial AI is a term coined to specifically refer to AI for the particular goals in the manufacturing industry. Industrial AI covers a wide range of machine learning where the keys to success are pattern recognition for highly nonlinear data, unstructured data analysis, robustness to repetitive tasks, fast computation speed, and high interpretability. Out of these industrial AI traits, recognizing a highly nonlinear pattern is essential, particularly because the relationship between input parameters and output parameters is only partially understood under simplified conditions. It is sometimes even unknown due to extremely high nonlinear correlations. To dispel the concerns, deep learning, a part of machine learning, is beginning to replace traditional data analysis techniques. Recently, the power of deep learning is already well known to the public. It not only captures complex patterns in train data, but it also can recognize various types of unstructured data, hence its tremendous success in object detection, natural language processing, speech recognition, and realistic image synthesis. Despite its drawbacks associated with interpretability and extrapolability, its potential is nearly limitless as its performance depends largely on the amount and quality of data and the design of its architecture. As such, it is widely studied worldwide with a huge amount of investment from both the governments and firms.</p><p id="Par4">Unfortunately, it still faces much reluctance when adopting it directly at manufacturing sites. One reason behind it could be that there is a lack of awareness of where and how it should be incorporated in the manufacturing pipeline and a few of its unsolved issues, as mentioned before, making it less trustworthy. This review aims to raise the awareness of possible AI applications by providing an extensive overview of its usage in various industrial sectors but only for limited objectives (i.e., product enhancement and manufacturing process enhancement) due to overly broad applicability. We hope this review paper would contribute to an even wider expansion of AI implementation in the industries. For clarity, details on machine learning [<xref ref-type="bibr" rid="CR1">1</xref>], deep learning [<xref ref-type="bibr" rid="CR2">2</xref>], and its sub-branches [<xref ref-type="bibr" rid="CR3">3</xref>â<xref ref-type="bibr" rid="CR8">8</xref>] should be referred to the attached references. The rest of this paper has the following sections remaining. SectionÂ 2 provides an extensive literature survey with four subsections that introduce various AI applied to improve the performance of specific products. SectionÂ 3 contains an overview of literature with two subsections, each of which talks about AI applications in the course of the manufacturing process. Finally, Sect.Â 4 concludes this overview with a summary and a brief insight into the future of AI.</p></sec><sec id="Sec2"><title>Applications of AI for Product Enhancement</title><sec id="Sec3"><title>Autonomous Vehicle</title><p id="Par5">Autonomous driving (AD) is a thriving field of study where AI is actively taking part. The main objectives of AD consist of road detection, lane detection, vehicle detection, pedestrian detection, drowsiness detection, collision avoidance, and traffic sign detection [<xref ref-type="bibr" rid="CR9">9</xref>]. These tasks mainly involve image-based object detection, localization, and segmentation in the context of computer science, and they are enhanced through the use of multiple sensors and appropriately fusing collected data from them. Sensor fusion is one of the vital aspects of self-driving cars. All of the detection schemes mentioned earlier could be useless and far from reality if there is a substantial error in the sensor signal. Despite its remarkable development in recent years, sensors are still vulnerable to noise and manufacturing defects. One practical solution to this issue is to merge multiple sensor readings to increase reliability by complementing the shortcomings of each sensor.</p><p id="Par6">In particular, sensors in AD are used for two main purposes: environmental perception and localization. While environmental perception refers to various object detection types (i.e., road detection and pedestrian detection) in a self-driving scenario, localization is meant by finding the absolute and relative positions of a driving vehicle. Different combinations of sensors are frequently used for each purpose, leading to the fusing of more than one sensor as input to deep neural networks. For example, LiDAR, radars, thermal cameras, and RGB cameras are the common choices for environmental perception. In contrast, for localization, inertial measurement units (IMU), inertial navigation systems (INS), LiDAR, global navigation satellite systems (GNSS) are selected [<xref ref-type="bibr" rid="CR10">10</xref>]. Moreover, multiple sensor readings are fused and then merged with a deep learning pipeline at different levels. A more detailed explanation of the possible routes of integration can be found in [<xref ref-type="bibr" rid="CR10">10</xref>]. While perception and localization may be challenging issues if readings from multiple sensors are mixed, deep learning is adopted to break the barrier. Here, we first introduce road detection and pedestrian detection as representative examples of the general image-based environmental perception using deep learning, followed by the cases of its applications based on multiple sensor fusion. Likewise, cases of localization is explained in detail afterwards.</p><p id="Par7">Road detection is the task of distinguishing the boundary between the road and the background. Limmer et al. [<xref ref-type="bibr" rid="CR11">11</xref>] showed a CNN-based road course prediction system for augmented reality applications. The proposed framework includes a multiscale CNN that receives multiple scales of the same input data simultaneously. Each scale of data is fed to the corresponding branch of the network. The branches do not share any weights, and they are joined at the end by a fully connected layer for scene labeling. It was shown that the approach performed well even for various weather conditions. Besides, Cheng et al. [<xref ref-type="bibr" rid="CR12">12</xref>] presented a cascaded end-to-end convolutional neural network (CasNet) for two tasks: road detection and road centerline extraction. The novelty of the proposed model lies in how a cascaded network is used to bridge two tasks together. Specifically, the first network has the form of an autoencoder for road detection. Feature maps generated at the last deconvolution layer in the first network are fed to the encoder network of the second network for the centerline extraction. In this way, two tasks are solved concurrently through an end-to-end fashion. A few other studies that tackle the road detection task uses a fully convolutional Siamese network [<xref ref-type="bibr" rid="CR13">13</xref>] and a specifically designed neural network (RBNet) [<xref ref-type="bibr" rid="CR14">14</xref>], respectively.</p><p id="Par8">Pedestrian detection refers to the task of differentiating pedestrians on roads from other objects. Special attention is given to this task due to its importance for preventing vehicle-to-human accidents. For this specific task, Wang et al. [<xref ref-type="bibr" rid="CR15">15</xref>] proposed the part and context network (PCN) that leverages the body part semantic information and the contextual information. The part branch, designed for occlusion handling, uses the long short-term memory (LSTM) module to communicate semantic information among the body parts. It was shown to boost the classification performance even for invisible parts. In the case of the context branch, contextual features with different scales are handled for pedestrian localization. They are important because they are the source of information for whether an object may be classified as a pedestrian or others by considering its background from different perspectives. In other studies regarding pedestrian detection, Ouyang et al. [<xref ref-type="bibr" rid="CR16">16</xref>] demonstrated that pedestrian detection could be enhanced by the joint handling of feature extraction, deformation, occlusion, and classification using a simple CNN. Cai et al. [<xref ref-type="bibr" rid="CR17">17</xref>] investigated the complexity-aware cascaded network, which leverages features of different complexities.</p><p id="Par9">As for the cases of sensor fusion in environmental perception, the frequent baseline deep neural networks turned out to be R-CNN, Faster R-CNN, YOLO, and etc. These networks are specifically designed for solving tasks involving object detection, localization, and segmentation. Wagner et al. [<xref ref-type="bibr" rid="CR18">18</xref>] compared the two types of sensor fusion (early fusion and late fusion) of RGB and thermal cameras. For an early fusion, the images of both cameras are concatenated side by side as channels and then are fed to R-CNN [<xref ref-type="bibr" rid="CR19">19</xref>]. For a late fusion, two separate networks are built for each input and are joined by a fully connected network right before the classifier at the end. The addition of thermal images is shown to solve the issue of low prediction accuracy in the nighttime. On the other hand, Schlosser et al. [<xref ref-type="bibr" rid="CR20">20</xref>] used LiDAR and RGB camera and performed an early fusion where the features representing different aspects of 3D scene were extracted from the LiDAR output and were used as additional image channels to be fed to R-CNN. Other studies which fuse features from LiDAR and visible images include [<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR22">22</xref>]. For these studies, YOLO [<xref ref-type="bibr" rid="CR23">23</xref>] was used for faster computation, and sensor fusion with LiDAR usually boosted discriminative performance for detecting pedestrians and objects on roads. Liu et al. [<xref ref-type="bibr" rid="CR24">24</xref>] demonstrated that RGB and thermal images could provide complementary information in detecting pedestrians by building four different fusion architectures based on faster R-CNN [<xref ref-type="bibr" rid="CR25">25</xref>]. The author shows that the halfway fusion where features extracted from both types of images are fused in halfway through the network achieves the best performance. A similar flow of work was presented in [<xref ref-type="bibr" rid="CR26">26</xref>] but with a few additional fusion architectures.</p><p id="Par10">Localization is another important area of sensor fusion in which deep learning is widely adopted. However, it normally requires different sensor fusions, as mentioned earlier. Multiple combinations of sensors are effective for the task: GNSS / INS or IMU and RGB / LiDAR. Firstly, Dai et al. [<xref ref-type="bibr" rid="CR27">27</xref>] recently introduced the deep learning-based integrated framework of GNSS and INS where the inaccurate GNSS signal is enhanced with the output of RNN fed with time-varying INS signal. Kim et al. [<xref ref-type="bibr" rid="CR28">28</xref>] also used a type of RNN, LSTM for the localization of a vehicle, but the work differs from the previous one because both GNSS and IMU are fed to the network simultaneously. Secondly, a sensor fusion of RGB images and LiDAR point clouds for improved localization by accurate depth estimation has been performed by Gao et al. [<xref ref-type="bibr" rid="CR29">29</xref>]. In the study, LiDAR point clouds images are projected to RGB images to create sparse depth images given as input to AlexNet [<xref ref-type="bibr" rid="CR30">30</xref>]. Similar works have been presented by Laidlow et al. [<xref ref-type="bibr" rid="CR31">31</xref>] that fuses depth predictions of multi-view stereo system with CNN output, and by Lee et al. [<xref ref-type="bibr" rid="CR32">32</xref>] that places RNN in addition to the existing CNN model. FigureÂ <xref rid="Fig1" ref-type="fig">1</xref> summarizes the aforementioned baseline deep learning models used in the surveyed literature where different pairs of sensors are fused.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>Baseline deep learning models found in the literature where different pairs of sensors are fused for improved perception and localization</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12541_2021_600_Fig1_HTML.png" id="MO1"/></fig></p><p id="Par11">In conclusion, the applications of AI in autonomous vehicle is mainly driven by image-based deep learning models such as CNN because autonomous vehicles generally collect information through sensors that characterize retrieved data into a 2-dimensional form that maintains rich spatial information. To improve the related studies in literature, it is necessary to have additional functions on top of the existing models that help capture the dynamics (i.e., temporal information) and complex patterns. Further research needs to be done to incorporate multiple sensor information into deep learning to allow for safer and fully automated driving on roads. The reviewed studies in this section are summarized in Table <xref rid="Tab1" ref-type="table">1</xref> with respect to some useful information including data type, data publicity/openness, investigated task and exploited algorithm or model.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the autonomous vehicle</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR11">11</xref>]</p></td><td align="left"><p>Radar signal, map database, vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (road detection)</p></td><td align="left"><p>Multi-scale CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR12">12</xref>]</p></td><td align="left"><p>Satellite image</p></td><td align="left"><p>Yes (Google Earth)</p></td><td align="left"><p>Classification (road and centerline extraction)</p></td><td align="left"><p>CasNet (Cascaded Network)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR13">13</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (road detection)</p></td><td align="left"><p>Siamesed FCN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR14">14</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (road and road boundary detection)</p></td><td align="left"><p>RBNet (Road and road Boundary Network)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR15">15</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (Caltech, INRIA and KITTI)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>PCN (Part and Context Network)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR16">16</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (ImageNet and Caltech)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>Fast R-CNN variant</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR17">17</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (Caltech and KITTI)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>CompACT (Complexity Aware Cascade Training)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR18">18</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KAIST dataset)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>R-CNN variant</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR20">20</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>R-CNN variant</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR21">21</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (vehicle detection)</p></td><td align="left"><p>YOLO variant</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR22">22</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (vehicle detection)</p></td><td align="left"><p>YOLO variant</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR24">24</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KAIST dataset)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>Faster R-CNN variant, ConvNet</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR26">26</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KAIST dataset)</p></td><td align="left"><p>Classification (pedestrian detection)</p></td><td align="left"><p>IAF R-CNN (Illumination-Aware Faster R-CNN)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR27">27</xref>]</p></td><td align="left"><p>INS signal, GPS signal</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (aerial vehicle navigation)</p></td><td align="left"><p>RNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR28">28</xref>]</p></td><td align="left"><p>IMU signal, GPS signal, map database</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (ground vehicle navigation)</p></td><td align="left"><p>LSTM-RNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR29">29</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI dataset)</p></td><td align="left"><p>Classification (object detection)</p></td><td align="left"><p>AlexNet</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR31">31</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (ICL-NUIM and TUM)</p></td><td align="left"><p>Regression (3D reconstruction)</p></td><td align="left"><p>DeepFusion</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR32">32</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (KITTI 
dataset)</p></td><td align="left"><p>Regression (depth estimation)</p></td><td align="left"><p>R-CNN</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec4"><title>Battery</title><p id="Par12">Recently, secondary battery has gained considerable interest worldwide due to its rising demand for electric vehicles (EVs) and hybrid electric vehicles (HEVs). One of the most commonly adopted secondary batteries for such vehicles is the lithium-ion battery because of its high power density, long battery life, high durability, low self-discharge rate, and fast charge rate compared to other types of secondary cells. However, its application to EVs and HEVs, which are exposed to extremely harsh conditions (in the perspective of battery usage) such as cold weather and long driving range as well as repetitive charge and discharge situations, constantly demands better batteries that have higher charge capacity, durability, cycle life, and faster charge rate than those of the existing batteries. These desirable traits are achievable not just by selecting the best type of materials (i.e., lithium iron phosphate and graphite) but also by optimizing the internal structure of batteries (i.e., separator thickness) and continuously monitoring the battery state to manage/control the external parameters (i.e., temperature) accordingly so that batteries are less exposed to unwilling conditions (i.e., 0% state-of-charge). The former is performed ex-situ by predicting in advance the battery behavior under specific conditions, while the latter is conducted in-situ by the battery management system (BMS) contained in a battery pack for EVs and HEVs.</p><p id="Par13">Two significant parameters in the battery must be tracked for battery health monitoring: state-of-charge (SOC) and state-of-health (SOH). SOC denotes the ratio of the current capacity to the fully charged capacity. On the other hand, SOH is the ratio of the fully charged capacity to the nominal capacity of the battery. The main difference can be intuitively understood such that while SOC can always be from 0 (fully discharged) to 1 (fully charged), SOH is 1 only at the time of manufacture and becomes 0.8 at its end-of-life (EOL). By being able to track the parameters correctly, one can design and control batteries much more effectively. Although the conventional ways of estimating SOC and SOH typically make use of several model-based and physics-based methods, a substantial amount of work is being made towards data-driven methods nowadays because of the incompleteness of the conventional methods. As for data-driven models, machine learning has been proven to demonstrate high accuracy, low computation, and the ability to learn from historical data, implicating its high feasibility to substitute the model-based and physics-based methods in the near future. The rest of this section describes various machine learning models that estimate SOC and SOH.</p><p id="Par14">SOC estimation by BMS that leverages data-driven methods has been accomplished fairly well, with test errors that range from 0.6 to 6.5% [<xref ref-type="bibr" rid="CR33">33</xref>]. Hu et al. [<xref ref-type="bibr" rid="CR34">34</xref>] presented SOC estimation using support vector regression (SVR) with a Gaussian kernel for which a double-step search discovers the optimal parameters. Since EVs run in diverse driving conditions in practice, parameters optimized using the available train data usually do not guarantee a good fit to unseen data unless the train data sufficiently reflects upon the real-world scenarios. The proposed methodology, however, exhibits a good generalization ability to test data under different operating conditions. Sahinoglu et al. [<xref ref-type="bibr" rid="CR35">35</xref>] introduced a novel approach of a recurrent Gaussian process regression (GPR) in which SOC estimate from the previous time step is fed back to the model as part of the input vector. The probabilistic nature of GPR allows for the quantification of the confidence intervals over the estimates and also for the identification of influential variables on the output, which are practical advantages of GPR over other machine learning models. The proposed model outperforms other models such as SVR, relevance vector machine (RVM), and neural network (NN) in RMSE and MAE but falls short in terms of computation time. Neal et al. [<xref ref-type="bibr" rid="CR36">36</xref>] used random forest, decision tree, and gradient boosted machine for SOC estimation of generated data using a physics-based simulation model. It is shown that they are generally capable of predicting the dynamics of the simulation model even though the computation times are much lower, implicating the high feasibility of machine learning as battery surrogate models.</p><p id="Par15">Similarly, SOH estimation has been conducted using many machine learning approaches, and it turned out to be more challenging than SOC estimation. Nuhic et al. [<xref ref-type="bibr" rid="CR37">37</xref>] presented SOH estimation on-board vehicles through SVR. The author creates an input training vector composed of operation history and Load Collective, a feature that reflects upon the change in environment, ambient, and load conditions. The train and test sets are split so that the test data contains information under a different driving profile to mimic the real-life scenario as much as possible. Guo et al. [<xref ref-type="bibr" rid="CR38">38</xref>] suggested a Bayesian formulation for the modeling of capacity fade where the coefficients of a linear regression model were formulated as probability distributions (e.g., normal distribution) to account for random effects in cell-to-cell variations. As such, this study contributes to modeling the random effects inherent in between-battery variations, which were usually neglected in prior studies. Tseng et al. [<xref ref-type="bibr" rid="CR39">39</xref>] stated that regression models that leverage fully discharge voltage and internal resistance as aging parameters could be more beneficial for SOH estimation than those with cycle numbers. The proposed regression model uses exponential terms with the aging parameters as input, and its coefficients are determined adaptively through particle swarm optimization. Khumprom et al. [<xref ref-type="bibr" rid="CR40">40</xref>] demonstrated a deep neural network-based approach and compared the performance against formerly used machine learning algorithms, including linear regression, k-nearest neighbors, SVR, and NN. Similarly, Ren et al. [<xref ref-type="bibr" rid="CR41">41</xref>] showed a deep neural network framework but it additionally comprises of multi-dimensional feature extraction step through an autoencoder model. The unsupervised way of feature extraction enables the exclusion of domain knowledge on the aging parameters. Severson et al. [<xref ref-type="bibr" rid="CR42">42</xref>] used a simple neural network but with a different optimization scheme, namely, elastic net, which places an additional particular regularization term. Several features, including the variance of the voltage-to-capacity slope, are used as input features. This approach proved to be very effective, showing the state-of-the-art prognosis result with 9.1% test error at the first 100 cycle point on the provided open dataset.</p><p id="Par16">To conclude, what mainly determined an accurate prognosis result on SOC and SOH estimation is not the machine learning approaches, but rather the aging parameters extracted based on authors' expert knowledge about the battery degradation. Furthermore, there is still room for improvement in the related studies since most of the experimental conditions are not fully representative of the real-world scenario, and the results for SOH and RUL estimation seem to be not accurate enough as of yet. It is advised to overcome these unsolved issues by tackling the hybrid approach that incorporates prior knowledge with more complex models in deep learning and reduces computational load. The reviewed studies in this section are summarized in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the battery</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR34">34</xref>]</p></td><td align="left"><p>Sensor signal (current, temperature, power, etc.)</p></td><td align="left"><p>Yes (ADVISOR simulator)</p></td><td align="left"><p>Regression (SOC estimation)</p></td><td align="left"><p>SVR</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR35">35</xref>]</p></td><td align="left"><p>Sensor signal (voltage, current, temperature)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (SOC estimation)</p></td><td align="left"><p>Recurrent GPR</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR36">36</xref>]</p></td><td align="left"><p>Sensor signal</p></td><td align="left"><p>Yes (P2D model)</p></td><td align="left"><p>Regression (SOC estimation)</p></td><td align="left"><p>DTs, RFs, GBMs</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR37">37</xref>]</p></td><td align="left"><p>Sensor signal (current, temperature, time, cycle number)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (SOH and RUL estimation)</p></td><td align="left"><p>SVM</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR38">38</xref>]</p></td><td align="left"><p>Sensor signal (SOC, cycle number)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (SOH and RUL estimation)</p></td><td align="left"><p>Bayesian method</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR39">39</xref>]</p></td><td align="left"><p>Sensor signal (voltage, internal resistance, cycle number)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (SOH and RUL estimation)</p></td><td align="left"><p>Statistical method (polynomial/exponential modelâ+âPSO optimization)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR40">40</xref>]</p></td><td align="left"><p>Sensor signal (<inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi mathvariant="italic">CT</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">DL</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${R}_{W}, {R}_{E}, {R}_{CT}, {C}_{DL}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12541_2021_600_Article_IEq1.gif"/></alternatives></inline-formula>)</p></td><td align="left"><p>Yes (NASA PCoE database)</p></td><td align="left"><p>Regression (SOH and RUL estimation)</p></td><td align="left"><p>DNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR41">41</xref>]</p></td><td align="left"><p>Sensor signal (voltage, current, temperature, cycle number)</p></td><td align="left"><p>Yes (NASA PCoE database)</p></td><td align="left"><p>Regression (RUL estimation)</p></td><td align="left"><p>Deep Auto-encoder</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR42">42</xref>]</p></td><td align="left"><p>Sensor signal (voltage, current, temperature, cycle number, charging time)</p></td><td align="left"><p>Yes (provided by the author)</p></td><td align="left"><p>Regression (RUL estimation)</p></td><td align="left"><p>Elastic net</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec5"><title>Robotics</title><p id="Par17">Robotics is one area that has been stimulated by the rise of AI to be at the core of automation and intelligent manufacturing process because a robot is what physically realizes the automation by the programmed movement of objects. A robot, however, can imply a different form of itself depending on the objective for which it is used. For example, a robot at the assembly line of the automobile industry typically reminds the general public of a dynamic multi-joint arm, whereas one in the military may look like a four-legged animal that can freely run, jump, and crawl around on bumpy terrain. Although they may differ in their looks, there are characteristics that all robots should share in common. All robots should have the capabilities to overcome the following challenges in order to operate as semi or fully autonomous robots [<xref ref-type="bibr" rid="CR43">43</xref>]:<list list-type="bullet"><list-item><p id="Par18">Challenge 1 and 2: learn complex, high dimensional dynamics and recognize an object</p></list-item><list-item><p id="Par19">Challenge 3: learn control policy in a dynamic environment</p></list-item></list></p><p id="Par20">The listed challenges are generally confronted by robots in a scenario where they need to process and comprehend given signal such as image and GPS signal (Challenge 1), spot any objects of interest nearby (Challenge 2), and stop moving and find another way if faced by an obstacle (Challenge 3).</p><p id="Par21">Understanding the complex meaning behind a spotted scene or an image requires a model to learn and find a hidden pattern or knowledge from a large dataset in a similar context. Mariolis et al. [<xref ref-type="bibr" rid="CR44">44</xref>] demonstrated that a robot could recognize the category (shirt, pants, and towel) and pose of hung garments through deep CNN, which has previously been considered a very challenging problem in computer vision due to the intricate state space of such highly deformable objects. Gao et al. [<xref ref-type="bibr" rid="CR45">45</xref>] fused two branches of CNN, a visual CNN and a haptic CNN, so that robots can get a better tactile understanding of an object, stressing the fact that humans also benefit from a cognitive pattern where both visual and haptic experiences of the physical world are involved for the understanding of things. To imitate such patterns, the branch of visual CNN is fed with real-world images of an object, while the haptic CNN branch is fed with signals of five types of physical quantities (e.g., fluid pressure and core temperature.). The proposed model shows a high classification accuracy of objects initially labeled as 24 different haptic adjectives (e.g., bumpy, soft, porous, compressible, sticky, and textured). Polydoros et al. [<xref ref-type="bibr" rid="CR46">46</xref>] proved the superiority of deep learning models in the learning of inverse dynamics of a robotic manipulator. The study suggests replacing the conventional physics-based models that cannot cope with the change in robot structure and dynamic environments with the proposed model, which is largely a DNN with one hidden layer modeled as an RNN. Similarly, Lenz et al. [<xref ref-type="bibr" rid="CR47">47</xref>] used a deep learning-based framework called DeepMPC to handle robotic food-cutting, wherein a deep recurrent model is devised to model a time-varying nonlinear dynamics involved in the task. The study shows that the model which implements recursive learning of internal representation as a new control signal is given constantly improves the prediction output, implying that deep learning can be very efficient in learning complex and high-dimensional dynamics.</p><p id="Par22">In particular, reinforcement learning, a subfield of machine learning, is widely adopted for learning control policies (Challenge 3) in robotics. This is mainly because reinforcement learning involves a computational agent that makes decisions by trial and error, and it has been shown to be highly efficient in modeling human-like cognitive behavior in the real world. Lillicrap et al. [<xref ref-type="bibr" rid="CR48">48</xref>] presented a model-free algorithm based on Deep Q-Network (DQN) that can operate over continuous action space. It is shown that even without getting full access to the dynamics of the domain, it outperforms the conventional planning algorithms with full access on more than 20 simulated physics tasks, including cart pole swing-up. This is made possible by adapting DQN to continuous domains by simply discretizing the action space. On the other hand, Levine et al. [<xref ref-type="bibr" rid="CR49">49</xref>] developed end-to-end learning of control policies by a novel CNN architecture trained using a guided policy search method. The method is evaluated by learning control policies for several physics tasks, such as placing a coat hanger on a rack with a robot. Such tasks require object localization, tracking, and understanding of contact dynamics. It is demonstrated that rather than training the perception and control systems separately, it is better to do it in an end-to-end fashion. The reviewed studies in this section are summarized in Table <xref rid="Tab3" ref-type="table">3</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the robotics</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR44">44</xref>]</p></td><td align="left"><p>Depth image</p></td><td align="left"><p>Yes (RD1 dataset)</p></td><td align="left"><p>Classification (category estimation)</p><p>Regression</p><p>(pose estimation)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR45">45</xref>]</p></td><td align="left"><p>Vision image, haptic signal</p></td><td align="left"><p>Yes (MINC and PHAC-2 dataset)</p></td><td align="left"><p>Classification (category estimation)</p></td><td align="left"><p>CNN-LSTM, ConvNet, GoogleNet</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR46">46</xref>]</p></td><td align="left"><p>Sensor signal (position, velocity and acceleration)</p></td><td align="left"><p>Yes (Sarcos and Barret dataset)</p></td><td align="left"><p>Regression (dynamics modeling)</p></td><td align="left"><p>GHL (Generalized Hebbian Learning), RNN, Bayesian linear regression</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR48">48</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression</p><p>(dynamics modeling)</p></td><td align="left"><p>DQN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR49">49</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression</p><p>(dynamics modeling)</p></td><td align="left"><p>CNNâ+âpolicy search method</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec6"><title>Renewable energy</title><p id="Par23">In this renewable energy section, wind energy and photovoltaic energy are thoroughly discussed. Nowadays, wind energy is one of the most importantÂ renewable energy sources. The market is growing and maturing itself, so it is necessary to improve operation stability, maintenance, and efficiency. Forecasting and monitoring energy production, fault detection and diagnosis (FDD), parameter optimization in wind energy systems are crucial. Here, we discuss FDD applied in the wind turbine (WT), which contains the following components: rotor, blade, gearbox, and bearing, as shown in Fig.Â <xref rid="Fig2" ref-type="fig">2</xref> [<xref ref-type="bibr" rid="CR50">50</xref>].<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>Components of a wind turbine</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12541_2021_600_Fig2_HTML.png" id="MO2"/></fig></p><p id="Par24">First, the WT component with a significant failure rate and downtime is the rotor blade. As the main fault, structural damage such as splitting, fatigue, wear, deterioration, deflection occurs [<xref ref-type="bibr" rid="CR51">51</xref>]. Due to the prolonged maintenance time during a shutdown cycle, manual inspection, which is the most general method, accompanies tiresome human involvement. To avoid such an inefficient procedure, Reddy et al. [<xref ref-type="bibr" rid="CR52">52</xref>]. suggested that an unmanned aerial vehicle (UAV) with optical cameras can monitor WT surface damage and noticeable visual features. By training a CNN with photos collected by UAVs and based on the intensity of the damage with feature recognition of a picture, structural health monitoring of WT blades can be conducted. Liu et al. [<xref ref-type="bibr" rid="CR53">53</xref>] developed a stiffness prediction method for WT blades built on deep learning networks. To achieve the prediction using historical fatigue stiffness data, three training models, CNN, LSTM, and CNN-LSTM, are used. The results show that the models can learn features straight from raw stiffness data and estimate residual stiffness. Combining the strength of different single network models such as CNN and LSTM, CNN-LSTM is shown to complement the weakness of a single network. Kong et al. [<xref ref-type="bibr" rid="CR54">54</xref>] showed that the health state of WTs was precisely defined by careful attention to the shift details on the spatial and temporal scale of the SCADA data. The author suggested a condition monitoring system of WTs based on spatiotemporal features fusion by CNN and the gated recurrent unit (GRU). The WT will be considered abnormal if the index obtained from the online SCADA data exceeds the established threshold for the successive time. For standard working environments, the index will alter in the range of the threshold.</p><p id="Par25">As a vital component in WTs, due to its long downtimes and complicated maintenance procedures, the gearbox commonly undergoes multiple failures, such as bearing damage, tooth breakage, and gear crack resulting in high maintenance cost. Jiang et al. [<xref ref-type="bibr" rid="CR55">55</xref>] introduced a new architecture, MSCNN, which acquires high-level, robust fault characteristics at various time scales directly by hierarchical learning from complex raw vibration signals in a parallel way. It substitutes for better robustness against noises of a wide range than the conventional CNN, heavily relying on the hand-crafted features, thus mainly reducing the demand for prior information and time-consuming signal processing steps. Therefore, due to its end-to-end feature learning capability, the proposed method can offer a valuable alternative as a general-purpose classification technique for intelligent fault diagnosis. Radford et al. [<xref ref-type="bibr" rid="CR56">56</xref>] proposed deep convolutional generative adversarial networks (DCGANs) for health condition monitoring (HCM) in an unsupervised manner. In contrast with existing unsupervised models such as autoencoder (AE), Chen et al. [<xref ref-type="bibr" rid="CR57">57</xref>] also utilized DCGAN for long-term accurate HCM of a WT bearing. In these monitoring frameworks, the critical obstacle is establishing a threshold for detecting different health conditions. Based on DCGAN, a self-setting threshold scheme is proposed to overcome the drawback. As DCGAN is adequately trained, a threshold for HCM can be automatically generated through its output. The threshold value can be automatically created by the output of the G network in the DCGAN model. To this end, the whole scheme creates a self-setting HCM threshold based on a DCGAN model to observe a WT bearing.</p><p id="Par26">Photovoltaics (PV) can provide electric energy directly from solar energy, and it is one of the promising renewable energy technologies. By converting sunlight photons directly into electricity, PV cells make energy conversion. As seen in Fig.Â <xref rid="Fig3" ref-type="fig">3</xref>, PV cells are arranged either in series or in parallel to form PV modules. PV modules constitute PV panels, and several PV panels make up a PV array. This section primarily discusses the FDD of PV systems.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>PV cell, module, panel, and array</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12541_2021_600_Fig3_HTML.png" id="MO3"/></fig></p><p id="Par27">Deitsch et al. [<xref ref-type="bibr" rid="CR58">58</xref>] proposed the general training system for SVM and CNN for the automated identification of defects in a single PV cell image. Simonyan et al. [<xref ref-type="bibr" rid="CR59">59</xref>] used deep convolutional networks consisting of up to 19 convolution layers (VGG-19) and stated that representation depth is beneficial for classification accuracy. Alcantarilla et al. [<xref ref-type="bibr" rid="CR60">60</xref>] suggested a KAZE feature related to the analogy with nonlinear diffusion processes in the image domain. This method is for multiscale 2D feature detection and description in nonlinear scale-spaces. Deitsch et al. [<xref ref-type="bibr" rid="CR58">58</xref>] suggested a fine-tuned regression CNN based on VGG-19, which is trained on enhanced module images. Both SVM and CNN classifiers fulfill equally well on monocrystalline and polycrystalline PV modules, with just a negligible advantage on average for the CNN. Cautiously built SVMs are trained on diverse features derived from PV cells EL images but can operate on random hardware. On the more inhomogeneous polycrystalline cells, however, the CNN classifier outperforms the SVM classifier by around 6% accuracy. Both automatic methodologies make constant, exceptionally accurate monitoring of PV cells feasible. For the diagnosis of different types of observable module defects, Li et al. [<xref ref-type="bibr" rid="CR61">61</xref>] proposed an automated UAV-based deep-learning CNN inspection method in order to determine the operational status of PV modules. The principle of the technique is to obtain in-depth features from module images and conduct pattern recognition of defects. The alternating convolution and sub-sampling operations are first conducted at CNN, and then a generalized multi-layer network is eventually implemented. For the completely linked sheet, the output is flattened as a vector, and the softmax function is exploited to identify the gained features to several classes. The result supports its usefulness with high precision in diagnosing numerous types of PV modules' general defects, including dust shading, encapsulant delamination, gridline corrosion, snail trails, and yellowing.</p><p id="Par28">For faults detection in PV panels, Herraiz et al. [<xref ref-type="bibr" rid="CR62">62</xref>] suggested a novel approach that uses a UAV-embedded thermographic camera to detect hot spots and set their positions on PV panels. To produce a stable detection structure, two novel region-based convolutional neural networks (R-CNNs) are unified. The combination of thermography and telemetry data to respond to panel condition monitoring is the key contribution. The data is collected and then converted automatically, facilitating fault detection during the examination. To encourage the performance, durability, and protection of PV systems, automated FDD techniques for PV arrays are critical. Belaout et al. [<xref ref-type="bibr" rid="CR63">63</xref>]. suggested a multiclass adaptive neuro-fuzzy classifier (MC-NFC) for automated PV array fault detection and classification, with more discriminative capability compared to an artificial neural network (ANN) classifier. Adopting space dimensionality reduction techniques provides the classifier with a clean way to select their inputs, strong classification precision, and lower space dimensionality characteristics to speed up the classification process. Chen et al. [<xref ref-type="bibr" rid="CR64">64</xref>] presented a smart FDD method for PV arrays based on a newly designed deep residual network model trained by the algorithm of adaptive moment estimation. The proposed model can automatically extract features from raw currentâvoltage curves, atmospheric irradiance, and temperature and effectively boost efficiency with a deeper network. Based on the output I-V characteristic curves and input ambient condition details, the method can detect numerous types and levels of typical early PV array faults, including partial shading, loss, short circuit, and open circuit faults.</p><p id="Par29">One of the major difficulties in PV solar power production is holding the designed PV systems running with the optimal operating performance. Harrou et al. [<xref ref-type="bibr" rid="CR65">65</xref>] proposed a model-based anomaly detection method for tracking the DC side of PV systems and transient shading. To replicate the monitored photovoltaic array characteristics, a model based on the one-diode model with binary clustering algorithms for more accurate fault detection is set up. The residuals from the simulation model are then exposed to a one-class support vector machine (1-SVM) protocol for fault detection.</p><p id="Par30">In this section, many deep learning methods for monitoring the system and detecting faults are discussed. Most studies seemed to be adaptable to only a small part of the system, so future works are needed to focus on big data from variable sensors. Some novel approaches have not yet been applied to the practical operation environments due to the limitation of large-scale, high-quality data, and intensely minimized images, and so on. The reviewed studies in this section are summarized in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the renewable energy</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR52">52</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR53">53</xref>]</p></td><td align="left"><p>Sensor signal (stiffness)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (stiffness prediction)</p></td><td align="left"><p>CNN, LSTM, CNN-LSTM</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR54">54</xref>]</p></td><td align="left"><p>Vision image,</p><p>sensor signal</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault diagnosis)</p></td><td align="left"><p>CNN, GRU</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR55">55</xref>]</p></td><td align="left"><p>Sensor signal (vibration)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>Multi-scale CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR57">57</xref>]</p></td><td align="left"><p>Sensor signal (vibration)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (fault diagnosis)</p></td><td align="left"><p>DCGAN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR58">58</xref>]</p></td><td align="left"><p>Electro-luminescence images</p></td><td align="left"><p>Yes (provided by the author)</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>CNN, SVM</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR61">61</xref>]</p></td><td align="left"><p>Vision images</p></td><td align="left"><p>Yes (available upon request)</p></td><td align="left"><p>Classification (fault diagnosis)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR62">62</xref>]</p></td><td align="left"><p>Thermographic images,</p><p>telemetry data</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault diagnosis)</p></td><td align="left"><p>R-CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR63">63</xref>]</p></td><td align="left"><p>Sensor signal (current, voltage, temperature and irradiance)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>MC-NFC (Multiclass Adaptive Neuro-Fuzzy Classifier)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR64">64</xref>]</p></td><td align="left"><p>Sensor signal (current, voltage, temperature, irradiance)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault diagnosis)</p></td><td align="left"><p>ResNet</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR65">65</xref>]</p></td><td align="left"><p>Sensor signal (current, voltage, temperature, irradiance)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (anomaly detection)</p></td><td align="left"><p>One-class SVM</p></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec7"><title>Applications of AI for Manufacturing Process Enhancement</title><sec id="Sec8"><title>Steel</title><p id="Par31">Steel mills, also known as steelworks, are one of the most fundamental industries in the modern world, which specialize in steel production. In this section, AI applications in various steelmaking such as ironmaking, casting, rolling, and galvanizing are introduced. To achieve more sustainable production and environmentally beneficial methods, this steel section mostly discusses FDD, and comparative study of several techniques, modeling, and forecasting of production.</p><p id="Par32">In ironmaking, a blast furnace (BF) is a key unit that consumes more than 70% of the energy in the whole steelmaking process. The ideal operation of the ironmaking method of BF relies greatly on the calculation of the molten iron quality (MIQ) indices. Zhou et al. [<xref ref-type="bibr" rid="CR66">66</xref>] introduced a novel data-driven robust modeling process for the online estimation and control of multivariate MIQ indices. First, for the MIQ indices, a nonlinear autoregressive exogenous (NARX) model is built to fully capture the nonlinear dynamics of the BF method. A multi-task transfer learning is then suggested to develop a new multi-output least-squares support vector regression (M-LS-SVR) to learn the NARX model, given that the standard LS-SVR does not directly cope with the multi-output issue. It has been demonstrated that the evolved model not only provides operators with accurate MIQ information to make an effective decision for optimum manufacturing operations with good consistency, adaptability, and robustness but also helps to implement input management of the BF process.</p><p id="Par33">For slag quality, tapping temperature, and hot metal quality, the silicon content of the hot metal is also a significant characterization parameter in ironmaking process. Han et al. [<xref ref-type="bibr" rid="CR67">67</xref>] suggested a parallelization scheme to build an SVM solution algorithm under the Hadoop platform to enhance the SVM solution speed on large data sample sets. On the Hadoop platform, dynamic estimation of blast furnace Si content is achieved. The greatest benefit of this algorithm is that, by way of the structural risk minimization theory, it can prevent dimensionality disasters with kernel features and realize the optimum generalization efficiency of the algorithm. The algorithm is primarily applicable to small sample results.</p><p id="Par34">Another important mechanism is to forecast hot metal temperature (HMT) in a BF to ensure that the ironmaking process runs smoothly. By comparing deep and shallow predictive approaches, the current period and multi-step-ahead HMT prognosis are given by Zhang et al. [<xref ref-type="bibr" rid="CR68">68</xref>]. From the point of implementation to an industrial BF, three advanced deep predictive models, including DNN, LSTM, and CNN, and seven successful shallow predictive models including partial least squares (PLS), locally weighted (LW)-PLS, Gaussian process regression (GPR), support vector regression (SVR), random forest (RF), boosted regression trees (BRT), and shallow neural network (SNN) are studied. The results demonstrated that the shallow neural network is preferred for current time HMT prediction. Moreover, GPR and SVR are selected for multi-step-ahead HMT predictions. The findings of the experiment are that PLS is the simplest approach with the cheapest cost of calculation but with less competitive prediction precision. In comparison, it is more expensive to calculate LW-PLS. Other than that, SNN and DNN are considered to attain better prediction precision in forecasting current time HMT than other techniques. SNN is favored for current HMT prediction because DNN has an acute model complexity and calculation expense than SNN. GPR and SVR are particularly appropriate for HMT forecasts of one hour ahead and two hours ahead. In comparison, both the current period and multi-step-ahead HMT forecasts have been particularly inappropriate for LSTM and CNN.</p><p id="Par35">Continuous casting is the procedure where molten steel is allowed to solidify. Such continuity of the process can save the cost of the casted steel. Moreover, carefully monitored and controlled casting can attain a high quality of steel casts. Early detection and prediction of the sticker, centerline segregation, mold level, mold breakout, and slab consistency are the main issues in continuous casting. Therefore, fault identification and prediction in continuous casting are studied as a second aspect of the steel industry application. For a better understanding of continuous casting, Fig.Â <xref rid="Fig4" ref-type="fig">4</xref> is shown below.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Continuous casting process</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12541_2021_600_Fig4_HTML.png" id="MO4"/></fig></p><p id="Par36">The most costly and hazardous problem of continuous casting is the breakout, which involves the loss of processing time and substantial penalties for yield. The sticker, a part of a stranded shell, which adheres to a mold surface, is the common reason for the breakout. A temperature pattern in a mold heat map will detect stickers. By monitoring and analyzing the temperature data from the fiber optical sensors installed on a mold, Fasizullin et al. [<xref ref-type="bibr" rid="CR69">69</xref>] presented a cyber-physical system that detects stickers. The author developed a special CNN, which identifies a sticker pattern and can be used as a full-fledged replacement or an assistant of the existing algorithm. Such an approach was implemented as the sticker detection system (SDS), a method when CNN works alone and the breakout prevention system (BPS) is idle. The BPSâ+âSDS approach suggests that, after the sticker warning from BPS, SDS analyzes only suspicious circumstances. The study demonstrates that CNN decreases the number of false alarms of the current algorithm.</p><p id="Par37">Decreasing centerline segregation of casting slabs in the continuous casting process is an important parameter for a better mechanical property. For early detection of the centerline segregation from operation input parameters measured in continuous cast steel slabs, Nieto et al. [<xref ref-type="bibr" rid="CR70">70</xref>] showed a novel hybrid algorithm based on SVM combined with the particle swarm optimization (PSO). In addition, for comparative purposes, the experimental results include an MLP and a multivariate adaptive regression splines (MARS) approach in conjunction with the PSO. First, the importance of each physicalâchemical variable on the segregation is addressed via the model. Second, models are obtained for forecasting segregation. Then, regression with optimal hyper-parameters is conducted. When this hybrid PSO-SVM-based model with RBF kernel function is tested on an experimental dataset, the coefficient of determination and average width are equal to 0.98 and 0.97, respectively. Wu et al. [<xref ref-type="bibr" rid="CR71">71</xref>] suggested a novel multiscale convolutional and recurrent neural network MCRNN architecture for which the input is converted at various scales and frequencies, recording both long-term patterns and short-term shifts in time series. The suggested system outperforms traditional time series classification approaches with improved feature representation. The experimental findings and comprehensive contrast with state-of-the-art techniques indicate the supremacy of the proposed MCRNN framework, which has adequate prediction efficiency and strong potential to enhance the quality of casting slabs.</p><p id="Par38">After the casting process, steel goes through the rolling mills to reduce the thickness and obtain high uniformity. In this process, a steel slab is placed between two rolls, and after undergoing several rolls, the thickness can be altered. The key factors in the rolling part are mainly based on the crown of the strip, temperature, rolling power, bending force, and flatness. Zhang et al. [<xref ref-type="bibr" rid="CR72">72</xref>], for the dynamic rolling process, suggested a nonlinear full condition monitoring model. First, for condition recognition, a dissimilarity index (DI) is decided, and a support vector model is developed to check the idle condition. Second, for sluggish feature analysis and co-integration analysis, t-distributed stochastic neighbor embedding (t-SNE) is utilized to remove nonlinear principal components. To gain a coil with a precise thickness after the rolling phase, it is important to pre-determine the exact rolling power.</p><p id="Par39">Li et al. [<xref ref-type="bibr" rid="CR73">73</xref>] proposed precise bending force prediction, which can enhance the control precision and flatness of the strip crown and further boost the strip shape quality. The author suggested six machine learning models, including ANN, SVR, classification and regression trees (CART), bagging regression tree (BRT), least absolute shrinkage and selection operator (LASSO), and gaussian process regression (GPR), which were implemented in the HSR process to predict the bending force. The findings indicate that GPR, with the best prediction precision, better stability, and reasonable computational expense, is the optimal model for bending force prediction.</p><p id="Par40">Strip shape prediction is a crucial task for a high-quality product. Sun et al. [<xref ref-type="bibr" rid="CR74">74</xref>] proposed an ensemble algorithm, random forest (RF), to forecast hot-rolled strip crowns. To develop three machine learning models, namely SVM, regression tree (RT), and RF, parameter tuning based on mean squared error is carried out. Results reveal that RF is the most preferred model to strip crown prediction because of the accurate results. For profile and flatness predictions, Wang et al. [<xref ref-type="bibr" rid="CR75">75</xref>] presented three hybrid models, including GA-MLP, MEAMLP, and PCA-MEA-MLP. In comparison with the hybrid GA-MLP model, the hybrid PCA-MEA-MLP model established after dimensionality reduction of input variables by PCA can improve training time without decreasing model prediction accuracy, which is an important means of model simplification.</p><p id="Par41">Hot-dip galvanizing is the process of submerging steel in a molten zinc bath to obtain corrosion resistance to protect the steel from harsh environments. As the last part of the steel industry application, the rest of this section discusses prediction and monitoring of tensile stress, yield stress, ultimate tensile strength, coating weight, and coating thickness of hot-dip galvanizing for a cost-effective process.</p><p id="Par42">By controlling the main process parameters within defined limits, mechanical properties, that is, yield strength and ultimate tensile strength, are obtained in the galvanizing line of the cold rolling mill. In order to predict the mechanical properties of a coil, Lalam et al. [<xref ref-type="bibr" rid="CR76">76</xref>] used an ANN. To prevent the consequences of redundancy and collinearity of input variables for the ANN, a key component analysis is used. To monitor the predicted mechanical properties and process parameters of a galvanized coil, an online quality management system is established. Colla et al. [<xref ref-type="bibr" rid="CR77">77</xref>] presented a machine learning-based system to enhance the homogeneity of tensile properties of steel strips. Two types of data-driven mechanical property prediction models have been adopted: a first-order polynomial model and a feedforward neural network (FFNN). The suggested system can improve its performance through time and keep up-to-date concerning the development of the product and evolving consumer demands.</p><p id="Par43">Pan et al. [<xref ref-type="bibr" rid="CR78">78</xref>] suggested an advanced neural network-based coating weight control approach for hot-dip galvanizing lines. The framework consisted of a feedforward control (FFC) and feedback control (FBC), together with a neural network predictive model, a bias-update module, and a real-time optimizer. Through this framework, nonlinearity, large time-variant delays, disturbances, and unsynchronized regulation of two manipulated variables (MVs) have been addressed. Both the coating weight variance and the transition time were greatly reduced as well. Mao et al. [<xref ref-type="bibr" rid="CR79">79</xref>] introduced a groundbreaking neural network model consisting of the BP algorithm and the genetic algorithm for the first time to model and predict the thickness of the hot-dip galvanized zinc sheet. In the model, the major influences of the coating thickness such as the stripline speed, air knife pressure, air knife to strip distance, and air knife height are used as the model input parameters. Furthermore, the coating thickness is the model output parameter of the hot-dip galvanizing system. Simulations demonstrate that the GA-BP algorithm, as opposed to standard coating thickness models, increases estimation precision and converges quicker and that it can be used as input in a closed loop zinc layer thickness control method.</p><p id="Par44">In conclusion, in each steelmaking process, newly adopted AI-based methodologies are discussed. It can be seen that they can facilitate high precision and an intense monitoring system, unlike conventional supervised processes, which are not profitable and efficient. However, some research still needs to be conducted further with more complex models or combined with other algorithms to improve its performance and reduce computational load. The reviewed studies in this section are summarized in Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the renewable energy</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR66">66</xref>]</p></td><td align="left"><p>Sensor signal (temperature, pressure, speed, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (estimation of multivariate MIQ indices)</p></td><td align="left"><p>M-LS-SVR (Multi-output Least-Squares)â+âNARX</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR67">67</xref>]</p></td><td align="left"><p>Sensor signal (volume, flux, temperature, etc.)</p></td><td align="left"><p>Yes (available upon request)</p></td><td align="left"><p>Regression (Si content prediction)</p></td><td align="left"><p>SVM</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR68">68</xref>]</p></td><td align="left"><p>Sensor signal (temperature, humidity, pressure, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (hot metal temperature prediction)</p></td><td align="left"><p>DNN, LSTM, CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR69">69</xref>]</p></td><td align="left"><p>Sensor signal (temperature)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (Sticker detection)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR70">70</xref>]</p></td><td align="left"><p>Sensor signal (flow, speed, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (centerline segregation)</p></td><td align="left"><p>PSO-SVM, PSO-MARS, MLP</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR71">71</xref>]</p></td><td align="left"><p>Sensor signal (mold level fluctuation)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (casting slab quality prediction)</p></td><td align="left"><p>MCRNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR72">72</xref>]</p></td><td align="left"><p>Sensor signal (stationary and non-stationary variables)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (condition monitoring)</p></td><td align="left"><p>NCA, NSFA, SVDD</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR73">73</xref>]</p></td><td align="left"><p>Sensor signal (temperature, thickness, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (bending force prediction)</p></td><td align="left"><p>ANN, SVR, CART, GPR, BRT, LASSO</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR74">74</xref>]</p></td><td align="left"><p>Sensor signal (thickness, temperature, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (strip crown prediction)</p></td><td align="left"><p>SVC, RT, RF</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR75">75</xref>]</p></td><td align="left"><p>Sensor signal (thickness, width, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (profile and flatness prediction)</p></td><td align="left"><p>GA-MLP, MEA-MLP, PCA-MEA-MLP</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR76">76</xref>]</p></td><td align="left"><p>Sensor signal (temperature, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (tensile strength prediction)</p></td><td align="left"><p>ANN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR77">77</xref>]</p></td><td align="left"><p>Sensor signal (thickness, speed, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (yield strength and ultimate tensile strength prediction)</p></td><td align="left"><p>ANN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR78">78</xref>]</p></td><td align="left"><p>Sensor signal (air knife gap, pressure, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression</p></td><td align="left"><p>FFC, FBC</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR79">79</xref>]</p></td><td align="left"><p>Sensor signal (speed, pressure, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Regression (zinc layer thickness prediction)</p></td><td align="left"><p>PB-GA-BP</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec9"><title>Semiconductor</title><p id="Par45">The technological advancement of fast computing chips from the semiconductor industry has made possible the current status of AI for various engineering applications. However, the one-sided relationship is now beginning to shift to a bi-directional one as the growth of both sides is mutually beneficial: AI is about to change the semiconductor industry in return.</p><p id="Par46">Generally, a semiconductor is processed on top of a silicon wafer. The wafer undergoes several procedures in which an insulator is formed through oxidation, a pattern is drawn by photolithography, etching is done using an etchant, a thin film is formed through evaporation or sputtering, and so on. These processes demand extremely high precision and thus prohibit anything (e.g., tiny dust) that may be a source of defect. Even though most semiconductor fabs are controlled under a near dust-free environment, defects that bring a severe impact on the yield are still detected in manufacturing lines due to other factors, including machine error and human mistakes. Approaches to detecting and localizing defects on wafers are twofold: image-based and signal-based fault detection. Here, studies that use both approaches are discussed.</p><p id="Par47">The more common way of wafer fault detection is the intricate image-based detection through deep learning. Not only do defects indicate the specific fault location, but the way defects tend to cluster and form a pattern can also provide information on the root causes of malfunction. Imoto et al. [<xref ref-type="bibr" rid="CR80">80</xref>] automated the classification process by using a CNN-based transfer learning method for monitoring the occurrence frequency of defect types that are useful for figuring out the root causes of process failures. The author suggests using transfer learning in a weakly supervised sense as there exists a massive incoherent labeled data in storage. The proposed methodology involves pre-training and fine-tuning. The former is done with massive data containing numerous incorrect labels. After that, the parameters of only the final layers are fine-tuned using few highly reliable labeled data. Yang [<xref ref-type="bibr" rid="CR81">81</xref>] achieved a classification accuracy of 99.2% and 98.1% using CNN and extreme gradient boosting (XGBoost), respectively. Such high accuracy is achieved by carefully exploring the wafer map and finding out that the defect signature of the wafer appears to have some spatial correlations in the background, thereby transforming the images in a way to enhance the contrast between the signal and the background. The images are pre-processed using singular value decomposition (SVD) that eventually removed much noise around the defects. In addition, data augmentation (e.g., random cropping, rotating, resizing, and flipping) is conducted to obtain a better model. On the other hand, Tello et al. [<xref ref-type="bibr" rid="CR82">82</xref>] classified both single-defect and mixed-defect patterns using randomized general regression network (RGRN) and CNN. It is stated that previously reported studies on defect classifications mainly focus on single-defect patterns and thus utilize simple models. Here, three stages are involved for the classification. In the first stage, a spatial filter is applied to remove noise in raw images. Then, a splitter based on information gain theory generates rules to identify and separate single-defect and mixed defect patterns. Lastly, the single-defect classified data are fed to RGRN while mixed-defect ones are given to CNN for training and testing. The separation of pattern types using the splitter turns out to be more effective than the traditional end-to-end deep learning techniques that are previously reported. OâLeary et al. [<xref ref-type="bibr" rid="CR83">83</xref>] presented an interesting study where the classification of the chemical composition of particle defects was carried out. Although a simple CNN model is used, an investigation is made to validate the merge of spectral data from EDX spectroscopy with fully connected layers of CNN. The CNN, therefore, extracts features from input raw images as well as the spectral data simultaneously, and the results show a significant rise in overall classification accuracy.</p><p id="Par48">The conventional method to inspect defects visually through a high-resolution camera faces limitations as it needs to be informed of all types of defects and their possible shapes in advance. Such inspection requires the labeling of numerous defects of shapes of a wide spectrum, which usually accompanies error and tedious human labor. Therefore, it is necessary to make use of the larger portion of unlabeled wafer maps that are available for training. Yu et al. [<xref ref-type="bibr" rid="CR84">84</xref>] devised a stacked convolutional sparse denoising autoencoder (SCSDAE) which is a combination of CNN and SDAE. A sparse autoencoder (SAE) is known to learn relatively sparse features as it constraints the learning process by adding a sparse penalty term, thereby optimizing the network weights better than the standard AEs. Coupled with SAE, the denoising feature of the network provides robustness in feature representation even if input data has been stochastically corrupted. Two SDAEs are involved in feature extraction steps that are further enhanced by convolution and max-pooling for learning even more discriminant features, hence the name SCSDAE. Nakazawa et al. [<xref ref-type="bibr" rid="CR85">85</xref>] not only detected abnormal defect patterns but also segmented them using an end-to-end deep convolutional encoder-decoder network. The proposed network is based on a fully convolutional network (FCN) that comprises mostly convolution layers for segmentation. Segmentation of defect patterns allows for the extraction of supplementary information such as location, size, major and minor axis length, and orientation. It is shown that it surpasses base architectures like FCN, SegNet, and U-Net in detection performance.</p><p id="Par49">Even though image-based detection is taking over much of the highlights, there is a substantial amount of studies regarding signal-based methods. Lee et al. [<xref ref-type="bibr" rid="CR86">86</xref>] showed that fault diagnosis to find root causes of process failures could be effectively carried out even using a black box CNN model. This is enabled particularly by tailoring the CNNâs receptive field over multivariate sensor signals along the time axis that allows for the association of its extracted features from hidden layers with the physical meaning of raw data. This further enables to locate the variable and time of process failures. Lee et al. [<xref ref-type="bibr" rid="CR87">87</xref>] focused on reducing the noise while maintaining valuable information as much as possible for reliable and robust fault monitoring. For reducing the noise, the author proposes SDAE for which several DAEs are pre-trained with latent representation from the previous time step given as input. Once the pre-training stage is done, the final layer is switched for defect classification, and further fine-tuning is carried out. The study shows that the proposed model is more robust as noise severity increased than twelve other machine learning models used for comparison. Kim et al. [<xref ref-type="bibr" rid="CR88">88</xref>] used sensor data called status variables identification (SVID) of varying lengths, which is important for early fault detection. The main difference of the proposed method from previous studies is that it shows robustness for SVID of different sequence lengths. Inspired by text classification in which sequences of varying length are encoded into a fixed-size vector, the author implements a self-attention mechanism to distribute attention over a fixed-size vector so that the model can pay attention to a certain time when faults occur. Azamfar et al. [<xref ref-type="bibr" rid="CR89">89</xref>] solved the issue of data drift; that is, data distribution is shifted as operating conditions and environment are changed. In a real-world scenario, due to various disturbances, operators often face times when the testing conditions become different from the normal, which leads to differently distributed train and test data. The proposed model is no different from a standard CNN. However, it has an additional loss term called maximum mean discrepancy (MMD) loss which is a metric that quantifies the distribution discrepancy between the source and the target domains. It is stated that the addition of the loss term contributes to a big increase in classification accuracy. Unlike previous works that mostly utilize CNNs, Kim et al. [<xref ref-type="bibr" rid="CR90">90</xref>] showed that RNN could be used as anomaly detection at an early stage. The effectiveness of the method is that it can pre-detect anomalies even if the model is not trained with defective data in advance. The model, so-called DeepNAP, consists of a detection module and a prediction module. The latter capitalize on the power of LSTM to forecast the future signal that is then passed to the detection module for latent representation and anomaly detection. In the detection module, the early part of the LSTM output is treated as the target signal used for optimization under partial reconstruction loss function. The partial reconstruction loss provides higher anomaly scores on the defective parts of the input signal, making it suitable for such application. It is shown that the proposed model outperforms other baseline architectures on the pre-detection of anomalies. The reviewed studies in this section are summarized in Table <xref rid="Tab6" ref-type="table">6</xref>.<table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>An overview of the surveyed literature regarding the semiconductor</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Reference nos.</p></th><th align="left"><p>Data type</p></th><th align="left"><p>Data publicity/openness</p></th><th align="left"><p>Task</p></th><th align="left"><p>Algorithm/model</p></th></tr></thead><tbody><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR80">80</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR81">81</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (pattern recognition)</p></td><td align="left"><p>CNNâ+âXGBoost</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR82">82</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (pattern recognition)</p></td><td align="left"><p>RGRN (Randomized General Regression Network), DSCN (Deep Structured Convolutional Network)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR83">83</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR84">84</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (synthesized wafer maps)</p></td><td align="left"><p>Classification (pattern recognition)</p></td><td align="left"><p>SCSDAE (Stacked Convolutional Sparse Denoising Auto-encoder)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR85">85</xref>]</p></td><td align="left"><p>Vision image</p></td><td align="left"><p>Yes (synthesized wafer maps)</p></td><td align="left"><p>Classification (fault detection and segmentation)</p></td><td align="left"><p>CAE</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR86">86</xref>]</p></td><td align="left"><p>Sensor signal (temperature, pressure, gas flow rate, etc.)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>FDC-CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR87">87</xref>]</p></td><td align="left"><p>Sensor signal (temperature, volume)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>SdA (Stacked denoising Auto-encoder)</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR88">88</xref>]</p></td><td align="left"><p>Sensor signal (SVID)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>Self-attentive CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR89">89</xref>]</p></td><td align="left"><p>Sensor signal (machine state variables)</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>CNN</p></td></tr><tr><td align="left"><p>[<xref ref-type="bibr" rid="CR90">90</xref>]</p></td><td align="left"><p>Sensor signal</p></td><td align="left"><p>No</p></td><td align="left"><p>Classification (fault detection)</p></td><td align="left"><p>DeepNAP</p></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec10" sec-type="conclusions"><title>Conclusion</title><p id="Par50">Applications of AI in manufacturing industries have been particularly challenging due to the demand for near immaculate modeling of highly nonlinear phenomena in a high-dimensional space. However, the vast amount of recent literature investigating AI in related industrial sectors (Table <xref rid="Tab7" ref-type="table">7</xref>) implies that although it is still in its infancy, it possesses huge potential as a modeling, analysis, and automation technique that can change the manufacturing paradigm in the near future. Apart from the aforementioned industrial sectors, it is widely studied for medical image analysis, bioinformatics, drug discovery, recommendation systems, financial fraud detection, visual art processing, and military. Some of the renowned commercial products that leverage the power of AI include 'Alexa' by Amazon, 'Watson' by IBM, and 'AlphaGo' by DeepMind, and many more products without names have already penetrated our daily lives. Furthermore, to overcome a number of limitations such as lack of interpretability and performance degradation under data shortage that hinders broader applications of AI in the industries, sub-branches of deep learning including physics-informed deep learning, explainable AI, domain adaptation, active learning, multi-task learning, graph neural networks are actively being studied. The convergence of AI with other engineering sectors is promising, and it should not be overlooked. Therefore, through this review, we truly hope that the community of precision engineering and manufacturing finds a way to utilize the upcoming AI for future-oriented manufacturing effectively.<table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>An overview of how deep learning models are used in the recent literature</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Industry</p></th><th align="left"><p>Objective</p></th><th align="left"><p>Topic</p></th><th align="left"><p>Related studies</p></th></tr></thead><tbody><tr><td align="left" rowspan="4"><p>Autonomous vehicle</p></td><td align="left" rowspan="10"><p>Product enhancement</p></td><td align="left"><p>Road detection</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR11">11</xref>â<xref ref-type="bibr" rid="CR14">14</xref>]</p></td></tr><tr><td align="left"><p>Pedestrian detection</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR15">15</xref>â<xref ref-type="bibr" rid="CR17">17</xref>]</p></td></tr><tr><td align="left"><p>Environmental perception</p><p>(sensor fusion)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR18">18</xref>â<xref ref-type="bibr" rid="CR22">22</xref>, <xref ref-type="bibr" rid="CR24">24</xref>â<xref ref-type="bibr" rid="CR26">26</xref>]</p></td></tr><tr><td align="left"><p>Localization</p><p>(sensor fusion)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR27">27</xref>â<xref ref-type="bibr" rid="CR29">29</xref>, <xref ref-type="bibr" rid="CR31">31</xref>, <xref ref-type="bibr" rid="CR32">32</xref>]</p></td></tr><tr><td align="left" rowspan="2"><p>Battery</p></td><td align="left"><p>SOC monitoring</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR34">34</xref>â<xref ref-type="bibr" rid="CR36">36</xref>]</p></td></tr><tr><td align="left"><p>SOH monitoring</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR37">37</xref>â<xref ref-type="bibr" rid="CR42">42</xref>]</p></td></tr><tr><td align="left" rowspan="2"><p>Robotics</p></td><td align="left"><p>Challenge 1&amp;2</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR44">44</xref>â<xref ref-type="bibr" rid="CR47">47</xref>]</p></td></tr><tr><td align="left"><p>Challenge 3</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR48">48</xref>, <xref ref-type="bibr" rid="CR49">49</xref>]</p></td></tr><tr><td align="left" rowspan="2"><p>Renewable energy</p></td><td align="left"><p>Wind energy</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR50">50</xref>â<xref ref-type="bibr" rid="CR57">57</xref>]</p></td></tr><tr><td align="left"><p>Photovoltaics</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR58">58</xref>â<xref ref-type="bibr" rid="CR65">65</xref>]</p></td></tr><tr><td align="left" rowspan="4"><p>Steel</p></td><td align="left" rowspan="6"><p>Manufacturing process enhancement</p></td><td align="left"><p>Ironmaking</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR66">66</xref>â<xref ref-type="bibr" rid="CR68">68</xref>]</p></td></tr><tr><td align="left"><p>Continuous casting</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR69">69</xref>â<xref ref-type="bibr" rid="CR71">71</xref>]</p></td></tr><tr><td align="left"><p>Rolling</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR72">72</xref>â<xref ref-type="bibr" rid="CR75">75</xref>]</p></td></tr><tr><td align="left"><p>Galvanizing</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR76">76</xref>â<xref ref-type="bibr" rid="CR79">79</xref>]</p></td></tr><tr><td align="left" rowspan="2"><p>Semiconductor</p></td><td align="left"><p>Image-based</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR80">80</xref>â<xref ref-type="bibr" rid="CR83">83</xref>]</p></td></tr><tr><td align="left"><p>Signal-based</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR84">84</xref>â<xref ref-type="bibr" rid="CR90">90</xref>]</p></td></tr></tbody></table></table-wrap></p></sec></body><back><ack><title>Acknowledgements</title><p>This work was supported by the National Research Foundation of Korea (NRF) Grant Funded by the Korea Government (MSIT) (No. 2020R1A2C1009744, 2018R1A2A1A05079477), in part by the Korea Institute for Advancement of Technology (KIAT) Grant Funded by the Korea Government (MOTIE) (P0008691, HRD Program for Industrial Innovation), and in part by the Intelligent Digital Power Plant (IDPP) project by KEPCO Research Institute.</p></ack><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jordan</surname><given-names>MI</given-names></name><name><surname>Mitchell</surname><given-names>TM</given-names></name></person-group><article-title xml:lang="en">Machine learning: Trends, perspectives, and prospects</article-title><source>Science</source><year>2015</year><volume>349</volume><issue>6245</issue><fpage>255</fpage><lpage>260</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3382217</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1355.68227</pub-id><pub-id pub-id-type="doi">10.1126/science.aaa8415</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <italic>Nature,</italic><italic>521</italic>(7553): 436â444.</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-David</surname><given-names>S</given-names></name><name><surname>Blitzer</surname><given-names>J</given-names></name><name><surname>Crammer</surname><given-names>K</given-names></name><name><surname>Pereira</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Analysis of representations for domain adaptation</article-title><source>Advances in neural information processing systems</source><year>2007</year><volume>19</volume><fpage>137</fpage></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Gunning, D. (2017). Explainable artificial intelligence (xai). <italic>Defense Advanced Research Projects Agency (DARPA), nd Web,</italic><italic>2</italic>(2).</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Ruder, S. (2017) An overview of multi-task learning in deep neural networks. arXiv preprint: arXiv:1706.05098</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Settles</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Active learning</article-title><source>Synthesis lectures on artificial intelligence and machine learning</source><year>2012</year><volume>6</volume><issue>1</issue><fpage>1</fpage><lpage>114</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3135867</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1270.68006</pub-id><pub-id pub-id-type="doi">10.1007/978-3-031-01560-1</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Raissi, M., Perdikaris, P.,&amp; Karniadakis, G. E. (2017).Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint: arXiv:1711.10561.</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Kim, S. W., Kim, I., Lee, J., &amp; Lee, S. (2021). Knowledge Integration into deep learning in dynamical systems: An overview and taxonomy. <italic>Journal of Mechanical Science and Technology,</italic> pp. 1â12.</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Muhammad, K., Ullah, A., Lloret, J., Del Ser, J., &amp; de Albuquerque, V. H. C. (2020). Deep learning for safe autonomous driving: Current challenges and future directions. <italic>IEEE Transactions on Intelligent Transportation Systems</italic>.</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fayyad</surname><given-names>J</given-names></name><name><surname>Jaradat</surname><given-names>MA</given-names></name><name><surname>Gruyer</surname><given-names>D</given-names></name><name><surname>Najjaran</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Deep learning sensor fusion for autonomous vehicle perception and localization: A review</article-title><source>Sensors</source><year>2020</year><volume>20</volume><issue>15</issue><fpage>4220</fpage><pub-id pub-id-type="doi">10.3390/s20154220</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Limmer, M., Forster, J., Baudach, D.<bold>, </bold>SchÃ¼le, F., Schweiger, R., &amp; Lensch, H. P. (2016). Robust deep-learning-based road-prediction for augmented reality navigation systems at night. In <italic>2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC)</italic>, IEEE, pp. 1888â1895.</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>G</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Xiang</surname><given-names>S</given-names></name><name><surname>Pan</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Automatic road detection and centerline extraction via cascaded end-to-end convolutional neural network</article-title><source>IEEE Transactions on Geoscience and Remote Sensing</source><year>2017</year><volume>55</volume><issue>6</issue><fpage>3322</fpage><lpage>3337</lpage><pub-id pub-id-type="doi">10.1109/TGRS.2017.2669341</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Embedding structured contour and location prior in siamesed fully convolutional networks for road detection</article-title><source>IEEE Transactions on Intelligent Transportation Systems</source><year>2017</year><volume>19</volume><issue>1</issue><fpage>230</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1109/TITS.2017.2749964</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Chen Z., &amp; Chen, Z. (2017). Rbnet: A deep neural network for unified road and road boundary detection. In: <italic>International Conference on Neural Information Processing</italic>, pp. 677â687. Springer.</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Cheng</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Pedestrian detection via body part semantic and contextual information with DNN</article-title><source>IEEE Transactions on Multimedia</source><year>2018</year><volume>20</volume><issue>11</issue><fpage>3148</fpage><lpage>3159</lpage><pub-id pub-id-type="doi">10.1109/TMM.2018.2829602</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ouyang</surname><given-names>W</given-names></name><name><surname>Zhou</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Yan</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title xml:lang="en">Jointly learning deep features, deformable parts, occlusion and classification for pedestrian detection</article-title><source>IEEE transactions on pattern analysis and machine intelligence</source><year>2017</year><volume>40</volume><issue>8</issue><fpage>1874</fpage><lpage>1887</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2017.2738645</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Cai, Z., Saberian, M. J., &amp; Vasconcelos, N. (2019). Learning complexity-aware cascades for pedestrian detection. <italic>IEEE transactions on pattern analysis and machine intelligence</italic>.</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>J</given-names></name><name><surname>Fischer</surname><given-names>V</given-names></name><name><surname>Herman</surname><given-names>M</given-names></name><name><surname>Behnke</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Multispectral pedestrian detection using deep fusion convolutional neural networks</article-title><source>ESANN</source><year>2016</year><volume>587</volume><fpage>509</fpage><lpage>514</lpage></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Girshick, R., Donahue, J., Darrell, T. &amp; Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, pp. 580â587.</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Schlosser, J., Chow, C. K. &amp; Kira, Z. (2016). Fusing lidar and images for pedestrian detection using convolutional neural networks. In <italic>2016 IEEE International Conference on Robotics and Automation (ICRA), IEEE</italic>, pp. 2198â2205</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asvadi</surname><given-names>A</given-names></name><name><surname>Garrote</surname><given-names>L</given-names></name><name><surname>Premebida</surname><given-names>C</given-names></name><name><surname>Peixoto</surname><given-names>P</given-names></name><name><surname>Nunes</surname><given-names>UJ</given-names></name></person-group><article-title xml:lang="en">Multimodal vehicle detection: Fusing 3D-LIDAR and color camera data</article-title><source>Pattern Recognition Letters</source><year>2018</year><volume>115</volume><fpage>20</fpage><lpage>29</lpage><pub-id pub-id-type="doi">10.1016/j.patrec.2017.09.038</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Wang, H., Lou, X., Cai, Y., Li, Y., &amp; Chen, L. (2019). Real-time vehicle detection algorithm based on vision and lidar point cloud fusion. <italic>Journal of Sensors,</italic> vol. 2019.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Redmon, J., Divvala, S., Girshick, R., &amp; Farhadi, A. (2016). You only look once: Unified, real-time object detection. In <italic>Proceedings of the IEEE conference on computer vision and pattern recognition</italic>, pp. 779â788.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Liu, J., Zhang, S., Wang, S., &amp; Metaxas, D. N. (2016). Multispectral deep neural networks for pedestrian detection. arXiv preprint: arXiv:1611.02644 .</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Ren, S., He, K., Girshick, R., Sun, J., &amp; F. R-CNN (2015). Towards real-time object detection with region proposal networks. CoRR. arXiv preprint: arXiv:1506.01497 .</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>D</given-names></name><name><surname>Tong</surname><given-names>R</given-names></name><name><surname>Tang</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Illumination-aware faster R-CNN for robust multispectral pedestrian detection</article-title><source>Pattern Recognition</source><year>2019</year><volume>85</volume><fpage>161</fpage><lpage>171</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2018.08.005</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>H-F</given-names></name><name><surname>Bian</surname><given-names>H-W</given-names></name><name><surname>Wang</surname><given-names>R-Y</given-names></name><name><surname>Ma</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">An INS/GNSS integrated navigation in GNSS denied environment using recurrent neural network</article-title><source>Defence Technology</source><year>2020</year><volume>16</volume><issue>2</issue><fpage>334</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/j.dt.2019.08.011</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Kim H.-U., &amp; Bae, T.-S. (2019). Deep learning-based GNSS network-based real-time kinematic improvement for autonomous ground vehicle navigation. <italic>Journal of Sensors,</italic> vol. <italic>2019</italic>.</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Cheng</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Object classification using CNN-based fusion of vision and LIDAR in autonomous vehicle environment</article-title><source>IEEE Transactions on Industrial Informatics</source><year>2018</year><volume>14</volume><issue>9</issue><fpage>4224</fpage><lpage>4231</lpage><pub-id pub-id-type="doi">10.1109/TII.2018.2822828</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title xml:lang="en">Imagenet classification with deep convolutional neural networks</article-title><source>Advances in neural information processing systems</source><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Laidlow, T., Czarnowski, J., &amp; Leutenegger, S. (2019). DeepFusion: Real-time dense 3D reconstruction for monocular SLAM using single-view depth and gradient predictions. In <italic>2019 International Conference on Robotics and Automation (ICRA)</italic>, <italic>IEEE</italic> , pp. 4068â4074</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SJ</given-names></name><name><surname>Choi</surname><given-names>H</given-names></name><name><surname>Hwang</surname><given-names>SS</given-names></name></person-group><article-title xml:lang="en">Real-time depth estimation using recurrent CNN with sparse depth cues for SLAM system</article-title><source>International Journal of Control, Automation and Systems</source><year>2020</year><volume>18</volume><issue>1</issue><fpage>206</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1007/s12555-019-0350-8</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuma</surname><given-names>MU</given-names></name><name><surname>Koroglu</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">A comprehensive review on estimation strategies used in hybrid and battery electric vehicles</article-title><source>Renewable and Sustainable Energy Reviews</source><year>2015</year><volume>42</volume><fpage>517</fpage><lpage>531</lpage><pub-id pub-id-type="doi">10.1016/j.rser.2014.10.047</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">State-of-charge estimation for battery management system using optimized support vector machine for regression</article-title><source>Journal of Power Sources</source><year>2014</year><volume>269</volume><fpage>682</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1016/j.jpowsour.2014.07.016</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahinoglu</surname><given-names>GO</given-names></name><name><surname>Pajovic</surname><given-names>M</given-names></name><name><surname>Sahinoglu</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Orlik</surname><given-names>PV</given-names></name><name><surname>Wada</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Battery state-of-charge estimation based on regular/recurrent gaussian process regression</article-title><source>IEEE Transactions on Industrial Electronics</source><year>2017</year><volume>65</volume><issue>5</issue><fpage>4311</fpage><lpage>4321</lpage><pub-id pub-id-type="doi">10.1109/TIE.2017.2764869</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dawson-Elli</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>SB</given-names></name><name><surname>Pathak</surname><given-names>M</given-names></name><name><surname>Mitra</surname><given-names>K</given-names></name><name><surname>Subramanian</surname><given-names>VR</given-names></name></person-group><article-title xml:lang="en">Data science approaches for electrochemical engineers: An introduction through surrogate model development for lithium-ion batteries</article-title><source>Journal of The Electrochemical Society</source><year>2018</year><volume>165</volume><issue>2</issue><fpage>A1</fpage><pub-id pub-id-type="doi">10.1149/2.1391714jes</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nuhic</surname><given-names>A</given-names></name><name><surname>Terzimehic</surname><given-names>T</given-names></name><name><surname>Soczka-Guth</surname><given-names>T</given-names></name><name><surname>Buchholz</surname><given-names>M</given-names></name><name><surname>Dietmayer</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Health diagnosis and remaining useful life prognostics of lithium-ion batteries using data-driven methods</article-title><source>Journal of power sources</source><year>2013</year><volume>239</volume><fpage>680</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1016/j.jpowsour.2012.11.146</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Pecht</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">A bayesian approach for li-ion battery capacity fade modeling and cycles to failure prognostics</article-title><source>Journal of Power Sources</source><year>2015</year><volume>281</volume><fpage>173</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.jpowsour.2015.01.164</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Tseng, K.-H., Liang, J.-W., Chang, W., &amp; Huang, S.-C. (2015). Regression models using fully discharged voltage and internal resistance for state of health estimation of lithium-ion batteries. <italic>Energies,</italic><italic>8</italic>(4): 2889â2907.</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khumprom</surname><given-names>P</given-names></name><name><surname>Yodo</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">A data-driven predictive prognostic model for lithium-ion batteries based on a deep learning algorithm</article-title><source>Energies</source><year>2019</year><volume>12</volume><issue>4</issue><fpage>660</fpage><pub-id pub-id-type="doi">10.3390/en12040660</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Hong</surname><given-names>S</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Remaining useful life prediction for lithium-ion battery: A deep learning approach</article-title><source>IEEE Access</source><year>2018</year><volume>6</volume><fpage>50587</fpage><lpage>50598</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2858856</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Severson</surname><given-names>KA</given-names></name><etal/></person-group><article-title xml:lang="en">Data-driven prediction of battery cycle life before capacity degradation</article-title><source>Nature Energy</source><year>2019</year><volume>4</volume><issue>5</issue><fpage>383</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1038/s41560-019-0356-8</pub-id></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pierson</surname><given-names>HA</given-names></name><name><surname>Gashler</surname><given-names>MS</given-names></name></person-group><article-title xml:lang="en">Deep learning in robotics: A review of recent research</article-title><source>Advanced Robotics</source><year>2017</year><volume>31</volume><issue>16</issue><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1080/01691864.2017.1365009</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Mariolis, I., Peleka, G., Kargakos, A., &amp; Malassiotis, S. (2015). Pose and category recognition of highly deformable objects using deep learning. In <italic>2015 International conference on advanced robotics (ICAR)</italic>, <italic>IEEE</italic>, pp. 655â662.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Gao, Y., Hendricks, L. A., Kuchenbecker, K. J., &amp; Darrell, T. (2016). Deep learning for tactile understanding from visual and haptic data. In <italic>2016 IEEE International Conference on Robotics and Automation (ICRA)</italic>, <italic>IEEE</italic>, pp. 536â543.</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Polydoros, A. S., Nalpantidis, L., &amp; KrÃ¼ger, V. (2015). Real-time deep learning of robotic manipulator inverse dynamics. In <italic>2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</italic>, <italic>IEEE</italic>, pp. 3442â3448.</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Lenz, I., Knepper, R. A., &amp; Saxena, A. (2015). DeepMPC: Learning deep latent features for model predictive control. In <italic>Robotics: Science and Systems</italic>. Rome, Italy.</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Lillicrap T. P. et al., (2015). Continuous control with deep reinforcement learning. arXiv preprint: arXiv:1509.02971.</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levine</surname><given-names>S</given-names></name><name><surname>Finn</surname><given-names>C</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Abbeel</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">End-to-end training of deep visuomotor policies</article-title><source>The Journal of Machine Learning Research</source><year>2016</year><volume>17</volume><issue>1</issue><fpage>1334</fpage><lpage>1373</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3491133</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1360.68687</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Liu Z., &amp; Zhang, L. (2020). A review of failure modes, condition monitoring and fault diagnosis methods for large-scale wind turbine bearings. <italic>Measurement 149</italic>, 107002.</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MarugÃ¡n</surname><given-names>AP</given-names></name><name><surname>MÃ¡rquez</surname><given-names>FPG</given-names></name><name><surname>Perez</surname><given-names>JMP</given-names></name><name><surname>Ruiz-HernÃ¡ndez</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">A survey of artificial neural network in wind energy systems</article-title><source>Applied energy</source><year>2018</year><volume>228</volume><fpage>1822</fpage><lpage>1836</lpage><pub-id pub-id-type="doi">10.1016/j.apenergy.2018.07.084</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Reddy, A., Indragandhi, V., Ravi, L., &amp; Subramaniyaswamy, V. (2019). Detection of cracks and damage in wind turbine blades using artificial intelligence-based image analytics. <italic>Measurement,</italic><italic>147</italic>, 106823.</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Liu, H., Zhang, Z., Jia, H., Li, Q., Liu, Y., &amp; Leng, J. (2020). A novel method to predict the stiffness evolution of in-service wind turbine blades based on deep learning models. <italic>Composite Structures,</italic><italic>252</italic>, 112702.</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kong</surname><given-names>Z</given-names></name><name><surname>Tang</surname><given-names>B</given-names></name><name><surname>Deng</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Condition monitoring of wind turbines based on spatio-temporal fusion of SCADA data by convolutional neural networks and gated recurrent units</article-title><source>Renewable Energy</source><year>2020</year><volume>146</volume><fpage>760</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.renene.2019.07.033</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>G</given-names></name><name><surname>He</surname><given-names>H</given-names></name><name><surname>Yan</surname><given-names>J</given-names></name><name><surname>Xie</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Multiscale convolutional neural networks for fault diagnosis of wind turbine gearbox</article-title><source>IEEE Transactions on Industrial Electronics</source><year>2018</year><volume>66</volume><issue>4</issue><fpage>3196</fpage><lpage>3207</lpage><pub-id pub-id-type="doi">10.1109/TIE.2018.2844805</pub-id></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Radford, A., Metz, L., &amp; Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint: arXiv:1511.06434</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Chen, P., Li, Y., Wang, K., Zuo, M. J., Heyns, P. S., &amp; BaggerÃ¶hr, S. (2021). A threshold self-setting condition monitoring scheme for wind turbine generator bearings based on deep convolutional generative adversarial networks. <italic>Measurement,</italic> 167: 108234.</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deitsch</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Automatic classification of defective photovoltaic module cells in electroluminescence images</article-title><source>Solar Energy</source><year>2019</year><volume>185</volume><fpage>455</fpage><lpage>468</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2019.02.067</pub-id></mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Simonyan K., &amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint: arXiv:1409.1556.</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Alcantarilla, P. F., Bartoli, A., &amp; Davison, A. J. (2012). KAZE features. In <italic>European conference on computer vision</italic>, pp. 214â227. Springer.</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Lou</surname><given-names>Z</given-names></name><name><surname>Yan</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Deep learning based module defect analysis for large-scale photovoltaic farms</article-title><source>IEEE Transactions on Energy Conversion</source><year>2018</year><volume>34</volume><issue>1</issue><fpage>520</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1109/TEC.2018.2873358</pub-id></mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herraiz</surname><given-names>ÃH</given-names></name><name><surname>MarugÃ¡n</surname><given-names>AP</given-names></name><name><surname>MÃ¡rquez</surname><given-names>FPG</given-names></name></person-group><article-title xml:lang="en">Photovoltaic plant condition monitoring using thermal images analysis by convolutional neural network-based structure</article-title><source>Renewable Energy</source><year>2020</year><volume>153</volume><fpage>334</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.renene.2020.01.148</pub-id></mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belaout</surname><given-names>A</given-names></name><name><surname>Krim</surname><given-names>F</given-names></name><name><surname>Mellit</surname><given-names>A</given-names></name><name><surname>Talbi</surname><given-names>B</given-names></name><name><surname>Arabi</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Multiclass adaptive neuro-fuzzy classifier and feature selection techniques for photovoltaic array fault detection and classification</article-title><source>Renewable Energy</source><year>2018</year><volume>127</volume><fpage>548</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1016/j.renene.2018.05.008</pub-id></mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Chen, Z., Chen, Y., Wu, L., Cheng, S., &amp; Lin, P. (2019). Deep residual network based fault detection and diagnosis of photovoltaic arrays using current-voltage curves and ambient conditions. <italic>Energy Conversion and Management,</italic><italic>198</italic>, 111793.</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harrou</surname><given-names>F</given-names></name><name><surname>Dairi</surname><given-names>A</given-names></name><name><surname>Taghezouit</surname><given-names>B</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">An unsupervised monitoring procedure for detecting anomalies in photovoltaic systems using a one-class support vector machine</article-title><source>Solar Energy</source><year>2019</year><volume>179</volume><fpage>48</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1016/j.solener.2018.12.045</pub-id></mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>P</given-names></name><name><surname>Guo</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Chai</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Data-driven robust M-LS-SVR-based NARX modeling for estimation and control of molten iron quality indices in blast furnace ironmaking</article-title><source>IEEE transactions on neural networks and learning systems</source><year>2017</year><volume>29</volume><issue>9</issue><fpage>4007</fpage><lpage>4021</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2017.2749412</pub-id></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Han, Y., Li, J., Yang, X.-L., Liu, W.-X., &amp; Zhang, Y.-Z. (2018). Dynamic prediction research of silicon content in hot metal driven by big data in blast furnace smelting process under hadoop cloud platform. <italic>Complexity,</italic> 2018.</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Zhang, X., Kano, M., &amp; Matsuzaki, S. (2019). A comparative study of deep and shallow predictive techniques for hot metal temperature prediction in blast furnace ironmaking. <italic>Computers and Chemical Engineering,</italic><italic>130</italic>, 106575.</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Faizullin, A., Zymbler, M., Lieftucht, D. &amp; FanghÃ¤nel, F. (2018). Use of deep learning for sticker detection during continuous casting. In <italic>2018 Global Smart Industry Conference (GloSIC)</italic>, <italic>IEEE</italic> , pp. 1â6.</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieto</surname><given-names>PG</given-names></name><name><surname>GarcÃ­a-Gonzalo</surname><given-names>E</given-names></name><name><surname>AntÃ³n</surname><given-names>JÃ</given-names></name><name><surname>SuÃ¡rez</surname><given-names>VG</given-names></name><name><surname>BayÃ³n</surname><given-names>RM</given-names></name><name><surname>MartÃ­n</surname><given-names>FM</given-names></name></person-group><article-title xml:lang="en">A comparison of several machine learning techniques for the centerline segregation prediction in continuous cast steel slabs and evaluation of its performance</article-title><source>Journal of Computational and Applied Mathematics</source><year>2018</year><volume>330</volume><fpage>877</fpage><lpage>895</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3717638</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1422.74082</pub-id><pub-id pub-id-type="doi">10.1016/j.cam.2017.02.031</pub-id></mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>X</given-names></name><etal/></person-group><article-title xml:lang="en">Multiscale convolutional and recurrent neural network for quality prediction of continuous casting slabs</article-title><source>Processes</source><year>2021</year><volume>9</volume><issue>1</issue><fpage>33</fpage><pub-id pub-id-type="doi">10.3390/pr9010033</pub-id></mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Zhang, C., Peng, K., &amp; Dong, J. (2020). A nonlinear full condition process monitoring method for hot rolling process with dynamic characteristic. <italic>ISA transactions</italic>.</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Luan</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">A comparative assessment of six machine learning models for prediction of bending force in hot strip rolling process</article-title><source>Metals</source><year>2020</year><volume>10</volume><issue>5</issue><fpage>685</fpage><pub-id pub-id-type="doi">10.3390/met10050685</pub-id></mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Peng</surname><given-names>W</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Strip crown prediction in hot rolling process using random forest</article-title><source>International Journal of Precision Engineering and Manufacturing</source><year>2021</year><volume>22</volume><issue>2</issue><fpage>301</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1007/s12541-020-00454-1</pub-id></mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Ma</surname><given-names>G</given-names></name><name><surname>Gong</surname><given-names>D</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Application of mind evolutionary algorithm and artificial neural networks for prediction of profile and flatness in hot strip rolling process</article-title><source>Neural Processing Letters</source><year>2019</year><volume>50</volume><issue>3</issue><fpage>2455</fpage><lpage>2479</lpage><pub-id pub-id-type="doi">10.1007/s11063-019-10021-z</pub-id></mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colla</surname><given-names>V</given-names></name><name><surname>Cateni</surname><given-names>S</given-names></name><name><surname>Maddaloni</surname><given-names>A</given-names></name><name><surname>Vignali</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">A modular machine-learning-based approach to improve tensile properties uniformity along hot dip galvanized steel strips for automotive applications</article-title><source>Metals</source><year>2020</year><volume>10</volume><issue>7</issue><fpage>923</fpage><pub-id pub-id-type="doi">10.3390/met10070923</pub-id></mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lalam</surname><given-names>S</given-names></name><name><surname>Tiwari</surname><given-names>PK</given-names></name><name><surname>Sahoo</surname><given-names>S</given-names></name><name><surname>Dalal</surname><given-names>AK</given-names></name></person-group><article-title xml:lang="en">Online prediction and monitoring of mechanical properties of industrial galvanised steel coils using neural networks</article-title><source>Ironmaking and Steelmaking</source><year>2019</year><volume>46</volume><issue>1</issue><fpage>89</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1080/03019233.2017.1342424</pub-id></mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Z-S</given-names></name><name><surname>Zhou</surname><given-names>X-H</given-names></name><name><surname>Chen</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Development and application of a neural network based coating weight control system for a hot-dip galvanizing line</article-title><source>Frontiers of Information Technology and Electronic Engineering</source><year>2018</year><volume>19</volume><issue>7</issue><fpage>834</fpage><lpage>846</lpage><pub-id pub-id-type="doi">10.1631/FITEE.1601397</pub-id></mixed-citation></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="other">Mao, K., Yang, Y.-L., Huang, Z., &amp; Yang, D.-Y. (2020). Coating thickness modeling and prediction for hot-dip galvanized steel strip based on GA-BP neural network. In <italic>2020 Chinese Control And Decision Conference (CCDC)</italic>, <italic>IEEE</italic>, pp. 3484â3489.</mixed-citation></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Imoto, K.<bold>, </bold>Nakai, T.<bold>, </bold>Ike, T., Haruki, K.<bold>, </bold>&amp; Sato, Y.( 2018). A CNN-based transfer learning method for defect classification in semiconductor manufacturing. In <italic>2018 International Symposium on Semiconductor Manufacturing (ISSM)</italic>, <italic>IEEE</italic>, pp. 1â3.</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Yuan-Fu, Y. (2019). A deep learning model for identification of defect patterns in semiconductor wafer map. In <italic>2019 30th Annual SEMI Advanced Semiconductor Manufacturing Conference (ASMC)</italic>, <italic>IEEE</italic>, pp. 1â6.</mixed-citation></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tello</surname><given-names>G</given-names></name><name><surname>Al-Jarrah</surname><given-names>OY</given-names></name><name><surname>Yoo</surname><given-names>PD</given-names></name><name><surname>Al-Hammadi</surname><given-names>Y</given-names></name><name><surname>Muhaidat</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>U</given-names></name></person-group><article-title xml:lang="en">Deep-structured machine learning model for the recognition of mixed-defect patterns in semiconductor fabrication processes</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2018</year><volume>31</volume><issue>2</issue><fpage>315</fpage><lpage>322</lpage><pub-id pub-id-type="doi">10.1109/TSM.2018.2825482</pub-id></mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>OâLeary</surname><given-names>J</given-names></name><name><surname>Sawlani</surname><given-names>K</given-names></name><name><surname>Mesbah</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep learning for classification of the chemical composition of particle defects on semiconductor wafers</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2020</year><volume>33</volume><issue>1</issue><fpage>72</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1109/TSM.2019.2963656</pub-id></mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Zheng</surname><given-names>X</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Stacked convolutional sparse denoising auto-encoder for identification of defect patterns in semiconductor wafer map</article-title><source>Computers in Industry</source><year>2019</year><volume>109</volume><fpage>121</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1016/j.compind.2019.04.015</pub-id></mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakazawa</surname><given-names>T</given-names></name><name><surname>Kulkarni</surname><given-names>DV</given-names></name></person-group><article-title xml:lang="en">Anomaly detection and segmentation for wafer defect patterns using deep convolutional encoderâdecoder neural network architectures in semiconductor manufacturing</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2019</year><volume>32</volume><issue>2</issue><fpage>250</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1109/TSM.2019.2897690</pub-id></mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>KB</given-names></name><name><surname>Cheon</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>CO</given-names></name></person-group><article-title xml:lang="en">A convolutional neural network for fault classification and diagnosis in semiconductor manufacturing processes</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2017</year><volume>30</volume><issue>2</issue><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1109/TSM.2017.2676245</pub-id></mixed-citation></ref><ref id="CR87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>CO</given-names></name></person-group><article-title xml:lang="en">A deep learning model for robust wafer fault monitoring with sensor measurement noise</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2016</year><volume>30</volume><issue>1</issue><fpage>23</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1109/TSM.2016.2628865</pub-id></mixed-citation></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Cho</surname><given-names>S</given-names></name><name><surname>Lee</surname><given-names>B</given-names></name><name><surname>Cho</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Fault detection and diagnosis using self-attentive convolutional neural networks for variable-length sensor data in semiconductor manufacturing</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2019</year><volume>32</volume><issue>3</issue><fpage>302</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1109/TSM.2019.2917521</pub-id></mixed-citation></ref><ref id="CR89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azamfar</surname><given-names>M</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Deep learning-based domain adaptation method for fault diagnosis in semiconductor manufacturing</article-title><source>IEEE Transactions on Semiconductor Manufacturing</source><year>2020</year><volume>33</volume><issue>3</issue><fpage>445</fpage><lpage>453</lpage><pub-id pub-id-type="doi">10.1109/TSM.2020.2995548</pub-id></mixed-citation></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>R</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Kang</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">DeepNAP: Deep neural anomaly pre-detection in a semiconductor fab</article-title><source>Information Sciences</source><year>2018</year><volume>457</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3807024</pub-id><pub-id pub-id-type="doi">10.1016/j.ins.2018.05.020</pub-id></mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><title>Publisher's Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Engineering</facet-value><facet-value count="1">Industrial and Production Engineering</facet-value><facet-value count="1">Materials Science, general</facet-value></facet><facet name="keyword"><facet-value count="1">Artificial intelligence</facet-value><facet-value count="1">Condition monitoring</facet-value><facet-value count="1">Deep learning</facet-value><facet-value count="1">Fault detection and diagnosis</facet-value><facet-value count="1">Manufacturing process</facet-value></facet><facet name="pub"><facet-value count="1">International Journal of Precision Engineering and Manufacturing</facet-value></facet><facet name="year"><facet-value count="1">2022</facet-value></facet><facet name="country"><facet-value count="1">South Korea</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
