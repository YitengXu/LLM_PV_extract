<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1007/s11265-012-0681-7</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">11265</journal-id><journal-title-group><journal-title>Journal of Signal Processing Systems</journal-title><journal-subtitle>for Signal, Image, and Video Technology
(formerly the Journal of VLSI Signal Processing Systems for Signal, Image, and Video Technology)</journal-subtitle><abbrev-journal-title abbrev-type="publisher">J Sign Process Syst </abbrev-journal-title></journal-title-group><issn pub-type="ppub">1939-8018</issn><issn pub-type="epub">1939-8115</issn><publisher><publisher-name>Springer US</publisher-name><publisher-loc>Boston</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s11265-012-0681-7</article-id><article-id pub-id-type="manuscript">681</article-id><article-id pub-id-type="doi">10.1007/s11265-012-0681-7</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Hardware-Software Implementation of a Sensor Network for City Traffic Monitoring Using the FPGA- and ASIC-Based Sensor Nodes</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Wójcikowski</surname><given-names>Marek</given-names></name><address><email>wujek@ue.eti.pg.gda.pl</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs1126501206817_cor1">a</xref><bio><sec><title>Marek Wójcikowski</title><p>received his M.Sc. degree in Electrical Engineering in 1993 from the Technical University of Gdańsk, Poland. In 1994, he was on leave at the University of Karlsruhe, Germany. In 200, he received his Ph.D. degree in Electrical Engineering from the Technical University of Gdańsk, Poland. Since 1994, he has been with the Department of Microelectronic Systems, Technical University of Gdańsk, Poland. His research interests lie in the design of microelectronic systems and sensor networks.
<fig id="Figa" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Figa_HTML.jpg" position="anchor" id="MO1"/></fig></p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Żaglewski</surname><given-names>Robert</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><bio><sec><title>Robert Żaglewski</title><p>received his M.Sc. degree in Electrical Engineering in 2007 from the Technical University of Gdańsk, Poland. In the years 2007–2008, he was with the Department of Microelectronic Systems, Technical University of Gdańsk, Poland. He is now with Intel Shannon Ltd, Shannon, Ireland. His research interests lie in FPGA and ASIC design.
<fig id="Figb" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Figb_HTML.jpg" position="anchor" id="MO2"/></fig></p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Pankiewicz</surname><given-names>Bogdan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec><title>Bogdan Pankiewicz</title><p>received his M.Sc. degree in Electrical Engineering in 1993 from the Technical University of Gdańsk, Poland. In 2002, he received his Ph.D. degree in Electrical Engineering from the Technical University of Gdańsk, Poland. Since 1994, he has been with the Department of Microelectronic Systems, Technical University of Gdańsk, Poland. His research interests lie in the design of analog and digital integrated circuits.
<fig id="Figc" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Figc_HTML.jpg" position="anchor" id="MO3"/></fig></p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Kłosowski</surname><given-names>Miron</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec><title>Miron Kłosowski</title><p>received his M.Sc. degree in Electrical Engineering in 1994 from the Technical University of Gdańsk, Poland. In 2001, he received his Ph.D. degree in Electrical Engineering from the Technical University of Gdańsk, Poland. Since 2001, he has been with the Department of Microelectronic Systems, Technical University of Gdańsk, Poland. His research interests lie in the applications of FPGAs in microelectronic systems.
<fig id="Figd" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Figd_HTML.jpg" position="anchor" id="MO4"/></fig></p></sec></bio></contrib><contrib contrib-type="author"><name><surname>Szczepański</surname><given-names>Stanisław</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><bio><sec><title>Stanisław Szczepański</title><p>received the M.Sc. and Ph.D. (with honors) degrees in electronic engineering from the Gdańsk University of Technology, Gdańsk, Poland, in 1975 and 1986 respectively. In 1986, he was a Visiting Research Associate with the Institute National Polytechnique de Toulouse (INPT), Toulouse, France. From 1990 to 1991, he was with the Department of Electrical Engineering, Portland State University, Portland, OR, on a Kosciuszko Foundation Fellowship. From August to September 1998, he was a Visiting Professor with the Faculty of Engineering and Information Sciences at the University of Hertfordshire, Hatfield, U.K. He is currently an Associate Professor and Head of the Department of Microelectronic Systems, Faculty of Electronics, Telecommunications and Informatics, Gdańsk University of Technology. He has published more than 140 papers and holds two patents. His teaching and research interests are in circuit theory, fully integrated analog filters, high-frequency transconductance amplifiers, analog integrated circuit design in bipolar and CMOS technology, and current-mode analog signal processing.
<fig id="Fige" position="anchor"><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Fige_HTML.jpg" position="anchor" id="MO5"/></fig></p></sec></bio></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.6868.0</institution-id><institution-id institution-id-type="ISNI">000000012187838X</institution-id><institution content-type="org-name">Gdańsk University of Technology</institution></institution-wrap><addr-line content-type="city">Gdańsk</addr-line><country country="PL">POLAND</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution content-type="org-name">Intel Shannon Ltd.</institution></institution-wrap><addr-line content-type="city">Shannon</addr-line><country country="IE">Ireland</country></aff></contrib-group><author-notes><corresp id="IDs1126501206817_cor1"><label>a</label><email>wujek@ue.eti.pg.gda.pl</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>27</day><month>6</month><year>2012</year></pub-date><pub-date date-type="pub" publication-format="print"><month>4</month><year>2013</year></pub-date><volume>71</volume><issue seq="5">1</issue><fpage>57</fpage><lpage>73</lpage><history><date date-type="registration"><day>22</day><month>5</month><year>2012</year></date><date date-type="received"><day>5</day><month>8</month><year>2011</year></date><date date-type="rev-recd"><day>19</day><month>5</month><year>2012</year></date><date date-type="accepted"><day>22</day><month>5</month><year>2012</year></date><date date-type="online"><day>27</day><month>6</month><year>2012</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2012</copyright-statement><copyright-year>2012</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by/2.0"><license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 2.0 International License (<ext-link xlink:href="https://creativecommons.org/licenses/by/2.0" ext-link-type="uri">https://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p>This paper presents a prototype sensor network for monitoring urban traffic. The sensor network node, equipped with a low-resolution camera, observes the street and detects moving objects. Object detection is based on the custom video segmentation algorithm, using dual background subtraction, edge detection and shadow detection, running on dedicated multi-processor SoC hardware. The number and the speed of the detected objects are transmitted using a low-power license-free radio transceiver to another neighboring node. All the nodes create a self-organized network, data are aggregated at the nodes and passed further to the nodes closer to data sinks. Finally, information about the traffic flow is collected from the sinks and visualized on a PC. The prototype sensor network node has been realized in two versions: FPGA and ASIC. The ASIC version consumes approximately 500 mW and it can be powered from a photovoltaic solar panel combined with a single cell Li-Po battery. The comparison of power consumption of both versions has also been made. Apart from collecting traffic data, the proposed sensor network can gather environmental data, such as the temperature, the acoustic noise or the intensity of the sunlight. The set of 26 prototype sensors has been mounted on street lamp-poles on streets and tested in real conditions.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Wireless sensor networks</kwd><kwd>Sensor systems</kwd><kwd>Object detection</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>3</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2013</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Springer Science+Business Media New York</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2013</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2012</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>5</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>22</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/11265_2012_Article_681.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-year</meta-name><meta-value>2013</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-month</meta-name><meta-value>2</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-day</meta-name><meta-value>9</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-year</meta-name><meta-value>2013</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-month</meta-name><meta-value>2</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-day</meta-name><meta-value>8</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Signal,Image and Speech Processing</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Circuits and Systems</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Electrical Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Image Processing and Computer Vision</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Pattern Recognition</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computer Imaging, Vision, Pattern Recognition and Graphics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta><notes notes-type="Misc"><p>Index Terms—wireless sensor networks, sensor systems, object detection</p></notes></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p>Sensor networks are very useful in collecting data from large areas. Many potential examples of sensor networks can be found in the literature [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR4">4</xref>]. The sensor network can also be effectively used for urban traffic. Traffic monitoring systems are very useful in providing management of the traffic flow and they can help in increasing the throughput and safety of the transportation and provide valuable information for planning of future road developments. Typically, detection of moving vehicles is done by inductive loops, passive infrared sensors, magnetometers, microphones, radars or microwave sensors [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref>]. Also, high-resolution cameras connected to the monitoring center using high-bandwidth cables or fiber optic links are often used but collecting data from remote sensors is usually very expensive as far as installation and usage are concerned. In recent years, an automatic video detection has been becoming more popular in the literature. There are many algorithms and systems for this purpose. Unfortunately, most of them work with high-resolution images and require significant computing power, which is not suitable for low-power sensor networks. The paper [<xref ref-type="bibr" rid="CR9">9</xref>] contains a survey of various image detection algorithms, from simple background subtraction to optical flow and stereo vision methods. In [<xref ref-type="bibr" rid="CR10">10</xref>], the authors present the algorithm dedicated for existing street cameras, which uses background subtraction with the Bayesian Network Classifier. The background subtraction is also used in the algorithm described in [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>], where pixels are modeled as the mixture of Gaussians and the pixel model is learned using the EM algorithm. The mixture of Gaussian for low-level tracking is also proposed in [<xref ref-type="bibr" rid="CR13">13</xref>], while high-level position estimation is done using the Kalman filter. The PC-based system described in [<xref ref-type="bibr" rid="CR14">14</xref>] utilizes the probability-based foreground extraction, with color image processing and shadow removal. Two separate algorithms for day and night operation are proposed in [<xref ref-type="bibr" rid="CR15">15</xref>]: the spatio-temporal analysis is used during daytime, the morphological analysis of headlights is used at night, the collected information is processed with the forward chaining rule production system. Another approach is described in [<xref ref-type="bibr" rid="CR16">16</xref>], where the outlines of the detected cars are represented as quadratic B-spline curves. The 3-D model consisting of the edge segments is matched with 2-D image segments in [<xref ref-type="bibr" rid="CR17">17</xref>]. In [<xref ref-type="bibr" rid="CR18">18</xref>] an adaptive background estimation on image divided into non-overlapped blocks is described, with PC-based implementation capable of 15 fps image processing. In [<xref ref-type="bibr" rid="CR19">19</xref>, <xref ref-type="bibr" rid="CR20">20</xref>], FPGA-based implementations are proposed, where some regions of interest are defined and analyzed in the observed image. The self-contained computer system running Linux with the attached camera is described in [<xref ref-type="bibr" rid="CR21">21</xref>], where the authors demonstrate the application of simple background detection algorithm. Traffic detection based on frame differencing, background subtraction and virtual induction coils are presented in [<xref ref-type="bibr" rid="CR22">22</xref>]. [<xref ref-type="bibr" rid="CR23">23</xref>] contains camera detection systems evaluation and optimal camera placement considerations. Other video detection systems use statistical approach using principal component analysis and independent component analysis [<xref ref-type="bibr" rid="CR24">24</xref>], neural network approach with multilayer feed-forward neural network [<xref ref-type="bibr" rid="CR25">25</xref>], support vector machine [<xref ref-type="bibr" rid="CR26">26</xref>] or symmetry-based vehicle detection [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>].</p><p>The motivation of the presented work was to propose the low cost method (in terms of the hardware and the maintenance costs) of measuring the city traffic. The set of small, low-power, autonomous devices (i.e. sensor network nodes), taken out-of-the-box and attached to the street lamp-poles, is able to evaluate the traffic using their built-in cameras and to transmit traffic data to the central computer, using the self-organized radio network (even ad-hoc installation is possible in emergency situations). Data collected by the network can be used for many purposes, such as: informing citizens by local radio station or internet, detecting extraordinary traffic events, guiding the emergency vehicles to achieve their goal more quickly or interacting with the traffic lights to improve the traffic flow, resulting in a decrease in traffic jams and environmental pollution. Compared to the standard vehicle detection method with the inductive loops, the use of small sensor network nodes has many benefits: simple installation, lack of wire connections, independent radio communication, possibility to estimate the speed, direction and size of the vehicles, capture and transmission of single images to the monitoring center.</p><p>In this paper, a sensor network for monitoring the traffic of the vehicles on the street of a city is described. The design target is a low-power sensor network node capable of estimating the traffic flow, due to the carefully developed video detection algorithms and proper hardware/software co-design. The node of the sensor network is designed to be installed on street lamp-poles to observe the scene from the location high above the road. Analysis of the video stream locally by each node of the sensor network significantly reduces the amount of data which needs to be transmitted over the radio, further decreasing the energy usage. The nodes are capable of detecting car traffic; additionally, single images from the nodes’ cameras can be sent, informing the operator, for example, about snow on the street.</p><p>The proposed sensor network node (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) uses a typical low-cost camera, thus the installation near the street lamp is mandatory to enable observation in the night.
<fig id="Fig1"><label>Figure 1</label><caption xml:lang="en"><p>A conceptual diagram of the sensor network node.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig1_HTML.gif" id="MO6"/></fig></p><p>The sensor network can be quickly and simply installed without the need for expensive cabling or permissions, since the nodes have a built-in, low-power radio working on license-free frequency band (see Fig. <xref rid="Fig2" ref-type="fig">2</xref>) and they can be powered from the solar panel.
<fig id="Fig2"><label>Figure 2</label><caption xml:lang="en"><p>An overview of the sensor network application for collecting traffic data.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig2_HTML.gif" id="MO7"/></fig></p><p>The layout of the paper is as follows: in <xref rid="Sec2" ref-type="sec">section II</xref>, the authors present the algorithm and realization of the image processing for moving object detection. <xref rid="Sec3" ref-type="sec">Section III</xref> describes the protocol and organization of the radio communication network. The details about the realization of the prototype nodes are given in <xref rid="Sec4" ref-type="sec">section IV</xref>. The results of the sensor network operation and conclusions are presented in <xref rid="Sec5" ref-type="sec">sections V</xref> and <xref rid="Sec6" ref-type="sec">VI</xref>, respectively</p></sec><sec id="Sec2"><title>Moving Object Detection</title><p>Monitoring of the traffic with a camera is a very challenging task. It requires an image-processing algorithm capable of detecting cars under variable light conditions. Implementation of an image-processing algorithm in the sensor network node is further constrained by the limited resources of the node. The sensor network node is usually a low-power device using as simple hardware as possible to decrease the size and the power consumption. Due to this fact, a careful design of an image-processing algorithm which would fit into those limited hardware resources is very important. In the presented solution, a monochrome camera is used to reduce the complexity of the hardware by about 3 times at a cost of decreasing the segmentation sensitivity; additionally, the image resolution has been decreased to 128 × 128 pixels.
<list list-type="order"><list-item><p><italic>Low–Level Image Processing</italic></p></list-item></list></p><p>In the presented implementation, the non-model-based approach for moving object detection with background subtraction is used, where each current frame from the camera is subtracted from the background image stored in the memory. Among the algorithms mentioned in <xref rid="Sec1" ref-type="sec">section I</xref>, the background subtraction is the easiest to implement in hardware, but it does not provide in its basic form the detection quality acceptable for the outdoor vehicle detection. The complete block diagram of the image-processing algorithm is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The detailed description of the low-level image segmentation algorithm can be found in [<xref ref-type="bibr" rid="CR29">29</xref>], where the earlier work of the authors is presented.
<fig id="Fig3"><label>Figure 3</label><caption xml:lang="en"><p>General diagram depicting the idea of the proposed image processing algorithm.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig3_HTML.gif" id="MO8"/></fig></p><p>The authors decided to use two background models concurrently [<xref ref-type="bibr" rid="CR30">30</xref>–<xref ref-type="bibr" rid="CR33">33</xref>]: long-term with non-selective update and short-term with selective background update. In this way, the stopped objects are initially not included into the selective background, but after a while become a part of the non-selective background. After being part of the non-selective background, the stopped object is not detected any more, thus it can be quickly included into the selective background too.</p><p>Both background models assume single Gaussian distribution of pixel intensities for time <italic>t</italic> with the average brightness μ<sub><italic>t</italic></sub> and the standard deviation σ<sub><italic>t</italic></sub> updated using running mode, which can be very efficiently realized in hardware.
<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left| {{{I}_t} - {{\mu }_t}} \right| &gt; k \cdot {{\sigma }_t} $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ1.gif"/></alternatives></disp-formula></p><p>The detection results, using inequality (1), in the form of the masks: <bold>m</bold><sub><bold><italic>N</italic></bold></sub> and <bold>m</bold><sub><bold><italic>S</italic></bold></sub> from non-selective and selective background model, respectively, are combined into a single binary mask <bold>m</bold><sub><bold><italic>B</italic></bold></sub>, using special combination of <italic>and</italic> and <italic>or</italic> operations, similarly as described in [<xref ref-type="bibr" rid="CR32">32</xref>]. For the simplicity of the hardware, apart from the current pixel, instead of eight neighboring pixels, only the four previously analyzed pixels are used. To improve the segmentation quality, two edge detection blocks have been introduced: temporal edge detection and spatial edge detection, resulting in mask <bold>m</bold><sub><bold><italic>ET</italic></bold></sub> and <bold>m</bold><sub><bold><italic>ES</italic></bold></sub>, respectively. To further increase the selectivity of the object detection, several additional blocks have been added, such as shadow detection block (mask <bold>m</bold><sub><bold><italic>SH</italic></bold></sub>) or highlight detection block detecting highlights from the car lights in the night (mask <bold>m</bold><sub><bold><italic>HI</italic></bold></sub>) and very dark pixels on a bright background (mask <bold>m</bold><sub><bold><italic>X</italic></bold></sub>). The basic detection of shadows is done by comparing the decrease in brightness [<xref ref-type="bibr" rid="CR34">34</xref>]. The Final Processing block from Fig. <xref rid="Fig3" ref-type="fig">3</xref> is responsible for basic morphological operations (erosion followed by dilation, using 2 × 2 rectangular structuring element) to condition the final mask <bold>m</bold><sub><bold><italic>BEHSX</italic></bold></sub>. The final detection result is processed by the Hough transform based block, which helps to obtain convex, filled blobs representing the detected objects. The Hough transform with the rectangular structuring element improves the shape of the blobs (Fig. <xref rid="Fig4" ref-type="fig">4</xref>). The mask <bold>m</bold><sub><bold><italic>V</italic></bold></sub> is the final detected vehicle mask image of elements equal to 0 or 1, where 1 denotes the pixels belonging to the detected moving objects.
<fig id="Fig4"><label>Figure 4</label><caption xml:lang="en"><p>An example of the operation of the Hough transform block: <bold>a</bold>—image <bold>m</bold><sub><bold><italic>BEHSX</italic></bold></sub> before Hough block, <bold>b</bold>—image <bold>m</bold><sub><bold><italic>V</italic></bold></sub> after Hough block.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig4_HTML.gif" id="MO9"/></fig></p><p>All the image-processing operations shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref> have been selected and adapted for efficient hardware implementation. The details of the pipelined implementation of the algorithm from Fig. <xref rid="Fig3" ref-type="fig">3</xref>, as well as the segmentation results, can be found in [<xref ref-type="bibr" rid="CR29">29</xref>]. In this paper, instead of typical pipelined hardware, a set of concurrently working, dedicated Pixel Processors (PP) has been developed to enable the modifications of the algorithm in the ASIC implementation. The PPs have been designed as scalable modules in VHDL: 16-bit, 8-bit and 1-bit versions are provided. The system contains nine PPs, as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. The processors are connected to the common data bus via a programmable switch matrix, where the bus contains data of the currently processed pixel. Each processor can read any of 128 data bits, but it can write only to the exclusively assigned selection of bits, except from 1-bit processors which can write to each bit of the bus. The 1-bit processors have a built-in memory for caching the previously analyzed pixels - data from this memory are used to perform simplified morphological operations that require information about the neighboring pixels, such as erosion or dilation. The processing tasks have been divided among the processors, as listed in Table <xref rid="Tab1" ref-type="table">1</xref>.
<fig id="Fig5"><label>Figure 5</label><caption xml:lang="en"><p>Block diagram of the image-processing hardware.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig5_HTML.gif" id="MO10"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>The assigned tasks for PPs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Processor</p></th><th><p>Tasks</p></th></tr></thead><tbody><tr><td><p>PP1</p></td><td><p>The non-selective background maintenance. Calculation of mask <bold>m</bold><sub><bold><italic>B</italic></bold></sub>. Storing <bold>I</bold><sub><bold><italic>t-</italic></bold><bold>1</bold></sub>.</p></td></tr><tr><td><p>PP2</p></td><td><p>The selective background maintenance. Calculation of mask <bold>m</bold><sub><bold><italic>S</italic></bold></sub>.</p></td></tr><tr><td><p>PP3</p></td><td><p>Calculation of masks <bold>m</bold><sub><bold><italic>SH</italic></bold></sub> and <bold>m</bold><sub><bold><italic>ET</italic></bold></sub>.</p></td></tr><tr><td><p>PP4</p></td><td><p>Calculation of masks <bold>m</bold><sub><bold><italic>ES</italic></bold></sub>, <bold>m</bold><sub><bold><italic>HI</italic></bold></sub> and <bold>m</bold><sub><bold><italic>X</italic></bold></sub>.</p></td></tr><tr><td><p>PP5</p></td><td><p>Indexing of the detected objects (phase 1).</p></td></tr><tr><td><p>PP6</p></td><td><p>Indexing of the detected objects (phase 2). Generating image after indexation. Generation of the tables with objects’ parameters.</p></td></tr><tr><td rowspan="3"><p>PP7</p></td><td><p>Calculating the combination of the masks <bold>m</bold><sub><bold><italic>N</italic></bold></sub> and <bold>m</bold><sub><bold><italic>S</italic></bold></sub>.</p></td></tr><tr><td><p>Calculating the combination of the masks <bold>m</bold><sub><bold><italic>ES</italic></bold></sub> and <bold>m</bold><sub><bold><italic>ET</italic></bold></sub>.</p></td></tr><tr><td><p>Erosion of mask <bold>m</bold><sub><bold><italic>SH</italic></bold></sub>.</p></td></tr><tr><td><p>PP8</p></td><td><p>Generating the final combination of masks. Erosion of the final mask <bold>m</bold><sub><bold><italic>BEHSX</italic></bold></sub>.</p></td></tr><tr><td><p>PP9</p></td><td><p>Dilation of the final mask <bold>m</bold><sub><bold><italic>BEHSX</italic></bold></sub>.</p></td></tr></tbody></table></table-wrap></p><p>The camera can view the street from various angles. To estimate the speed and the size of the moving objects, the image must be geometrically transformed. An example of the transformation is shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, presenting the input image before and after the transformation. In the real system, the mask <bold>m</bold><sub><bold><italic>V</italic></bold></sub> is transformed into the mask <bold>m</bold><sub><bold><italic>TF</italic></bold></sub>. The image transformation is realized by the hardware block equipped with the memory block containing the image-mapping coordinates.
<fig id="Fig6"><label>Figure 6</label><caption xml:lang="en"><p>An example of the input image before (<bold>a</bold>) and after (<bold>b</bold>) the geometrical transformation canceling the perspective.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig6_HTML.gif" id="MO11"/></fig></p><p>The detected blobs are labeled and their basic parameters are calculated, such as: object’s boundaries, the center of the object and the area in pixels. The labeling and calculating of the parameters of the objects is done during pixel by pixel revision of the image by the selected PPs.
<list list-type="order"><list-item><p><italic>High–Level Tracking</italic></p></list-item></list></p><p>The detection results from PPs in the form of the table containing the basic parameters of the blobs are passed to the main 32-bit processor of the system, where the objects are traced, classified and measured. The detected blobs are usually noisy and inaccurate, i.e. they can change shape, disappear, split or join with the other blobs. The main processor services the radio network and it also has to extract as much information as possible to infer the moving cars, but the limited computing resources did not allow the use of the Kalman filter approach in real time, therefore a simplified method has been used. For that purpose, a set of <italic>if-then</italic> heuristic rules has been applied, working on simple rectangular models of the blobs. For every picture frame, each detected blob is always modeled as the rectangle <italic>B</italic> (denoted as <italic>blob B</italic> in the further text) with the assigned attributes shown in Table <xref rid="Tab2" ref-type="table">2</xref>.
<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>The attributes assigned to each detected blob <italic>B.</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Attribute</p></th><th><p>Description</p></th></tr></thead><tbody><tr><td><p><italic>t</italic><sub>0<italic>B</italic></sub></p></td><td><p>The time of creation of the blob <italic>B.</italic></p></td></tr><tr><td><p><italic>x</italic><sub>0<italic>B</italic></sub>, <italic>y</italic><sub>0<italic>B</italic></sub></p></td><td><p>The coordinates of the center of the rectangle representing the blob <italic>B</italic> at the time <italic>t</italic><sub><italic>0B</italic></sub><italic>.</italic></p></td></tr><tr><td><p><italic>x</italic><sub><italic>B</italic></sub>, <italic>y</italic><sub><italic>B</italic></sub></p></td><td><p>The coordinates of the center of the rectangle representing the blob <italic>B</italic> at the current time.</p></td></tr><tr><td><p><italic>X</italic><sub><italic>B</italic></sub>, <italic>Y</italic><sub><italic>B</italic></sub></p></td><td><p>The current size of the rectangle representing the blob <italic>B.</italic></p></td></tr><tr><td><p><inline-formula id="IEq1"><alternatives><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {{X}} }_{{B}}}{,}\,{{\overline {{Y}} }_{{B}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq1.gif"/></alternatives></inline-formula></p></td><td><p>The average size of the rectangle representing the blob <italic>B</italic> since <italic>t</italic><sub><italic>0B</italic></sub><italic>,</italic> updated at every frame.</p></td></tr><tr><td><p><inline-formula id="IEq2"><alternatives><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {{V}} }_{{B}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq2.gif"/></alternatives></inline-formula></p></td><td><p>The average velocity of the blob <italic>B</italic> since <italic>t</italic><sub><italic>0B</italic></sub><italic>,</italic> updated at every frame.</p></td></tr><tr><td><p><inline-formula id="IEq3"><alternatives><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_B} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq3.gif"/></alternatives></inline-formula></p></td><td><p>The average movement direction of the blob <italic>B</italic> since <italic>t</italic><sub><italic>0B</italic></sub><italic>,</italic> updated at every frame.</p></td></tr><tr><td><p><italic>P</italic><sub><italic>B</italic></sub></p></td><td><p>The number of pixels of the real blob represented by the rectangle <italic>B.</italic></p></td></tr><tr><td rowspan="6"><p><italic>q</italic><sub><italic>B</italic></sub></p></td><td><p>The quality of the movement path of the blob <italic>B</italic>, calculated according to the equations:</p></td></tr><tr><td><p><inline-formula id="IEq4"><alternatives><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \matrix{ {{{\text{q}}_{{{\text{B,}}{{\text{t}}_{\text{0B}}}}}}\,{ = }\,0} \\ {{{\text{q}}_{{{\text{B,t}}}}}\,{ = }\,{{\text{q}}_{{{\text{B,t - 1}}}}}{ + }2Q - \left| {{{\text{x}}_{{{\text{B,t}}}}} - {{\text{x}}_{{{\text{B,t - 1}}}}} - {{{\overline {\Delta X} }}_{{{\text{B,t - 1}}}}}} \right| - \left| {{{\text{y}}_{{{\text{B,t}}}}} - {{\text{y}}_{{{\text{B,t - 1}}}}} - {{{\overline {\Delta Y} }}_{{{\text{B,t - 1}}}}}} \right|} \\ }&lt;!end array&gt; $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq4.gif"/></alternatives></inline-formula></p></td></tr><tr><td><p>where:</p></td></tr><tr><td><p><inline-formula id="IEq5"><alternatives><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {{X}}} }_{{{\text{B,t}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq5.gif"/></alternatives></inline-formula>, <inline-formula id="IEq6"><alternatives><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {{Y}}} }_{{{\text{B,t}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq6.gif"/></alternatives></inline-formula>—the average differences between the traveled distance by the blob <italic>B</italic> at two consecutive time moments <italic>t</italic>-1 and <italic>t</italic> for the direction <italic>x</italic> and <italic>y</italic>, respectively;</p></td></tr><tr><td><p><italic>Q</italic>—acceptable change of <inline-formula id="IEq7"><alternatives><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {{X}}} }_{{{\text{B,t}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq7.gif"/></alternatives></inline-formula>, <inline-formula id="IEq8"><alternatives><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {{Y}}} }_{{{\text{B,t}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq8.gif"/></alternatives></inline-formula> for consecutive frames, assumed value of <italic>Q</italic> = 2.</p></td></tr><tr><td><p>In further considerations, the blob’s and model’s time indexes will be omitted, unless necessary.</p></td></tr></tbody></table></table-wrap></p><p>For simplicity, all the average values are calculated as running mean and the movement path is approximated with a straight line. The overlapping of the blobs in the current and the previous frame is detected by the PP and both such blobs are treated as representing the same moving object, providing the continuity of the existence of the blob’s rectangle <italic>B</italic>. If the blob <italic>B</italic> disappears, the virtual model <italic>M</italic> is created, continuing the movement of the blob <italic>B</italic>, using the last known values for size, direction and speed of the disappeared blob <italic>B</italic>. The attributes of the model <italic>M</italic> are listed in Table <xref rid="Tab3" ref-type="table">3</xref>.
<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>The attributes assigned to each model <italic>M.</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Attribute</p></th><th><p>Description</p></th></tr></thead><tbody><tr><td><p><italic>t</italic><sub>0<italic>M</italic></sub></p></td><td><p>The time of creation of the blob replaced by the model <italic>M.</italic></p></td></tr><tr><td><p><italic>x</italic><sub>0<italic>M</italic></sub>, <italic>y</italic><sub>0<italic>M</italic></sub></p></td><td><p>The coordinates of the center of the rectangle <italic>M</italic> at the time <italic>t</italic><sub>0<italic>M</italic></sub>.</p></td></tr><tr><td><p><italic>x</italic><sub><italic>M</italic></sub>, <italic>y</italic><sub><italic>M</italic></sub></p></td><td><p>The current coordinates of the center of the rectangle <italic>M,</italic> calculated as: <inline-formula id="IEq9"><alternatives><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{*{20}{c}}  {{{{\text{x}}}_{{{\text{M}},{\text{t}}}}} = {{{\text{x}}}_{{{\text{M}},{\text{t}} - 1}}} + {{{\text{V}}}_{{\text{M}}}}\,\cos \left( {{{\alpha }_{{\text{M}}}}} \right)} \\  {{{{\text{y}}}_{{{\text{M}},{\text{t}}}}} = {{{\text{y}}}_{{{\text{M}},{\text{t}} - 1}}} + {{{\text{V}}}_{{\text{M}}}}\,\sin \left( {{{\alpha }_{{\text{M}}}}} \right)} \\  \end{array}  $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq9.gif"/></alternatives></inline-formula></p></td></tr><tr><td><p><italic>X</italic><sub><italic>M</italic></sub>, <italic>Y</italic><sub><italic>M</italic></sub></p></td><td><p>The size of the rectangle <italic>M.</italic></p></td></tr><tr><td><p><italic>V</italic><sub><italic>M</italic></sub></p></td><td><p>The velocity of the model <italic>M.</italic></p></td></tr><tr><td><p><italic>α</italic><sub><italic>M</italic></sub></p></td><td><p>The movement direction of the model <italic>M</italic>.</p></td></tr><tr><td><p><italic>T</italic><sub><italic>M</italic></sub></p></td><td><p>Time to live for the model <italic>M.</italic> At every time step, the value of <italic>T</italic><sub><italic>M</italic></sub> is decreased by 1. If <italic>T</italic><sub><italic>M</italic></sub> = 0, the model is deleted (applicable only for the <italic>short-term</italic> model, as explained in the text).</p></td></tr><tr><td rowspan="7"><p><italic>S</italic><sub><italic>M</italic></sub></p></td><td><p>The value informing about the size of the model, relative to the size of a typical car:</p></td></tr><tr><td><p>1 = <italic>small</italic> when <italic>w</italic> &lt; 0.5·<italic>W</italic><sub><italic>t</italic></sub></p></td></tr><tr><td><p>2 = <italic>normal</italic> when 0.5·<italic>W</italic><sub><italic>t</italic></sub> ≤ <italic>w</italic> ≤ <italic>3·W</italic><sub><italic>t</italic></sub></p></td></tr><tr><td><p>3 = <italic>big</italic> when <italic>w</italic> &gt; <italic>3·W</italic><sub><italic>t</italic></sub></p></td></tr><tr><td><p>where:</p></td></tr><tr><td><p><italic>w</italic> is calculated as the width of car cast in the direction of its movement;</p></td></tr><tr><td><p><italic>W</italic><sub><italic>t</italic></sub> is the width of a typical car (2 m).</p></td></tr></tbody></table></table-wrap></p><p>A simple example of blob processing is shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref>, where at the time <italic>t</italic>, a new blob has been detected and marked as <italic>B</italic><sub><italic>t</italic></sub> (the speed <inline-formula id="IEq10"><alternatives><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \overline {\text{V}} {\text{B,t}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq10.gif"/></alternatives></inline-formula> and the direction <inline-formula id="IEq11"><alternatives><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \overline \alpha {\text{B}},{\text{t}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq11.gif"/></alternatives></inline-formula> are unknown at this time). At the next time step <italic>t</italic> + 1, the blob changes its position and shape, but, due to the overlapping with the blob from time <italic>t</italic>, it is treated as the same moving blob, so the attributes of <italic>B</italic><sub><italic>t</italic></sub> are copied to <italic>B</italic><sub><italic>t+</italic>1</sub>, also the first approximation of the speed of the blob <inline-formula id="IEq12"><alternatives><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\text{V}} }_{{{\text{B,t}} + 1}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq12.gif"/></alternatives></inline-formula> can be calculated, as well as <inline-formula id="IEq13"><alternatives><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_{{{\text{B}},{\text{t}} + 1}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq13.gif"/></alternatives></inline-formula>. At the time step <italic>t</italic> + 2, the blob disappears, but its existence is represented by the newly created model <italic>M</italic><sub><italic>t+</italic>2</sub> with the following attributes:
<fig id="Fig7"><label>Figure 7</label><caption xml:lang="en"><p>An example of transforming the blob <italic>B</italic> into the model <italic>M.</italic> At the time <italic>t</italic> a new blob has been detected and marked as <italic>B</italic><sub><italic>t</italic></sub>. Next, at <italic>t + </italic>1, the new blob overlaps with the blob from time <italic>t,</italic> so it is considered as the continuation of <italic>B</italic><sub><italic>t</italic></sub>. At the time <italic>t + </italic>2 the blob disappears, but the model <italic>M</italic><sub><italic>t + </italic>2</sub> continues the movement of <italic>B.</italic></p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig7_HTML.gif" id="MO12"/></fig><disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \begin{array}{*{20}{c}} {{{{\text{t}}}_{{0{\text{M}}}}} = {{{\text{t}}}_{{0{\text{B}}}}}} \\ {\left( {XM,YM} \right) = \left( {{{{\overline {\text{X}} }}_{{{\text{B}},{\text{t}} + 1}}},{{{\overline {\text{Y}} }}_{{{\text{B}},{\text{t}} + 1}}}} \right)} \\ {{{{\text{V}}}_{{\text{M}}}} = {{{\overline {\text{V}} }}_{{{\text{B}},{\text{t}} + 1}}},\,{{\alpha }_{{\text{M}}}} = {{{\overline \alpha }}_{{{\text{B}},{\text{t}} + 1}}}} \\ \end{array} $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ2.gif"/></alternatives></disp-formula></p><p>When the size of the blob <italic>B</italic> changes (which could mean joining with another blob or splitting), new objects are created: a new model <italic>M</italic>, as a continuation of the <italic>B</italic>, and the new blob (or blobs in the case of splitting) representing the current blob. As an example, two moving blobs are shown in Fig. <xref rid="Fig8" ref-type="fig">8</xref>, represented by <italic>B</italic><sub><italic>a</italic></sub> and <italic>B</italic><sub><italic>b</italic></sub> at the time <italic>t</italic>. In the next frame at the time <italic>t</italic> + 1, both blobs have joined together and from this moment <italic>B</italic><sub><italic>a</italic></sub> is representing the connected blobs. However, due to the size change of the blob <italic>B</italic><sub><italic>a</italic></sub> and disappearing of the blob <italic>B</italic><sub><italic>b</italic></sub>, the blobs <italic>B</italic><sub><italic>a</italic></sub> and <italic>B</italic><sub><italic>b</italic></sub> from time <italic>t</italic> have been converted into the models <italic>M</italic><sub><italic>d</italic></sub> and <italic>M</italic><sub><italic>c</italic></sub>, respectively. Later, at the time <italic>t</italic> + 2, the blob <italic>B</italic><sub><italic>a</italic></sub> splits again into two blobs <italic>B</italic><sub><italic>a</italic></sub> and <italic>B</italic><sub><italic>b</italic></sub>; the overlapping models <italic>M</italic><sub><italic>c</italic></sub> and <italic>M</italic><sub><italic>d</italic></sub> are then absorbed by the blobs <italic>B</italic><sub><italic>a</italic></sub> and <italic>B</italic><sub><italic>b</italic></sub>, respectively, providing the continuity of their movement.
<fig id="Fig8"><label>Figure 8</label><caption xml:lang="en"><p>An example of blobs’ tracking after blob joining.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig8_HTML.gif" id="MO13"/></fig></p><p>Having a set of blobs <italic>B</italic> and models <italic>M</italic> for each frame, the main processor executes the heuristic rules to extract data about moving vehicles. For example, if any blob and model are close together and have similar speed and direction of movement, depending on the situation, the model can be deleted or its position can be centered with the blob. Small models that are close to a bigger one and have similar speed and direction are deleted. The models that overlap with each other and move in the similar direction and with similar speed are consolidated. The most important rules concerning the blobs and models are summarized in Table <xref rid="Tab4" ref-type="table">4</xref> and below:
<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Overview of situations when model is created and deleted. The details are presented in form of the rules in the text.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Action</p></th><th><p>Situation</p></th></tr></thead><tbody><tr><td><p>Model is created</p></td><td><p>A blob disappears or changes its shape.</p></td></tr><tr><td rowspan="4"><p>Model is deleted</p></td><td><p>A blob and a model are close together, move in similar direction with similar speed.</p></td></tr><tr><td><p>A blob and a model overlap and their position is constant.</p></td></tr><tr><td><p>Two models are close together, they move in similar direction with similar speed and one of them is small.</p></td></tr><tr><td><p>Two models overlap and they move in similar direction with similar speed.</p></td></tr></tbody></table></table-wrap></p><p>High level tracking rules:
<list list-type="order"><list-item><p><bold>If</bold><italic>BlobSizeChange</italic>(<italic>B</italic><sub><italic>a,t</italic></sub><italic>, B</italic><sub><italic>a,t+</italic>1</sub>) ≥ 50 % <bold>then</bold></p><p><italic>M</italic><sub><italic>a,t+</italic>1</sub> = <italic>NewModelFromBlob</italic>(<italic>B</italic><sub><italic>a,t</italic></sub>)</p><p><italic>B</italic><sub><italic>b,t+</italic>1</sub> = <italic>NewBlob</italic>(<italic>B</italic><sub><italic>a,t+</italic>1</sub>)</p><p><italic>DeleteBlob</italic>(<italic>B</italic><sub><italic>a,t+</italic>1</sub>)</p></list-item><list-item><p><bold>If</bold><italic>BlobDisappeared</italic>(<italic>B</italic><sub><italic>t</italic></sub>) <bold>then</bold><italic>M</italic><sub><italic>t+</italic>1</sub> = <italic>NewModelFromBlob</italic>(<italic>B</italic><sub><italic>t</italic></sub>)</p></list-item><list-item><p><bold>If</bold><italic>Distance</italic>(<italic>B, M</italic>) &lt; <italic>D</italic><sub><italic>min</italic></sub><bold>and</bold><inline-formula id="IEq14"><alternatives><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \left| {{{\text{V}}_M} - {{{\overline {\text{V}} }}_B}} \right| &lt; {\text{V}}\,min $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq14.gif"/></alternatives></inline-formula><bold>and</bold><italic>AngleDifference</italic>(<inline-formula id="IEq15"><alternatives><tex-math id="IEq15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_{B}},{{\alpha }_{M}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq15.gif"/></alternatives></inline-formula>) &lt; <italic>α</italic><sub><italic>min</italic></sub><bold>and</bold><italic>t</italic><sub>0<italic>B</italic></sub> 
                                    <italic>&lt; t</italic><sub>0<italic>M</italic></sub><bold>then</bold><italic>BlobAbsorbsModel</italic>(<italic>B, M</italic>).</p></list-item><list-item><p><bold>If</bold><italic>Overlap</italic>(<italic>B, M</italic>) &gt; 50 % <bold>and</bold><inline-formula id="IEq16"><alternatives><tex-math id="IEq16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {{V}} }_B} = 0 $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq16.gif"/></alternatives></inline-formula><bold>and</bold><italic>V</italic><sub><italic>M</italic></sub> = 0 <bold>then</bold><italic>DeleteModel</italic>(<italic>M</italic>)</p></list-item><list-item><p><bold>If</bold><italic>SpeedDifference</italic>(<inline-formula id="IEq17"><alternatives><tex-math id="IEq17_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{{V}}_{{{{{M}}_{{a}}}}}}{, }{{{V}}_{{{{{M}}_{{b}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq17.gif"/></alternatives></inline-formula>) &lt; 50 % <bold>and</bold><italic>AngleDifference</italic>(<inline-formula id="IEq18"><alternatives><tex-math id="IEq18_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\alpha }_{{{{{\text{M}}}_{{\text{a}}}}}}},\,{{\alpha }_{{{{{\text{M}}}_{{\text{b}}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq18.gif"/></alternatives></inline-formula>) &lt; <italic>α</italic><sub><italic>min</italic></sub><bold>and</bold><italic>Distance</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>) &lt; <italic>D</italic><sub><italic>min</italic></sub><bold>and</bold> (<inline-formula id="IEq19"><alternatives><tex-math id="IEq19_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{{\text{S}}}_{{{{M}_{a}}}}} = {\text{small}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq19.gif"/></alternatives></inline-formula><bold>or</bold><inline-formula id="IEq20"><alternatives><tex-math id="IEq20_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\text{S}}_{{{{{M}}_{{b}}}}}} = {\text{small}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq20.gif"/></alternatives></inline-formula>) <bold>then</bold><italic>ConsolidateModels</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>)</p></list-item><list-item><p><bold>If</bold><italic>SpeedDifference</italic>(<inline-formula id="IEq21"><alternatives><tex-math id="IEq21_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{{V}}_{{{{{M}}_{{a}}}}}}{, }{{{V}}_{{{{{M}}_{{b}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq21.gif"/></alternatives></inline-formula>) &lt; 50 % <bold>and</bold><italic>AngleDifference</italic>(<inline-formula id="IEq22"><alternatives><tex-math id="IEq22_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\alpha }_{{{{{\text{M}}}_{{\text{a}}}}}}},\,{{\alpha }_{{{{{\text{M}}}_{{\text{b}}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq22.gif"/></alternatives></inline-formula>) &lt; <italic>α</italic><sub><italic>min</italic></sub><bold>and</bold><italic>Overlap</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>) &lt; min(<inline-formula id="IEq23"><alternatives><tex-math id="IEq23_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{t}} - {{\text{t}}_{{0{{{M}}_{{a}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq23.gif"/></alternatives></inline-formula>, 100) <bold>then</bold><italic>ConsolidateModels</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>)</p></list-item><list-item><p><bold>If</bold><italic>SpeedDifference</italic>(<inline-formula id="IEq24"><alternatives><tex-math id="IEq24_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{{V}}_{{{{{M}}_{{a}}}}}}{, }{{{V}}_{{{{{M}}_{{b}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq24.gif"/></alternatives></inline-formula>) &lt; 50 % <bold>and</bold><italic>AngleDifference</italic>(<inline-formula id="IEq25"><alternatives><tex-math id="IEq25_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\alpha }_{{{{{\text{M}}}_{{\text{a}}}}}}},\,{{\alpha }_{{{{{\text{M}}}_{{\text{b}}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq25.gif"/></alternatives></inline-formula>) &lt; <italic>α</italic><sub><italic>min</italic></sub><bold>and</bold> (<italic>Overlap</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>) &lt; min(<inline-formula id="IEq26"><alternatives><tex-math id="IEq26_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{t}} - {{\text{t}}_{{0{{{M}}_{{a}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq26.gif"/></alternatives></inline-formula>, 100) <bold>or</bold><italic>Overlap</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>) &lt; min(<inline-formula id="IEq27"><alternatives><tex-math id="IEq27_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{t}} - {{{t}}_{{0{{{M}}_{{b}}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq27.gif"/></alternatives></inline-formula>, 100)) <bold>then</bold><italic>ConsolidateModels</italic>(<italic>M</italic><sub><italic>b</italic></sub><italic>, M</italic><sub><italic>a</italic></sub>)</p></list-item></list></p><p>Rules used for conversion of blob <italic>B</italic> into model <italic>M</italic>:
<list list-type="order"><list-item><p><bold>If</bold><italic>NewModelFromBlob</italic>(<italic>B</italic>) <bold>and</bold><inline-formula id="IEq28"><alternatives><tex-math id="IEq28_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\text{V}} }_{\text{B}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq28.gif"/></alternatives></inline-formula> and <inline-formula id="IEq29"><alternatives><tex-math id="IEq29_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_{{\text{B}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq29.gif"/></alternatives></inline-formula> are not known <bold>then</bold> create a new <italic>short-term</italic> model <italic>M</italic> using the parameters of <italic>B</italic> and <italic>T</italic><sub><italic>M</italic></sub> 
                                    <italic>= T</italic><sub><italic>MIN</italic></sub> .</p><p><italic>Short-term</italic> model is supposed to live shortly and it will disappear after <italic>T</italic><sub><italic>MIN</italic></sub> frames. It is used to track “blinking” moving blobs, for which it is difficult to estimate <inline-formula id="IEq30"><alternatives><tex-math id="IEq30_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\text{V}} }_{\text{B}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq30.gif"/></alternatives></inline-formula> and <inline-formula id="IEq31"><alternatives><tex-math id="IEq31_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_{{\text{B}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq31.gif"/></alternatives></inline-formula> due to their short existence on the screen. <italic>T</italic><sub><italic>MIN</italic></sub> 
                                    <italic>=</italic> 6 has been set experimentally for frame rate 32 fps.</p></list-item><list-item><p><bold>If</bold><italic>NewModelFromBlob</italic>(<italic>B</italic>) <bold>and</bold><inline-formula id="IEq32"><alternatives><tex-math id="IEq32_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\text{V}} }_{\text{B}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq32.gif"/></alternatives></inline-formula> and <inline-formula id="IEq33"><alternatives><tex-math id="IEq33_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline \alpha }_{{\text{B}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq33.gif"/></alternatives></inline-formula> are known <bold>and</bold><inline-formula id="IEq34"><alternatives><tex-math id="IEq34_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\text{V}} }_{\text{B}}} &lt; {{V}_{\text{MAX}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq34.gif"/></alternatives></inline-formula><bold>and not</bold> (<italic>is_human(B)</italic>) <bold>and</bold><inline-formula id="IEq35"><alternatives><tex-math id="IEq35_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\text{q}}_{\text{B}}} &gt; {{\text{Q}}_{\text{MIN}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq35.gif"/></alternatives></inline-formula><bold>then</bold> create new model <italic>M</italic> using the parameters of <italic>B.</italic></p></list-item></list>where:
<def-list><def-item><term><italic>V</italic><sub><italic>MAX</italic></sub></term><def><p>the maximum allowed speed for the vehicle</p></def></def-item><def-item><term><italic>D</italic><sub><italic>min</italic></sub></term><def><p>the minimum allowed distance (assumed 4 pixels)</p></def></def-item><def-item><term><italic>α</italic><sub><italic>min</italic></sub></term><def><p>the minimum allowed angle, assumed <italic>α</italic><sub><italic>min</italic></sub> 
                                    <italic>=</italic> 45°</p></def></def-item></def-list></p><p>The function <italic>is_human()</italic> is a simple and rough estimate, if the detected blob could be a person walking along the street (assuming that the camera is situated high above the road) and is calculated as:
<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{is}}\_ {\text{human}}(B) = \left\{ {\matrix{ {\text{true}} &amp;{when\,\,{{{{{{\overline {\text{V}} }}_{\text{B}}} &lt; {{\text{V}}_{\text{MIN}}} \wedge 3 &lt; {{{\overline {\text{Y}} }}_{\text{B}}}}} \left/ {{{{{\overline {\text{X}} }}_{\text{B}}} &lt; 6 \wedge {{\text{P}}_{\text{HMNIN}}} &lt; {{\text{P}}_{\text{B}}} &lt; {{\text{P}}_{\text{HMAX}}}}} \right.}} \\ {\text{false}} &amp;{otherwise} \\ }&lt;!end array&gt; } \right\} $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ3.gif"/></alternatives></disp-formula>where:
<def-list><def-item><term><italic>V</italic><sub><italic>MIN</italic></sub></term><def><p>the maximum speed for walking human</p></def></def-item><def-item><term><italic>P</italic><sub><italic>HMIN</italic></sub>, <italic>P</italic><sub><italic>HMAX</italic></sub></term><def><p>the constant defining the minimum and maximum number of pixels, respectively, for the blob recognized as a person. The values of <italic>P</italic><sub><italic>HMIN</italic></sub> and <italic>P</italic><sub><italic>HMAX</italic></sub> are calculated to represent the area of 0.75 m<sup>2</sup> and 3 m<sup>2</sup>, respectively, at the image after geometrical transformation.</p></def></def-item><def-item><term><italic>Q</italic><sub><italic>MIN</italic></sub></term><def><p>the constant defining the minimum allowed object’s movement path quality <italic>q</italic><sub><italic>B</italic></sub>.</p></def></def-item></def-list></p><p>The quality <italic>q</italic><sub><italic>B</italic></sub> of the movement path of the blob <italic>B</italic> is calculated according to the equations:
<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ \matrix{ {{{\text{q}}_{{{\text{B}},{{\text{t}}_{{0{\text{B}}}}}}}} = 0} \\ {{{\text{q}}_{{{\text{B,t}}}}} = {{\text{q}}_{{{\text{B,t - 1}}}}} + 2{\text{Q}} - \left| {{{\text{x}}_{{{\text{B,t}}}}} - {{\text{x}}_{{{\text{B,t - 1}}}}} - {{{\overline {\Delta {\text{X}}} }}_{{{\text{B,t - 1}}}}}} \right| - \left| {{{\text{y}}_{{{\text{B,t}}}}} - {{\text{y}}_{{{\text{B,t - 1}}}}} - {{{\overline {\Delta {\text{Y}}} }}_{{{\text{B,t - 1}}}}}} \right|} \\ }&lt;!end array&gt; $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ4.gif"/></alternatives></disp-formula>where:
<def-list><def-item><term><inline-formula id="IEq36"><alternatives><tex-math id="IEq36_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {\text{X}}} }_{{{\text{B,t}}}}},\,{{\overline {\Delta {\text{Y}}} }_{{{\text{B,T}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq36.gif"/></alternatives></inline-formula></term><def><p>the average differences between the traveled distance by the blob <italic>B</italic> at two consecutive time moments <italic>t</italic>-1 and <italic>t</italic> for the direction <italic>x</italic> and <italic>y</italic>, respectively.</p></def></def-item><def-item><term><italic>Q</italic></term><def><p>acceptable change of <inline-formula id="IEq37"><alternatives><tex-math id="IEq37_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {{\overline {\Delta {\text{X}}} }_{{{\text{B,t}}}}},\,{{\overline {\Delta {\text{Y}}} }_{{{\text{B,T}}}}} $$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_IEq37.gif"/></alternatives></inline-formula> for consecutive frames, assumed value of <italic>Q</italic> = 2.
<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\text{BlobSizeChange}}\left( {{{\text{B}}_{\text{t}}},{{B}_{{{\text{t}} + 1}}}} \right) = max\left( {\frac{{\left| {{{\text{X}}_{{{\text{B,t}}}}} - {{\text{X}}_{{{\text{B,t}} + 1}}}} \right|}}{{max\left( {{{\text{X}}_{{{\text{B,t}}}}},{{\text{X}}_{{{\text{B,t}}}}} + 1} \right)}},\frac{{\left| {{{\text{Y}}_{{{\text{B,t}}}}} - {{\text{Y}}_{{{\text{B,t}} + 1}}}} \right|}}{{max\left( {{{\text{Y}}_{{{\text{B,t}}}}},{{\text{Y}}_{{{\text{B,t}}}}} + 1} \right)}}} \right) $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ5.gif"/></alternatives></disp-formula></p></def></def-item><def-item><term><italic>Distance</italic>(<italic>B, M</italic>)</term><def><p>the shortest distance between the outlines of two rectangles: <italic>B</italic> and <italic>M</italic>. If the rectangles overlap, the distance is 0.
<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ SpeedDifference\left( {{{\text{V}}_1}{,}{{\text{V}}_2}} \right) = \frac{{\left| {{{\text{V}}_1} - {{\text{V}}_2}} \right|}}{{max\left( {{{\text{V}}_1},{{\text{V}}_2}} \right)}}100\% $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ6.gif"/></alternatives></disp-formula></p></def></def-item><def-item><term><italic>AngleDifference</italic>(<italic>α</italic><sub>1,</sub><italic>α</italic><sub>2</sub>)</term><def><p>the difference between two angles <italic>α</italic><sub>1</sub> and <italic>α</italic><sub>2</sub>.</p></def></def-item><def-item><term><italic>Overlap</italic>(<italic>B, M</italic>)</term><def><p>returns % of overlapping area between the rectangles <italic>B</italic> and <italic>M</italic> with respect to the area of <italic>B</italic>.</p></def></def-item><def-item><term><italic>BlobAbsorbsModel</italic>(<italic>B, M</italic>)</term><def><p>the attributes’ values of the model <italic>M</italic> are copied to the blob <italic>B</italic> and the model <italic>M</italic> is deleted.</p></def></def-item><def-item><term><italic>ConsolidateModels</italic>(<italic>M</italic><sub><italic>a</italic></sub><italic>, M</italic><sub><italic>b</italic></sub>)</term><def><p>the models <italic>M</italic><sub><italic>a</italic></sub> and <italic>M</italic><sub><italic>b</italic></sub> are compared and the model which exists shorter is deleted, the remaining model is updated with the speed and moving direction as the average of the respective values from <italic>M</italic><sub><italic>a</italic></sub> and <italic>M</italic><sub><italic>b</italic></sub>.</p></def></def-item></def-list></p><p>The rules have been presented in a simplified form to aid the readability, in fact more conditions are checked, i.e. to protect from processing invalid data, division by 0, etc.</p><p>The moving models which leave the picture frame are filtered and only those considered as reliable are counted. The model is considered reliable if it comes from a blob that has been moving smoothly for some time (depending on the quality <italic>q</italic><sub><italic>B</italic></sub> of the movement path) or it has been often overlapping with blobs moving with similar size, speed and direction. The traffic flow data are constantly updated, the moving vehicles are classified according to their speed and direction—this information is periodically transmitted by the radio to a nearby node which is closer to the data sink.</p></sec><sec id="Sec3"><title>Data Transfer in the Sensor Network</title><p>To avoid the overheads of the standard wireless communication protocols such as 802.11 or 802.15.4, the authors decided to develop and implement the proprietary protocol optimized for this application. The proposed protocol has shorter lengths of the headers (Table <xref rid="Tab5" ref-type="table">5</xref>), moreover, the header already contains information about the state of the node (battery state, data buffer occupancy), which is important for neighboring nodes in routing the traffic. Also, data length is suited to carry only important information on car traffic. The simple comparison of the proposed protocol to the standard 802.15.4 is shown in Table <xref rid="Tab6" ref-type="table">6</xref>. The sink nodes have a simple serial interface with text commands, so connecting the network to a PC is very easy.
<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>The bit lengths of the transmission headers in different protocols.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Protocol/remarks</p></th><th><p>Description</p></th><th><p>Length [bits]</p></th></tr></thead><tbody><tr><td><p>802.11</p></td><td><p>MAC header</p></td><td><p>240</p></td></tr><tr><td rowspan="3"><p>802.15.4, (PHY + MAC header)</p></td><td><p>Beacon:</p></td><td><p>80–176</p></td></tr><tr><td><p>Data:</p></td><td><p>96–192</p></td></tr><tr><td><p>Ack</p></td><td><p>32</p></td></tr><tr><td rowspan="3"><p>Protocol used in this paper. Header already contains its own 16-bit CRC. Each header (including Ack) contains additional data about the state of the transmitting node: battery state, external power supply, data buffer level.</p></td><td><p>Beacon</p></td><td><p>64</p></td></tr><tr><td><p>Data</p></td><td><p>64</p></td></tr><tr><td><p>Ack</p></td><td><p>64</p></td></tr></tbody></table></table-wrap><table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>The comparison of the most important parameters of 802.15.4 and the protocol presented in this paper.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Description</p></th><th><p>802.15.4</p></th><th><p>The protocol used in this the presented sensor network node</p></th></tr></thead><tbody><tr><td><p>Beacon interval</p></td><td><p>approx. 15 ms–4 min.</p></td><td><p>100 s</p></td></tr><tr><td><p>Access to the transmission medium</p></td><td><p>Contention based and contention free periods</p></td><td><p>Only contention free.</p></td></tr><tr><td rowspan="3"><p>Number of channels</p></td><td><p>1 at 868 MHz</p></td><td rowspan="3"><p>1 at 868.5 MHz</p></td></tr><tr><td><p>10 at 902–928 MHz</p></td></tr><tr><td><p>16 at 2400–2483.5 MHz</p></td></tr><tr><td><p>Data rate (at 868 MHz band)</p></td><td><p>20 kbps</p></td><td><p>38.4 kbps</p></td></tr><tr><td><p>Preamble length</p></td><td><p>32 bits</p></td><td><p>10 bits</p></td></tr></tbody></table></table-wrap></p><p>The access to the transmission medium is based on decentralized Time Division Multiple Access (TDMA). Each node is equipped with a radio transceiver module using ISM 869.5 MHz band utilizing data rate of 38.4 kbps with Manchester encoding. The nodes’ transmitters have the power of 500 mW (transmitter duty cycle &lt;10 %), which helps in providing longer communication range in the harsh environment. The transmitter’s power can be remotely changed by the software to the values 40 mW, 125 mW, 250 mW and 500 mW, if needed. The dedicated operating system of the node, running on the main 32-bit on-chip processor, controls all the operations of the node and collects data from the image-processing part and the PP processors.
<list list-type="order"><list-item><p><italic>Transmission Time t</italic><sub><italic>tx</italic></sub></p></list-item></list></p><p>Each node has its own unique address and a table for storing data about its neighbors. All the nodes use the same period <italic>T</italic><sub><italic>t</italic></sub> of the transmission schedule, basing on their local clocks. Each node selects its own transmission start time <italic>t</italic><sub><italic>tx</italic></sub> in the TDMA transmission period <italic>T</italic><sub><italic>t</italic></sub>, providing:
<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ _{\text{i}}^{\forall }\left| {{{\text{t}}_{{{\text{tx,i}}}}} - {{\text{t}}_{\text{tx}}}} \right| &gt; {{\text{T}}_{\text{MIN}}} $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="11265_2012_681_Article_Equ7.gif"/></alternatives></disp-formula>where <italic>t</italic><sub><italic>tx,i</italic></sub> is the transmission start time of the other previously discovered node <italic>i</italic> (this can be also an indirect neighbor, i.e. the neighbor of the neighbor), <italic>T</italic><sub><italic>MIN</italic></sub> is a constant defined globally for the network, providing the required time distance between the transmissions. The times <italic>t</italic><sub><italic>tx</italic></sub> and <italic>t</italic><sub><italic>tx,i</italic></sub> are relative to the node’s local time source. The time distances of all the transmission start times <italic>t</italic><sub><italic>tx,i</italic></sub> of the neighboring nodes are continuously monitored. When a node detects that among the neighboring nodes the condition (7) is not satisfied, it asks the problematic node to change its <italic>t</italic><sub><italic>tx</italic></sub>.
<list list-type="order"><list-item><p><italic>Beacon</italic></p></list-item></list></p><p>The beacon contains basic information about the node, such as the node’s address, the distance of the node from the sink (measured in transmission hops), the node’s battery condition, the node’s data buffer occupancy and data about the node’s neighbors. Beacon is transmitted at <italic>t</italic><sub><italic>tx</italic></sub> every <italic>N</italic><sub><italic>B</italic></sub> data transmission periods, typical value of <italic>N</italic><sub><italic>B</italic></sub> is from 3 to 10. Beacons are also used for establishing data links between the nodes. Each node infers from its neighbors’ beacons the existence of the indirect neighboring nodes, which helps to prevent the hidden terminal problem [<xref ref-type="bibr" rid="CR2">2</xref>].
<list list-type="order"><list-item><p><italic>Startup and Discovery of the Neighbors</italic></p></list-item></list></p><p>At the startup, each node enters the discovery mode, when it continuously listens to the transmitted beacons, identifies neighbors and saves neighbors’ transmission times <italic>t</italic><sub><italic>tx,i</italic></sub>. To find new nodes and accommodate the network topology changes, the discovery mode is periodically repeated. This is similar to the procedure used in S-MAC protocol [<xref ref-type="bibr" rid="CR35">35</xref>].</p><p>There are four types of frames that can be transmitted through the network: beacon, data, acknowledge and configuration packets. All the frames have a 64-bit header of a similar structure. The beacon frame may contain information for the neighboring nodes needed for organizing the network (Table <xref rid="Tab7" ref-type="table">7</xref>). The data frame carries the encrypted 128-bit packets (PKT) with car traffic information (Table <xref rid="Tab8" ref-type="table">8</xref>), which can be aggregated from several nodes and transmitted together in a single frame. The acknowledge frame, transmitted just after the successful reception of the data frame, is used to confirm the reception of data, so the data transmitting node can free its buffer.
<table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>The general structure of the frame.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Field:</p></th><th><p>HEADER (64-bits)</p></th><th><p>PAYLOAD (0…1920 bits)</p></th><th><p>CRC (32 bits)</p></th></tr></thead><tbody><tr><td><p>Data:</p></td><td><p>frame type, source and destination address, frame length, header’s CRC</p></td><td><p>beacon data, 128-bit data packets (PKT)</p></td><td><p>CRC of the payload</p></td></tr></tbody></table></table-wrap><table-wrap id="Tab8"><label>Table 8</label><caption xml:lang="en"><p>The structure of the single 128-bit payload packet PKT.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Field:</p></th><th><p>PKT_ID (4-bits) (type of the packet)</p></th><th><p>PKT_ADDR (12 bits)</p></th><th><p>PKT_REALTIME (32 bits)</p></th><th><p>PKT_DATA (64-bits) (content depends on the PKT_ID)</p></th><th><p>PKT_CRC (16 bits)</p></th></tr></thead><tbody><tr><td rowspan="4"><p>Data:</p></td><td><p>PKTID_DATA1 (basic car traffic info)</p></td><td rowspan="4"><p>Address of the source node</p></td><td rowspan="4"><p>Time stamp</p></td><td><p>The number and the average speed of the detected cars in the last minute for each of four directions (N, E, W, S).</p></td><td rowspan="4"><p>CRC of the payload</p></td></tr><tr><td><p>PKTID_DATA2 (environmental data)</p></td><td><p>Ambient temperature, sun intensity, etc.</p></td></tr><tr><td><p>PKTID_DATA3 (node working parameters)</p></td><td><p>Node’s battery state, number of detected neighbors, IDs of the first eight neighbors.</p></td></tr><tr><td><p>PKTID_DATA4 (camera image)</p></td><td><p>64 bits of camera image data.</p></td></tr></tbody></table></table-wrap></p><p>Each node switches on its receiver at the time <italic>t</italic><sub><italic>tx,i</italic></sub> to listen to the possible transmission of the <italic>i</italic>-th neighbor. If the receiving node is not an addressee of the data transmission, it switches off its receiver just after receiving the header, in order to avoid overhearing.</p><p>Car traffic data in the sensor network are regularly transmitted from the nodes to the data sinks using data frames. To provide the ability of sending information in the reverse direction, i.e. from the sink to distributed nodes, the configuration frame is used. The configuration frame is transmitted without acknowledgement. Each node, after receiving a new configuration packet, simply retransmits it several times. This simple mechanism causes a heavy load of radio links but it is used rarely and only for service purposes, such as configuring remote nodes’ parameters, updating the firmware (FPGA nodes only) or the software.</p><p>Each data packet (PKT) contains the time stamp coming from the node’s real clock. The time stamp is used for checking the validity of data at the host computer. The sensor network utilizes global real clock synchronization scheme. The sync packets are periodically generated by the main sink node and distributed through the network by each node using the beacons.
<list list-type="order"><list-item><p><italic>Data Protection</italic></p></list-item></list></p><p>Each packet header is protected with 16-bit CRC, so the receiver can quickly verify the integrity of the header and the addressee of the packet without receiving the whole packet. Additionally, the payload has its own 32-bit CRC.</p><p>The header is transmitted using plain text, while the payload (PKT_DATA) is encrypted using AES with symmetric key. The hardware encryption, compared to the software encryption, has better energy efficiency measured in energy/bit, thus each node has the hardware encryption block. Decryption is used occasionally by the nodes (except the sink nodes), so it has been implemented in the software.</p><p>The 3 prioritized AES keys have been used: KEY1 and KEY2 are the same for all the nodes, the KEY3 is different for each node. During regular transmission, the KEY1 is used. To change the KEY1, the KEY2 is required. At a regular time interval, the change of the KEY1 (protected by KEY2) is commanded by the sink node. The KEY3 enables changes to the KEY1 and KEY2 and it should be used in case of a node’s takeover.</p></sec><sec id="Sec4"><title>Hardware Realization of the Sensor Network</title><p><list list-type="order"><list-item><p><italic>Block Diagram of the Sensor Network Nodes</italic></p></list-item></list></p><p>The block diagram of the node’s hardware, common for FPGA and ASIC prototype, is presented in Fig. <xref rid="Fig9" ref-type="fig">9</xref>. The main part of the hardware has been integrated into a Sensor Network Processing Module (SNPM), containing the custom microelectronic system with 32-bit processor BA12 from Beyond Semiconductor (the same class as Arm’s ARM9™) and the peripherals connected to the Wishbone bus. The hardware moving object detection system, described in <xref rid="Sec2" ref-type="sec">section II</xref>, has also been integrated, together with the additional hardware blocks providing quick AES encryption and control of the low level operations of the transceiver (Fig. <xref rid="Fig10" ref-type="fig">10</xref>).
<fig id="Fig9"><label>Figure 9</label><caption xml:lang="en"><p>Block diagram of the prototype node.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig9_HTML.gif" id="MO14"/></fig><fig id="Fig10"><label>Figure 10</label><caption xml:lang="en"><p>Block diagram of the Sensor Network Processing Module.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig10_HTML.gif" id="MO15"/></fig><list list-type="order"><list-item><p><italic>Hardware Implementation</italic></p></list-item></list></p><p>The first prototype sensor network node has been developed using Xilinx’s XC4VLX60 FPGA. The main board contains all the elements from Fig. <xref rid="Fig10" ref-type="fig">10</xref> with the camera MT9V111 from Micron and the transceiver ARF29 from Adeunis. The power supply has been implemented on additional boards using 12 V 6 Ah gel cell sealed lead acid battery. The resources used in FPGA implementation are listed in Table <xref rid="Tab9" ref-type="table">9</xref> and the pictures of the prototypes with the FPGA are shown in Fig. <xref rid="Fig11" ref-type="fig">11</xref>.
<table-wrap id="Tab9"><label>Table 9</label><caption xml:lang="en"><p>The resources used in the FPGA implementation using Xilinx Virtex-4 XC4VLX60.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Resource</p></th><th><p>Total used</p></th></tr></thead><tbody><tr><td><p>Flip Flops</p></td><td><p>10 325</p></td></tr><tr><td><p>LUTs (used as logic)</p></td><td><p>30 784</p></td></tr><tr><td><p>Total number of LUTs</p></td><td><p>31 676</p></td></tr><tr><td><p>Occupied Slices</p></td><td><p>19 220</p></td></tr><tr><td><p>18 Kb Block RAMs</p></td><td><p>157</p></td></tr></tbody></table></table-wrap><fig id="Fig11"><label>Figure 11</label><caption xml:lang="en"><p>Prototype sensor network node with FPGA, <bold>a</bold>—picture of the node, <bold>b</bold>—two test nodes installed on a street lamp-pole.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig11_HTML.gif" id="MO16"/></fig></p><p>After the successful startup with the FPGA, the ASIC has been designed in 130 nm UMC CMOS process and manufactured through Europractice (Fig. <xref rid="Fig12" ref-type="fig">12</xref>), using the RTL code from the FPGA prototype. Both FPGA and ASIC provide almost the same functionality. The design of ASIC has been made using Cadence SoC software with Faraday L130FSG (LP and HS) library of digital gates. The code consisting of ~95 000 lines of VHDL and Verilog has been synthesized using Cadence RTL Compiler with clock gating optimization. DFT flip-flops and JTAG controller have been added to enable future tests. For the implementation, Cadence SOC Encounter GXL 6.2 has been used, with crosstalk and signal integrity analysis and power supply analysis (electromigration, IR drop). The parameters of the designed ASIC are listed in Table <xref rid="Tab10" ref-type="table">10</xref>. Comparing Tables <xref rid="Tab9" ref-type="table">9</xref> and <xref rid="Tab10" ref-type="table">10</xref>, a large difference in the number of Flip Flops can be observed. The reason is that small memories and shift registers in FPGA are implemented using LUTs, while ASIC implementation uses FFs for that purpose. Moreover, in the FPGA all the memory is composed of 18Kbit block memories, resulting in wasted bits. Each memory block used in ASIC is designed to exactly fit the required size.
<fig id="Fig12"><label>Figure 12</label><caption xml:lang="en"><p>A photo of the manufactured ASIC realized in 130 nm UMC process packed in CQFP208 package.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/11265_2012_681_Fig12_HTML.jpg" id="MO17"/></fig><table-wrap id="Tab10"><label>Table 10</label><caption xml:lang="en"><p>The resources used in ASIC implementation using Faraday library and UMC 130 nm CMOS process.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Resource</p></th><th><p>Total used</p></th></tr></thead><tbody><tr><td><p>Flip Flops</p></td><td><p>18 478</p></td></tr><tr><td><p>Primitives (gates, buffers, flip flops, etc.)</p></td><td><p>549 062</p></td></tr><tr><td><p>Chip area</p></td><td><p>25 mm<sup>2</sup></p></td></tr><tr><td><p>Memory blocks</p></td><td><p>75</p></td></tr><tr><td><p>Memory bits</p></td><td><p>1 661 952</p></td></tr></tbody></table></table-wrap></p><p>The manufactured ASIC has been used to build the low-power version of the node shown in Fig. <xref rid="Fig13" ref-type="fig">13</xref>, which works with Li-Ion 3.7 V 3.5 Ah single cell battery and can be supplied by a solar panel of area of 0.5 m<sup>2</sup> (50 W peak power).
<fig id="Fig13"><label>Figure 13</label><caption xml:lang="en"><p>Photo of the ASIC version of the node.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig13_HTML.gif" id="MO18"/></fig></p><p>The set of the nodes has been installed on the street lamp-poles on several streets, as shown in Fig. <xref rid="Fig14" ref-type="fig">14</xref>. The detailed plan of the deployment is presented in the next section of the paper. The nodes have been using their batteries during the day; at night the batteries have been charging from the lamps’ power supply. The ASIC node consumes less power than the FPGA counterpart and therefore it is capable of working with a solar panel, instead of using the lamp’s power supply.
<fig id="Fig14"><label>Figure 14</label><caption xml:lang="en"><p>Photographs of the test nodes installed on the street lamp-poles, <bold>a</bold>—set of nodes along the street, <bold>b</bold>—two nodes with FPGA, <bold>c</bold>—the node with ASIC from Fig. <xref rid="Fig13" ref-type="fig">13</xref> installed in a typical housing of an industrial security camera.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig14_HTML.gif" id="MO19"/></fig></p></sec><sec id="Sec5"><title>Simulation and Test Results</title><p><list list-type="order"><list-item><p><italic>Object Detection</italic></p></list-item></list></p><p>The vehicles detected by the system have been verified on-line by the human operator. The road selected for traffic detection tests had various traffic conditions: from traffic jams to speeding vehicles. The sensor network node installed above the road was detecting and counting passing vehicles. Concurrently, the human operator was manually counting the cars. The comparison of the detection for 100 cars for various conditions has been presented in Table <xref rid="Tab11" ref-type="table">11</xref>. More results, including the object segmentation simulations and videos, are available on-line at <ext-link xlink:href="http://www.ue.eti.pg.gda.pl/sn" ext-link-type="uri">http://www.ue.eti.pg.gda.pl/sn</ext-link>.
<table-wrap id="Tab11"><label>Table 11</label><caption xml:lang="en"><p>Evaluation of vehicle detection rate for the sensor network node for various scenes. As the reference, 100 cars were counted by the human operator in each test.</p></caption><graphic xlink:href="MediaObjects/11265_2012_681_Tab11_HTML.gif" id="MO20"/></table-wrap></p><p>As can be seen from Table <xref rid="Tab11" ref-type="table">11</xref>, the image detection system recognized 63–93 % of the moving vehicles registered by the human. There were typically 1–9 % additional false detections of non-existent vehicles. During the sunny day, the most important problem are the shadows, resulting in joining the blobs from different cars. The basic shadow detection used in this system is not able to detect all the shadows. On a cloudy day, the most errors come from the dark cars of gray level similar to the color of the road. In this situation, the edge detection blocks are very helpful in increasing the detected pixel rate. The detection quality is significantly lower in the night, where mostly the car lights are detected. The achieved accuracy is the result of the compromises made during the design, such as a processing of monochrome and low resolution images and 4-bit representation of the pixel values, to obtain a low-power operation and simpler hardware. The detection rates are satisfactory for the collecting of the statistical data on average traffic flow and getting the overall general picture of the traffic. However, the authors feel that improvements in accuracy should be the next step in the further development of the presented idea.
<list list-type="order"><list-item><p><italic>Network Data Transmission</italic></p></list-item></list></p><p>The sensor network has been simulated to estimate the traffic parameters. The proposed sensor network is based on non-standard communication protocol and the use of popular simulation packages such as <italic>ns2</italic> would require the creation of the exact communication model of the node. Instead of this, the authors decided to embed the exact copy of the node’s software into the custom simulator written in C++. In this way there is no need to create the separate simulation model of the node and the simulation results are very close to reality.</p><p>In the simulator, ideal radio links have been assumed, so all transmissions are successful. Assuming that each node regularly generates on average 9 bps of data stream containing information about the car traffic and basic node’s information such as battery condition or node’s neighbors, the maximal throughput for the data packets at the single radio link is 620 bps. Other parameters are listed in Table. <xref rid="Tab12" ref-type="table">12</xref>.
<table-wrap id="Tab12"><label>Table 12</label><caption xml:lang="en"><p>Parameters of the proposed radio protocol.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Parameter</p></th><th><p>Value</p></th></tr></thead><tbody><tr><td><p>Typical data stream generated by the node</p></td><td><p>9 bps</p></td></tr><tr><td><p>Maximal throughput—raw frames at single link (node to node)</p></td><td><p>651 bps</p></td></tr><tr><td><p>Maximal throughput—data packets at single link (node to node)</p></td><td><p>620 bps</p></td></tr><tr><td><p>Node’s maximal number of neighbors</p></td><td><p>17</p></td></tr></tbody></table></table-wrap></p><p>Two basic network configurations of Fig. <xref rid="Fig15" ref-type="fig">15</xref> have been simulated. For the network configuration shown in Fig. <xref rid="Fig15" ref-type="fig">15(a)</xref>, the delay and radio activity have been analyzed. Both source nodes were generating the test messages consisting of single 128-bit packet. The total average delay of packets was 2.7 s, when the source nodes generate packets every <italic>t</italic><sub><italic>gen</italic></sub> = 0.5–2 s (Fig. <xref rid="Fig16" ref-type="fig">16</xref>). The activity of the radio receiver and transmitter is shown in Fig. <xref rid="Fig17" ref-type="fig">17(a) and (b)</xref>, respectively. The radio activity is highest for the middle node, as it has the largest number of neighbors. The average delay from the node 1 to the node <italic>n</italic> in the network configuration from Fig <xref rid="Fig15" ref-type="fig">15(b)</xref> is shown in Fig. <xref rid="Fig18" ref-type="fig">18</xref>. As can be seen, the average delay of packets per single hop in this configuration is almost constant and equal to approx. 1 s.
<fig id="Fig15"><label>Figure 15</label><caption xml:lang="en"><p>Test configurations of sensor network.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig15_HTML.gif" id="MO21"/></fig><fig id="Fig16"><label>Figure 16</label><caption xml:lang="en"><p>Simulation results of average delay of packets for the network configuration from Fig. <xref rid="Fig15" ref-type="fig">15(a)</xref>.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig16_HTML.gif" id="MO22"/></fig><fig id="Fig17"><label>Figure 17</label><caption xml:lang="en"><p>Simulation results of the activity of radio receiver <bold>a</bold> and radio transmitter <bold>b</bold> for the network configuration from Fig. <xref rid="Fig15" ref-type="fig">15(a)</xref>.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig17_HTML.gif" id="MO23"/></fig><fig id="Fig18"><label>Figure 18</label><caption xml:lang="en"><p>Simulation results of average delay of packets for the network configuration from Fig. <xref rid="Fig15" ref-type="fig">15(b)</xref> for <italic>t</italic><sub><italic>gen</italic></sub><italic> = 1 s</italic>.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig18_HTML.gif" id="MO24"/></fig></p><p>The effective radio range of the transceivers installed in real environment was about 900 m at the direct visibility of the antennas. The direct visibility can be easily achieved when the nodes are mounted on the street lamp-poles. After the installation of the nodes, the network organized itself and was correctly transmitting data to the sink, in the same way as during the simulation, except for sporadically occurring radio interference. The nodes have large data buffers for collecting unsent data (for at least 30 minutes); in the case of transmission problems, data is retransmitted at a later time. All the transmission times have been verified in practice and conform to the simulations. Apart from regular data about the car traffic, the operator can ask a node to send a single frame of the picture (Fig. <xref rid="Fig19" ref-type="fig">19</xref>), and uploading new firmware or software to the selected node, or all the nodes, is also possible.
<fig id="Fig19"><label>Figure 19</label><caption xml:lang="en"><p>The picture uploaded from the node.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig19_HTML.gif" id="MO25"/></fig><list list-type="order"><list-item><p><italic>Power Consumption</italic></p></list-item></list></p><p>The comparison of the power consumption of FPGA and ASIC prototypes is shown in Fig. <xref rid="Fig20" ref-type="fig">20</xref>. As could be expected, the main power savings are at the core in ASIC version. The other blocks (SDRAM, camera, I/O) have also slightly lower power consumption in ASIC prototype, but it is only due to the fact that they work with lower supply voltage (3 V instead of 3.3 V used at FPGA prototype).
<fig id="Fig20"><label>Figure 20</label><caption xml:lang="en"><p>Power consumed by the main blocks of the sensor network node prototypes during typical work (detecting traffic and exchanging data with 5 neighbors). FPGA prototype works with the following power supply voltages: 1.2 V and 2.5 V for the core, 3.3 V for SNPM I/O, camera and SDRAM and 3 V for the transceiver. ASIC prototype uses 1.2 V for the core, 3 V for all other blocks. The power losses at the power supply and battery charging circuitry are not included.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig20_HTML.gif" id="MO26"/></fig><list list-type="order"><list-item><p><italic>The Sensor Network Installation</italic></p></list-item></list></p><p>The sensor network consisting of 26 nodes (22 FPGA-based and 4 ASIC-based) has been tested in real conditions. Data from the remote nodes are transferred to the sink node. The sink node is connected via RS-232 to the PC computer, which plays the role of operator’s console, where the collected data are visualized. The map of the installation is shown in Fig. <xref rid="Fig21" ref-type="fig">21</xref>, the installed nodes are configured to measure only the traffic along the street, as indicated by the arrows on the map. For the deployment of the nodes, the following criteria have been chosen: the vicinity of the university, the possibility to test multi-hop radio transmission, one-way and two-way streets and the various congestion of the nodes’ positions.
<fig id="Fig21"><label>Figure 21</label><caption xml:lang="en"><p>The location of the sensor network nodes (<italic>black dots</italic>) on the streets. The arrows indicate the directions of measured traffic.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig21_HTML.gif" id="MO27"/></fig></p><p>Each node accumulates information about the traffic and transmits it every 60 s. Finally, this information is presented at the console as simple histograms (Fig. <xref rid="Fig22" ref-type="fig">22</xref>), showing the number of vehicles detected for each direction. Each node maintains statistics about average tracks of the moving vehicles, so standing vehicles, for which the moving direction cannot be determined, shall be classified according to their location on the street. The project had a strictly scientific purpose, therefore the collected data were used only by the researchers, the controlling of the city lights was not introduced at this stage.
<fig id="Fig22"><label>Figure 22</label><caption xml:lang="en"><p>Histograms representing the traffic detected by a node.</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/11265_2012_681_Fig22_HTML.gif" id="MO28"/></fig></p><p>Apart from the current state of the traffic, the history of the traffic for each node and each direction can be displayed in the form of a time graph. The operator can also: configure a single parameter of each node, reset the node, update its software or download logs or single camera pictures, however these operations work slowly due to the low throughput of the radio network. Each node is also equipped with on-board temperature sensor and A/D converters for measuring the solar panel and battery voltages and currents. The proposed network protocol has dedicated data slots for transmitting 8-bit values of the measured temperature, noise, sun intensity and node’s internal battery state.</p></sec><sec id="Sec6" sec-type="conclusion"><title>Conclusions</title><p>This work represents an attempt to solve the problem of measuring street traffic using a sensor network. For the purpose of video detection, dedicated multi-processor hardware and algorithms have been developed. The system has been realized as a relatively low-power device, with low hardware and software resource usage.</p><p>The video detection of the vehicles is realized using low-resolution images and simplified algorithms, thus its accuracy is not high, but it seems appropriate for a rough estimation of the traffic flow. The reduction of video data from 8 bits to 4 bits resulted in a decreased sensitivity for low-light scenes.</p><p>The sensor network nodes acquire data about car traffic and environmental parameters, such as temperature or the voltage from the solar panel, and they transmit them by the radio in the compressed form to the node closer to the sink. Other measured parameters, such as acoustic noise level, can be easily added, giving the possibility to monitor the noise in the area of the city. For that purpose, a self-organizing multi-hop radio network protocol has been designed, responsible for downloading data from the remote nodes to the sinks. Data are aggregated at the nodes to decrease the number of radio transmissions. The radio protocol also enables the uploading of data to configure the remote nodes. Despite using 500 mW transceivers, the transmission range was only 900 m, which requires dense location of the nodes. In some situations, it might be more convenient to install GSM transceivers for the remote nodes.</p><p>The sensor network node has been practically realized in two versions: FPGA and ASIC and compared with each other. The network consisting of 26 nodes has been tested in real conditions for more than 12 months. The achieved detection rate (as shown in Table <xref rid="Tab11" ref-type="table">11</xref>) was enough to report the traffic flows in the monitored area. The radio transmission performance was very close to the simulation results. In the case of transmission problems, the large nodes’ data buffers were able to buffer data locally for up to 30 min, waiting for the radio link improvement. The installation was successfully collecting data, which were recorded and visualized on a PC, proving the usefulness of this idea.</p></sec></body><back><ack><title>Acknowledgments</title><sec><p>This work was supported in part by the Polish Ministry of Science and Higher Education under R&amp;D grant no. R02 014 01.</p></sec><sec><title>Open Access</title><p>This article is distributed under the terms of the Creative Commons Attribution License which permits any use, distribution, and reproduction in any medium, provided the original author(s) and the source are credited.</p></sec></ack><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akyildiz</surname><given-names>IF</given-names></name><name><surname>Su</surname><given-names>W</given-names></name><name><surname>Sankarasubramaniam</surname><given-names>Y</given-names></name><name><surname>Cayirci</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">“A survey on sensor networks”</article-title><source>Communications Magazine IEEE</source><year>2002</year><volume>40</volume><issue>8</issue><fpage>102</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1109/MCOM.2002.1024422</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Karl, H., Willig, A. (2007) Protocol and architecture for wireless sensor networks, John Wiley and Sons, Ltd.</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Stojmenovic, I. (2005). Handbook of sensor networks, John Wiley and Sons, Inc.</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yick</surname><given-names>J</given-names></name><name><surname>Mukherjee</surname><given-names>B</given-names></name><name><surname>Ghosal</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Wireless sensor network survey</article-title><source>Computer Networks</source><year>2008</year><volume>52</volume><issue>12</issue><fpage>2292</fpage><lpage>2330</lpage><pub-id pub-id-type="doi">10.1016/j.comnet.2008.04.002</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Leduc, G. (2008). “Road traffic data: Collection methods and applications”, JCR technical notes. European commission, joint research centre, institute for prospective technological studies</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">“Traffic Detector Handbook: 3rd edition”, vol I, II, FHWA-HRT-06-139, FHWA-HRT-06-108, US Department of Transportation, Federal Highway Administration 2006.</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Eng-Han Ng, Su-Lim Tan, Guzman, J.G. (2009). “Road traffic monitoring using a wireless vehicle sensor network”, <italic>Int. Symposium on Intelligent Signal Processing and Communications Systems ISPACS 2008</italic>, pp. 1–4, 8–11</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Yang, S.S., Kim, Y. G., Choi, H. (2007). “Vehicle identification using wireless sensor networks”, <italic>Proc. IEEE SoutheastCon, 2007</italic>, pp. 41–46, 22–25.</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kastrinaki</surname><given-names>V</given-names></name><name><surname>Zervakis</surname><given-names>M</given-names></name><name><surname>Kalaitzakis</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">A survey of video processing techniques for traffic applications</article-title><source>Image And Vision Computing</source><year>2003</year><volume>21</volume><issue>4</issue><fpage>359</fpage><lpage>381</lpage><pub-id pub-id-type="doi">10.1016/S0262-8856(03)00004-0</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kumar</surname><given-names>P</given-names></name><name><surname>Ranganath</surname><given-names>S</given-names></name><name><surname>Weimin</surname><given-names>H</given-names></name><name><surname>Sengupta</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">“Framework for real-time behavior interpretation from traffic video”</article-title><source>IEEE Transactions on Intelligent Transportation Systems</source><year>2005</year><volume>6</volume><issue>1</issue><fpage>43</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1109/TITS.2004.838219</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Friedman, N., Russell, S. (1997). “Image segmentation in video sequences: A probabilistic approach”, <italic>Proc. Thirteenth Conf. on Uncertainty in Artificial Intelligence</italic> (UAI 97).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Stauffer, C., Grimson, W. (1999). “Adaptive background mixture models for real-time tracking”, <italic>Proc. IEEE Conf. Compu. Vis. Pattern Recognit</italic>., pp. 246–252.</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Veeraraghavan, H., Masoud, O., Papanikolopoulos, N. (2003). “Computer algorithms for intersection monitor”, <italic>IEEE Trans. Intell. Transp. Syst.</italic>, no. 2, pp. 78–89.</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chiu</surname><given-names>C</given-names></name><name><surname>Ku</surname><given-names>M</given-names></name><name><surname>Liang</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">A robust object segmentation system using a probability-based background extraction algorithm</article-title><source>IEEE Trans. Circuits Syst. Video Technol.</source><year>2010</year><volume>20</volume><issue>4</issue><fpage>518</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2009.2035843</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cucchiara</surname><given-names>R</given-names></name><name><surname>Piccardi</surname><given-names>M</given-names></name><name><surname>Mello</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Image analysis and rule-based reasoning for a traffic monitoring system</article-title><source>IEEE Transactions on Intelligent Transportation Systems</source><year>2000</year><volume>1</volume><issue>2</issue><fpage>119</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1109/6979.880969</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Ferrier, N.J., Rowe, S. M., Blake, A. (1994). “Real-time traffic monitoring”, in Proc. 2nd <italic>IEEE Workshop on Applications of Computer Vision</italic>, Sarasota, FL, pp. 81–88.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koller</surname><given-names>D</given-names></name><name><surname>Daniilidis</surname><given-names>K</given-names></name><name><surname>Nagel</surname><given-names>HH</given-names></name></person-group><article-title xml:lang="en">Model-based object tracking in monocular image sequences of road traffic scenes</article-title><source>Int. J. Comput. Vision</source><year>1993</year><volume>10</volume><issue>3</issue><fpage>257</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1007/BF01539538</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Gao</surname><given-names>D</given-names></name><name><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Moving vehicle detection for automatic traffic monitoring</article-title><source>IEEE Trans. Vehicular Technology</source><year>2007</year><volume>56</volume><issue>1</issue><fpage>51</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1109/TVT.2006.883735</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Gorgon, M., Pawlik, P., Jablonski, M., Przybylo, J. (2007). “FPGA-based road traffic videodetector”, in <italic>Proc. 12th Euromicro Conf. on Digital System Design, Architectures, Methods and Tools (DSD '09)</italic>, Lubeck, Germany.</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Pamula, W. (2009). “Vehicle detection algorithm for FPGA based implementation”, <italic>Computer Recognition Systems 3</italic>, Springer Berlin/Heidelberg, 57: 585–592.</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Chen, P., Ahammad, P., Boyer, C., Shih-I Huang, Lin, L., Lobaton, E., Meingast, M., Oh, S., Wang, S., Yan, P., Yang, A. Y., Yeo, C., Chang, L-C., Tygar, J. D., Sastry, S. S. (2008). “CITRIC: A low-bandwidth wireless camera network platform”, <italic>Second ACM/IEEE International Conference on Distributed Smart Cameras ICDSC 2008</italic>, pp.1–10.</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Xu, L., Bu, W. (2011) “Traffic flow detection method based on fusion of frames differencing and background differencing”, Second Int. Conf. Mechanic Automation and Control Engineering (MACE), pp.1847–1850.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Kwon, C.H., Park, H.J. (2004). “A Study on Camera-Detector System for Traffic Control in Korea”, Lecture Notes in Computer Science, Conceptual Modeling for Advanced Application Domains 3289: 566–576, Springer.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Lien</surname><given-names>J-JJ</given-names></name></person-group><article-title xml:lang="en">Automatic vehicle detection using local features—A statistical approach</article-title><source>IEEE Transactions on Intelligent Transportation Systems</source><year>2008</year><volume>9</volume><issue>1</issue><fpage>83</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1109/TITS.2007.908572</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Junior</surname><given-names>OL</given-names></name><name><surname>Nunes</surname><given-names>U</given-names></name></person-group><source>“Improving the generalization properties of neural networks: An application to vehicle detection”</source><year>2008</year><publisher-loc>Oct</publisher-loc><publisher-name>Proc IEEE Conf. Intell. Transp. Syst</publisher-name><fpage>310</fpage><lpage>315</lpage></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Bebis</surname><given-names>G</given-names></name><name><surname>Miller</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Monocular precrash vehicle detection: Features and classifiers</article-title><source>IEEE Transactions on Image Processing</source><year>2006</year><volume>15</volume><issue>7</issue><fpage>2019</fpage><lpage>2034</lpage><pub-id pub-id-type="doi">10.1109/TIP.2006.877062</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Arrospide, J., Salgado, L., Nieto, M. and Jaureguizar, F. (2008). “On-board robust vehicle detection and tracking using adaptive quality evaluation,” in Proc.IEEE Int. Conf. Image Process. pp. 2008–2011.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Fu</surname><given-names>L</given-names></name><name><surname>Hsiao</surname><given-names>P</given-names></name></person-group><source>“Vehicle detection under various lighting conditions by incorporating particle filter”</source><year>2007</year><publisher-loc>Oct</publisher-loc><publisher-name>Proc IEEE Conf. Intell. Transp. Syst</publisher-name><fpage>534</fpage><lpage>539</lpage></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wójcikowski</surname><given-names>M</given-names></name><name><surname>Żaglewski</surname><given-names>R</given-names></name><name><surname>Pankiewicz</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">FPGA-based real-time implementation of detection algorithm for automatic traffic surveillance sensor network</article-title><source>Journal of Signal Processing Systems</source><year>2012</year><volume>68</volume><issue>1</issue><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1007/s11265-010-0569-3</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Cucchiara, R., Grana, C., Piccardi, M., Prati, A. (2000). “Statistic and knowledge-based moving object detection in traffic scenes”, in <italic>IEEE Proc. Intell. Transp. Syst.</italic>, Dearborn, MI, pp. 27–32.</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Elgammal, A., Harwood, D., Davis, L.S. (2000). “Non-parametric model for background subtraction” in <italic>European Conf. Computer Vision</italic>, Dublin, Ireland, vol. II, pp. 751–767.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Duque, D., Santos, H., Cortez, P. (2005). “Moving Object Detection Unaffected by Cast Shadows, Highlights and Ghosts”, in <italic>Proc. IEEE Int. Conf. Image Processing</italic>, pp. 413–416.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Porikli, F., Ivanov, Y., Haga, T. (2008). “Robust abandoned object detection using dual foregrounds”, <italic>EURASIP J. Adv. Signal Process</italic>.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Cucchiara, R., Granna, C., Piccardi, M., Prati, A., Sirotti, S. (2001). “Improving Shadow Suppression in Moving Object Detection with HSV Color Information”, in Proc. IEEE Intell. Transp. Syst. Conf., Oakland, CA, pp. 334–339.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Heidemann</surname><given-names>J</given-names></name><name><surname>Estrin</surname><given-names>D</given-names></name></person-group><source>“An energy-efficient mac protocol for wireless sensor networks”</source><year>2002</year><publisher-loc>New York</publisher-loc><publisher-name>Proc INFOCOM 2002 IEEE Press</publisher-name></mixed-citation></ref></ref-list></ref-list></back></article></records><facets><facet name="subject"><facet-value count="1">Circuits and Systems</facet-value><facet-value count="1">Computer Imaging, Vision, Pattern Recognition and Graphics</facet-value><facet-value count="1">Electrical Engineering</facet-value><facet-value count="1">Engineering</facet-value><facet-value count="1">Image Processing and Computer Vision</facet-value><facet-value count="1">Pattern Recognition</facet-value><facet-value count="1">Signal,Image and Speech Processing</facet-value></facet><facet name="keyword"><facet-value count="1">Object detection</facet-value><facet-value count="1">Sensor systems</facet-value><facet-value count="1">Wireless sensor networks</facet-value></facet><facet name="pub"><facet-value count="1">Journal of Signal Processing Systems</facet-value></facet><facet name="year"><facet-value count="1">2013</facet-value></facet><facet name="country"><facet-value count="1">Ireland</facet-value><facet-value count="1">Poland</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
