<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-023-01016-5</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-023-01016-5</article-id><article-id pub-id-type="manuscript">1016</article-id><article-id pub-id-type="doi">10.1038/s41524-023-01016-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/638/563</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">TransPolymer: a Transformer-based language model for polymer property predictions</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2689-3313</contrib-id><name><surname>Xu</surname><given-names>Changwen</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0723-6246</contrib-id><name><surname>Wang</surname><given-names>Yuyang</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2952-8576</contrib-id><name><surname>Barati Farimani</surname><given-names>Amir</given-names></name><address><email>barati@cmu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="corresp" rid="IDs41524023010165_cor3">c</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Materials Science and Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="street">5000 Forbes Ave</addr-line><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="street">5000 Forbes Ave</addr-line><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Machine Learning Department</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="street">5000 Forbes Ave</addr-line><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Chemical Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="street">5000 Forbes Ave</addr-line><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs41524023010165_cor3"><label>c</label><email>barati@cmu.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>22</day><month>4</month><year>2023</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2023</year></pub-date><volume>9</volume><issue seq="64">1</issue><elocation-id>64</elocation-id><history><date date-type="registration"><day>5</day><month>4</month><year>2023</year></date><date date-type="received"><day>2</day><month>9</month><year>2022</year></date><date date-type="accepted"><day>3</day><month>4</month><year>2023</year></date><date date-type="online"><day>22</day><month>4</month><year>2023</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2023</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Accurate and efficient prediction of polymer properties is of great significance in polymer design. Conventionally, expensive and time-consuming experiments or simulations are required to evaluate polymer functions. Recently, Transformer models, equipped with self-attention mechanisms, have exhibited superior performance in natural language processing. However, such methods have not been investigated in polymer sciences. Herein, we report TransPolymer, a Transformer-based language model for polymer property prediction. Our proposed polymer tokenizer with chemical awareness enables learning representations from polymer sequences. Rigorous experiments on ten polymer property prediction benchmarks demonstrate the superior performance of TransPolymer. Moreover, we show that TransPolymer benefits from pretraining on large unlabeled dataset via Masked Language Modeling. Experimental results further manifest the important role of self-attention in modeling polymer sequences. We highlight this model as a promising computational tool for promoting rational polymer design and understanding structure-property relationships from a data science view.</p></abstract><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>64</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>4</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>5</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2023_Article_1016.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">The accurate and efficient property prediction is essential to the design of polymers in various applications, including polymer electrolytes<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, organic optoelectronics<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR4">4</xref></sup>, energy storage<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>, and many others<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>. Rational representations which map polymers to continuous vector space are crucial to applying machine learning tools in polymer property prediction. Fingerprints (FPs), which have been proven to be effective in molecular machine learning models, are introduced for polymer-related tasks<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Recently, deep neural networks (DNNs) have revolutionized polymer property prediction by directly learning expressive representations from data to generate deep fingerprints, instead of relying on manually engineered descriptors<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Rahman et al. used convolutional neural networks (CNNs) for the prediction of mechanical properties of polymer-carbon nanotube surfaces<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, whereas CNNs suffered from failure to consider molecular structure and interactions between atoms. Graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, which have outperformed many other models on several molecules and polymer benchmarks<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>, are capable of learning representations from graphs and finding optimal fingerprints based on downstream tasks<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. For example, Park et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> trained graph convolutional neural networks (GCNN) for predictions of thermal and mechanical properties of polymers and discovered that the GCNN representations for polymers resulted in comparable model performance to the popular extended-connectivity circular fingerprint (ECFP)<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup> representation. Recently, Aldeghi et al. adapted a graph representation of molecular ensembles along with a GNN architecture to capture pivotal features and accomplish accurate predictions of electron affinity and ionization potential of conjugated polymers<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. However, GNN-based models require explicitly known structural and conformational information, which would be computationally or experimentally expensive to obtain. Plus, the degree of polymerization varies for each polymer, which makes it even harder to accurately represent polymers as graphs. Using the repeating unit only as graph is likely to result in missing structural information. Therefore, the optimal method of graph representation for polymers is still obscure.</p><p id="Par3">Meanwhile, language models, like recurrent neural networks (RNNs) based models<sup><xref ref-type="bibr" rid="CR22">22</xref>–<xref ref-type="bibr" rid="CR25">25</xref></sup>, treat polymers as character sequences for featurization. Chemistry sequences have the same structure as a natural language like English, as suggested by Cadeddu et al., in terms of the distribution of text fragments and molecular fragments<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. This elucidates the development of sequence models similar to those in computational linguistics for extracting information from chemical sequences and realizing the intuition of understanding chemical texts just like understanding natural languages. Multiple works have investigated the development of deep language models for polymer science. Simine et al. managed to predict spectra of conjugated polymers by long short-term memory (LSTM) from coarse-grained representations of polymers<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Webb et al. proposed coarse-grained polymer genomes as sequences and applied LSTM to predict the properties of different polymer classes<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Patel et al. further extended the coarse-grained string featurization to copolymer systems and developed GNN, CNN, as well as LSTM to model encoded copolymer sequences<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Bhattacharya et al. leveraged RNNs with sequence embedding to predict aggregate morphology of macromolecules<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Plus, sequence models could represent molecules and polymers with Simplified Molecular-Input Line-Entry system (SMILES)<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> and convert the strings to embeddings for vectorization. Some works, like BigSMILES<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, have also investigated the string-based encoding of macromolecules. Goswami et al. created encodings from polymer SMILES as input for the LSTM model for polymer glass transition temperature prediction<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. However, RNN-based models are generally not competitive enough to encode chemical knowledge from polymer sequences because they rely on previous hidden states for dependencies between words and tend to lose information when they reach deeper steps. In recent years, the exceptionally superior performance demonstrated by Transformer<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> on numerous natural language processing (NLP) tasks has shed light on studying chemistry and materials science by language models. Since proposed, Transformer and its variants have soon brought about significant changes in NLP tasks over the past few years. Transformer is featured with using attention mechanism only so that it can capture relationships between tokens in a sentence without relying on past hidden states. Many Transformer-based models like BERT<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, RoBERTa<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, GPT<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, ELMo<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, and XLM<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> have emerged as effective pretraining methods by self-supervised learning of representations from unlabeled texts, leading to performance enhancement on various downstream tasks. On this account, many works have already applied Transformer on property predictions of small organic molecules<sup><xref ref-type="bibr" rid="CR40">40</xref>–<xref ref-type="bibr" rid="CR43">43</xref></sup>. SMILES-BERT was proposed to pretrain the model of BERT-like architecture through a masked SMILES recovery task and then generalize into different molecular property prediction tasks<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Similarly, ChemBERTa<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, a RoBERTa-like model for molecular property prediction, was also introduced, following the pretrain-finetune pipeline. ChemBERTa demonstrated competitive performance on multiple downstream tasks and scaled well with the size of pretraining datasets. Transformer-based models could even be used for processing reactions. Schwaller et al. mimicked machine translation tasks and trained Transformer on reaction sequences represented by SMILES for reaction prediction with high accuracy<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Recently, Transformer has been further proven to be effective as a structure-agnostic model in material science tasks, for example, predicting MOF properties based on a text string representation<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. Despite the wide investigation of Transformer for molecules and materials, such models have not yet been leveraged to learn representations of polymers. Compared with small molecules, designing Transformer-based models for polymers is more challenging because the standard SMILES encoding fails to model the polymer structure and misses fundamental factors influencing polymer properties like degree of polymerization and temperature of measurement. Moreover, the polymer sequences used as input should contain information on not only the definition of monomers but also the arrangement of monomers in polymers<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. In addition, sequence models for polymers are confronted with an inherent scarcity of handy, well-labeled data, considering the hard work in the characterization process in the laboratory. The situation becomes even worse when some of the polymer data sources are not fully accessible<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup>.</p><p id="Par4">Herein, we propose TransPolymer, a Transformer-based language model for polymer property predictions. To the best of our knowledge, it is the first work to introduce the Transformer-based model to polymer sciences. Polymers are represented by sequences based on SMILES of their repeating units as well as structural descriptors and then tokenized by a chemically-aware tokenizer as the input of TransPolymer, shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. Even though there is still information which cannot be explicitly obtained from input sequences, like bond angles or overall polymer chain configuration, such information can still be learned implicitly by the model. TransPolymer consists of a RoBERTa architecture and a multi-layer perceptron (MLP) regressor head, for predictions of various polymer properties. In the pretraining phase, TransPolymer is trained through Masked Language Modeling (MLM) with approximately 5M augmented unlabeled polymers from the PI1M database<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. In MLM, tokens in sequences are randomly masked and the objective is to recover the original tokens based on the contexts. Afterward, TransPolymer is finetuned and evaluated on ten datasets of polymers concerning various properties, covering polymer electrolyte conductivity, band gap, electron affinity, ionization energy, crystallization tendency, dielectric constant, refractive index, and p-type polymer OPV power conversion efficiency<sup><xref ref-type="bibr" rid="CR52">52</xref>–<xref ref-type="bibr" rid="CR55">55</xref></sup>. For each entry in the datasets, the corresponding polymer sequence, containing polymer SMILES as well as useful descriptors like temperature and special tokens are tokenized as input of TransPolymer. The pretraining and finetuning processes are illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b and d. Data augmentation is also implemented for better learning of features from polymer sequences. TransPolymer achieves state-of-the-art (SOTA) results on all ten benchmarks and surpasses other baseline models by large margins in most cases. Ablation studies provide further evidence of what contributes to the superior performance of TransPolymer by investigating the roles of MLM pretraining on large unlabeled data, finetuning both Transformer encoders and the regressor head, and data augmentation. The evidence from visualization of attention scores illustrates that TransPolymer can encode chemical information about internal interactions of polymers and influential factors of polymer properties. Such a method learns generalizable features that can be transferred to property prediction of polymers, which is of great significance in polymer design.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Overview of TransPolymer.</title><p>(<bold>a</bold>) Polymer tokenization. Illustrated by the example, the sequence which comprises components with polymer SMILES and other descriptors is tokenized with chemical awareness. <bold>b</bold> The whole TransPolymer framework with a pretrain-finetune pipeline. <bold>c</bold> Sketch of Transformer encoder and multi-head attention. <bold>d</bold> Illustration of the pretraining (left) and finetuning (right) phases of TransPolymer. The model is pretrained with Masked Language Modeling to recover original tokens, while the feature vector corresponding to the special token ‘〈<italic>s</italic>〉’ of the last hidden layer is used for prediction when finetuning. Within the TransPolymer block, lines of deeper color and larger width stand for higher attention scores.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig1_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>TransPolymer framework</title><p id="Par5">Our TransPolymer framework consists of tokenization, Transformer encoder, pretraining, and finetuning. Each polymer data is first converted to a string of tokens through tokenization. Polymer sequences are more challenging to design than molecule or protein sequences as polymers contain complex hierarchical structures and compositions. For instance, two polymers that have the same repeating units can vary in terms of the degree of polymerization. Therefore, we propose a chemical-aware polymer tokenization method as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. The repeating units of polymers are embedded using SMILES and additional descriptors (e.g., degree of polymerization, polydispersity, and chain conformation) are included to model the polymer system. Plus, copolymers are modeled by combining the SMILES of each constituting repeating unit along with the ratios and the arrangements of those repeating units. Moreover, materials consisting of mixtures of polymers are represented by concatenating the sequences for each component as well as the descriptors for the materials. Besides, each token represents either an element, the value of a polymer descriptor, or a special separator. Therefore, the tokenization strategy is chemical-aware and thus has an edge over the tokenizer trained for natural languages which tokenizes based on single letters. More details about the design of our chemical-aware tokenization strategy could be found in the Methods section.</p><p id="Par6">Transformer encoders are built upon stacked self-attention and point-wise, fully connected layers<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c. Unlike RNN or CNN models, Transformer depends on the self-attention mechanism that relates tokens at different positions in a sequence to learn representations. Scaled dot-product attention across tokens is applied which relies on the query, key, and value matrices. More details about self-attention can be found in the Methods section. In our case, the Transformer encoder is made up of 6 hidden layers and each hidden layer contains 12 attention heads. The hyperparameters of TransPolymer are chosen by starting from the common setting of RoBERTa<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> and then tuned according to model performance.</p><p id="Par7">To learn better representations from large unlabeled polymer data, the Transformer encoder is pretrained via Masked Language Modeling (MLM), a universal and effective pretraining method for various NLP tasks<sup><xref ref-type="bibr" rid="CR56">56</xref>–<xref ref-type="bibr" rid="CR58">58</xref></sup>. As shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d (left), 15% of tokens of a sequence are randomly chosen for possible replacement, and the pretraining objective is to predict the original tokens by learning from the contexts. The pretrained model is then finetuned for predicting polymer properties with labeled data. Particularly, the final hidden vector of the special token ‘〈<italic>s</italic>〉’ at the beginning of the sequence is fed into a regressor head which is made up of one hidden layer with SiLU as the activation function for prediction as illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>d (right).</p></sec><sec id="Sec4"><title>Experimental settings</title><p id="Par8">PI1M, the benchmark of polymer informatics, is used for pretraining. The benchmark, whose size is around 1M, was built by Ma et al. by training a generative model on polymer data collected from the PolyInfo database<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>. The generated sequences consist of monomer SMILES and ‘*’ signs representing the polymerization points. The ~1M database was demonstrated to cover similar chemical space as PolyInfo but populate space where data in PolyInfo are sparse. Therefore, the database can serve as an important benchmark for multiple tasks in polymer informatics.</p><p id="Par9">To finetune the pretrained TransPolymer, ten datasets are used in our experiments which cover various properties of different polymer materials, and the distributions of polymer sequence lengths vary from each other (shown in Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>). Plus, data in all the datasets are of different types: sequences from Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets are about polymers only so that the inputs are just polymer SMILES; while PE-I, PE-II, and OPV datasets describe polymer-based materials so that the sequences contain additional descriptors. In particular, PE-I which is about polymer electrolytes involves mixtures of multiple components in polymer materials. Hence, these datasets provide challenging and comprehensive benchmarks to evaluate the performance of TransPolymer. A summary of the ten datasets for downstream tasks is shown in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Summary of datasets for downstream tasks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th><p>Property</p></th><th><p># Data</p></th><th><p># Augmented train data</p></th><th><p># Test data</p></th><th><p>Data split</p></th></tr></thead><tbody><tr><td><p>PE-I<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></p></td><td><p>conductivity</p></td><td><p>9185</p></td><td><p>34803</p></td><td><p>146</p></td><td><p>train-test split by year</p></td></tr><tr><td><p>PE-II<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></p></td><td><p>conductivity</p></td><td><p>271</p></td><td><p>8864</p></td><td><p>55</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Egc<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>bandgap (chain)</p></td><td><p>3380</p></td><td><p>5408</p></td><td><p>676</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Egb<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>bandgap (bulk)</p></td><td><p>561</p></td><td><p>6443</p></td><td><p>113</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Eea<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>electron affinity</p></td><td><p>368</p></td><td><p>3993</p></td><td><p>74</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Ei<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>ionization energy</p></td><td><p>370</p></td><td><p>4000</p></td><td><p>74</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Xc<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>crystallization tendency</p></td><td><p>432</p></td><td><p>8837</p></td><td><p>87</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>EPS<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>dielectric constant</p></td><td><p>382</p></td><td><p>4188</p></td><td><p>77</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>Nc<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>refractive index</p></td><td><p>382</p></td><td><p>4188</p></td><td><p>77</p></td><td><p>5-fold cross-validation</p></td></tr><tr><td><p>OPV<sup><xref ref-type="bibr" rid="CR55">55</xref></sup></p></td><td><p>power conversion efficiency</p></td><td><p>1203</p></td><td><p>4810</p></td><td><p>241</p></td><td><p>5-fold cross-validation</p></td></tr></tbody></table></table-wrap></p><p id="Par10">We apply data augmentation to each dataset that we use by removing canonicalization from SMILES and generating non-canonical SMILES which correspond to the same structure as the canonical ones. For PI1M database, each data entry is augmented to five so that the augmented dataset with the size of ~5M is used for pretraining. For downstream datasets, we limit the numbers of augmented SMILES for large datasets with long SMILES for the following reasons: long SMILES tend to generate more non-canonical SMILES which might alter the original data distribution; we are not able to use all the augmented data for finetuning given the limited computation resources. We include the number of data points after augmentation in Table <xref rid="Tab1" ref-type="table">1</xref> and summarize the augmentation strategy for each downstream dataset in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>.</p></sec><sec id="Sec5"><title>Polymer property prediction results</title><p id="Par11">The performance of our pretrained TransPolymer model on ten property prediction tasks is illustrated below. We use root mean square error (RMSE) and <italic>R</italic><sup>2</sup> as metrics for evaluation. For each benchmark, the baseline models and data splitting are adopted from the original literature. Except for PE-I which is trained on data from the year 2018 and evaluated on data from the year 2019, all other datasets are split by five-fold cross-validation. When cross-validation is used, the metrics are calculated by taking the average of those by each fold. We also train Random Forest models using Extended Connectivity Fingerprint (ECFP)<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>, one of the state-of-the-art fingerprint approaches, to compare with TransPolymer. Besides, we develop long short-term memory (LSTM), another widely used language model, as well as unpretrained TransPolymer trained purely via supervised learning as baseline models in all the benchmarks. TransPolymer<sub>unpretrained</sub> and TransPolymer<sub>pretrained</sub> denote unpretrained and pretrained TransPolymer, respectively.</p><p id="Par12">The results of TransPolymer and baselines on PE-I are illustrated in Table <xref rid="Tab2" ref-type="table">2</xref>. The original literature used gated GNN to generate fingerprints for the prediction of polymer electrolyte conductivity by Gaussian Process<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. The fingerprints are also passed to random forest and supporting vector machine (SVM) for comparison. Another random forest is trained based on ECFP fingerprints. The results of most baseline models indicate strong overfitting which is attributed to the introduction of unconventional conductors consisting of conjugated polybenzimidazole and ionic liquid. For instance, Gaussian Process trained on GNN fingerprints achieves a <italic>R</italic><sup>2</sup> of 0.90 on the training set but only 0.16 on the test set, and Random Forest trained on GNN FP gets a negative test <italic>R</italic><sup>2</sup> even the train <italic>R</italic><sup>2</sup> is 0.91. Random Forest trained on ECFP stands out among all the baseline models, whereas its performance on test dataset is still poor. However, TransPolymer<sub>pretrained</sub> not only achieves the highest scores on the training set but also improves the performance on the test set significantly, which is illustrated by the <italic>R</italic><sup>2</sup> of 0.69 on the test set. Such information demonstrates that TransPolymer is capable of learning the intrinsic relationship between polymers and their properties and suffers less from overfitting. Notably, TransPolymer<sub>unpretrained</sub> also achieves competitive results and shows mild overfitting compared with other baseline models. This indicates the effectiveness of the attention mechanism of Transformer-based models. The scatter plots of ground truth vs. predicted values for PE-I by TransPolymer<sub>pretrained</sub> are illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>a and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2a</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Performance of TransPolymer and baseline models on PE-I.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Model</p></th><th><p>Train RMSE (<italic>S</italic> ⋅ <italic>cm</italic><sup>−1</sup>*) (<italic>↓</italic>)</p></th><th><p>Test RMSE (<italic>S</italic> ⋅ <italic>cm</italic><sup>−1</sup>*) (<italic>↓</italic>)</p></th><th><p>Train <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th><th><p>Test <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr></thead><tbody><tr><td><p>Gaussian Process (GNN FP)</p></td><td><p>0.55</p></td><td><p>0.97</p></td><td><p>0.90</p></td><td><p>0.16</p></td></tr><tr><td><p>Random Forest (GNN FP)</p></td><td><p>0.50</p></td><td><p>2.23</p></td><td><p>0.91</p></td><td><p>−2.64</p></td></tr><tr><td><p>SVM (GNN FP)</p></td><td><p>1.34</p></td><td><p>2.12</p></td><td><p>0.04</p></td><td><p>−1.94</p></td></tr><tr><td><p>Random Forest (ECFP)</p></td><td><p>0.15</p></td><td><p>1.00</p></td><td><p>0.99</p></td><td><p>0.32</p></td></tr><tr><td><p>LSTM</p></td><td><p>1.03</p></td><td><p>1.36</p></td><td><p>0.67</p></td><td><p>−0.25</p></td></tr><tr><td><p>TransPolymer<sub>unpretrained</sub></p></td><td><p>0.88</p></td><td><p>1.02</p></td><td><p>0.70</p></td><td><p>0.30</p></td></tr><tr><td><p>TransPolymer<sub>pretrained</sub></p></td><td><p><bold>0.20</bold></p></td><td><p><bold>0.67</bold></p></td><td><p><bold>0.98</bold></p></td><td><p><bold>0.69</bold></p></td></tr></tbody></table><table-wrap-foot><p>*The units are in logarithm scale.</p><p>The bold values indicate the best results in terms of the metrics we use.</p></table-wrap-foot></table-wrap><fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Ground truth vs. predicted values by TransPolymer<sub>pretrained</sub>.</title><p>Scatter plots of ground truth vs. predicted values for downstream tasks: <bold>a</bold> PE-I, <bold>b</bold> PE-II, <bold>c</bold> Egc, <bold>d</bold> Egb, <bold>e</bold> Eea, <bold>f</bold> Ei, <bold>g</bold> Xc, <bold>h</bold> EPS, <bold>i</bold> Nc, and <bold>j</bold> OPV. The dashed lines on diagonals stand for perfect regression.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig2_HTML.png"/></fig></p><p id="Par13">As is shown in Table <xref rid="Tab3" ref-type="table">3</xref>, the results of TransPolymer and baselines including Ridge, Random Forest, Gradient Boosting, and Extra Trees which were trained on chemical descriptors generated from polymers from PE-II in the original paper<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> are listed, as well as Random Forest trained on ECFP. Although Gradient Boosting surpasses other models on training sets by obtaining nearly perfect regression outcomes, its performance on test sets drops significantly. In contrast, TransPolymer<sub>pretrained</sub>, which achieves the lowest RMSE of 0.61 and highest <italic>R</italic><sup>2</sup> of 0.73 on the average of cross-validation sets, exhibits better generalization. The scatter plots of ground truth vs. predicted values for PE-II by TransPolymer<sub>pretrained</sub> are illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>b and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2b</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Performance of TransPolymer and baseline models on PE-II.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Model</p></th><th><p>Train RMSE (<italic>S</italic> ⋅ <italic>c</italic><italic>m</italic><sup>−1</sup>*) (<italic>↓</italic>)</p></th><th><p>Test RMSE (<italic>S</italic> ⋅ <italic>cm</italic><sup>−1</sup>*) (↓)</p></th><th><p>Train <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th><th><p>Test <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr></thead><tbody><tr><td><p>Ridge (Chemical descriptors)</p></td><td><p>0.58</p></td><td><p>0.67</p></td><td><p>0.77</p></td><td><p>0.58</p></td></tr><tr><td><p>Random Forest (Chemical descriptors)</p></td><td><p>0.26</p></td><td><p>0.64</p></td><td><p>0.96</p></td><td><p>0.71</p></td></tr><tr><td><p>Gradient Boosting (Chemical descriptors)</p></td><td><p><bold>0.00</bold></p></td><td><p>0.66</p></td><td><p><bold>0.99</bold></p></td><td><p>0.68</p></td></tr><tr><td><p>Extra Trees (Chemical descriptors)</p></td><td><p>0.10</p></td><td><p>0.63</p></td><td><p>0.98</p></td><td><p>0.72</p></td></tr><tr><td><p>Random Forest (ECFP)</p></td><td><p>0.22</p></td><td><p>0.94</p></td><td><p>0.96</p></td><td><p>0.27</p></td></tr><tr><td><p>LSTM</p></td><td><p>1.16</p></td><td><p>1.18</p></td><td><p>0.05</p></td><td><p>0.00</p></td></tr><tr><td><p>TransPolymer<sub>unpretrained</sub></p></td><td><p>0.18</p></td><td><p>0.80</p></td><td><p>0.97</p></td><td><p>0.54</p></td></tr><tr><td><p>TransPolymer<sub>pretrained</sub></p></td><td><p>0.18</p></td><td><p><bold>0.61</bold></p></td><td><p>0.96</p></td><td><p><bold>0.73</bold></p></td></tr></tbody></table><table-wrap-foot><p>*The units are in logarithm scale.</p><p>The bold values indicate the best results in terms of the metrics we use.</p></table-wrap-foot></table-wrap></p><p id="Par14">Table <xref rid="Tab4" ref-type="table">4</xref> summarizes the performance of TransPolymer and baselines on Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets from Kuenneth et al.<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>. In the original literature, both Gaussian process and neural networks were trained on each dataset with polymer genome (PG) fingerprints<sup><xref ref-type="bibr" rid="CR60">60</xref></sup> as input, some of which resulted in desirable performance while some of which did not. Meanwhile, PG fingerprints are demonstrated to surpass ECFP on the datasets used by Kuenneth et al. For Egc, Egb, and Eea, despite the high scores by other models, TransPolymer<sub>pretrained</sub> is still able to enhance the performance, lowering RMSE and enhancing <italic>R</italic><sup>2</sup>. In contrast, baseline models perform poorly on Xc whose test <italic>R</italic><sup>2</sup> scores are less than 0. However, TransPolymer<sub>pretrained</sub> significantly lowers test RMSE and increases <italic>R</italic><sup>2</sup> to 0.50. Notably, The authors of the original paper used multi-task learning to enhance model performance and achieved higher scores than TransPolymer<sub>pretrained</sub> on some of the datasets, like Egb, EPS, and Nc (the average test RMSE and <italic>R</italic><sup>2</sup> are 0.43 and 0.95 for Egb, 0.39 and 0.86 for EPS, and 0.07 and 0.91 for Nc, respectively). Access to multiple properties of one polymer, however, may not be available from time to time, which limits the application of multi-task learning. In addition, the TransPolymer<sub>pretrained</sub> still outperforms multi-task learning models on four out of the seven chosen datasets. Hence the improvement by TransPolymer compared with single-task baselines should still be highly valued. The scatter plots of ground truth vs. predicted values for Egc, Egb, Eea, Ei, Xc, EPS, and Nc datasets by TransPolymer<sub>pretrained</sub> are depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>c–i and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2c</xref>–<xref ref-type="supplementary-material" rid="MOESM1">i</xref>, respectively.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Performance of TransPolymer and baseline models on datasets from literature by Kuenneth et al.<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Model</p></th><th colspan="7"><p>Test RMSE (<italic>↓</italic>)</p></th><th colspan="7"><p>Test <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr><tr><th/><th><p>Egc (eV)</p></th><th><p>Egb (eV)</p></th><th><p>Eea (eV)</p></th><th><p>Ei (eV)</p></th><th><p>Xc (%)</p></th><th><p>EPS</p></th><th><p>Nc</p></th><th><p>Egc</p></th><th><p>Egb</p></th><th><p>Eea</p></th><th><p>Ei</p></th><th><p>Xc</p></th><th><p>EPS</p></th><th><p>Nc</p></th></tr></thead><tbody><tr><td><p>Gaussian Process (PG)</p></td><td><p>0.48</p></td><td><p>0.55</p></td><td><p><bold>0.32</bold></p></td><td><p>0.42</p></td><td><p>24.42</p></td><td><p>0.53</p></td><td><p><bold>0.10</bold></p></td><td><p>0.90</p></td><td><p>0.91</p></td><td><p>0.90</p></td><td><p>0.77</p></td><td><p>&lt;0</p></td><td><p>0.68</p></td><td><p>0.79</p></td></tr><tr><td><p>Neural Network (PG)</p></td><td><p>0.49</p></td><td><p>0.57</p></td><td><p><bold>0.32</bold></p></td><td><p>0.45</p></td><td><p>20.74</p></td><td><p>0.54</p></td><td><p><bold>0.10</bold></p></td><td><p>0.89</p></td><td><p>0.89</p></td><td><p>0.87</p></td><td><p>0.74</p></td><td><p>&lt;0</p></td><td><p>0.71</p></td><td><p>0.78</p></td></tr><tr><td><p>Random Forest (ECFP)</p></td><td><p>0.81</p></td><td><p>0.88</p></td><td><p>0.56</p></td><td><p>0.58</p></td><td><p>25.61</p></td><td><p>0.75</p></td><td><p>0.14</p></td><td><p>0.65</p></td><td><p>0.66</p></td><td><p>0.70</p></td><td><p>0.57</p></td><td><p>−0.29</p></td><td><p>0.50</p></td><td><p>0.56</p></td></tr><tr><td><p>LSTM</p></td><td><p>0.58</p></td><td><p>1.94</p></td><td><p>1.04</p></td><td><p>0.94</p></td><td><p>23.67</p></td><td><p>1.11</p></td><td><p>0.23</p></td><td><p>0.86</p></td><td><p>0.00</p></td><td><p>0.06</p></td><td><p>0.10</p></td><td><p>0.00</p></td><td><p>−0.02</p></td><td><p>0.02</p></td></tr><tr><td><p>TransPolymer<sub>unpretrained</sub></p></td><td><p>0.63</p></td><td><p>0.61</p></td><td><p>0.36</p></td><td><p>0.46</p></td><td><p>20.11</p></td><td><p>0.59</p></td><td><p><bold>0.10</bold></p></td><td><p>0.84</p></td><td><p>0.90</p></td><td><p>0.89</p></td><td><p>0.78</p></td><td><p>0.27</p></td><td><p>0.70</p></td><td><p>0.80</p></td></tr><tr><td><p>TransPolymer<sub>pretrained</sub></p></td><td><p><bold>0.44</bold></p></td><td><p><bold>0.52</bold></p></td><td><p><bold>0.32</bold></p></td><td><p><bold>0.39</bold></p></td><td><p><bold>16.57</bold></p></td><td><p><bold>0.52</bold></p></td><td><p><bold>0.10</bold></p></td><td><p><bold>0.92</bold></p></td><td><p><bold>0.93</bold></p></td><td><p><bold>0.91</bold></p></td><td><p><bold>0.84</bold></p></td><td><p><bold>0.50</bold></p></td><td><p><bold>0.76</bold></p></td><td><p><bold>0.82</bold></p></td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best results in terms of the metrics we use.</p></table-wrap-foot></table-wrap></p><p id="Par15">TransPolymer and baselines are trained on p-type polymer OPV dataset whose results are shown in Table <xref rid="Tab5" ref-type="table">5</xref>. The original paper trained random forest and artificial neural network (ANN) on the dataset using ECFP<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>. TransPolymer<sub>pretrained</sub>, in comparison with baselines, gives a slightly better performance as the average RMSE is the same as that of random forest, and the average test <italic>R</italic><sup>2</sup> is increased by 0.05. Although all the model performance is not satisfying enough, possibly attributed to the noise in data, TransPolymer<sub>pretrained</sub> still outperforms baselines. The scatter plots of ground truth vs. predicted values for OPV by TransPolymer<sub>pretrained</sub> are depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>j and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2j</xref>.<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>Performance of TransPolymer and baseline models on p-type polymer OPV.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Model</p></th><th><p>Train RMSE (%) (<italic>↓</italic>)</p></th><th><p>Test RMSE (%) (↓)</p></th><th><p>Train <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th><th><p>Test <italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr></thead><tbody><tr><td><p>Random Forest (ECFP)</p></td><td><p>0.66</p></td><td><p><bold>1.92</bold></p></td><td><p>0.92</p></td><td><p>0.27</p></td></tr><tr><td><p>ANN (ECFP)</p></td><td><p>1.58</p></td><td><p>2.03</p></td><td><p>0.55</p></td><td><p>0.20</p></td></tr><tr><td><p>LSTM</p></td><td><p>2.35</p></td><td><p>2.34</p></td><td><p>−0.01</p></td><td><p>0.00</p></td></tr><tr><td><p>TransPolymer<sub>unpretrained</sub></p></td><td><p>1.91</p></td><td><p>2.10</p></td><td><p>0.33</p></td><td><p>0.19</p></td></tr><tr><td><p>TransPolymer<sub>pretrained</sub></p></td><td><p><bold>1.19</bold></p></td><td><p><bold>1.92</bold></p></td><td><p><bold>0.74</bold></p></td><td><p><bold>0.32</bold></p></td></tr></tbody></table><table-wrap-foot><p>The bold values indicate the best results in terms of the metrics we use.</p></table-wrap-foot></table-wrap></p><p id="Par16">Table <xref rid="Tab6" ref-type="table">6</xref> summarizes the improvement of TransPolymer<sub>pretrained</sub> over the best baseline models as well as TransPolymer<sub>unpretrained</sub> on each dataset. TransPolymer<sub>pretrained</sub> has outperformed all other models on all ten datasets, further providing evidence for the generalization of TransPolymer. TransPolymer<sub>pretrained</sub> exhibits an average decrease of evaluation RMSE by 7.70% (in percentage) and an increase of evaluation <italic>R</italic><sup>2</sup> by 0.11 (in absolute value) compared with the best baseline models, and the two values become 18.5% and 0.12, respectively, when it comes to comparison with TransPolymer<sub>unpretrained</sub>. Therefore, the pretrained TransPolymer could hopefully be a universal pretrained model for polymer property prediction tasks and applied to other tasks by finetuning. Besides, TransPolymer equipped with MLM pretraining technique shows significant advantages over other models in dealing with complicated polymer systems. Specifically, on PE-I benchmark, TransPolymer<sub>pretrained</sub> improves <italic>R</italic><sup>2</sup> by 0.37 comparing with the previous best baseline model and by 0.39 comparing with TransPolymer<sub>unpretrained</sub>. PE-I contains not only polymer SMILES but also key descriptors of the materials like temperature and component ratios within the materials. The data in PE-I is noisy due to the existence of different types of components in the polymer materials, for instance, copolymers, anions, and ionic liquids. Also, models are trained on data from the year 2018 and evaluated on data from the year 2019, which gives a more challenging setting. Therefore it is reasonable to infer that TransPolymer is better at learning features out of noisy data and giving a robust performance. It is noticeable that LSTM becomes the least competitive model in almost every downstream task, such evidence demonstrates the significance of attention mechanisms in understanding chemical knowledge from polymer sequences.<table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>Improvement of performance of TransPolymer<sub>pretrained</sub> compared with baselines and TransPolymer<sub>unpretrained</sub> in terms of decrease of test RMSE (in percentage) and increase of test <italic>R</italic><sup>2</sup> (in absolute value).</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th colspan="2"><p>vs. best baselines</p></th><th colspan="2"><p>vs. TransPolymer<sub>unpretrained</sub></p></th></tr><tr><th/><th><p>RMSE (<italic>↓</italic>)</p></th><th><p><italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th><th><p>RMSE (<italic>↓</italic>)</p></th><th><p><italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr></thead><tbody><tr><td><p>PE-I</p></td><td><p>−30.9%</p></td><td><p>+0.37</p></td><td><p>−52.2%</p></td><td><p>+0.39</p></td></tr><tr><td><p>PE-II</p></td><td><p>−3.17%</p></td><td><p>+0.01</p></td><td><p>−23.8%</p></td><td><p>+0.19</p></td></tr><tr><td><p>Egc</p></td><td><p>−8.33%</p></td><td><p>+0.02</p></td><td><p>−30.2%</p></td><td><p>+0.08</p></td></tr><tr><td><p>Egb</p></td><td><p>−5.45%</p></td><td><p>+0.02</p></td><td><p>14.8%</p></td><td><p>+0.03</p></td></tr><tr><td><p>Eea</p></td><td><p>0.00%</p></td><td><p>+0.01</p></td><td><p>−11.1%</p></td><td><p>+0.02</p></td></tr><tr><td><p>Ei</p></td><td><p>−7.14%</p></td><td><p>+0.07</p></td><td><p>−15.2%</p></td><td><p>+0.06</p></td></tr><tr><td><p>Xc</p></td><td><p>−20.1%</p></td><td><p>+0.50</p></td><td><p>−17.6%</p></td><td><p>+0.23</p></td></tr><tr><td><p>EPS</p></td><td><p>−1.89%</p></td><td><p>+0.05</p></td><td><p>−11.9%</p></td><td><p>+0.06</p></td></tr><tr><td><p>Nc</p></td><td><p>0.00%</p></td><td><p>+0.03</p></td><td><p>0.00%</p></td><td><p>+0.02</p></td></tr><tr><td><p>OPV</p></td><td><p>0.00%</p></td><td><p>+0.05</p></td><td><p>−8.57%</p></td><td><p>+0.13</p></td></tr><tr><td><p>Average</p></td><td><p>−7.70%</p></td><td><p>+0.11</p></td><td><p>−18.5%</p></td><td><p>+0.12</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec6"><title>Abaltion studies</title><p id="Par17">The effects of pretraining could be further demonstrated by the chemical space taken up by polymer SMILES from the pretraining and downstream datasets visualized by t-SNE<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>, shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Each polymer SMILES is converted to TransPolymer embedding with the size of sequence length × embedding size. Max pooling is implemented to convert the embedding matrices to vectors so that the strong characteristics in embeddings could be preserved in the input of t-SNE. We use openTSNE library<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> to create 2D embeddings via pretraining data and map downstream data to the same 2D space. As illustrated in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a, almost every downstream data point lies in the space covered by the original ~1M pretraining data points, indicating the effectiveness of pretraining in better representation learning of TransPolymer. Data points from datasets like Xc which exhibit minor evidence of clustering in the chemical space cover a wide range of polymers, explaining the phenomenon that other models struggle on Xc while pretrained TransPolymer learns reasonable representations. Meanwhile, for datasets that cluster in the chemical space, other models can obtain reasonable results whereas TransPolymer achieves better results. Additionally, it should be pointed out that the numbers of unique polymer SMILES in PE-I and PE-II are much smaller than the sizes of the datasets as many instances share the same polymer SMILES while differing in descriptors like molecular weight and temperature, hence the visualization of polymer SMILES cannot fully reflect the chemical space taken up by the polymers from these datasets.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>t-SNE visualization of pretraining and downstream data.</title><p>The embeddings are obtained by first fitting on the (<bold>a</bold>) 1M (original), (<bold>b</bold>) 50K, and (<bold>c</bold>) 5K pretraining data and then transforming downstream data to the corresponding data space.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig3_HTML.png"/></fig></p><p id="Par18">Besides, we have also investigated how the size of the pretraining dataset affects the downstream performance. We randomly pick up 5K, 50K, 500K, and 1M (original size) data points from the initial pretraining dataset without augmentation, and pretrain TransPolymer with them and compare the results with those by TransPolymer trained with 5M augmented data. The results are summarized in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">5</xref>. Plus, Fig. <xref rid="Fig4" ref-type="fig">4</xref> presents the bar plot of <italic>R</italic><sup>2</sup> for each experiment we have performed. Error bars are included in the figure if cross-validation is implemented in experiments. As shown in the table and the figure, the results demonstrate a clear trend of enhanced downstream performance (decreasing RMSE and increasing <italic>R</italic><sup>2</sup>) with increasing pretraining size. In particular, the model performance on some datasets, for example, PE-I, Nc, and OPV, are even worse than training TransPolymer from scratch (the results by TransPolymer<sub>unpretrained</sub> in Tables <xref rid="Tab2" ref-type="table">2</xref>–<xref rid="Tab5" ref-type="table">5</xref>). A possible explanation is that the small amount of pretraining size results in the limited data space covered by pretraining data, thus making some downstream data points out of the distribution of pretraining data. Figure <xref rid="Fig3" ref-type="fig">3</xref>b, c visualize the data space by fitting on 50K and 5K pretraining data, respectively, in which a lot of space taken up downstream data points is not covered by pretraining data. Therefore, the results emphasize the effects of pretraining with a large number of unlabeled sequences.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Model performance with varying pretraining data sizes.</title><p>The <italic>R</italic><sup>2</sup> for each downstream task with different pretraining data sizes are presented in the bar plot. Error bars are included if cross-validation is implemented.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig4_HTML.png"/></fig></p><p id="Par19">The results from TransPolymer<sub>pretrained</sub> so far are all derived by pretraining first and then finetuning the whole model on the downstream datasets. Besides, we also consider another setting where in downstream tasks only the regressor head is finetuned while the pretrained Transformer encoder is frozen. The comparison of the performance of TransPolymer<sub>pretrained</sub> between finetuning the regressor head only and finetuning the whole model is presented in Table <xref rid="Tab7" ref-type="table">7</xref>. Standard deviation is included in the results if cross-validation is applied for downstream tasks. Reasonable results could be obtained by freezing the pretrained encoders and training the regressor head only. For instance, the model performance on Xc dataset already surpasses the baseline models, and the model performance on Ei, Nc, and OPV datasets is slightly worse than the corresponding best baselines. However, the performance on all the downstream tasks increases significantly if both the Transformer encoders and the regressor head are finetuned, which indicates that the regressor head only is not enough to learn task-specific information. In fact, the attention mechanism plays a key role in learning not only generalizable but also task-specific information. Even though the pretrained TransPolymer is transferable to various downstream tasks and more efficient, it is necessary to finetune the Transformer encoders with task-related data points for better performance.<table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>Comparison of performance of TransPolymer<sub>pretrained</sub> between finetuning the regressor head only and finetuning the whole model in terms of test RMSE and <italic>R</italic><sup>2</sup>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th colspan="2"><p>Finetuning the regressor head</p></th><th colspan="2"><p>Finetuning the whole model</p></th></tr><tr><th/><th><p>RMSE</p></th><th><p><italic>R</italic><sup>2</sup></p></th><th><p>RMSE</p></th><th><p><italic>R</italic><sup>2</sup></p></th></tr></thead><tbody><tr><td><p>PE-I</p></td><td><p>1.14</p></td><td><p>0.12</p></td><td><p>0.67</p></td><td><p>0.69</p></td></tr><tr><td><p>PE-II</p></td><td><p>0.91 ± 0.12</p></td><td><p>0.40 ± 0.10</p></td><td><p>0.61 ± 0.07</p></td><td><p>0.73 ± 0.04</p></td></tr><tr><td><p>Egc</p></td><td><p>0.69 ± 0.03</p></td><td><p>0.81 ± 0.02</p></td><td><p>0.44 ± 0.01</p></td><td><p>0.92 ± 0.00</p></td></tr><tr><td><p>Egb</p></td><td><p>0.83 ± 0.05</p></td><td><p>0.81 ± 0.02</p></td><td><p>0.52 ± 0.05</p></td><td><p>0.93 ± 0.01</p></td></tr><tr><td><p>Eea</p></td><td><p>0.52 ± 0.04</p></td><td><p>0.76 ± 0.04</p></td><td><p>0.32 ± 0.02</p></td><td><p>0.91 ± 0.03</p></td></tr><tr><td><p>Ei</p></td><td><p>0.51 ± 0.05</p></td><td><p>0.73 ± 0.05</p></td><td><p>0.39 ± 0.07</p></td><td><p>0.84 ± 0.06</p></td></tr><tr><td><p>Xc</p></td><td><p>19.18 ± 2.15</p></td><td><p>0.34 ± 0.10</p></td><td><p>16.57 ± 0.68</p></td><td><p>0.50 ± 0.06</p></td></tr><tr><td><p>EPS</p></td><td><p>0.72 ± 0.09</p></td><td><p>0.58 ± 0.07</p></td><td><p>0.52 ± 0.07</p></td><td><p>0.76 ± 0.11</p></td></tr><tr><td><p>Nc</p></td><td><p>0.13 ± 0.02</p></td><td><p>0.70 ± 0.06</p></td><td><p>0.10 ± 0.02</p></td><td><p>0.82 ± 0.07</p></td></tr><tr><td><p>OPV</p></td><td><p>2.04 ± 0.06</p></td><td><p>0.24 ± 0.03</p></td><td><p>1.92 ± 0.06</p></td><td><p>0.32 ± 0.05</p></td></tr></tbody></table></table-wrap></p><p id="Par20">Data augmentation is implemented not only in pretraining but also in finetuning. The comparison between the model performance on downstream tasks with pretraining on the original ~1M dataset and the augmented ~5M dataset (shown in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">5</xref>) has already demonstrated the significance of data augmentation in model performance enhancement. In this part, we use the model pretrained on the ~5M augmented pretraining dataset but finetune TransPolymer without augmenting the downstream datasets to investigate to what extent the TransPolymer model can improve the best baseline models for downstream tasks. The model performance enhancement with or without data augmentation compared with best baseline models is summarized in Table <xref rid="Tab8" ref-type="table">8</xref>. For most downstream tasks, TransPolymer<sub>pretrained</sub> can improve model performance without data augmentation, while such improvement would become more significant if data augmentation is applied. For PE-II dataset, however, TransPolymer<sub>pretrained</sub> is not comparable to the best baseline model without data augmentation since the original dataset contains only 271 data points in total. Because of the data-greedy characteristics of Transformer, data augmentation could be a crucial factor in finetuning, especially when data are scarce (which is very common in chemical and materials science regimes). Therefore, data augmentation can help generalize the model to sequences unseen in training data.<table-wrap id="Tab8"><label>Table 8</label><caption xml:lang="en"><p>Improvement of performance of TransPolymer<sub>pretrained</sub> without and with data augmentation in finetuning compared with best baselines in terms of decrease of test RMSE (in percentage) and increase of test <italic>R</italic><sup>2</sup> (in absolute value).</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th colspan="2"><p>No Augmentation</p></th><th colspan="2"><p>Augmentation</p></th></tr><tr><th/><th><p>RMSE (<italic>↓</italic>)</p></th><th><p><italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th><th><p>RMSE (<italic>↓</italic>)</p></th><th><p><italic>R</italic><sup>2</sup> (<italic>↑</italic>)</p></th></tr></thead><tbody><tr><td><p>PE-I</p></td><td><p>−15.5%</p></td><td><p>+0.22</p></td><td><p>−30.9%</p></td><td><p>+0.37</p></td></tr><tr><td><p>PE-II</p></td><td><p>+22.2%</p></td><td><p>−0.15</p></td><td><p>−3.17%</p></td><td><p>+0.01</p></td></tr><tr><td><p>Egc</p></td><td><p>−4.17%</p></td><td><p>+0.01</p></td><td><p>−8.33%</p></td><td><p>+0.02</p></td></tr><tr><td><p>Egb</p></td><td><p>+9.09%</p></td><td><p>−0.02</p></td><td><p>−5.45%</p></td><td><p>+0.02</p></td></tr><tr><td><p>Eea</p></td><td><p>+9.38%</p></td><td><p>0.00</p></td><td><p>0.00%</p></td><td><p>+0.01</p></td></tr><tr><td><p>Ei</p></td><td><p>0.00%</p></td><td><p>+0.03</p></td><td><p>−7.14%</p></td><td><p>+0.07</p></td></tr><tr><td><p>Xc</p></td><td><p>−14.5%</p></td><td><p>+0.43</p></td><td><p>−20.1%</p></td><td><p>+0.50</p></td></tr><tr><td><p>EPS</p></td><td><p>+7.55%</p></td><td><p>0.00</p></td><td><p>−1.89%</p></td><td><p>+0.05</p></td></tr><tr><td><p>Nc</p></td><td><p>0.00%</p></td><td><p>+0.02</p></td><td><p>0.00%</p></td><td><p>+0.03</p></td></tr><tr><td><p>OPV</p></td><td><p>+0.52%</p></td><td><p>+0.03</p></td><td><p>0.00%</p></td><td><p>+0.05</p></td></tr><tr><td><p>Average</p></td><td><p>+1.46%</p></td><td><p>+0.06</p></td><td><p>−7.70%</p></td><td><p>+0.11</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec7"><title>Self-attention visualization</title><p id="Par21">Attention scores, serving as an indicator of how closely two tokens align with each other, could be used for understanding how much chemical knowledge TransPolymer learns from pretraining and how each token contributes to the prediction results. Take poly(ethylene oxide) (*CCO*), which is one of the most prevailing polymer electrolytes, as an example. The attention scores between each token in the first and last hidden layer are shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a and b, respectively. The attention score matrices of 12 attention heads generated from the first hidden layer indicate strong relationships between tokens in the neighborhood, which could be inferred from the emergence of high attention scores around the diagonals of matrices. This trend makes sense because the nearby tokens in polymer SMILES usually represent atoms bonded to each other in the polymer, and atoms are most significantly affected by their local environments. Therefore,i the first hidden layer, which is the closest layer to inputs, could capture such chemical information. In contrast, the attention scores from the last hidden layer tend to be more uniform, thus lacking an interpretable pattern. Such phenomenon has also been observed by Abnar et al. who discovered that the embeddings of tokens would become contextualized for deeper hidden layers and might carry similar information<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Visualization of attention scores from pretrained TransPolymer.</title><p><bold>a</bold> Attention scores in the first hidden layer. <bold>b</bold> Attention scores in the last hidden layer.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig5_HTML.png"/></fig></p><p id="Par22">When finetuning TransPolymer, the vector of the special token ‘〈<italic>s</italic>〉’ from the last hidden state is used for prediction. Hence, to check the impacts of tokens on prediction results, the attention scores between ‘〈<italic>s</italic>〉’ and other tokens from all 6 hidden layers in each attention head are illustrated with the example of the PEC-PEO blend electrolyte coming from PE-II whose polymer SMILES is ‘*COC(=O)OC*.*CCO*’. In addition to polymer SMILES, the sequence also includes ‘F[B-](F)(F)F’, ‘0.17’, ‘95.2’, ‘37.0’, ‘−23’, and ‘S_1’ which stand for the anion in the electrolyte, the ratio between lithium ions and functional groups in the polymer, comonomer percentage, molecular weight (kDa), glass transition temperature (T<sub>g</sub>), and linear chain structure, respectively. As is illustrated in Fig. <xref rid="Fig6" ref-type="fig">6</xref>, the ‘〈<italic>s</italic>〉’ token tends to focus on certain tokens, like ‘*’, ‘$’, and ‘−23’, which are marked in red in the example sequence in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. Since T<sub>g</sub> usually plays an important role in determining the conductivity of polymers<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>, the finetuned Transpolyemr could understand the influential parts on properties in a polymer sequence. However, it is also widely argued that the attention weights cannot fully depict the relationship between tokens and prediction results because a high attention score does not necessarily guarantee that the pair of tokens is important to the prediction results given that attention scores do not consider Value matrices<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. More related work is needed to fully address the attention interpretation problem.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>Visualization of attention scores from finetuned TransPolymer.</title><p>The attention scores between the ‘〈<italic>s</italic>〉’ token and other tokens at different hidden layers in each attention head after finetuning are visualized. At the bottom is the sequence used for visualization in which the tokens having high attention scores with ‘〈<italic>s</italic>〉’ are marked in red.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1016_Fig6_HTML.png"/></fig></p></sec></sec><sec id="Sec8" sec-type="discussion"><title>Discussion</title><p id="Par23">In summary, we have proposed TransPolymer, a Transformer-based model with MLM pretraining, for accurate and efficient polymer property prediction. By rationally designing a polymer tokenization strategy, we can map a polymer instance to a sequence of tokens. Data augmentation is implemented to enlarge the available data for representation learning. TransPolymer is first pretrained on approximately 5M unlabeled polymer sequences by MLM, then finetuned on different downstream datasets, outperforming all the baselines and unpretrained TransPolymer. The superior model performance could be further explained by the impact of pretraining with large unlabeled data, finetuning Transformer encoders, and data augmentation for data space enlargement. The attention scores from hidden layers in TransPolymer provide evidence of the efficacy of learning representations with chemical awareness and suggest the influential tokens on final prediction results.</p><p id="Par24">Given the desirable model performance and outstanding generalization ability out of a small number of labeled downstream data, we anticipate that TransPolymer would serve as a potential solution to predicting newly designed polymer properties and guiding polymer design. For example, the pretrained TransPolymer could be applied in the active-learning-guided polymer discovery framework<sup><xref ref-type="bibr" rid="CR66">66</xref>,<xref ref-type="bibr" rid="CR67">67</xref></sup>, in which TransPolymer serves to virtually screen the polymer space, recommend the potential candidates with desirable properties based on model predictions, and get updated by learning on data from experimental evaluation. In addition, the outstanding performance of TransPolymer on copolymer datasets compared with existing baseline models has shed light on the exploration of copolymers. In a nutshell, even though the main focus of this paper is placed on regression, TransPolymer can pave the way for several promising (co)polymer discovery frameworks.</p></sec><sec id="Sec9" sec-type="methods"><title>Methods</title><sec id="Sec10"><title>Polymer tokenization</title><p id="Par25">Unlike small molecules which are easily represented by SMILES, polymers are more complex to be converted to sequences since SMILES fails to incorporate pivotal information like connectivity between repeating units and degree of polymerization. As a result, we need to design the polymer sequences to take account of that information. To design the polymer sequences, each repeating unit of the polymer is first recognized and converted to SMILES, then ‘*’ signs are added at the places which represent the ends of the repeating unit to indicate the connectivity between repeating units. Such a strategy to indicate repeating units has been widely used in string-based polymer representations<sup><xref ref-type="bibr" rid="CR68">68</xref>,<xref ref-type="bibr" rid="CR69">69</xref></sup>. For the cases of copolymers, ‘.’ is used to separate different constituents, and ‘^’ is used to indicate branches in copolymers. Other information like the degree of polymerization and molecular weight, if accessible, will be put after the polymer SMILES separated by special tokens. Take the example of the sequence given in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a, the sequence describes a polymer electrolyte system including two components separated by the special token ‘∣’. Descriptors like the ratio between repeating units in the copolymer, component type, and glass transition temperature (T<sub>g</sub> for short) are added for each component separated by ‘$’, and the ratio between components and temperature are put at the end of the sequence. Adding these descriptors can improve the performance of property predictions as suggested by Patel et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Unique ‘NAN’ tokens are assigned for missing values of each descriptor in the dataset. For example, ‘NAN_Tg’ indicates the missing value of glass transition temperature, and ‘NAN_MW’ indicates the missing molecular weight at that place. These unique NAN tokens are added during finetuning to include available chemical descriptors in the datasets. Therefore, different datasets can contain different NAN tokens. Notably, other descriptors like molecular weight and degree of polymerization are omitted in this example because their values for each component are missing. However, for practical usage, these values should also be included with unique ‘NAN’ characters. Besides, considering the varying constituents in copolymers as well as components in composites, the ‘NAN’ tokens for ratios are padded to the maximum possible numbers.</p><p id="Par26">When tokenizing the polymer sequences, the regular expression in the tokenizer adapted from the RoBERTa tokenizer is transformed to search for all the possible elements in polymers as well as the vocabulary for descriptors and special tokens. Consequently, the polymer tokenizer can correctly slice polymers into constituting atoms. For example, ‘Si’ which represents a silicon atom in polymer sequences would be recognized as a single token by our polymer tokenizer whereas ‘S’ and ‘i’ are likely to be separated into different tokens when using the RoBERTa tokenizer. Values for descriptors and special tokens are converted to single tokens as well, where all the non-text values, e.g., temperature, are discretized and treated as one token by the tokenizer.</p></sec><sec id="Sec11"><title>Data augmentation</title><p id="Par27">To enlarge the available polymer data for better representation learning, data augmentation is applied to the polymer SMILES within polymer sequences from each dataset we use. The augmentation technique is borrowed from Lambard et al.<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>. First, canonicalization is removed from SMILES representations; then, atoms in SMILES are renumbered by rotation of their indices; finally, for each renumbering case, grammatically correct SMILES which preserve isomerism of original polymers or molecules and prevent Kekulisation are reconstructed<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR71">71</xref></sup>. Also, duplicate SMILES are removed from the expanded list. SMILES augmentation is implemented by RDKit library<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>. In particular, data augmentation is only applied to training sets after the train-test split to avoid information leakage.</p></sec><sec id="Sec12"><title>Transformer-based encoder</title><p id="Par28">Our TransPolymer model is based on Transformer encoder architecture<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Unlike RNN-based models which encoded temporal information by recurrence, Transformer uses self-attention layers instead. The attention mechanism used in Transformer is named Scaled Dot-Product Attention, which maps input data into three vectors: queries (Q), keys (K), and values (V). The attention is computed by first computing the dot product of the query with all keys, dividing each by <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sqrt{{d}_{k}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1016_Article_IEq1.gif"/></alternatives></inline-formula> for scaling where <italic>d</italic><sub><italic>k</italic></sub> is the dimension of keys, applying softmax function to obtain the weights of values, and finally deriving the attention. The dot product between queries and keys computes how closely aligned the keys are with the queries. Therefore, the attention score is able to reflect how closely related the two embeddings of tokens are. The formula of Scaled Dot-Product Attention can be written as:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:mi mathvariant="normal">Attention</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Q</mml:mi><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo>,</mml:mo><mml:mi>V</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">softmax</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac><mml:mrow><mml:mi>Q</mml:mi><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi>V</mml:mi></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\rm{Attention}}}}(Q,K,V)={{{\rm{softmax}}}}\left(\frac{Q{K}^{{{{\rm{T}}}}}}{\sqrt{{d}_{{{{\rm{k}}}}}}}\right)V$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1016_Article_Equ1.gif"/></alternatives></disp-formula>Multi-head attention is performed instead of single attention by linearly projecting Q, K, and V with different projections and applying the attention function in parallel. The outputs are concatenated and projected again to obtain the final results. In this way, information from different subspaces could be learned by the model.</p><p id="Par29">The input of Transformer model, namely embeddings, maps tokens in sequences to vectors. Due to the absence of recurrence, word embeddings only are not sufficient to encode sequence order. Therefore, positional encodings are introduced so that the model can know the relative or absolute position of the token in the sequence. In Transformer, position encodings are represented by trigonometric functions:<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">PE</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pos</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">pos</mml:mi><mml:mo>/</mml:mo><mml:mn>1000</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">i</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">model</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\rm{PE}}}}}_{{{{\rm{pos}}}},2{{{\rm{i}}}}}=\sin ({{{\rm{pos}}}}/1000{0}^{2{{{\rm{i}}}}/{d}_{{{{\rm{model}}}}}})$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1016_Article_Equ2.gif"/></alternatives></disp-formula><disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">PE</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pos</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mi mathvariant="normal">i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">pos</mml:mi><mml:mo>/</mml:mo><mml:mn>1000</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">i</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">model</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\rm{PE}}}}}_{{{{\rm{pos}}}},2{{{\rm{i}}}}+1}=\cos ({{{\rm{pos}}}}/1000{0}^{2{{{\rm{i}}}}/{d}_{{{{\rm{model}}}}}})$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1016_Article_Equ3.gif"/></alternatives></disp-formula>where pos is the position of the token and i is the dimension. By this means, the relative positions of tokens could be learned by the model.</p></sec><sec id="Sec13"><title>Pretraining with MLM</title><p id="Par30">To pretrain TransPolymer with Masked Language Modeling (MLM), 15% of tokens of a sequence are chosen for possible replacement. Among the chosen tokens, 80% of which are masked, 10% of which are replaced by randomly selected vocabulary tokens, and 10% are left unchanged, in order to generate proper contextual embeddings for all tokens and bias the representation towards the actual observed words<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Such a pretraining strategy enables TransPolymer to learn the “chemical grammar" of polymer sequences by recovering the original tokens so that chemical knowledge is encoded by the model.</p><p id="Par31">The pretraining database is split into training and validation sets by a ratio of 80/20. We use AdamW as the optimizer, where the learning rate is 5 × 10<sup>−5</sup>, betas parameters are (0.9, 0.999), epsilon is 1 × 10<sup>−6</sup>, and weight decay is 0. A linear scheduler with a warm-up ratio of 0.05 is set up so that the learning rate increases from 0 to the learning rate set in the optimizer in the first 5% training steps then decreases linearly to zero. The batch size is set to 200, and the hidden layer dropout and attention dropout are set to 0.1. The model is pretrained for 30 epochs during which the binary cross entropy loss decreases steadily from over 1 to around 0.07, and the one with the best performance on the validation set is used for finetuning. The whole pretraining process takes approximately 3 days on two RTX 6000 GPUs.</p></sec><sec id="Sec14"><title>Finetuning for polymer property prediction</title><p id="Par32">The finetuning process involves the pretrained Transformer encoder and a one-layer MLP regressor head so that representations of polymer sequences could be used for property predictions.</p><p id="Par33">For the experimental settings of finetuning, AdamW is set to be the optimizer whose betas parameters are (0.9, 0.999), epsilon is 1 × 10<sup>−6</sup>, and weight decay is 0.01. Different learning rates are used for the pretrained TransPolymer and regressor head. Particularly, for some experiments a strategy of layer-wise learning rate the decay (LLRD), suggested by Zhang et al.<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>, is applied. Specifically, in LLRD, the learning rate is decreased layer-by-layer from top to bottom with a multiplicative decay rate. The strategy is based on the observation that different layers learn different information from sequences. Top layers near the output learn more local and specific information, thus requiring larger learning rates; while bottom layers near inputs learn more general and common information. The specific choices of learning rates for each dataset as well as other hyperparameters of the optimizer and scheduler are exhibited in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">2</xref>. For each downstream dataset, the model is trained for 20 epochs and the best model is determined in terms of the RMSE and <italic>R</italic><sup>2</sup> on the test set for evaluation.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>We thank the Advanced Research Projects Agency—Energy (ARPA-E), U.S. Department of Energy, under Award No. DE-AR0001221 and the start-up fund provided by the Department of Mechanical Engineering at Carnegie Mellon University.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>A.B.F., Y.W., and C.X. conceived the idea; C.X. trained and evaluated the TransPolymer model; C.X. wrote the manuscript; A.B.F. supervised the work; all authors modified and approved the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>All data used in this work are publicly available. Original datasets could be found in corresponding literature<sup><xref ref-type="bibr" rid="CR51">51</xref>–<xref ref-type="bibr" rid="CR55">55</xref></sup>. Besides, the original and processed datasets used in this work are also available at <ext-link xlink:href="https://github.com/ChangwenXu98/TransPolymer.git" ext-link-type="uri">https://github.com/ChangwenXu98/TransPolymer.git</ext-link>.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The codes developed for this work are available at <ext-link xlink:href="https://github.com/ChangwenXu98/TransPolymer.git" ext-link-type="uri">https://github.com/ChangwenXu98/TransPolymer.git</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par34">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Toward designing highly conductive polymer electrolytes by machine learning assisted coarse-grained molecular dynamics</article-title><source>Chem. Mater.</source><year>2020</year><volume>32</volume><fpage>4144</fpage><lpage>4151</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXnslehtrs%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.9b04830</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerating amorphous polymer electrolyte screening by learning to reduce errors in molecular dynamics simulated properties</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41467-022-30994-1</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>St. John</surname><given-names>PC</given-names></name><etal/></person-group><article-title xml:lang="en">Message-passing neural networks for high-throughput polymer screening</article-title><source>J. Chem. Phys.</source><year>2019</year><volume>150</volume><fpage>234111</fpage><pub-id pub-id-type="doi">10.1063/1.5099132</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munshi</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name><name><surname>Chien</surname><given-names>T</given-names></name><name><surname>Balasubramanian</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Transfer learned designer polymers for organic solar cells</article-title><source>J. Chem. Inf. Model.</source><year>2021</year><volume>61</volume><fpage>134</fpage><lpage>142</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXkvVCntA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.0c01157</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Core–shell nanostructure design in polymer nanocomposite capacitors for energy storage applications</article-title><source>ACS Sustain. Chem. Eng.</source><year>2018</year><volume>7</volume><fpage>3145</fpage><lpage>3153</lpage><pub-id pub-id-type="doi">10.1021/acssuschemeng.8b04943</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Recent advances in rational design of polymer nanocomposite dielectrics for energy storage</article-title><source>Nano Energy</source><year>2020</year><volume>74</volume><fpage>104844</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXpvF2qtL4%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.nanoen.2020.104844</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bai</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerated discovery of organic polymer photocatalysts for hydrogen evolution from water through the integration of experiment and theory</article-title><source>J. Am. Chem. Soc.</source><year>2019</year><volume>141</volume><fpage>9063</fpage><lpage>9071</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXptlenu7Y%3D</pub-id><pub-id pub-id-type="doi">10.1021/jacs.9b03591</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>L</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>X</given-names></name></person-group><article-title xml:lang="en">Machine-learning-assisted low dielectric constant polymer discovery</article-title><source>Mater. Chem. Front.</source><year>2021</year><volume>5</volume><fpage>3823</fpage><lpage>3829</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXmt1Onur0%3D</pub-id><pub-id pub-id-type="doi">10.1039/D0QM01093F</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mannodi-Kanakkithodi</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Scoping the polymer genome: a roadmap for rational polymer dielectrics design and beyond</article-title><source>Mater. Today</source><year>2018</year><volume>21</volume><fpage>785</fpage><lpage>796</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhvFahurfP</pub-id><pub-id pub-id-type="doi">10.1016/j.mattod.2017.11.021</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Polymer informatics: current status and critical next steps</article-title><source>Mater. Sci. Eng. R. Rep.</source><year>2021</year><volume>144</volume><fpage>100595</fpage><pub-id pub-id-type="doi">10.1016/j.mser.2020.100595</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rahman</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">A machine learning framework for predicting the shear strength of carbon nanotube-polymer interfaces based on molecular dynamics simulation data</article-title><source>Compos Sci. Technol.</source><year>2021</year><volume>207</volume><fpage>108627</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXmt1Onsrk%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.compscitech.2020.108627</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarselli</surname><given-names>F</given-names></name><name><surname>Gori</surname><given-names>M</given-names></name><name><surname>Tsoi</surname><given-names>AC</given-names></name><name><surname>Hagenbuchner</surname><given-names>M</given-names></name><name><surname>Monfardini</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">The graph neural network model</article-title><source>IEEE trans. neural netw.</source><year>2008</year><volume>20</volume><fpage>61</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1109/TNN.2008.2005605</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>145301</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSnu7c%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.145301</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. <italic>Adv. Neural. Inf. Process. Syst.</italic><bold>28</bold>, (2015).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Analyzing learned molecular representations for property prediction</article-title><source>J. Chem. Inf. Model.</source><year>2019</year><volume>59</volume><fpage>3370</fpage><lpage>3388</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsVOhsLfL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00237</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamad</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Orbital graph convolutional neural network for material property prediction</article-title><source>Phys. Rev. Mater.</source><year>2020</year><volume>4</volume><fpage>093801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitVOktr3M</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.4.093801</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Barati Farimani</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Molecular contrastive learning of representations via graph neural networks</article-title><source>Nat. Mach. Intell.</source><year>2022</year><volume>4</volume><fpage>279</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1038/s42256-022-00447-x</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Park, J. et al. Prediction and interpretation of polymer properties using the graph convolutional network. <italic>ACS polym. Au</italic>. <bold>2</bold>, 213-222 (2022).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cereto-Massagué</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Molecular fingerprint similarity search in virtual screening</article-title><source>Methods</source><year>2015</year><volume>71</volume><fpage>58</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.ymeth.2014.08.005</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Extended-connectivity fingerprints</article-title><source>J. Chem. Inf. Model</source><year>2010</year><volume>50</volume><fpage>742</fpage><lpage>754</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3cXlt1Onsbg%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci100050t</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aldeghi</surname><given-names>M</given-names></name><name><surname>Coley</surname><given-names>CW</given-names></name></person-group><article-title xml:lang="en">A graph representation of molecular ensembles for polymer property prediction</article-title><source>Chem. Sci.</source><year>2022</year><volume>13</volume><fpage>10486</fpage><lpage>10498</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38Xit1ahs73N</pub-id><pub-id pub-id-type="doi">10.1039/D2SC02839E</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Cho, K. et al. Learning phrase representations using rnn encoder-decoder for statistical machine translation. In <italic>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</italic>, 1724–1734 (ACL, 2014).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwaller</surname><given-names>P</given-names></name><name><surname>Gaudin</surname><given-names>T</given-names></name><name><surname>Lanyi</surname><given-names>D</given-names></name><name><surname>Bekas</surname><given-names>C</given-names></name><name><surname>Laino</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">"found in translation”: predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>6091</fpage><lpage>6098</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtFyjtb%2FE</pub-id><pub-id pub-id-type="doi">10.1039/C8SC02339E</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>S-T</given-names></name><name><surname>Kuo</surname><given-names>E-J</given-names></name><name><surname>Tiwary</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Learning molecular dynamics with simple language model built upon long short-term memory neural network</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-18959-8</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flam-Shepherd</surname><given-names>D</given-names></name><name><surname>Zhu</surname><given-names>K</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Language models can learn complex molecular distributions</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XhsF2mtr7I</pub-id><pub-id pub-id-type="doi">10.1038/s41467-022-30839-x</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cadeddu</surname><given-names>A</given-names></name><name><surname>Wylie</surname><given-names>EK</given-names></name><name><surname>Jurczak</surname><given-names>J</given-names></name><name><surname>Wampler-Doty</surname><given-names>M</given-names></name><name><surname>Grzybowski</surname><given-names>BA</given-names></name></person-group><article-title xml:lang="en">Organic chemistry as a language and the implications of chemical linguistics for structural and retrosynthetic analyses</article-title><source>Angew. Chem. Int. Ed.</source><year>2014</year><volume>53</volume><fpage>8108</fpage><lpage>8112</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXhtFCgtbnL</pub-id><pub-id pub-id-type="doi">10.1002/anie.201403708</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simine</surname><given-names>L</given-names></name><name><surname>Allen</surname><given-names>TC</given-names></name><name><surname>Rossky</surname><given-names>PJ</given-names></name></person-group><article-title xml:lang="en">Predicting optical spectra for optoelectronic polymers using coarse-grained models and recurrent neural networks</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2020</year><volume>117</volume><fpage>13945</fpage><lpage>13948</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsVCgs73K</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1918696117</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Webb</surname><given-names>MA</given-names></name><name><surname>Jackson</surname><given-names>NE</given-names></name><name><surname>Gil</surname><given-names>PS</given-names></name><name><surname>Pablo</surname><given-names>JJ</given-names></name></person-group><article-title xml:lang="en">Targeted sequence design within the coarse-grained polymer genome</article-title><source>Sci. Adv.</source><year>2020</year><volume>6</volume><fpage>6216</fpage><pub-id pub-id-type="doi">10.1126/sciadv.abc6216</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>RA</given-names></name><name><surname>Borca</surname><given-names>CH</given-names></name><name><surname>Webb</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Featurization strategies for polymer sequence or composition design by machine learning</article-title><source>Mol. Syst. Des. Eng.</source><year>2022</year><volume>7</volume><fpage>661–676</fpage><pub-id pub-id-type="doi">10.1039/D1ME00160D</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhattacharya</surname><given-names>D</given-names></name><name><surname>Kleeblatt</surname><given-names>DC</given-names></name><name><surname>Statt</surname><given-names>A</given-names></name><name><surname>Reinhart</surname><given-names>WF</given-names></name></person-group><article-title xml:lang="en">Predicting aggregate morphology of sequence-defined macromolecules with recurrent neural networks</article-title><source>Soft Matter</source><year>2022</year><volume>18</volume><fpage>5037</fpage><lpage>5051</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XhslSmsrbM</pub-id><pub-id pub-id-type="doi">10.1039/D2SM00452F</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weininger</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules</article-title><source>J. Chem. Inf. Comput.</source><year>1988</year><volume>28</volume><fpage>31</fpage><lpage>36</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1cXnsVeqsA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T-S</given-names></name><etal/></person-group><article-title xml:lang="en">Bigsmiles: a structurally-based line notation for describing macromolecules</article-title><source>ACS Cent. Sci.</source><year>2019</year><volume>5</volume><fpage>1523</fpage><lpage>1531</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhslCntb7E</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00476</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goswami</surname><given-names>S</given-names></name><name><surname>Ghosh</surname><given-names>R</given-names></name><name><surname>Neog</surname><given-names>A</given-names></name><name><surname>Das</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Deep learning based approach for prediction of glass transition temperature in polymers</article-title><source>Mater. Today.: Proc.</source><year>2021</year><volume>46</volume><fpage>5838</fpage><lpage>5843</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhs1ynsrbE</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. <italic>Adv. Neural. Inf. Process. Syst.</italic><bold>30</bold>, (2017).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In <italic>Proceedings of NAACL-HLT</italic> 4171–4186 (2019).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1907.11692" ext-link-type="uri">https://arxiv.org/abs/1907.11692</ext-link> (2019).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Language models are few-shot learners</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>1877</fpage><lpage>1901</lpage></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Peters, M. E., Neumann, M., Zettlemoyer, L. &amp; Yih, W.-t. Dissecting contextual word embeddings: architecture and representation. In <italic>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</italic> 1499–1509 (2018).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Conneau, A. &amp; Lample, G. Cross-lingual language model pretraining. <italic>Adv. Neural. Inf. Process. Syst.</italic><bold>32</bold>, (2019).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Honda, S., Shi, S. &amp; Ueda, H. R. Smiles transformer: pre-trained molecular fingerprint for low data drug discovery. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1911.04738" ext-link-type="uri">https://arxiv.org/abs/1911.04738</ext-link> (2019).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ying</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Do transformers really perform badly for graph representation?</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2021</year><volume>34</volume><fpage>28877</fpage><lpage>28888</lpage></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irwin</surname><given-names>R</given-names></name><name><surname>Dimitriadis</surname><given-names>S</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Bjerrum</surname><given-names>EJ</given-names></name></person-group><article-title xml:lang="en">Chemformer: a pre-trained transformer for computational chemistry</article-title><source>Mach. Learn.: Sci. Technol.</source><year>2022</year><volume>3</volume><fpage>015022</fpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magar</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Barati Farimani</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Crystal twins: self-supervised learning for crystalline material property prediction</article-title><source>NPJ Comput. Mater.</source><year>2022</year><volume>8</volume><fpage>231</fpage><pub-id pub-id-type="doi">10.1038/s41524-022-00921-5</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Wang, S., Guo, Y., Wang, Y., Sun, H. &amp; Huang, J. Smiles-bert: large scale unsupervised pre-training for molecular property prediction. In <italic>Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics</italic> 429–436 (2019).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Chithrananda, S., Grand, G. &amp; Ramsundar, B. Chemberta: large-scale self-supervised pretraining for molecular property prediction. Preprint at <ext-link xlink:href="https://arxiv.org/abs/2010.09885" ext-link-type="uri">https://arxiv.org/abs/2010.09885</ext-link> (2020).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwaller</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction</article-title><source>ACS Cent. Sci.</source><year>2019</year><volume>5</volume><fpage>1572</fpage><lpage>1583</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhs1Krtb%2FJ</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00576</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Z</given-names></name><name><surname>Magar</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Barati Farimani</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Moformer: self-supervised transformer model for metal–organic framework property prediction</article-title><source>J. Am. Chem. Soc.</source><year>2023</year><volume>145</volume><fpage>2958</fpage><lpage>2967</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3sXhvFWltbc%3D</pub-id><pub-id pub-id-type="doi">10.1021/jacs.2c11420</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perry</surname><given-names>SL</given-names></name><name><surname>Sing</surname><given-names>CE</given-names></name></person-group><article-title xml:lang="en">100th anniversary of macromolecular science viewpoint: opportunities in the physics of sequence-defined polymers</article-title><source>ACS Macro Lett.</source><year>2020</year><volume>9</volume><fpage>216</fpage><lpage>225</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvVeqsrg%3D</pub-id><pub-id pub-id-type="doi">10.1021/acsmacrolett.0c00002</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>T</given-names></name><name><surname>Epa</surname><given-names>VC</given-names></name><name><surname>Burden</surname><given-names>FR</given-names></name><name><surname>Winkler</surname><given-names>DA</given-names></name></person-group><article-title xml:lang="en">Quantitative structure–property relationship modeling of diverse materials properties</article-title><source>Chem. Rev.</source><year>2012</year><volume>112</volume><fpage>2889</fpage><lpage>2919</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XpsVyisA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/cr200066h</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Persson</surname><given-names>N</given-names></name><name><surname>McBride</surname><given-names>M</given-names></name><name><surname>Grover</surname><given-names>M</given-names></name><name><surname>Reichmanis</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Silicon valley meets the ivory tower: searchable data repositories for experimental nanomaterials research</article-title><source>Curr. Opin. Solid State Mater. Sci.</source><year>2016</year><volume>20</volume><fpage>338</fpage><lpage>343</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtVaiurfF</pub-id><pub-id pub-id-type="doi">10.1016/j.cossms.2016.06.002</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>R</given-names></name><name><surname>Luo</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Pi1m: a benchmark database for polymer informatics</article-title><source>J. Chem. Inf. Model</source><year>2020</year><volume>60</volume><fpage>4684</fpage><lpage>4690</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvFaqs73E</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.0c00726</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schauser</surname><given-names>NS</given-names></name><name><surname>Kliegle</surname><given-names>GA</given-names></name><name><surname>Cooke</surname><given-names>P</given-names></name><name><surname>Segalman</surname><given-names>RA</given-names></name><name><surname>Seshadri</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Database creation, visualization, and statistical learning for polymer li+-electrolyte design</article-title><source>Chem. Mater.</source><year>2021</year><volume>33</volume><fpage>4863</fpage><lpage>4876</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhtlCqu7fM</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c04767</pub-id></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatakeyama-Sato</surname><given-names>K</given-names></name><name><surname>Tezuka</surname><given-names>T</given-names></name><name><surname>Umeki</surname><given-names>M</given-names></name><name><surname>Oyaizu</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Ai-assisted exploration of superionic glass-type li+ conductors with aromatic structures</article-title><source>J. Am. Chem. Soc.</source><year>2020</year><volume>142</volume><fpage>3301</fpage><lpage>3305</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXpt1Wjsg%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/jacs.9b11442</pub-id></mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kuenneth</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Polymer informatics with multi-task learning</article-title><source>Patterns</source><year>2021</year><volume>2</volume><fpage>100238</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XhvF2nur3I</pub-id><pub-id pub-id-type="doi">10.1016/j.patter.2021.100238</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagasawa</surname><given-names>S</given-names></name><name><surname>Al-Naamani</surname><given-names>E</given-names></name><name><surname>Saeki</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Computer-aided screening of conjugated polymers for organic solar cell: classification by random forest</article-title><source>J. Phys. Chem. Lett.</source><year>2018</year><volume>9</volume><fpage>2639</fpage><lpage>2646</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXovVyms74%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpclett.8b00635</pub-id></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Salazar, J., Liang, D., Nguyen, T. Q. &amp; Kirchhoff, K. Masked language model scoring. In <italic>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</italic> 2699–2712 (ACL, 2020).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Bao, H. et al. Unilmv2: Pseudo-masked language models for unified language model pre-training. In <italic>International Conference on Machine Learning (ICML)</italic> 642–652 (ICML, 2020).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Yang, Z., Yang, Y., Cer, D., Law, J. &amp; Darve, E. Universal sentence representation learning with conditional masked language model. In <italic>Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</italic> 6216–6228 (2021).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Otsuka, S., Kuwajima, I., Hosoya, J., Xu, Y. &amp; Yamazaki, M. Polyinfo: Polymer database for polymeric materials design. In <italic>2011 International Conference on Emerging Intelligent Data and Web Technologies</italic> 22–29 (2011).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Chandrasekaran</surname><given-names>A</given-names></name><name><surname>Huan</surname><given-names>TD</given-names></name><name><surname>Das</surname><given-names>D</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Polymer genome: a data-powered polymer informatics platform for property predictions</article-title><source>J. Phys. Chem. C.</source><year>2018</year><volume>122</volume><fpage>17575</fpage><lpage>17585</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtlaktLnL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpcc.8b02913</pub-id></mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Visualizing data using t-sne</article-title><source>J. Mach. Learn Res.</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Poličar, P.G., Stražar, M. &amp; Zupan, B. Opentsne: a modular python library for t-sne dimensionality reduction and embedding. Preprint at <ext-link xlink:href="https://www.biorxiv.org/content/10.1101/731877v3.abstract" ext-link-type="uri">https://www.biorxiv.org/content/10.1101/731877v3.abstract</ext-link> (2019).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Abnar, S. &amp; Zuidema, W. Quantifying attention flow in transformers. In <italic>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</italic> 4190–4197 (ACL, 2020).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schauser</surname><given-names>NS</given-names></name><etal/></person-group><article-title xml:lang="en">Glass transition temperature and ion binding determine conductivity and lithium–ion transport in polymer electrolytes</article-title><source>ACS Macro Lett.</source><year>2020</year><volume>10</volume><fpage>104</fpage><lpage>109</lpage><pub-id pub-id-type="doi">10.1021/acsmacrolett.0c00788</pub-id></mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Hao, Y., Dong, L., Wei, F. &amp; Xu, K. Self-attention attribution: interpreting information interactions inside transformer. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic><bold>35</bold>, 12963–12971 (2021).</mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reis</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Machine-learning-guided discovery of 19f mri agents enabled by automated copolymer synthesis</article-title><source>J. Am. Chem. Soc.</source><year>2021</year><volume>143</volume><fpage>17677</fpage><lpage>17689</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXit1Wnu7fO</pub-id><pub-id pub-id-type="doi">10.1021/jacs.1c08181</pub-id></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tamasi</surname><given-names>MJ</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning on a robotic platform for the design of polymer–protein hybrids</article-title><source>Adv. Mater.</source><year>2022</year><volume>34</volume><fpage>2201809</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XhsFCisLbK</pub-id><pub-id pub-id-type="doi">10.1002/adma.202201809</pub-id></mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batra</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Polymers for extreme conditions designed using syntax-directed variational autoencoders</article-title><source>Chem. Mater.</source><year>2020</year><volume>32</volume><fpage>10489</fpage><lpage>10500</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXisFGksrbP</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c03332</pub-id></mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Tao</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Predicting polymers’ glass transition temperature by a chemical language processing model</article-title><source>Polymers</source><year>2021</year><volume>13</volume><fpage>1898</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhsVSrur%2FN</pub-id><pub-id pub-id-type="doi">10.3390/polym13111898</pub-id></mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lambard</surname><given-names>G</given-names></name><name><surname>Gracheva</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Smiles-x: autonomous molecular compounds characterization for small datasets without descriptors</article-title><source>Mach. Learn.: Sci. Technol.</source><year>2020</year><volume>1</volume><fpage>025004</fpage></mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Eyben, F., Wöllmer, M. &amp; Schuller, B. Opensmile: the munich versatile and fast open-source audio feature extractor. In <italic>Proceedings of the 18th ACM International Conference on Multimedia</italic> 1459–1462 (2010).</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Landrum, G. et al. Rdkit: open-source cheminformatics. <ext-link xlink:href="https://www.rdkit.org" ext-link-type="uri">https://www.rdkit.org</ext-link> (2006).</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Zhang, T., Wu, F., Katiyar, A., Weinberger, K. Q. &amp; Artzi, Y. Revisiting few-sample bert fine-tuning. In <italic>International Conference on Learning Representations (ICLR)</italic> (ICLR, 2021).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec15"><title>Supplementary information</title><p id="Par35"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2023_1016_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information for TransPolymer: a Transformer-based Language Model for Polymer Property Predictions</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-023-01016-5" ext-link-type="doi">https://doi.org/10.1038/s41524-023-01016-5</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2023</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
