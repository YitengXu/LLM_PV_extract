<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-023-01003-w</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-023-01003-w</article-id><article-id pub-id-type="manuscript">1003</article-id><article-id pub-id-type="doi">10.1038/s41524-023-01003-w</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/923/1028</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2015-9556</contrib-id><name><surname>Shetty</surname><given-names>Pranav</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Rajan</surname><given-names>Arunkumar Chitteth</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6958-4679</contrib-id><name><surname>Kuenneth</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Gupta</surname><given-names>Sonakshi</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" id="Au5"><name><surname>Panchumarti</surname><given-names>Lakshmi Prerana</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au6"><name><surname>Holm</surname><given-names>Lauren</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au7"><name><surname>Zhang</surname><given-names>Chao</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au8"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-4630-1565</contrib-id><name><surname>Ramprasad</surname><given-names>Rampi</given-names></name><address><email>rampi.ramprasad@mse.gatech.edu</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs4152402301003w_cor8">h</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution content-type="org-name">School of Computational Science &amp; Engineering</institution></institution-wrap><addr-line content-type="city">Atlanta</addr-line><addr-line content-type="state">GA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.213917.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 4943</institution-id><institution content-type="org-division">School of Materials Science and Engineering</institution><institution content-type="org-name">Georgia Institute of Technology</institution></institution-wrap><addr-line content-type="street">771 Ferst Drive NW</addr-line><addr-line content-type="postcode">30332</addr-line><addr-line content-type="city">Atlanta</addr-line><addr-line content-type="state">GA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.450280.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1769 7721</institution-id><institution content-type="org-division">Department of Metallurgy Engineering and Materials Science</institution><institution content-type="org-name">Indian Institute of Technology</institution></institution-wrap><addr-line content-type="city">Indore</addr-line><addr-line content-type="state">Madhya Pradesh</addr-line><country country="IN">India</country></aff></contrib-group><author-notes><corresp id="IDs4152402301003w_cor8"><label>h</label><email>rampi.ramprasad@mse.gatech.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>5</day><month>4</month><year>2023</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2023</year></pub-date><volume>9</volume><issue seq="52">1</issue><elocation-id>52</elocation-id><history><date date-type="registration"><day>17</day><month>3</month><year>2023</year></date><date date-type="received"><day>16</day><month>9</month><year>2022</year></date><date date-type="accepted"><day>16</day><month>3</month><year>2023</year></date><date date-type="online"><day>5</day><month>4</month><year>2023</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2023</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">The ever-increasing number of materials science articles makes it hard to infer chemistry-structure-property relations from literature. We used natural language processing methods to automatically extract material property data from the abstracts of polymer literature. As a component of our pipeline, we trained MaterialsBERT, a language model, using 2.4 million materials science abstracts, which outperforms other baseline models in three out of five named entity recognition datasets. Using this pipeline, we obtained ~300,000 material property records from ~130,000 abstracts in 60 hours. The extracted data was analyzed for a diverse range of applications such as fuel cells, supercapacitors, and polymer solar cells to recover non-trivial insights. The data extracted through our pipeline is made available at <ext-link xlink:href="https://polymerscholar.org" ext-link-type="uri">polymerscholar.org</ext-link> which can be used to locate material property data recorded in abstracts. This work demonstrates the feasibility of an automatic pipeline that starts from published literature and ends with extracted material property information.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>United States Department of Defense | United States Navy | Office of Naval Research (ONR)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100000006</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">N00014-19-1-2103</award-id><award-id award-type="FundRef grant">N00014-20-1-2175</award-id><principal-award-recipient><name><surname>Ramprasad</surname><given-names>Rampi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Ramprasad</surname><given-names>Rampi</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>52</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>3</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>17</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2023_Article_1003.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">The number of materials science papers published annually grows at the rate of 6% compounded annually. Quantitative and qualitative material property information is locked away in these publications written in natural language that is not machine-readable. The explosive growth in published literature makes it harder to see quantitative trends by manually analyzing large amounts of literature. Searching the literature for material systems that have desirable properties also becomes more challenging. Moreover, material information published in a non-machine-readable form contributes to data scarcity in the field of materials informatics where the training of property predictors requires painstaking manual curation of the data of interest from literature. Here, we propose adapting techniques for information extraction from the natural language processing (NLP) literature to address these issues.</p><p id="Par3">Information extraction from the written text is well-studied within NLP and involves several key components such as named entity recognition (NER), i.e., identifying categories to which words in the text belong; relation extraction, i.e., classifying relationships between extracted entities; co-referencing, i.e., identifying clusters of named entities in the text referring to the same object such as a polymer and its abbreviation, and named entity normalization, i.e., identifying all the variations in the name for an entity across a large number of documents. The idea of “self-supervised learning” through transformer-based models such as BERT<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>, pre-trained on massive corpora of unlabeled text to learn contextual embeddings, is the dominant paradigm of information extraction today. A common architecture for NER and relation extraction is to feed a labeled input to BERT and use the output vector embedding for each word along with the corresponding labels (which could be entity labels or relation labels) as inputs to a task-specific machine learning model (typically a neural network) that learns to predict those labels. The tasks mentioned above are label intensive. Extending these methods to new domains requires labeling new data sets with ontologies that are tailored to the domain of interest.</p><p id="Par4">ChemDataExtractor<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, ChemSpot<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, and ChemicalTagger<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> are tools that perform NER to tag material entities. For example, ChemDataExtractor has been used to create a database of Neel temperatures and Curie temperatures that were automatically mined from literature<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. It has also been used to generate a literature-extracted database of magnetocaloric materials and train property prediction models for key figures of merit<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. In the space of polymers, the authors of Ref. <sup><xref ref-type="bibr" rid="CR8">8</xref></sup> used a semi-automated approach that crawled papers automatically and used students to extract the Flory-Huggins parameter (a measure of the affinity between two materials, eg., a polymer and a solvent). Word embedding approaches were used in Ref. <sup><xref ref-type="bibr" rid="CR9">9</xref></sup> to generate entity-rich documents for human experts to annotate which were then used to train a polymer named entity tagger. Most previous NLP-based efforts in materials science have focused on inorganic materials<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup> and organic small molecules<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup> but limited work has been done to address information extraction challenges in polymers. Polymers in practice have several non-trivial variations in name for the same material entity which requires polymer names to be normalized. Moreover, polymer names cannot typically be converted to SMILES strings<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> that are usable for training property-predictor machine learning models. The SMILES strings must instead be inferred from figures in the paper that contain the corresponding structure.</p><p id="Par5">Past work to automatically extract material property information from literature has focused on specific properties typically using keyword search methods or regular expressions<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. However, there are few solutions in the literature that address building general-purpose capabilities for extracting material property information, i.e., for any material property. Moreover, property extraction and analysis of polymers from a large corpus of literature have also not yet been addressed. Automatically analyzing large materials science corpora has enabled many novel discoveries in recent years such as Ref. <sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, where a literature-extracted data set of zeolites was used to analyze interzeolite relations. Using word embeddings trained on such corpora has also been used to predict novel materials for certain applications in inorganics and polymers<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>.</p><p id="Par6">We built a general-purpose pipeline for extracting material property data in this work. Starting with a corpus of 2.4 million materials science papers described in Ref. <sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, we selected 750 abstracts from the polymer domain and annotated each of the abstracts using our own ontology that was designed for the purpose of extracting information from materials science literature. Using these 750 annotated abstracts we trained an NER model, using our MaterialsBERT language model to encode the input text into vector representations. MaterialsBERT in turn was trained by starting from PubMedBERT, another language model, and using 2.4 million materials science abstracts to continue training the model<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. The trained NER model was applied to polymer abstracts and heuristic rules were used to combine the predictions of the NER model and obtain material property records from all polymer-relevant abstracts. This pipeline is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. We restricted our focus to abstracts as associating property value pairs with their corresponding materials is a more tractable problem in abstracts. We analyzed the data obtained using this pipeline for applications as diverse as polymer solar cells, fuel cells, and supercapacitors and showed that several known trends and phenomena in materials science can be inferred using this data. Moreover, we trained a machine learning predictor for the glass transition temperature using automatically extracted data (Supplementary Discussion <xref ref-type="supplementary-material" rid="MOESM1">3)</xref>.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Pipeline used for extracting material property records from a corpus of abstracts.</title><p>The training of MaterialsBERT, training of the NER model as well as the use of the NER model in conjunction with heuristic rules to extract material property data.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig1_HTML.png"/></fig></p><p id="Par7">This work builds a general-purpose material property data extraction pipeline, for any material property. MaterialsBERT, the language model that powers our information extraction pipeline is released in order to enable the information extraction efforts of other materials researchers. There are other BERT-based language models for the materials science domain such as MatSciBERT<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> and the similarly named MaterialBERT<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> which have been benchmarked on materials science specific NLP tasks. This work goes beyond benchmarking the language model on NLP tasks and demonstrates how it can be used in combination with NER and relation extraction methods to extract all material property records in the abstracts of our corpus of papers. In addition, we show that MaterialsBERT outperforms other similar BERT-based language models such as BioBERT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> and ChemBERT<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> on three out of five materials science NER data sets. The data extracted using this pipeline can be explored using a convenient web-based interface (<ext-link xlink:href="https://polymerscholar.org" ext-link-type="uri">polymerscholar.org</ext-link>) which can aid polymer researchers in locating material property information of interest to them.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Abstract annotation</title><p id="Par8">Our ontology for extracting material property information consists of 8 entity types namely POLYMER, POLYMER_CLASS, PROPERTY_VALUE, PROPERTY_NAME, MONOMER, ORGANIC_MATERIAL, INORGANIC_MATERIAL, and MATERIAL_AMOUNT. For a detailed description of these entity types, see Table <xref rid="Tab1" ref-type="table">1</xref>. This ontology captures the key pieces of information commonly found in abstracts and the information we wish to utilize for downstream purposes. Unlike some other studies<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, our ontology does not annotate entities using the BIO tagging scheme, i.e., <bold>B</bold>eginning-<bold>I</bold>nside-<bold>O</bold>utside of the labeled entity. Instead, we opt to keep the labels simple and annotate only tokens belonging to our ontology and label all other tokens as ‘OTHER’. This is because, as reported in Ref. <sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, for BERT-based sequence labeling models, the advantage offered by explicit BIO tags is negligible and IO tagging schemes suffice. More detailed annotation guidelines are provided in Supplementary Methods <xref ref-type="supplementary-material" rid="MOESM1">1</xref>. The corpus of papers described previously was filtered to obtain a data set of abstracts that were polymer relevant and likely to contain the entity types of interest to us. We did so by filtering abstracts containing the string ‘poly’ to find polymer-relevant abstracts and using regular expressions to find abstracts that contained numeric information.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Description of each entity type in the ontology used for annotating PolymerAbstracts.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Entity type</p></th><th><p>Description</p></th><th><p>Total occurrences</p></th></tr></thead><tbody><tr><td><p>POLYMER</p></td><td><p>Material entities that are polymers</p></td><td><p>7364</p></td></tr><tr><td><p>ORGANIC_MATERIAL</p></td><td><p>Material entities that are organic but not polymers. Typically used as plasticizers or cross-linking agents</p></td><td><p>914</p></td></tr><tr><td><p>MONOMER</p></td><td><p>Material entities which are explicitly indicated as being the repeat units for a POLYMER entity</p></td><td><p>2074</p></td></tr><tr><td><p>POLYMER_CLASS</p></td><td><p>Material entities that don’t refer to a specific chemical substance but are broad terms used for a class of polymers</p></td><td><p>1476</p></td></tr><tr><td><p>INORGANIC_MATERIAL</p></td><td><p>Material entities which are inorganic and are typically used as additives in a polymer formulation</p></td><td><p>1272</p></td></tr><tr><td><p>MATERIAL_AMOUNT</p></td><td><p>Entity type indicating the amount of a particular material in a material formulation</p></td><td><p>1143</p></td></tr><tr><td><p>PROPERTY_NAME</p></td><td><p>Entity type for a material property</p></td><td><p>4535</p></td></tr><tr><td><p>PROPERTY_VALUE</p></td><td><p>Entity type including a numeric value and its unit corresponding to a material property</p></td><td><p>5800</p></td></tr><tr><td><p>OTHER</p></td><td><p>Default entity type used for all tokens that do not lie in any of the above categories</p></td><td><p>147,115</p></td></tr></tbody></table><table-wrap-foot><p>Total occurrences here refers to the number of occurrences of each entity type in PolymerAbstracts.</p></table-wrap-foot></table-wrap></p><p id="Par9">Using the above-described ontology, we annotated 750 abstracts and split the abstracts into 85% for training, 5% for validation, and 10% for testing. Prior to manual annotation, we automatically pre-annotated the abstracts using dictionaries of entities for the entity types where one was available<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>. This was intended to speed up the annotation process. This data set was annotated by three domain experts using the tool Prodigy (<ext-link xlink:href="https://prodi.gy" ext-link-type="uri">https://prodi.gy</ext-link>). Annotation was done over three rounds using a small sample of abstracts in each round. With each round, the annotation guidelines were refined and the abstracts in the previous rounds were re-annotated using the refined guidelines. We refer to this data set as PolymerAbstracts.</p><p id="Par10">In order to assess the inter-annotator agreement between the three annotators, 10 abstracts were annotated by all the annotators to measure the Cohen’s Kappa and Fleiss Kappa<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> metrics. The Fleiss Kappa metric was computed to be 0.885 and the pairwise Cohen’s Kappa metric to be (0.906, 0.864, 0.887) for each of the three pairs of annotators. These metrics are comparable to inter-annotator agreements reported elsewhere in the literature<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and indicate good homogeneity in the annotations.</p></sec><sec id="Sec4"><title>NER model</title><p id="Par11">The architecture used for training our NER model is depicted in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. BERT and BERT-based models have become the de-facto solutions for a large number of NLP tasks<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. It embodies the transfer learning paradigm in which a language model is trained on a large amount of unlabeled text using unsupervised objectives (not shown in Fig. <xref rid="Fig2" ref-type="fig">2)</xref> and then reused for other NLP tasks. The resulting BERT encoder can be used to generate token embeddings for the input text that are conditioned on all other input tokens and hence are context-aware. We used a BERT-based encoder to generate representations for tokens in the input text as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The generated representations were used as inputs to a linear layer connected to a softmax non-linearity that predicted the probability of the entity type of each token. The cross-entropy loss was used during training to learn the entity types and on the test set, the highest probability label was taken to be the predicted entity type for a given input token. Dropout was used in the linear layer with a dropout probability of 0.2. The BERT model has an input sequence length limit of 512 tokens and most abstracts fall within this limit. Sequences longer than this length were truncated to 512 tokens as per standard practice<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. We used a number of different encoders and compared the performance of the resulting models on PolymerAbstracts. We compared these models for a number of different publicly available materials science data sets as well. All experiments were performed by us and the training and evaluation setting was identical across the encoders tested, for each data set.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Model architecture used for named entity recognition.</title><p>Each token in the input sequence is converted to a contextual embedding by a BERT-based encoder which is then input to a single-layer neural network. The output of the neural network is the entity type of the input token.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig2_HTML.png"/></fig></p></sec><sec id="Sec5"><title>Evaluation methods</title><p id="Par12">The performance of the NER model is evaluated using precision, recall, and F1 score of the predicted entity tag compared to the ground truth labels. These are defined below:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">Precision</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">FP</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">Recall</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">TP</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">FN</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="normal">F1</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Precision</mml:mi><mml:mo>×</mml:mo><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Precision</mml:mi><mml:mspace width="0.25em"/><mml:mo>+</mml:mo><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Recall</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{{{\rm{Precision}}}}\,=\,\frac{{{{\rm{TP}}}}}{{{{\rm{TP}}}}\,+\,{{{\rm{FP}}}}}\\ {{{\rm{Recall}}}}\,=\,\frac{{{{\rm{TP}}}}}{{{{\rm{TP}}}}\,+\,{{{\rm{FN}}}}}\\ {{{\rm{F1}}}}\,=\,\frac{2\times {{{\rm{Precision}}}}\times {{{\rm{Recall}}}}}{{{{\rm{Precision}}}}\,+\,{{{\rm{Recall}}}}}\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1003_Article_Equ1.gif"/></alternatives></disp-formula>where TP are the true positives, FP are the false positives and FN are the false negatives. Each of the above metrics is reported as a % value. We consider a predicted label to be a true positive only when the label of a complete entity is predicted correctly. For instance, for the polymer ‘polyvinyl ethylene’, both ‘polyvinyl’ and ‘ethylene’ must be correctly labeled as a POLYMER, else the entity is deemed to be predicted incorrectly.</p></sec><sec id="Sec6"><title>NER model performance</title><p id="Par13">The performance of various BERT-based language models tested for training an NER model on PolymerAbstracts is shown in Table <xref rid="Tab2" ref-type="table">2</xref>. We observe that MaterialsBERT, the model fine-tuned by us on 2.4 million materials science abstracts using PubMedBERT as the starting point, outperforms PubMedBERT as well as other language models used. This is in agreement with previously reported results where the fine-tuning of a BERT-based language model on a domain-specific corpus resulted in improved downstream task performance<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. Similar trends are observed across two of the four materials science data sets as reported in Table <xref rid="Tab3" ref-type="table">3</xref> and thus MaterialsBERT outperforms other BERT-based language models in three out of five materials science data sets. These NER datasets were chosen to span a range of subdomains within materials science, i.e., across organic and inorganic materials. A more detailed description of these NER datasets is provided in Supplementary Methods <xref ref-type="supplementary-material" rid="MOESM1">2</xref>. All encoders tested in Table <xref rid="Tab2" ref-type="table">2</xref> used the BERT-base architecture, differing in the value of their weights but having the same number of parameters and hence are comparable. MaterialsBERT outperforms PubMedBERT on all datasets except ChemDNER, which demonstrates that fine-tuning on a domain-specific corpus indeed produces a performance improvement on sequence labeling tasks. ChemBERT<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> is BERT-base fine-tuned on a corpus of ~400,000 organic chemistry papers and also out-performs BERT-base<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> across the NER data sets tested. BioBERT<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> was trained by fine-tuning BERT-base using the PubMed corpus and thus has the same vocabulary as BERT-base in contrast to PubMedBERT which has a vocabulary specific to the biomedical domain. Ref. <sup><xref ref-type="bibr" rid="CR28">28</xref></sup> describes the model MatBERT which was pre-trained from scratch using a corpus of 2 million materials science articles. Despite MatBERT being a model that was pre-trained from scratch, MaterialsBERT outperforms MatBERT on three out of five datasets. While the vocabulary of MatBERT and MaterialsBERT are both relevant to the domain of materials science, this performance difference can likely be attributed to the fact that PubMedBERT, the initial model for MaterialsBERT was pre-trained on a much larger corpus of text (14 million abstracts and full text). All experiments shown in Tables <xref rid="Tab2" ref-type="table">2</xref> and <xref rid="Tab3" ref-type="table">3</xref> were performed by us. We did not test BiLSTM-based architectures<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> as past work has shown that BERT-based architectures typically outperform BiLSTM-based ones<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. The performance of MaterialsBERT for each entity type in our ontology is described in Supplementary Discussion <xref ref-type="supplementary-material" rid="MOESM1">1</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Performance of various BERT-based encoders on the test set of PolymerAbstracts.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Model</p></th><th><p>Precision</p></th><th><p>Recall</p></th><th><p>F1</p></th></tr></thead><tbody><tr><td><p>MaterialsBERT (ours)</p></td><td><p>62.5</p></td><td><p>70.6</p></td><td><p><bold>66.4</bold></p></td></tr><tr><td><p>PubMedBERT</p></td><td><p>61.4</p></td><td><p>70.7</p></td><td><p>65.8</p></td></tr><tr><td><p>MatBERT</p></td><td><p>60.9</p></td><td><p>70.1</p></td><td><p>65.2</p></td></tr><tr><td><p>BioBERT</p></td><td><p>59.2</p></td><td><p>66.3</p></td><td><p>62.6</p></td></tr><tr><td><p>ChemBERT</p></td><td><p>52.2</p></td><td><p>62.6</p></td><td><p>57.0</p></td></tr><tr><td><p>BERT-base</p></td><td><p>52.1</p></td><td><p>61.0</p></td><td><p>56.2</p></td></tr></tbody></table><table-wrap-foot><p>Values are reported in %.</p><p>MaterialsBERT has the highest F1 score (shown in bold).</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Performance of different BERT-based encoders on the test sets of publicly available materials science NER datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>BERT-based encoder</p></th><th colspan="3"><p>ChemDNER<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></p></th><th colspan="3"><p>Inorganic Synthesis recipes<sup><xref ref-type="bibr" rid="CR71">71</xref></sup></p></th><th colspan="3"><p>Inorganic Abstracts<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></p></th><th colspan="3"><p>ChemRxnExtractor<sup><xref ref-type="bibr" rid="CR23">23</xref></sup></p></th></tr><tr><th/><th><p>P</p></th><th><p>R</p></th><th><p>F1</p></th><th><p>P</p></th><th><p>R</p></th><th><p>F1</p></th><th><p>P</p></th><th><p>R</p></th><th><p>F1</p></th><th><p>P</p></th><th><p>R</p></th><th><p>F1</p></th></tr></thead><tbody><tr><td><p>MaterialsBERT (ours)</p></td><td><p>70.1</p></td><td><p>68.2</p></td><td><p>69.2</p></td><td><p>69.1</p></td><td><p>68.3</p></td><td><p><bold>68.6</bold></p></td><td><p>85.3</p></td><td><p>86.7</p></td><td><p>86.0</p></td><td><p>73.5</p></td><td><p>69.5</p></td><td><p><bold>71.4</bold></p></td></tr><tr><td><p>PubMedBERT</p></td><td><p>71.5</p></td><td><p>69.0</p></td><td><p><bold>70.2</bold></p></td><td><p>69.9</p></td><td><p>65.3</p></td><td><p>67.6</p></td><td><p>84.0</p></td><td><p>86.2</p></td><td><p>85.0</p></td><td><p>68.1</p></td><td><p>59.5</p></td><td><p>63.6</p></td></tr><tr><td><p>MatBERT</p></td><td><p>71.7</p></td><td><p>66.9</p></td><td><p>69.2</p></td><td><p>68.6</p></td><td><p>67.7</p></td><td><p>68.2</p></td><td><p>85.6</p></td><td><p>86.7</p></td><td><p>86.2</p></td><td><p>67.4</p></td><td><p>58.0</p></td><td><p>62.4</p></td></tr><tr><td><p>BioBERT</p></td><td><p>70.6</p></td><td><p>65.7</p></td><td><p>68.0</p></td><td><p>64.4</p></td><td><p>63.7</p></td><td><p>64.0</p></td><td><p>85.6</p></td><td><p>87.1</p></td><td><p><bold>86.4</bold></p></td><td><p>74.8</p></td><td><p>65.4</p></td><td><p>69.8</p></td></tr><tr><td><p>ChemBERT</p></td><td><p>72.5</p></td><td><p>66.4</p></td><td><p>69.4</p></td><td><p>66.8</p></td><td><p>64.3</p></td><td><p>65.6</p></td><td><p>83.2</p></td><td><p>86.4</p></td><td><p>84.8</p></td><td><p>65.0</p></td><td><p>64.0</p></td><td><p>64.4</p></td></tr><tr><td><p>BERT-base</p></td><td><p>71.2</p></td><td><p>65.7</p></td><td><p>68.4</p></td><td><p>62.4</p></td><td><p>60.3</p></td><td><p>61.4</p></td><td><p>81.0</p></td><td><p>81.9</p></td><td><p>81.4</p></td><td><p>57.7</p></td><td><p>54.9</p></td><td><p>56.2</p></td></tr></tbody></table><table-wrap-foot><p>Values are reported in %.</p><p>The encoders with the highest F1 score for each dataset tested are shown in bold.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec7"><title>Quantifying the extracted data</title><p id="Par14">Using our pipeline, we extracted ~300,000 material property records from ~130,000 abstracts. Out of our corpus of 2.4 million articles, ~650,000 abstracts are polymer relevant and around ~130,000 out of those contain material property data. This extraction process took 60 hours using a single Quadro 16 GB GPU. To place this number in context, PoLyInfo a comparable database of polymer property records that is publicly available has 492,645 property records as of this writing<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. This database was manually curated by domain experts over many years while the material property records we have extracted using automated methods took 2.5 days using only abstracts and is yet of comparable size. However, the curation of datasets is not eliminated by automated extraction as we will still need domain experts to carefully curate text-mined data sets but these methods can dramatically reduce the amount of work needed. It is easier to flag bad entries in a structured format than to manually parse and enter data from natural language. The composition of these material property records is summarized in Table <xref rid="Tab4" ref-type="table">4</xref> for specific properties (grouped into a few property classes) that are utilized later in this paper. For the general property class, we computed the number of neat polymers as the material property records corresponding to a single material of the POLYMER entity type. Blends correspond to material property records with multiple POLYMER entities while composites contain at least one material entity that is not of the POLYMER or POLYMER_CLASS entity type. To compute the number of unique neat polymer records, we first counted all unique normalized polymer names from records that had a normalized polymer name. This accounts for the majority of polymers with multiple reported names as detailed in Ref. <sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Out of the remaining neat polymer records that did not have a normalized polymer name, we then counted all unique polymer names (accounting for case variations) and added them to the number of unique normalized polymer names to arrive at the estimated number of unique polymers. For the general property class, we note that elongation at break data for an estimated 413 unique neat polymers was extracted. In contrast, Ref. <sup><xref ref-type="bibr" rid="CR32">32</xref></sup> used 77 polymers to train a machine learning model. For tensile strength, an estimated 926 unique neat polymer data points were extracted while Ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup> used 672 data points to train a machine learning model. Thus the amount of data extracted in the aforementioned cases by our pipeline is already comparable to or greater than the amount of data being utilized to train property predictors in the literature. Table <xref rid="Tab4" ref-type="table">4</xref> accounts for only 39207 data points which is 13% of the total extracted material property records. More details on the extracted material property records can be found in Supplementary Discussion <xref ref-type="supplementary-material" rid="MOESM1">2</xref>. The reader is also encouraged to explore this data further through <ext-link xlink:href="https://polymerscholar.org" ext-link-type="uri">polymerscholar.org</ext-link>.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Number of material property records extracted for several key polymer properties and figures of merit for certain applications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Property class</p></th><th><p>Property</p></th><th><p>Total number of datapoints</p></th><th><p>neat polymers/ blends/ composites</p></th><th><p>Estimated number of unique neat polymers</p></th></tr></thead><tbody><tr><td><p>General</p></td><td><p>Molecular Weight</p></td><td><p>9053</p></td><td><p>9053/-/-</p></td><td><p>2623</p></td></tr><tr><td/><td><p>Glass Transition Temperature</p></td><td><p>6155</p></td><td><p>4612/1036/507</p></td><td><p>1732</p></td></tr><tr><td/><td><p>Electrical conductivity</p></td><td><p>6030</p></td><td><p>3202/606/2222</p></td><td><p>1017</p></td></tr><tr><td/><td><p>Tensile Strength</p></td><td><p>4382</p></td><td><p>2679/651/1052</p></td><td><p>926</p></td></tr><tr><td/><td><p>Elongation at Break</p></td><td><p>1499</p></td><td><p>954/234/311</p></td><td><p>413</p></td></tr><tr><td><p>Polymer Solar Cells</p></td><td><p>Power Conversion Efficiency</p></td><td><p>3595</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Open Circuit Voltage</p></td><td><p>1386</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Short Circuit Current</p></td><td><p>1049</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Fill Factor</p></td><td><p>966</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td><p>Fuel Cells</p></td><td><p>Proton conductivity</p></td><td><p>1359</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Areal Power Density</p></td><td><p>1235</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Areal Current Density</p></td><td><p>295</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Methanol permeability</p></td><td><p>174</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td><p>Supercapacitors</p></td><td><p>Gravimetric Energy Density</p></td><td><p>1131</p></td><td><p>–</p></td><td><p>–</p></td></tr><tr><td/><td><p>Gravimetric Power Density</p></td><td><p>898</p></td><td><p>–</p></td><td><p>–</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec8"><title>General property class</title><p id="Par15">We now analyze the properties extracted class-by-class in order to study their qualitative trend. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows property data extracted for the five most common polymer classes in our corpus (columns) and the four most commonly reported properties (rows). Polymer classes are groups of polymers that share certain chemical attributes such as functional groups. The properties reported in Fig. <xref rid="Fig3" ref-type="fig">3</xref> fall under the general property class described in Table <xref rid="Tab4" ref-type="table">4</xref>. The material property data in Fig. <xref rid="Fig3" ref-type="fig">3</xref> corresponds to cases when a polymer of a particular polymer class is part of the formulation for which a property is reported and does not necessarily correspond to homopolymers but instead could correspond to blends or composites. The polymer class is “inferred” through the POLYMER_CLASS entity type in our ontology and hence must be mentioned explicitly for the material property record to be part of this plot. Several key trends are captured in this plot. From the glass transition temperature (<italic>T</italic><sub>g</sub>) row, we observe that polyamides and polyimides typically have higher <italic>T</italic><sub>g</sub> than other polymer classes. Molecular weights unlike the other properties reported are not intrinsic material properties but are determined by processing parameters. The reported molecular weights are far more frequent at lower molecular weights than at higher molecular weights; mimicking a power-law distribution rather than a Gaussian distribution. This is consistent with longer chains being more difficult to synthesize than shorter chains. For electrical conductivity, we find that polyimides have much lower reported values which is consistent with them being widely used as electrical insulators. Also note that polyimides have higher tensile strengths as compared to other polymer classes, which is a well-known property of polyimides<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Material property data extracted from abstracts for material systems that contain a polymer from the polymer classes of polyurethane, polyimide, polyamide, polyester, and polysiloxane in each corresponding column.</title><p>These are the most commonly reported polymer classes and the properties reported are the most commonly reported properties in our corpus of papers.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig3_HTML.png"/></fig></p><p id="Par16">Figure <xref rid="Fig4" ref-type="fig">4</xref> shows mechanical properties measured for films which demonstrates the trade-off between elongation at break and tensile strength that is well known for materials systems (often called the strength-ductility trade-off dilemma). Materials with high tensile strength tend to have a low elongation at break and conversely, materials with high elongation at break tend to have low tensile strength<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. This known fact about the physics of material systems emerges from an amalgamation of data points independently gathered from different papers. In the next section, we take a closer look at pairs of properties for various devices that reveal similarly interesting trends.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Tensile Strength Vs Elongation at break for films demonstrating the strength-ductility trade-off.</title><p>Materials with high elongation at break demonstrate lower tensile strength and conversely, those with high tensile strength have lower elongation at break.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig4_HTML.png"/></fig></p></sec><sec id="Sec9"><title>Knowledge extraction</title><p id="Par17">Next, we consider a few device applications and co-relations between the most important properties reported for these applications to demonstrate that non-trivial insights can be obtained by analyzing this data. We consider three device classes namely polymer solar cells, fuel cells, and supercapacitors, and show that their known physics is being reproduced by NLP-extracted data. We find documents specific to these applications by looking for relevant keywords in the abstract such as ‘polymer solar cell’ or ‘fuel cell’. The total number of data points for key figures of merit for each of these applications is given in Table <xref rid="Tab4" ref-type="table">4</xref>. The number of extracted data points reported in Table <xref rid="Tab4" ref-type="table">4</xref> is higher than that in Fig. <xref rid="Fig5" ref-type="fig">5</xref> and Fig. <xref rid="Fig6" ref-type="fig">6</xref> as additional constraints are imposed in the latter cases to better study this data.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Correlations between key properties extracted automatically from literature for polymer solar cells.</title><p><bold>a</bold> Power conversion efficiency against short circuit current <bold>b</bold> Power conversion efficiency against fill factor <bold>c</bold> Power conversion efficiency against open circuit voltage. Correlations between key properties extracted manually from literature for polymer solar cells <bold>d</bold> Power conversion efficiency against short circuit current <bold>e</bold> Power conversion efficiency against fill factor <bold>f</bold> Power conversion efficiency against open circuit voltage. Figure 5<bold>d</bold>–<bold>f</bold> is adapted here with permission from Ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Copyright 2018 American Chemical Society.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig5_HTML.png"/></fig><fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>Correlations between key properties extracted automatically from literature for three different applications.</title><p><bold>a</bold> Areal current density Vs Areal power density for fuel cells. the slope of the best-fit line has a slope of 0.42 V which is the typical operating voltage of a fuel cell <bold>b</bold> Proton conductivity vs. Methanol permeability for fuel cells. The red box shows the desirable region of the property space <bold>c</bold> Up-to-date Ragone plot for supercapacitors showing energy density Vs power density. <bold>d</bold> lower conversion efficiency against time for fullerene acceptors and <bold>e</bold> Power conversion efficiency against time for non-fullerene acceptors <bold>f</bold> Trend of the number of data points extracted by our pipeline over time. The dashed lines represent the number of papers published for each of the three applications in the plot and correspond to the dashed Y-axis.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1003_Fig6_HTML.png"/></fig></p><sec id="Sec10"><title>Polymer solar cells</title><p id="Par18">Polymer solar cells, in contrast to conventional silicon-based solar cells, have the benefit of lower processing costs but suffer from lower power conversion efficiencies. Improving their power conversion efficiency by varying the materials used in the active layer of the cell is an active area of research<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>. Figure <xref rid="Fig5" ref-type="fig">5</xref>a–c shows the power conversion efficiency for polymer solar cells plotted against the corresponding short circuit current, fill factor, and open circuit voltage for NLP extracted data while Fig. <xref rid="Fig5" ref-type="fig">5</xref>d–f shows the same pairs of properties for data extracted manually as reported in Ref. <sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Each data point in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a–c is taken from a particular paper and corresponds to a single material system. It is clear from Fig. <xref rid="Fig5" ref-type="fig">5</xref>c that the peak power conversion efficiencies reported are around 16.71% which is close to the maximum known values reported in the literature<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> as of this writing. The open-circuit voltages (OCV) appear to be Gaussian distributed at around 0.85 V. Figure <xref rid="Fig5" ref-type="fig">5</xref>a) shows a linear trend between short circuit current and power conversion efficiency. It is clear that the trends observed in Fig. <xref rid="Fig5" ref-type="fig">5</xref>a–c for NLP extracted data are quite similar to the trends observed from manually curated data in Fig. <xref rid="Fig5" ref-type="fig">5</xref>d–f.</p></sec><sec id="Sec11"><title>Fuel cells</title><p id="Par19">Fuel cells are devices that convert a stream of fuel such as methanol or hydrogen and oxygen to electricity. Water is one of the primary by-products of this conversion making this a clean source of energy. A polymer membrane is typically used as a separating membrane between the anode and cathode in fuel cells<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Improving the proton conductivity and thermal stability of this membrane to produce fuel cells with higher power density is an active area of research. Figure <xref rid="Fig6" ref-type="fig">6</xref>a and b show plots for fuel cells comparing pairs of key performance metrics. The points on the power density versus current density plot (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a)) lie along the line with a slope of 0.42 V which is the typical operating voltage of a fuel cell under maximum current densities<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Each point in this plot corresponds to a fuel cell system extracted from the literature that typically reports variations in material composition in the polymer membrane. Figure <xref rid="Fig6" ref-type="fig">6</xref>b illustrates yet another use-case of this capability, i.e., to find material systems lying in a desirable range of property values for the more specific case of direct methanol fuel cells. For such fuel cell membranes, low methanol permeability is desirable in order to prevent the methanol from crossing the membrane and poisoning the cathode<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. High proton conductivity is simultaneously desirable. The box shown in the figure illustrates the desirable region and can thus be used to easily locate promising material systems.</p></sec><sec id="Sec12"><title>Trends across time</title><p id="Par20">We show that known trends across time in polymer literature are also reproduced in our extracted data. A Ragone plot illustrates the trade-off between energy and power density for devices. Supercapacitors are a class of devices that have high power density but low energy density. Figure <xref rid="Fig6" ref-type="fig">6</xref>c illustrates the trade-off between gravimetric energy density and gravimetric power density for supercapacitors and is effectively an up-to-date version of the Ragone plot for supercapacitors<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Historically, in most Ragone plots, the energy density of supercapacitors ranges from 1 to 10 Wh/kg<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. However, this is no longer true as several recent papers have demonstrated energy densities of up to 100 Wh/kg<sup><xref ref-type="bibr" rid="CR44">44</xref>–<xref ref-type="bibr" rid="CR46">46</xref></sup>. As seen in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c, the majority of points beyond an energy density of 10 Wh/kg are from the previous two years, i.e., 2020 and 2021.</p><p id="Par21">Figure <xref rid="Fig6" ref-type="fig">6</xref>d and e show the evolution of the power conversion efficiency of polymer solar cells for fullerene acceptors and non-fullerene acceptors respectively. These are the two major classes of acceptors used in polymer solar cells. An acceptor along with a polymer donor forms the active layer of a bulk heterojunction polymer solar cell. Observe that more papers with fullerene acceptors are found in earlier years with the number dropping in recent years while non-fullerene acceptor-based papers have become more numerous with time. They also exhibit higher power conversion efficiencies than their fullerene counterparts in recent years. This is a known trend within the domain of polymer solar cells reported in Ref. <sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. It is worth noting that the authors realized this trend by studying the NLP extracted data and then looking for references to corroborate this observation.</p><p id="Par22">Figure <xref rid="Fig6" ref-type="fig">6</xref>f shows the number of data points extracted by our pipeline over time for the various categories described in Table <xref rid="Tab4" ref-type="table">4</xref>. Observe that the number of data points of the general category has grown exponentially at the rate of 6% per year. Out of the three applications considered in Fig. <xref rid="Fig6" ref-type="fig">6</xref>f, polymer solar cells have historically had the largest number of papers as well as data points, although that appears to be declining over the past few years. Observe that there is a decline in the number of data points as well as the number of papers in 2020 and 2021. This is likely attributable to the COVID-19 pandemic<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> which appears to have led to a drop in the number of experimental papers published that form the input to our pipeline<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>.</p></sec></sec></sec><sec id="Sec13" sec-type="discussion"><title>Discussion</title><p id="Par23">A natural language processing pipeline that extracts material property records from abstracts has been built and demonstrated. This however has some limitations in practice that we describe below:<list list-type="order"><list-item><p id="Par24">Material property information is multi-modal and can be found in the text, tables, and figures of the paper. Co-referencing material entity mentions across large spans of text and across figures and tables is a challenging problem. In addition to this, relation extraction of material entities and property value pairs occurring across sentences, are challenges that need to be addressed when extending this work from abstracts to full-text.</p></list-item><list-item><p id="Par25">The ontology used in this work consists of the most important entity types found in materials science literature. This makes it easier to combine material and property information using heuristic rules but misses other information about the material property record such as processing conditions, measurement methods, or measurement conditions which in most cases would influence the property value. This work can be extended to include this metadata by extending the ontology to other entities that would influence the measured property values for any given domain by explicitly labeling those entities and training a new NER model. This approach would however be time-intensive. Another approach would be to use a combination of fully supervised and weakly supervised approaches<sup><xref ref-type="bibr" rid="CR50">50</xref>–<xref ref-type="bibr" rid="CR52">52</xref></sup>. The ontology used in this work can be used to extract entities like material names as well as property names and values through supervised NER. Our ontology is relevant across all sub-domains of materials science and additional information relevant to a specific sub-domain can be obtained through heuristic rules and regular expressions which would be unsupervised.</p></list-item><list-item><p id="Par26">Converting polymer names to a structure (typically a SMILES string<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>) is also a bottleneck to training property predictor models as this must be done manually. While tools have been developed to handle molecule images<sup><xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>, reliably and robustly converting images of polymer structures typically found in the literature to SMILES strings is an area of future work for the community. The SMILES string so generated can be used to generate a structural fingerprint vector of the polymer which in turn can serve as the input to a machine learning model. Expanding the scope of this pipeline to figures in the paper would allow training property predictor models without any additional curation for converting images to SMILES strings. Training robust property predictors in this manner would in turn allow the continuous and semi-automatic design of new materials, thus addressing a missing link in materials informatics. An example of manually converting polymer names to SMILES strings followed by the training of a property prediction model for glass transition temperature is shown in Supplementary Discussion <xref ref-type="supplementary-material" rid="MOESM1">3</xref>.</p></list-item></list></p><p id="Par27">The automated extraction of material property records enables researchers to search through literature with greater granularity and find material systems in the property range of interest. It also enables insights to be inferred by analyzing large amounts of literature that would not otherwise be possible. As shown in the section “Knowledge extraction”, a diverse range of applications were analyzed using this pipeline to reveal non-trivial albeit known insights. This work built a general-purpose capability to extract material property records from published literature. ~300,000 material property records were extracted from ~130,000 polymer abstracts using this capability. Through our web interface (<ext-link xlink:href="https://polymerscholar.org" ext-link-type="uri">polymerscholar.org</ext-link>) the community can conveniently locate material property data published in abstracts. As part of this work, we also train and release MaterialsBERT, a language model that is fine-tuned on 2.4 million materials science abstracts using PubMedBERT as the starting point and obtains the best F1 score across three of five materials science NER data sets tested.</p><p id="Par28">Growing the extracted material property data set further would require extending this capability to the body of the paper. This would require more robust methods to associate the entities extracted using named entity recognition. A few steps also remain in order to utilize the extracted data to produce trained machine learning property prediction models. The biggest bottleneck in the case of organic materials is obtaining SMILES strings for material entities which can then be used to generate structural fingerprints for property predictor machine learning models. There is also a wealth of additional information such as processing conditions or measurement conditions that are not captured in our ontology. Addressing these bottlenecks would enable automatic and continuous updates of materials databases that can seamlessly power property predictor machine learning models<sup><xref ref-type="bibr" rid="CR55">55</xref>,<xref ref-type="bibr" rid="CR56">56</xref></sup>.</p></sec><sec id="Sec14" sec-type="methods"><title>Methods</title><sec id="Sec15"><title>Corpus of papers</title><p id="Par29">We have created a corpus of ~2.4 million journal articles from the materials science domain. These papers were downloaded from the APIs and websites of publishers such as Elsevier, Wiley, Royal Society of Chemistry, American Chemical Society, Springer Nature, Taylor &amp; Francis, and the American Institute of Physics. The corpus used in this work is an expanded version of the corpus described previously in Ref. <sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. 750 abstracts of this corpus were annotated and used to train an NER model. Furthermore, the trained NER model along with heuristic rules was used to extract material property records from the abstracts of the full corpus.</p></sec><sec id="Sec16"><title>Preprocessing of documents</title><p id="Par30">Because the documents in our corpus are HTML formatted, we stripped all HTML tags to parse the plain text. Moreover, we replaced HTML superscripts and subscripts (&lt;sup&gt; and &lt;sub&gt;) with plain text using the LaTeX convention of ^{} and _ {}, respectively. This is important in order to extract units of quantities as well as property values reported in scientific notation. Property values recorded in this notation were converted back to floating-point numbers downstream when the numeric value was to be recovered. We also mapped characters such as spaces or special characters that have multiple Unicode representations but have a similar appearance by creating a custom mapping.</p></sec><sec id="Sec17"><title>Tokenization</title><p id="Par31">For tokenization, i.e., breaking up text into units known as tokens which are used for downstream processing, we used wordpiece tokenization which is the standard tokenization scheme used with BERT and BERT-based models<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>. For instance ‘The molecular weight of the result ##ant P ##LL ##A - rich polymer was enhanced .’ is what a sentence would look like post-tokenization. The word ‘resultant’ and the polymer ‘PLLA’ have been broken into sub-word tokens. This is necessary in order to tokenize arbitrary text inputs using a fixed-sized vocabulary as a larger vocabulary would increase the size of the model. Starting with a set of characters (alphabets, numbers, etc), certain combinations of characters are iteratively merged and added to the vocabulary till the vocabulary reaches a certain fixed size<sup><xref ref-type="bibr" rid="CR58">58</xref></sup>. The characters to be merged are selected based on combinations that maximize the likelihood of the input text such that the most frequently occurring sequences of text in the corpus are included in the vocabulary. This typically breaks up words into meaningful subunits such as ‘resultant’ being separated into ‘result’ and ‘##ant’ which reduces the size of the vocabulary. This does not always happen though, as seen with the example of ‘PLLA’. The embedding associated with the first subword for each word is used as the input to the NER model in accordance with conventional practice<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. Thus, only the label predicted for the first subword is used for evaluating the model predictions.</p></sec><sec id="Sec18"><title>NER model training</title><p id="Par32">We used the Adam optimizer with an initial learning rate of 5 × 10<sup>−5</sup> which was linearly damped to train the model<sup><xref ref-type="bibr" rid="CR59">59</xref></sup>. We used early stopping while training the NER model, i.e., the number of epochs of training was determined by the peak F1 score of the model on the validation set as evaluated after every epoch of training<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>. During, this stage, also referred to as ‘fine-tuning’ the model, all the weights of the BERT-based encoder and the linear classifier are updated.</p></sec><sec id="Sec19"><title>Training MaterialsBERT</title><p id="Par33">BERT-base, the original BERT model, was trained using an unlabeled corpus that included English Wikipedia and the Books Corpus<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. The training objectives included using the masked language modeling task, which masks a random subset of the input text and asks the language model to predict it, and the next sentence prediction task which determines for a given sentence pair whether one sentence follows the other in the training data<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The vocabulary of the tokenizer was fixed at 30,000 tokens. It is known that a domain-specific BERT encoder improves performance on NLP tasks for that domain because the vocabulary used for tokenization is more representative of the application of interest and because the unlabeled text is also closer to the domain of interest resulting in “better" contextual embeddings<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. BERT-base was pre-trained from scratch using a general English language corpus<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p><p id="Par34">Even though computationally expensive, pre-training NLP models from scratch has the advantage of creating a model with a vocabulary that is customized for the domain of interest. To give an idea of how resource-intensive this can be, note that RoBERTa, a similarly pre-trained encoder used the computing power of 1024 V100 GPUs for one day<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. As this is not a viable route for us, we fine-tuned a model starting from previous checkpoints. The vocabulary used while fine-tuning a model in contrast remains the same as the underlying model which is a compromise we must accept. We used PubMedBERT as our starting point and fine-tuned it using 2.4 million materials science abstracts<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. These abstracts were not restricted to polymers and span many different sub-domains of materials science. We restricted ourselves to abstracts because Ref. <sup><xref ref-type="bibr" rid="CR19">19</xref></sup> found that a language model pre-trained on abstracts only, outperforms a language model pre-trained on abstracts as well as full text when the downstream task only involved abstracts. The computational cost incurred if abstracts, as well as full text, were to be used would also be significantly higher. The PubMedBERT model used here is itself pre-trained from scratch using the PubMed corpus (14 million abstracts from PubMed as well as full-text articles from PubMedCentral), using the BERT-base architecture. We picked PubMedBERT as our starting point as its vocabulary is specific to the biomedical domain which overlaps with materials science as material entities are frequently mentioned in biomedical papers. During fine-tuning, the model weights of PubMedBERT were loaded and training was continued using the same training objectives used to pre-train PubMedBERT but using the unlabeled text from the fine-tuning corpus as the input. The hyperparameters used during fine-tuning were identical to those used to train PubMedBERT. We used the “Transformers" library for fine-tuning PubMedBERT<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. A similar strategy was employed in ChemBERT<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, ClinicalBert<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>, and FinBERT<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. We fine-tuned PubMedBERT for 3 epochs which took 90 hours on four RTX6000 16 GB GPUs to obtain MaterialsBERT.</p></sec><sec id="Sec20"><title>Material property records extraction</title><p id="Par35">The trained NER model is one component of our pipeline that is used to extract material property records. Each component is explained below (Refer Fig. <xref rid="Fig1" ref-type="fig">1)</xref>:<list list-type="order"><list-item><p id="Par36"><bold>Train NER model</bold>: A subset of our corpus of 2.4 million papers was selected and annotated with a given ontology to train an NER model (described in the Section “NER model”). This model was used to generate entity labels for abstracts in the corpus.</p></list-item><list-item><p id="Par37"><bold>Pick documents with ‘poly’</bold>: The corpus of 2.4 million abstracts was down-selected by searching for the string ‘poly’ in the abstract as a proxy for polymer-relevant documents.</p></list-item><list-item><p id="Par38"><bold>Run NER model</bold>: The NER model previously trained was used for predicting entity labels on each polymer-relevant document obtained from the previous step.</p></list-item><list-item><p id="Par39"><bold>Abstract filtering</bold>: As not all polymer abstracts contain material property information, the output of the NER was used as a heuristic to filter out those that do. Only abstracts with specific material entities, i.e., POLYMER, POLYMER_FAMILY, or MONOMER as well as the PROPERTY_NAME and PROPERTY_VALUE tags were allowed through this stage. This acts as a second filter to locate polymer-relevant documents.</p></list-item><list-item><p id="Par40"><bold>Entity extraction</bold>: The material entities, (PROPERTY_NAME, PROPERTY_VALUE) and MATERIAL_AMOUNT entities were extracted and processed separately.</p></list-item><list-item><p id="Par41"><bold>Co-reference material entities</bold>: This step was applied to co-reference all mentions of the same material entity. A common example of this is when a material is mentioned next to its abbreviation. We used the abbreviation detection system in ChemDataExtractor<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> to find material entity abbreviation pairs. In addition, we co-referenced material entities that were within a Levenshtein distance<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> of one. Co-referencing is a tractable problem in abstracts compared to the body of a paper as there are no long-range dependencies in the former and typically no anaphora resolution is required<sup><xref ref-type="bibr" rid="CR67">67</xref></sup>.</p></list-item><list-item><p id="Par42"><bold>Normalizing polymer names</bold>: Polymers can have several different variations in names referring to the same chemical entity. In this step, we normalized these variations to the most commonly occurring name for that particular polymer. For instance, ‘poly(ethylene)’ and ‘poly-ethylene’ occurring in different abstracts are both normalized to ‘polyethylene’. This is done using a dictionary lookup on a data set of polymer name clusters that were normalized using the workflow described in Ref. <sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Note that we do not normalize all polymer names but only the ones which are included in our dictionary. In practice, this includes the most commonly occurring polymers that have multiple names in the literature.</p></list-item><list-item><p id="Par43"><bold>Extract Property Value pairs</bold>: The PROPERTY_NAME and PROPERTY_VALUE tag were associated by co-occurrence within a context window. The numeric value of the property was separated from the units using regular expressions and all parsed property values were converted to a standard set of units. The unit used was the most commonly reported unit for that particular property. Any standard deviation reported with the numeric value was also parsed using regular expressions.</p></list-item><list-item><p id="Par44"><bold>Extract Material amounts</bold>: Entities with the MATERIAL_AMOUNT tag were extracted and the closest material entity within a context window was associated with it.</p></list-item><list-item><p id="Par45"><bold>Relation extraction</bold>: In order to obtain a material property record, it is necessary to associate the material entities and the property value pair that correspond to a single record. This problem has been addressed in the literature using supervised methods<sup><xref ref-type="bibr" rid="CR68">68</xref>,<xref ref-type="bibr" rid="CR69">69</xref></sup>. However, the annotation process for relation labeling is time-intensive and hence we employed heuristics in this work to obtain relations between entities. To associate material entities with property value pairs, we associated the closest material entity tagged in the same sentence as the property value pair. If no such material entity was found, then all the material entities mentioned in the abstract were associated with the property value pair. This is because most commonly, an abstract mentions a major material system described in the paper and reports its measured property values. This step is reasonable in abstracts, which report this information compactly. In contrast, the body of the paper would require coreferencing the entities in text, tables, and figures to extract material property records.</p></list-item></list></p></sec></sec></body><back><ack><title>Acknowledgements</title><p>This work was supported by the Office of Naval Research through grants N00014-19-1-2103 and N00014-20-1-2175. Helpful discussions and feedback from Dr. Lihua Chen are acknowledged. Pranav Shetty was partially funded by a fellowship by JPMorgan Chase &amp; Co. that helped to support this research. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase &amp; Co. or its affiliates.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>Conceptualization: P.S., C.Z., R.R., Methodology: P.S., Investigation: P.S., Data Curation: P.S., A.C.R., C.K., S.G., L.P.P., L.H., Visualization: P.S., Supervision: C.Z., R.R., Writing-original draft: P.S., Writing-review &amp; editing: P.S., C.K., C.Z., R.R.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The journal articles used to train MaterialsBERT and to extract material property data were downloaded through licensing arrangements that Georgia Tech has with Elsevier, Wiley, Royal Society of Chemistry, American Chemical Society, Springer Nature, Taylor &amp; Francis, and the American Institute of Physics. The pre-trained language model MaterialsBERT is available in the HuggingFace model zoo at <ext-link xlink:href="https://huggingface.co/pranav-s/MaterialsBERT" ext-link-type="uri">huggingface.co/pranav-s/MaterialsBERT</ext-link>. The DOIs of the journal articles used to train MaterialsBERT are also provided at the aforementioned link. The data set PolymerAbstracts can be found at <ext-link xlink:href="https://www.github.com/Ramprasad-Group/polymer_information_extraction" ext-link-type="uri">www.github.com/Ramprasad-Group/polymer_information_extraction</ext-link>. The material property data mentioned in this paper can be explored through <ext-link xlink:href="https://polymerscholar.org" ext-link-type="uri">polymerscholar.org</ext-link>.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The code used in this work can be found at <ext-link xlink:href="https://www.github.com/Ramprasad-Group/polymer_information_extraction" ext-link-type="uri">www.github.com/Ramprasad-Group/polymer_information_extraction</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing Interests</title><p id="Par46">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Kenton, J. D. M.-W. C. &amp; Toutanova, L. K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, vol. 1, p. 2 (2019).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. Adv Neural Inf Process Syst <bold>30</bold> (2017).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swain</surname><given-names>MC</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature</article-title><source>J. Chem. Inf. Model</source><year>2016</year><volume>56</volume><fpage>1894</fpage><lpage>1904</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsFKjsr%2FK</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.6b00207</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rocktäschel</surname><given-names>T</given-names></name><name><surname>Weidlich</surname><given-names>M</given-names></name><name><surname>Leser</surname><given-names>U</given-names></name></person-group><article-title xml:lang="en">Chemspot: a hybrid system for chemical named entity recognition</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>1633</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts183</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawizy</surname><given-names>L</given-names></name><name><surname>Jessop</surname><given-names>DM</given-names></name><name><surname>Adams</surname><given-names>N</given-names></name><name><surname>Murray-Rust</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Chemicaltagger: a tool for semantic text-mining in chemistry</article-title><source>J. Cheminformatics</source><year>2011</year><volume>3</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXmsV2hu7g%3D</pub-id><pub-id pub-id-type="doi">10.1186/1758-2946-3-17</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Court</surname><given-names>CJ</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Auto-generated materials database of curie and néel temperatures via semi-supervised relationship extraction</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.111</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Court</surname><given-names>CJ</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Inverse design of materials that exhibit the magnetocaloric effect by text-mining of the scientific literature and generative deep learning</article-title><source>Chem. Mater.</source><year>2021</year><volume>33</volume><fpage>7217</fpage><lpage>7231</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXitVaitL%2FM</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.1c01368</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tchoua</surname><given-names>RB</given-names></name><etal/></person-group><article-title xml:lang="en">Blending education and polymer science: semiautomated creation of a thermodynamic property database</article-title><source>J. Chem. Educ.</source><year>2016</year><volume>93</volume><fpage>1561</fpage><lpage>1568</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtlaktbvI</pub-id><pub-id pub-id-type="doi">10.1021/acs.jchemed.5b01032</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Tchoua, R. B. et al. <italic>Creating training data for scientific named entity recognition with minimal human effort</italic>, 398–411 (Springer, 2019).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>E</given-names></name><etal/></person-group><article-title xml:lang="en">Materials synthesis insights from scientific literature via text extraction and machine learning</article-title><source>Chem. Mater.</source><year>2017</year><volume>29</volume><fpage>9436</fpage><lpage>9444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs1Kqu7fJ</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.7b03500</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kononova</surname><given-names>O</given-names></name><etal/></person-group><article-title xml:lang="en">Text-mined dataset of inorganic materials synthesis recipes</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kemp</surname><given-names>N</given-names></name><name><surname>Lynch</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Extraction of information from the text of chemical patents. 1. identification of specific chemical names</article-title><source>J. Chem. Inf. Comput Sci.</source><year>1998</year><volume>38</volume><fpage>544</fpage><lpage>551</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK1cXkt1Whur0%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci980324v</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinger</surname><given-names>R</given-names></name><name><surname>Kolářik</surname><given-names>C</given-names></name><name><surname>Fluck</surname><given-names>J</given-names></name><name><surname>Hofmann-Apitius</surname><given-names>M</given-names></name><name><surname>Friedrich</surname><given-names>CM</given-names></name></person-group><article-title xml:lang="en">Detection of IUPAC and IUPAC-like chemical names</article-title><source>Bioinformatics</source><year>2008</year><volume>24</volume><fpage>i268</fpage><lpage>i276</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXnvV2rtLs%3D</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btn181</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weininger</surname><given-names>D</given-names></name><name><surname>Weininger</surname><given-names>A</given-names></name><name><surname>Weininger</surname><given-names>JL</given-names></name></person-group><article-title xml:lang="en">Smiles. 2. algorithm for generation of unique smiles notation</article-title><source>J. Chem. Inf. Comput Sci.</source><year>1989</year><volume>29</volume><fpage>97</fpage><lpage>101</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1MXitFWlt7s%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci00062a008</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Friedl, J. E. <italic>Mastering regular expressions</italic> (O’Reilly Media, Inc., 2006).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwalbe-Koda</surname><given-names>D</given-names></name><name><surname>Jensen</surname><given-names>Z</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Gómez-Bombarelli</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Graph similarity drives zeolite diffusionless transformations and intergrowth</article-title><source>Nat. Mater.</source><year>2019</year><volume>18</volume><fpage>1177</fpage><lpage>1181</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvFCntbbP</pub-id><pub-id pub-id-type="doi">10.1038/s41563-019-0486-1</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shetty</surname><given-names>P</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Automated knowledge extraction from polymer literature using natural language processing</article-title><source>iScience</source><year>2020</year><volume>24</volume><fpage>101922</fpage><pub-id pub-id-type="doi">10.1016/j.isci.2020.101922</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tshitoyan</surname><given-names>V</given-names></name><etal/></person-group><article-title xml:lang="en">Unsupervised word embeddings capture latent knowledge from materials science literature</article-title><source>Nature</source><year>2019</year><volume>571</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtlamurrK</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1335-8</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Domain-specific language model pretraining for biomedical natural language processing</article-title><source>ACM Trans. Comput. Healthc. (HEALTH)</source><year>2021</year><volume>3</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXitlGksbjO</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>T</given-names></name><name><surname>Zaki</surname><given-names>M</given-names></name><name><surname>Krishnan</surname><given-names>NA</given-names></name></person-group><article-title xml:lang="en">Matscibert: A materials domain language model for text mining and information extraction</article-title><source>Npj Comput. Mater.</source><year>2022</year><volume>8</volume><fpage>102</fpage><pub-id pub-id-type="doi">10.1038/s41524-022-00784-w</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshitake</surname><given-names>M</given-names></name><name><surname>Sato</surname><given-names>F</given-names></name><name><surname>Kawano</surname><given-names>H</given-names></name><name><surname>Teraoka</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Materialbert for natural language processing of materials science texts</article-title><source>Sci. Technol. Adv. Mater.</source><year>2022</year><volume>2</volume><fpage>372</fpage><lpage>380</lpage></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Biobert: a pre-trained biomedical language representation model for biomedical text mining</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><fpage>1234</fpage><lpage>1240</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhslCisLrL</pub-id><pub-id pub-id-type="doi">10.1093/bioinformatics/btz682</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Guo, J. et al. Automated chemical reaction extraction from scientific literature. <italic>J. Chem. Inf. Model.</italic><bold>62</bold>, 2035–2045 (2021).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weston</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Named entity recognition and normalization applied to large-scale information extraction from the materials science literature</article-title><source>J. Chem. Inf. Model</source><year>2019</year><volume>59</volume><fpage>3692</fpage><lpage>3702</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsVOhsL7M</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00470</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleiss</surname><given-names>JL</given-names></name></person-group><article-title xml:lang="en">Measuring nominal scale agreement among many raters</article-title><source>Psychol. Bull.</source><year>1971</year><volume>76</volume><fpage>378</fpage><pub-id pub-id-type="doi">10.1037/h0031619</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Tabassum, J., Lee, S., Xu, W. &amp; Ritter, A. Wnut-2020 task 1 overview: Extracting entities and relations from wet lab protocols. <italic>arXiv preprint arXiv:2010.14576.</italic><ext-link xlink:href="https://arxiv.org/abs/2010.14576" ext-link-type="uri">https://arxiv.org/abs/2010.14576</ext-link> (2020).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Beltagy, I., Peters, M. E. &amp; Cohan, A. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150. <ext-link xlink:href="https://arxiv.org/abs/2004.05150" ext-link-type="uri">https://arxiv.org/abs/2004.05150</ext-link> (2020).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trewartha</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Quantifying the advantage of domain-specific pre-training on named entity recognition tasks in materials science</article-title><source>Patterns</source><year>2022</year><volume>3</volume><fpage>100488</fpage><pub-id pub-id-type="doi">10.1016/j.patter.2022.100488</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Huang, Z., Xu, W. &amp; Yu, K. Bidirectional lstm-crf models for sequence tagging. <italic>arXiv preprint arXiv:1508.01991.</italic><ext-link xlink:href="https://arxiv.org/abs/1508.01991" ext-link-type="uri">https://arxiv.org/abs/1508.01991</ext-link> (2015).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Otsuka, S., Kuwajima, I., Hosoya, J., Xu, Y. &amp; Yamazaki, M. <italic>Polyinfo: Polymer database for polymeric materials design</italic>, In 2011 International Conference on Emerging Intelligent Data and Web Technologies, pp. 22–29 (2011).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shetty</surname><given-names>P</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Machine-guided polymer knowledge extraction using natural language processing: The example of named entity normalization</article-title><source>J. Chem. Inf. Model</source><year>2021</year><volume>61</volume><fpage>5377</fpage><lpage>5385</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXisVegu7%2FL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.1c00554</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palomba</surname><given-names>D</given-names></name><name><surname>Vazquez</surname><given-names>GE</given-names></name><name><surname>Díaz</surname><given-names>MF</given-names></name></person-group><article-title xml:lang="en">Prediction of elongation at break for linear polymers</article-title><source>Chemom. Intell. Lab Syst.</source><year>2014</year><volume>139</volume><fpage>121</fpage><lpage>131</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXhs12ntrnE</pub-id><pub-id pub-id-type="doi">10.1016/j.chemolab.2014.09.009</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doan Tran</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Machine-learning predictions of polymer properties with polymer genome</article-title><source>J. Appl. Phys.</source><year>2020</year><volume>128</volume><fpage>171104</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXit1Ois7%2FK</pub-id><pub-id pub-id-type="doi">10.1063/5.0023759</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M</given-names></name><name><surname>Niu</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Polyimide fibers with high strength and high modulus: preparation, structures, properties, and applications</article-title><source>Macromol. Rapid Commun.</source><year>2018</year><volume>39</volume><fpage>1800141</fpage><pub-id pub-id-type="doi">10.1002/marc.201800141</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Evading the strength–ductility trade-off dilemma of rigid thermosets by incorporating triple cross-links of varying strengths</article-title><source>Polym. Chem.</source><year>2020</year><volume>11</volume><fpage>6281</fpage><lpage>6287</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsl2hsLbL</pub-id><pub-id pub-id-type="doi">10.1039/D0PY00928H</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Z-G</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Polymerized small-molecule acceptors for high-performance all-polymer solar cells</article-title><source>Angew. Chem. Int</source><year>2021</year><volume>60</volume><fpage>4422</fpage><lpage>4433</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXislShurfE</pub-id><pub-id pub-id-type="doi">10.1002/anie.202009666</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nagasawa</surname><given-names>S</given-names></name><name><surname>Al-Naamani</surname><given-names>E</given-names></name><name><surname>Saeki</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Computer-aided screening of conjugated polymers for organic solar cell: classification by random forest</article-title><source>J. Phys. Chem.</source><year>2018</year><volume>9</volume><fpage>2639</fpage><lpage>2646</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXovVyms74%3D</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Metallated terpolymer donors with strongly absorbing iridium complex enables polymer solar cells with 16.71% efficiency</article-title><source>Chem. Eng. J.</source><year>2022</year><volume>430</volume><fpage>132832</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXitlWnur7I</pub-id><pub-id pub-id-type="doi">10.1016/j.cej.2021.132832</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abdelkareem</surname><given-names>MA</given-names></name><etal/></person-group><article-title xml:lang="en">Environmental aspects of fuel cells: A review</article-title><source>Sci. Total Environ.</source><year>2021</year><volume>752</volume><fpage>141803</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhslOgurnK</pub-id><pub-id pub-id-type="doi">10.1016/j.scitotenv.2020.141803</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Larminie, J., Dicks, A. &amp; McDonald, M. S. Fuel <italic>cell systems explained</italic> Vol. 2 (J. Wiley Chichester, UK, 2003).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shaari</surname><given-names>N</given-names></name><etal/></person-group><article-title xml:lang="en">Enhanced proton conductivity and methanol permeability reduction via sodium alginate electrolyte-sulfonated graphene oxide bio-membrane</article-title><source>Nanoscale Res. Lett.</source><year>2018</year><volume>13</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXksFeju7o%3D</pub-id><pub-id pub-id-type="doi">10.1186/s11671-018-2493-6</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Catenaro</surname><given-names>E</given-names></name><name><surname>Rizzo</surname><given-names>DM</given-names></name><name><surname>Onori</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Experimental analysis and analytical modeling of enhanced-ragone plot</article-title><source>Appl. Energy</source><year>2021</year><volume>291</volume><fpage>116473</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXnt1emsLY%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.apenergy.2021.116473</pub-id></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shown</surname><given-names>I</given-names></name><name><surname>Ganguly</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>L-C</given-names></name><name><surname>Chen</surname><given-names>K-H</given-names></name></person-group><article-title xml:lang="en">Conducting polymer-based flexible supercapacitor</article-title><source>Energy Sci. Eng.</source><year>2015</year><volume>3</volume><fpage>2</fpage><lpage>26</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhslCqug%3D%3D</pub-id><pub-id pub-id-type="doi">10.1002/ese3.50</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uppugalla</surname><given-names>S</given-names></name><name><surname>Srinivasan</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Polyaniline nanofibers and porous ni [oh] 2 sheets coated carbon fabric for high performance super capacitor</article-title><source>J. Appl. Polym. Sci.</source><year>2019</year><volume>136</volume><fpage>48042</fpage><pub-id pub-id-type="doi">10.1002/app.48042</pub-id></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Coupled and decoupled hierarchical carbon nanomaterials toward high-energy-density quasi-solid-state na-ion hybrid energy storage devices</article-title><source>Energy Stor. Mater.</source><year>2019</year><volume>23</volume><fpage>530</fpage><lpage>538</lpage></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Javed</surname><given-names>MS</given-names></name><etal/></person-group><article-title xml:lang="en">Achieving high rate and high energy density in an all-solid-state flexible asymmetric pseudocapacitor through the synergistic design of binder-free 3d znco 2 o 4 nano polyhedra and 2d layered ti 3 c 2 t x-mxenes</article-title><source>J. Mater. Chem.</source><year>2019</year><volume>7</volume><fpage>24543</fpage><lpage>24556</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvVCkur3L</pub-id><pub-id pub-id-type="doi">10.1039/C9TA08227A</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Polymer donors for high-performance non-fullerene organic solar cells</article-title><source>Angew. Chem. Int</source><year>2019</year><volume>58</volume><fpage>4442</fpage><lpage>4453</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsVOiurc%3D</pub-id><pub-id pub-id-type="doi">10.1002/anie.201806291</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ciotti</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">The covid-19 pandemic</article-title><source>Crit. Rev. Clin. Lab Sci.</source><year>2020</year><volume>57</volume><fpage>365</fpage><lpage>388</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFShsrnM</pub-id><pub-id pub-id-type="doi">10.1080/10408363.2020.1783198</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gao</surname><given-names>J</given-names></name><name><surname>Yin</surname><given-names>Y</given-names></name><name><surname>Myers</surname><given-names>KR</given-names></name><name><surname>Lakhani</surname><given-names>KR</given-names></name><name><surname>Wang</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Potentially long-lasting effects of the pandemic on scientists</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>1</fpage><lpage>6</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-26428-z</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Li, Y., Shetty, P., Liu, L., Zhang, C. &amp; Song, L. <italic>BERTifying the hidden Markov model for multi-source weakly supervised named entity recognition</italic>, 6178–6190 (Association for Computational Linguistics, Online, 2021). <ext-link xlink:href="https://aclanthology.org/2021.acl-long.482" ext-link-type="uri">https://aclanthology.org/2021.acl-long.482</ext-link>.</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Ratner, A. et al. <italic>Snorkel: Rapid training data creation with weak supervision</italic>, Vol. 11, 269 (NIH Public Access, 2017).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Zhang, R., Yu, Y., Shetty, P., Song, L. &amp; Zhang, C. <italic>Prompt-based rule discovery and boosting for interactive weakly-supervised learning</italic>, 745–758 (Association for Computational Linguistics, Dublin, Ireland, 2022). <ext-link xlink:href="https://aclanthology.org/2022.acl-long.55" ext-link-type="uri">https://aclanthology.org/2022.acl-long.55</ext-link>.</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name><name><surname>Zielesny</surname><given-names>A</given-names></name><name><surname>Steinbeck</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Decimer: towards deep learning for chemical image recognition</article-title><source>J. Cheminformatics</source><year>2020</year><volume>12</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1186/s13321-020-00469-w</pub-id></mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khokhlov</surname><given-names>I</given-names></name><name><surname>Krasnov</surname><given-names>L</given-names></name><name><surname>Fedorov</surname><given-names>MV</given-names></name><name><surname>Sosnin</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Image2smiles: Transformer-based molecular optical recognition engine</article-title><source>Chem.-Methods</source><year>2022</year><volume>2</volume><fpage>e202100069</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XitlWit7%2FM</pub-id><pub-id pub-id-type="doi">10.1002/cmtd.202100069</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Polymer informatics: Current status and critical next steps</article-title><source>Mater. Sci. Eng. R. Rep.</source><year>2021</year><volume>144</volume><fpage>100595</fpage><pub-id pub-id-type="doi">10.1016/j.mser.2020.100595</pub-id></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Dielectric polymers tolerant to electric field and temperature extremes: Integration of phenomenology, informatics, and experimental validation</article-title><source>ACS Appl Mater. Interfaces</source><year>2021</year><volume>13</volume><fpage>53416</fpage><lpage>53424</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhvVOhtLjI</pub-id><pub-id pub-id-type="doi">10.1021/acsami.1c11885</pub-id></mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Wu, Y. et al. Google’s neural machine translation system: Bridging the gap between human and machine translation. <italic>arXiv preprint arXiv:1609.08144.</italic><ext-link xlink:href="https://arxiv.org/abs/1609.08144" ext-link-type="uri">https://arxiv.org/abs/1609.08144</ext-link> (2016).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Song, X., Salcianu, A., Song, Y., Dopson, D. &amp; Zhou, D. <italic>Fast WordPiece tokenization</italic>, 2089–2103 (Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021). <ext-link xlink:href="https://aclanthology.org/2021.emnlp-main.160" ext-link-type="uri">https://aclanthology.org/2021.emnlp-main.160</ext-link>.</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: A method for stochastic optimization. <italic>arXiv preprint arXiv:1412.6980.</italic><ext-link xlink:href="https://arxiv.org/abs/1412.6980" ext-link-type="uri">https://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Liang, C. et al. <italic>Bond: Bert-assisted open-domain named entity recognition with distant supervision</italic>, KDD ’20, 1054-1064 (Association for Computing Machinery, New York, NY, USA, 2020). <ext-link xlink:href="10.1145/3394486.3403149" ext-link-type="doi">https://doi.org/10.1145/3394486.3403149</ext-link>.</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Zhu, Y. et al. <italic>Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</italic>, In Proceedings of the IEEE international conference on computer vision, pp. 19–27 (2015).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Liu, Y. et al. Roberta: A robustly optimized bert pretraining approach. <italic>arXiv preprint arXiv:1907.11692.</italic><ext-link xlink:href="https://arxiv.org/abs/1907.11692" ext-link-type="uri">https://arxiv.org/abs/1907.11692</ext-link> (2019).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Wolf, T. et al. <italic>Transformers: State-of-the-art natural language processing</italic>, 38–45 (Association for Computational Linguistics, Online, 2020). <ext-link xlink:href="https://aclanthology.org/2020.emnlp-demos.6" ext-link-type="uri">https://aclanthology.org/2020.emnlp-demos.6</ext-link>.</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Huang, K., Altosaar, J. &amp; Ranganath, R. Clinicalbert: Modeling clinical notes and predicting hospital readmission. <italic>arXiv preprint arXiv:1904.05342.</italic><ext-link xlink:href="https://arxiv.org/abs/1904.05342" ext-link-type="uri">https://arxiv.org/abs/1904.05342</ext-link> (2019).</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Araci, D. Finbert: Financial sentiment analysis with pre-trained language models. <italic>arXiv preprint arXiv:1908.10063.</italic><ext-link xlink:href="https://arxiv.org/abs/1908.10063" ext-link-type="uri">https://arxiv.org/abs/1908.10063</ext-link> (2019).</mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Levenshtein</surname><given-names>VI</given-names></name></person-group><article-title xml:lang="en">Binary codes capable of correcting deletions, insertions, and reversals</article-title><source>Dokl. Phys.</source><year>1966</year><volume>10</volume><fpage>707</fpage><lpage>710</lpage></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Mitkov, R. Anaphora <italic>resolution</italic> (Routledge, 2014).</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Wang, L., Cao, Z., De Melo, G. &amp; Liu, Z. <italic>Relation classification via multi-level attention</italic> CNNS, 1298–1307 (2016).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Zhong, Z. &amp; Chen, D. <italic>A frustratingly easy approach for entity and relation extraction</italic>, 50–61 (Association for Computational Linguistics, Online, 2021). <ext-link xlink:href="https://aclanthology.org/2021.naacl-main.5" ext-link-type="uri">https://aclanthology.org/2021.naacl-main.5</ext-link>.</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krallinger</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">The Chemdner corpus of chemicals and drugs and its annotation principles</article-title><source>J. Cheminformatics</source><year>2015</year><volume>7</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S1</pub-id></mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Mysore, S. et al. The materials science procedural text corpus: Annotating materials synthesis procedures with shallow semantic structures. <italic>arXiv preprint arXiv:1905.06939.</italic><ext-link xlink:href="https://arxiv.org/abs/1905.06939" ext-link-type="uri">https://arxiv.org/abs/1905.06939</ext-link> (2019).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec21"><title>Supplementary information</title><p id="Par47"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2023_1003_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-023-01003-w" ext-link-type="doi">https://doi.org/10.1038/s41524-023-01003-w</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2023</facet-value></facet><facet name="country"><facet-value count="1">India</facet-value><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
