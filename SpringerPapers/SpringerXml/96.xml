<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s43246-020-00052-8</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">43246</journal-id><journal-id journal-id-type="doi">10.1038/43246.2662-4443</journal-id><journal-title-group><journal-title>Communications Materials</journal-title><abbrev-journal-title abbrev-type="publisher">Commun Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2662-4443</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s43246-020-00052-8</article-id><article-id pub-id-type="manuscript">52</article-id><article-id pub-id-type="doi">10.1038/s43246-020-00052-8</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/705/258</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/638/675</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Integrating multiple materials science projects in a single neural network</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-1959-5430</contrib-id><name><surname>Hatakeyama-Sato</surname><given-names>Kan</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au2"><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name><address><email>oyaizu@waseda.jp</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs43246020000528_cor2">b</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.5290.e</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 9975</institution-id><institution content-type="org-division">Department of Applied Chemistry</institution><institution content-type="org-name">Waseda University</institution></institution-wrap><addr-line content-type="postcode">169-8555</addr-line><addr-line content-type="city">Tokyo</addr-line><country country="JP">Japan</country></aff></contrib-group><author-notes><corresp id="IDs43246020000528_cor2"><label>b</label><email>oyaizu@waseda.jp</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>30</day><month>7</month><year>2020</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2020</year></pub-date><volume>1</volume><issue seq="49">1</issue><elocation-id>49</elocation-id><history><date date-type="registration"><day>15</day><month>7</month><year>2020</year></date><date date-type="received"><day>18</day><month>4</month><year>2020</year></date><date date-type="accepted"><day>2</day><month>7</month><year>2020</year></date><date date-type="online"><day>30</day><month>7</month><year>2020</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2020</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">In data-intensive science, machine learning plays a critical role in processing big data. However, the potential of machine learning has been limited in the field of materials science because of the difficulty in treating complex real-world information as a digital language. Here, we propose to use graph-shaped databases with a common format to describe almost any materials science experimental data digitally, including chemical structures, processes, properties, and natural languages. The graphs can express real world’s data with little information loss. In our approach, a single neural network treats the versatile materials science data collected from over ten projects, whereas traditional approaches require individual models to be prepared to process each individual database and property. The multitask learning of miscellaneous factors increases the prediction accuracy of parameters synergistically by acquiring broad knowledge in the field. The integration is beneficial for developing general prediction models and for solving inverse problems in materials science.</p></abstract><abstract xml:lang="en" id="Abs2" abstract-type="ShortSummary"><p id="Par2">Traditionally, machine learning for materials science is based on database-specific models and is limited in the number of predictable parameters. Here, a versatile graph-based neural network can integrate multiple data sources, allowing the prediction of more than 40 parameters simultaneously.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>MEXT | Japan Society for the Promotion of Science (JSPS)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100001691</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">17H03072</award-id><award-id award-type="FundRef grant">18K19120</award-id><award-id award-type="FundRef grant">18H05515</award-id><award-id award-type="FundRef grant">20H05298</award-id><award-id award-type="FundRef grant">19K15638</award-id><principal-award-recipient><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Oyaizu</surname><given-names>Kenichi</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Research</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>103</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2019</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>7</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>15</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/43246_2020_Article_52.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-day</meta-name><meta-value>14</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-day</meta-name><meta-value>14</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">Data-driven science is becoming increasingly important amidst the worldwide deluge of data<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. Recent developments in deep learning have provided a way to extract important features from big data automatically and to understand new phenomena<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Integration of data by machine learning is also important in materials science. New devices, such as next-generation batteries and photovoltaics, could be developed more efficiently by automatically exploring materials with superior properties, chemical structures, and processes<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p id="Par4">Despite the high expectations around materials informatics, even cutting-edge prediction models are not yet able to integrate big data from materials science, due to the lack of general knowledge in this field<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup>. A number of models predict a variety of material parameters, including physical and chemical properties, structures, and spectroscopic responses<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR18">18</xref></sup>. The recent development of data mining techniques from scientific literature is also helpful to increase the number of databases and to enhance prediction accuracy<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>. However, a critical drawback has been that the previous models could not predict more than two parameters (Table <xref rid="Tab1" ref-type="table">1</xref> and Fig. <xref rid="Fig1" ref-type="fig">1a, b</xref>) and contained as many individual prediction algorithms and models as predicting parameters<sup><xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup>. Therefore, the models could not perform essential tasks that are easy for humans, such as learning, considering, and predicting multiple real-world phenomena with a single intelligence. This limitation arises from the use of traditional, inflexible table databases. To integrate knowledge, varied information must be inputted and outputted (i.e., multimodal learning)<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Comparison of the present work with previous prediction models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2"/><th rowspan="2"><p>Targets</p></th><th rowspan="2"><p>Predictable parameters<sup>a</sup></p></th><th rowspan="2"><p>Inputted databases<sup>b</sup></p></th><th colspan="3"><p>Input<sup>b</sup></p></th><th colspan="3"><p>Output<sup>b</sup></p></th><th rowspan="2"><p>Ref.</p></th></tr><tr><th><p>Num.</p></th><th><p>Chem.</p></th><th><p>Misc.</p></th><th><p>Num.</p></th><th><p>Chem.</p></th><th><p>Misc.</p></th></tr></thead><tbody><tr><td><p>1</p></td><td><p>Properties, chemicals, processes, etc.</p></td><td><p>&gt;40</p></td><td><p>&gt;10</p></td><td><p>+</p></td><td><p>+</p></td><td><p>+</p></td><td><p>+</p></td><td><p>+</p></td><td><p>+</p></td><td><p>This work</p></td></tr><tr><td><p>2</p></td><td><p>Ionic conductivity</p></td><td><p>1</p></td><td><p>1 (2)<sup>c</sup></p></td><td><p>+</p></td><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR6">6</xref></sup></p></td></tr><tr><td><p>3</p></td><td><p>Material properties</p></td><td><p>1 (&gt;40)<sup>c</sup></p></td><td><p>1 (&gt;10)<sup>c</sup></p></td><td/><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR8">8</xref></sup></p></td></tr><tr><td><p>4</p></td><td><p>Polymer properties</p></td><td><p>1 (7)<sup>c</sup></p></td><td><p>1</p></td><td/><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR7">7</xref></sup></p></td></tr><tr><td><p>5</p></td><td><p>Nanosheet yield</p></td><td><p>1</p></td><td><p>1</p></td><td/><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR9">9</xref></sup></p></td></tr><tr><td><p>6</p></td><td><p>OLED performance<sup>d</sup></p></td><td><p>1</p></td><td><p>1</p></td><td/><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR11">11</xref></sup></p></td></tr><tr><td><p>7</p></td><td><p>Chemical reaction<sup>e</sup></p></td><td><p>1</p></td><td><p>1</p></td><td/><td><p>+</p></td><td/><td><p>+</p></td><td/><td/><td><p><sup><xref ref-type="bibr" rid="CR12">12</xref></sup></p></td></tr><tr><td><p>8</p></td><td><p>NMR spectra<sup>f</sup></p></td><td><p>1</p></td><td><p>1</p></td><td/><td/><td><p>+</p></td><td/><td/><td><p>+</p></td><td><p><sup><xref ref-type="bibr" rid="CR10">10</xref></sup></p></td></tr><tr><td><p>9</p></td><td><p>Molecular structure<sup>g</sup></p></td><td><p>1</p></td><td><p>1</p></td><td/><td><p>+</p></td><td/><td/><td><p>+</p></td><td/><td><p><sup><xref ref-type="bibr" rid="CR13">13</xref></sup></p></td></tr><tr><td><p>10</p></td><td><p>Inorganic semiconductors, etc.<sup>h</sup></p></td><td><p>–</p></td><td><p>(1)</p></td><td/><td/><td><p>+</p></td><td/><td/><td><p>+</p></td><td><p><sup><xref ref-type="bibr" rid="CR18">18</xref></sup></p></td></tr><tr><td><p>11</p></td><td><p>Properties, chemicals, etc.<sup>i</sup></p></td><td><p>(&gt;3)</p></td><td><p>(&gt;3)</p></td><td><p>(+)</p></td><td><p>(+)</p></td><td><p>(+)</p></td><td><p>(+)</p></td><td><p>(+)</p></td><td><p>(+)</p></td><td><p><sup><xref ref-type="bibr" rid="CR17">17</xref></sup></p></td></tr></tbody></table><table-wrap-foot><p><sup>a</sup>Number of parameters and databases predicted or interpreted by a single prediction model.</p><p><sup>b</sup>Inputs and outputs of the models. “Num.”, “Chem.”, and “Misc.” indicate numbers, chemical structures, and miscellaneous parameters (e.g., images, sounds, and spectra). For example, prediction model number 2 in the table predicts ionic conductivity (numbers) of polymer composites from their chemical structures and numeric parameters (composition ratio, etc).</p><p><sup>c</sup>Multiple models were generated to input/predict multiple databases and parameters by transfer learning. Each model shares mutual algorithms for the improved recognition of important features, but final predictions themselves are done individually (see Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">12</xref>).</p><p><sup>d</sup>OLED: organic light emitting diode.</p><p><sup>e</sup>Predict yield of chemical reactions.</p><p><sup>f</sup>Predict NMR spectra of chemicals.</p><p><sup>g</sup>Known as an autoencoder of chemical structures.</p><p><sup>h</sup>From text abstracts, the model calculates the embedding vectors of chemicals, expressed by words.</p><p><sup>i</sup>Individual databases and machine learning models were connected and analyzed by graph approaches.</p></table-wrap-foot></table-wrap><fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Concept for the present work.</title><p><bold>a</bold> A single neural network model is trained to learn and predict diverse materials informatics projects. <bold>b</bold> Traditionally, a model can process only one database and predict one parameter. The Wikipedia logo is reprinted with the CC-BY-SA 3.0 license.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/43246_2020_52_Fig1_HTML.png"/></fig></p><p id="Par5">In this study, we introduced graphs with a common format to integrate diverse materials science projects (Fig. <xref rid="Fig2" ref-type="fig">2a</xref>, Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">1</xref> and <xref ref-type="supplementary-material" rid="MOESM1">2</xref>). The format can express almost all experimental materials science information, such as structure, properties, processes, text, images, and even sounds. All related information from more than ten projects was inputted into a single neural network to predict more than 40 parameters simultaneously, including numeric properties, chemical structures, and text (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Graph approaches have been employed to analyze the relationships of atom-connections, chemical features, and reactions<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. In this study, we extended the approach to train a neural network with the general phenomena of science, which are expressed by graphs. The multitask training of versatile information was essential to acquire broad knowledge about materials science. Our graph approach will be the key to developing general-purpose artificial intelligence for materials science, including inverse problem solving.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>About graph approach.</title><p><bold>a</bold> Concept of using a graph database for materials informatics. Traditionally, the experimental, real-world information (Z) is converted to table databases manually, which require unique algorithms to be formatted as numeric arrays for each case (right part of the figure). Machine learning models, trained with individual format databases, cannot interpret the other databases. On the other hand, common-formatted graphs were made to express versatile experimental information in this study (left part). A single machine learning model interprets all inputted experimental information. <bold>b</bold> Overview of processing graph structures for machine learning (see Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4</xref> for further information).</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/43246_2020_52_Fig2_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Process informatics for electron-conducting polymers</title><p id="Par6">As a model case to demonstrate the effect of the graph format, we examined the process informatics of poly(3,4-ethylenedioxythiophene) doped with poly(4-styrenesulfonate) (PEDOT-PSS; Fig. <xref rid="Fig3" ref-type="fig">3</xref>). The polymer is known for its high electron conductivity and can be used in transparent flexible conductive films, capacitors, solar cells, thermoelectrics, and other energy-related devices<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. The conductivity reaches over 3000 S cm<sup>−1</sup> after the careful chemical treatment of the polymer film<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. To achieve higher conductivity, a number of new annealing methods, including repeated chemical treatments with strong acids, bases, and solvents, have been reported<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Informatic approaches have been partially introduced to predict the properties of PEDOT-PSS<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. However, the post-treatment methods have become too long and complex to be analyzed by conventional machine learning approaches or to be understood except by a few specialists (example scheme is shown in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>).<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Prediction of electric conductivity.</title><p><bold>a</bold> Predicting the electrical conductivity of PEDOT-PSS from its post-treatment method. <bold>b</bold> Structure of PEDOT-PSS. The mixture gives higher conductivity with appropriate post-treatment, which induces the optimum connection of conductive moieties. <bold>c</bold> Experimental and predicted conductivity of PEDOT-PSS. Seventy percent of data are selected randomly as the training dataset. The rest is used only for prediction. <italic>R</italic><sup>2</sup> scores for training and test datasets are 0.87 and 0.70, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/43246_2020_52_Fig3_HTML.png"/></fig></p><p id="Par7">Process informatics aims to optimize procedures by using statistical tools<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>. The supposedly important factors for the target performance are extracted manually, and recorded in table databases (e.g., heating temperature, mixing speed, and duration; Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>). The table format is normally used because most machine learning models can only accept numeric arrays<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par8">The intrinsic problems with the traditional approach are the inflexible format of the table and ignorance of the experimental context. The database format must be changed whenever a new experimental step (e.g., additional mixing) is considered, although additional steps are often examined to optimize the procedure. It may not be possible to describe complex experimental information fully in a numeric table alone. Even if the table is constructed, reusing it in other projects may be difficult because it does not contain the context for the values (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>).</p><p id="Par9">In the present study, text-based experimental procedures were automatically converted to graphs while maintaining the text context and inputted directly to a machine learning model (Fig. <xref rid="Fig3" ref-type="fig">3a</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). Graph data describe the relationships among things (expressed as nodes) by connecting them with edges; in contrast to table databases, this flexibility enables the expression of diverse information easily, such as text structures, social networks, and molecular structures<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. A recent development in deep learning has enabled the automatic recognition of the graphs and calculation of their characteristic features<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Still, individual prediction models had to be prepared for each genre. Here, we demonstrate that even a single graph and a prediction model can process multidisciplinary information, including chemicals, text, and numbers (Fig. <xref rid="Fig1" ref-type="fig">1</xref>).</p><p id="Par10">A simple yet powerful approach to describe versatile information in graphs was to record both genre and content information in each node (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>, Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">4</xref> and <xref ref-type="supplementary-material" rid="MOESM1">5a</xref>). The original text of the post treatments was converted to dependency trees as undirected graphs by a natural language parser<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The nodes in graphs were classified as words, chemical structures, and values. The node information was then converted to numeric vectors by three algorithms (see “Methods” and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4</xref>). To process words, a state-of-the-art language-understanding deep-learning model called BERT<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> was used. Molecular information was converted to vectors by using molecular fingerprints<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>. Values in numeric nodes were kept unchanged. To distinguish the genres, three numeric arrays were added to the headers. Apart from the three classes, any information can be embedded in graphs if it can be converted to numeric arrays, thereby paving the way to learning general information about materials science (e.g., inorganic structures, images, and sounds)<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par11">More than 350 types of graphs related to the post-treatment of PEDOT-PSS (from over 20 papers) were prepared and inputted to a graph neural network<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. The model was trained to predict the electrical conductivity from the post-treatment methods of the polymer films. In the original database, the procedures were written as text (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). After automatically converting the text to graphs, only the nodes containing electrical conductivity were replaced with the keyword “unknown”. Here, no significant information is basically lost during the graph conversion because the quasi-reversibility of text parsing<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The graphs were used as the inputs (questions) to the model. The model was trained to predict the conductivity from the graph-shaped questions. As the overall user interface, the model can answer the performances of the polymers only from text. This style is more convenient and reliable for most users; special effort and knowledge are needed to prepare traditional table databases, which require the careful, manual selection of important features for the target phenomena and formatting into numeric arrays for machine learning.</p><p id="Par12">The prediction accuracy of the conductivity by the neural network was high. To check the accuracy, the database was split into training (70%) and test (30%) datasets randomly. Although the model was trained only with the training dataset, the <italic>R</italic><sup>2</sup> score of the prediction and experimental conductivity was greater than 0.7 (Fig. <xref rid="Fig3" ref-type="fig">3c</xref>). The score was comparable or slightly higher than the control experiment, where conductivity was predicted directly from texts using a conventional natural language model (<italic>R</italic><sup>2</sup> = 0.66, Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">5b, c</xref>). The high accuracy supported the validity of the graph approach. Except for a few specialists, such accurate predictions are difficult to make due to the excessively complex preparation procedure (see the long preparation method shown in Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). Because the neural network can find essential features from graphs automatically, manual parameter selection to prepare the database is not necessary. Automatic text parsing<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and the general graph approach enable automatic data collection from materials science big data, where recognition of unstructured data has been a bottleneck<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>.</p></sec><sec id="Sec4"><title>Multitask learning in different projects</title><p id="Par13">A key advantage of using general graphs is their high capability for describing diverse experimental information. Because the text context is maintained in the graphs, users can easily change the target parameters of the prediction by replacing the target node with the keyword “unknown”. In contrast to normal table databases, the graph questions themselves contain the information about what is to be predicted. This enables one model to learn and predict multiple databases and parameters easily (multitask and multimodal training). For instance, we prepared a graph database containing more than 1000 chemical compounds from Wikipedia (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">6</xref>). The relationships among chemical structures and their physical properties were recorded as graphs. Similarly, a lithium-ion conducting polymer database, which we constructed previously<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>, was converted into graphs. In the previous study, a long script was needed to process the complex conductor information so it could be interpreted by a machine learning model (i.e., into numeric arrays)<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. However, in the present study, no additional script was necessary because the conductor information could be expressed in the general graph format (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">6</xref>).</p><p id="Par14">Chemical graph databases were easily converted to question graphs by replacing the property nodes with the keyword “unknown”. More than ten properties, including ionic conductivity, melting point, pKa, viscosity, and vapor pressure, were set as questions (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">6</xref>). A machine learning model was trained with the PEDOT-PSS and Wikipedia databases to predict the recorded properties (Fig. <xref rid="Fig4" ref-type="fig">4a</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">7</xref>). Multitask and multimodal training is not feasible with the traditional table databases, due to their inflexible format; the process information about PEDOT-PSS and the chemical properties in Wikipedia cannot be described fully in an integrated table.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Multitask learning.</title><p><bold>a</bold> Preparation of datasets for multitask learning. If the split ratio is 0.6, 60% of the randomly selected PEDOT-PSS data and all of the other database are used to train the model. The other 40% is selected as the test dataset. <bold>b</bold><italic>R</italic><sup>2</sup> scores for the test dataset after multitask learning with Wikipedia or the lithium-ion conducting polymer database. Only the PEDOT-PSS database is trained as a control. The values show the averages of five experiments with different random shuffling (<italic>n</italic> = 5 independent experiments). Error bars show the standard errors. See Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">7</xref> for the results of the training dataset. <bold>c</bold> Representative heatmap of the latent vectors for the PEDOT-PSS and Wikipedia databases, outputted by a model trained only with the PEDOT-PSS database. See Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">9</xref> for further information. <bold>d</bold> Two-dimensional UMAP projection of the latent vectors shown in Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">9</xref> and <xref ref-type="supplementary-material" rid="MOESM1">10</xref>.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/43246_2020_52_Fig4_HTML.png"/></fig></p><p id="Par15">Although there was no obvious relationship between the post-treatment of PEDOT-PSS and compounds in Wikipedia, the prediction accuracy of the electrical conductivity of PEDOT-PSS was improved by multitask training. The PEDOT-PSS database was split into training and test datasets randomly with different split ratios (0–0.9). All data from Wikipedia were combined with the training dataset (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). As expected, the <italic>R</italic><sup>2</sup> score for the test dataset increased as the split ratio increased (Fig. <xref rid="Fig4" ref-type="fig">4b</xref>). Most importantly, the scores were always higher when Wikipedia was learned simultaneously. A similar improvement was observed for the multitask learning of the lithium-ion conducting polymer database. The score was more than three times higher with the multitask learning than with only learning PEDOT-PSS, with a split ratio of 0.3 (corresponding to learning ca. 100 cases of PEDOT-PSS). To our knowledge, this is the first report of multitask learning of different databases and improvement of prediction accuracy in materials science.</p><p id="Par16">To reveal the detailed process of multitask learning, we analyzed the intermediate calculation steps in the model, by visualizing the outputs of a hidden layer in the neural network (Fig. <xref rid="Fig4" ref-type="fig">4c</xref>, Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">8</xref>–<xref ref-type="supplementary-material" rid="MOESM1">11</xref>). The hidden layer converted the inputted graphs to 32-dimensional numeric arrays as the vector representation (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">5a</xref>). The vectors contain essential information about the input and output, termed ‘latent space’<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. We compressed the 32-dimensional vectors into two-dimensional arrays<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> for easier understanding (Fig. <xref rid="Fig4" ref-type="fig">4d</xref>). When only the Wikipedia database was used for training (split ratio of 0), the plots from PEDOT-PSS and Wikipedia databases were separate, indicating that the model interpreted them as different species. In contrast, the plots were combined after multitask learning because the model found hidden mutuality among the data and partially shared calculation algorithms for predicting the different parameters. Further mechanism analysis of the multitask is not accessible due to the “black box” problem of deep learning<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. However, the recent idea of machine learning, represented by influence functions<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, may help researchers reveal the internal processes (e.g., clarify the relationships among specific databases and parameters).</p><p id="Par17">A similar idea to multitask learning, called ‘transfer learning’, has also been proposed to improve prediction<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR8">8</xref></sup>, in which different prediction models partially shared the calculation steps to recognize important data features efficiently. However, the final calculations were done by individual algorithms (for details see Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">12</xref>). The individuality limits the advantages of the synergistic effects of learning multiple databases and acquiring broad knowledge of the field. In contrast, in the graph approach, a single intelligence interpreted multiple databases and properties. This finding is essential for exploring the materials informatics of experimental projects, most of which have limited database capacity owing to the high experimental cost<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>.</p></sec><sec id="Sec5"><title>General materials informatics prediction model</title><p id="Par18">A more general materials informatics prediction model was pursued by increasing the number of learning databases. From public data, we collected 14 experimental materials science databases, containing over 40 properties (see <xref ref-type="supplementary-material" rid="MOESM1">Supplementary Information</xref>). The main compounds in the databases were monomeric molecules, organic polymers, and their mixtures. In addition to their basic physical properties, advanced features, such as redox potentials were included (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Prediction of redox potential is necessary to develop energy-related devices but was not fully successful, mainly because the potentials are changed by the effects of solvents and salts and complex systems are difficult to handle in the table format and simulations<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. In the graph databases, the redox potentials were easily recorded as a function of redox molecules, solvents, and electrolyte salts.</p><p id="Par19">For machine learning, some databases were selected randomly and learned with a single prediction model (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">13</xref>). By increasing the number of training databases, the number of predictable parameters increased because the model could understand the larger amount of information inputted. When the model was trained with all 14 databases, it could predict over 40 properties with high accuracy (Fig. <xref rid="Fig5" ref-type="fig">5</xref>, Supplementary Fig <xref ref-type="supplementary-material" rid="MOESM1">14a</xref>, Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>, and Supplementary Data <xref ref-type="supplementary-material" rid="MOESM3">1</xref>). The prediction accuracy was not high enough (<italic>R</italic><sup>2</sup> &lt; 0) with the parameters with insufficient amounts of training data (typically less than 100 cases, Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">14b</xref>). We emphasize that the prediction errors by the multitask training were basically smaller than the control experiments, introducing different random forest regressors to predict each parameter from corresponding chemical fingerprints (i.e., standard single-task training, Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">14c</xref>). For higher accuracy, we are integrating other public experimental databases and even computational results. Together with revealing the synergetic effects of multitask learning, even one- or zero-shot learning<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> may be achievable with the model, which can benefit from both human-like context understanding capability and hugecomputational power to process big data. General prediction models will be beneficial to the wider research community because of their broad knowledge and ability to answer unknown questions; internet search engines can only answer questions about known issues and human professional resources are limited.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Prediction of versatile parameters.</title><p>Experimental (<italic>x</italic>-axis) and predicted (<italic>y</italic>-axis) values of each parameter after multitask learning of the 14 databases (90% of data was used for training). Blue and red plots correspond to the training and test datasets, respectively. The values are shown as <italic>z</italic>-scores. <italic>R</italic><sup>2</sup> scores for test datasets are displayed. If test cases were not enough for evaluation, train scores are shown with brackets instead. Statistical results are also summarized in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>. Larger graphs are shown in Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">14a</xref>.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/43246_2020_52_Fig5_HTML.png"/></fig></p></sec><sec id="Sec6"><title>Inverse problem solving by graph approach</title><p id="Par20">One of the ultimate goals of materials informatics is fully solving inverse problems. Instead of predicting the results of conditions carefully specified by humans, machine learning models are expected to answer much more ambitious questions, such as “Which post-treatment protocol for PEDOT-PSS will yield a conductivity of 10<sup>4</sup> S cm<sup>−1</sup>?” or “What is the organic polymer structure that gives a melting point of 500 °C?”. Although there are no available procedures or structures that give high performances, integration of big scientific data may find the answers. In contrast, many informatics challenges must be overcome. The main difficulties are related to uniqueness of mapping, common sense, and generation of complex answers (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">15</xref>). For example, there would be multiple polymer structures exhibiting a melting point of the desired value. Therefore, an inverse function of melting point to structure cannot be determined uniquely (uniqueness problem). Furthermore, most candidate structures must be excluded automatically based on common sense from the field, namely by filtering out inappropriate compounds, for example, synthetically challenging or unstable compounds. Finally, generating complex information, represented by chemical structures, is still an open question in deep learning<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p><p id="Par21">A general prediction model with the graph approach may be the key to solving inverse problems in materials science. Here, a graph neural network was trained with all the information from the 14 databases. In the previous section, nodes of numeric values (material properties) were set as the targets for prediction, whereas all types of nodes (number, word, and compound) in each graph were selected for prediction here (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">16</xref>). Final answers were constructed by finding the most similar vectors in the text and compound databases with the predicted values (see “Methods”). In the future, it may be possible to generate completely new answers by integrating an autoencoder<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. After training, the model could predict the original text in the graphs with a high accuracy of 96% (Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">2</xref>). Even the 4% failed predictions were close to the answers (Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). Typical mistakes were predicting “electrical conductivity” instead of “ionic conductivity” (answer) and “melting point” instead of “boiling point” (answer). This was because the model could understand the similar meanings of natural language by neural networks<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, resulting in near-miss answers.</p><p id="Par22">The prediction accuracy of compounds was lower (36%), mainly because of the uniqueness problem (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">16</xref>). For instance, a chemical structure with a specific density (1.58 g cm<sup>−3</sup>) and a melting point (203 °C) was questioned. The answer was “trehalose”; however, there should be other structures satisfying these two conditions. This problem with uniqueness lowered the apparent accuracy of compound prediction. In practical use, users can add optional conditions freely for desirable compounds. The structure will be determined uniquely if other properties such as heat capacity, hydrophilicity, and chemical stability are specified. Because machine learning models have no actual experimental experience, including this type of tacit knowledge that researchers have will be the key to ensuring the quality of predictions.</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p id="Par23">We used a graph format to express diverse materials informatics information. This common format allows databases from different projects to be combined easily. A single neural network interpreted miscellaneous information, including chemical structures, more than 40 material properties, and text. Multitask and multimodal learning was essential not only for increasing prediction accuracy, but also for developing general-purpose prediction models for materials science. Integrating big data and improving the inverse problem-solving methods will allow this method to be used as an artificial materials science expert, which will change the traditional research and development cycle.</p></sec><sec id="Sec8" sec-type="methods"><title>Methods</title><sec id="Sec9"><title>General information</title><p id="Par24">Databases were constructed or collected from public data. All experimental data collected from the literature were converted into undirected graphs using original Python 3 scripts. Graph nodes were classified into three types: values, text, and chemical compounds, which were converted into numeric arrays automatically by different algorithms. All graph edges were treated equally. To train a neural network, values of target nodes (=<italic>y</italic>) in graphs were replaced with a specific keyword “__unknown__”. The generated graphs were inputted as <italic>x</italic> to the model.</p></sec><sec id="Sec10"><title>Databases</title><p id="Par25">The PEDOT-PSS database was constructed in this work. The lithium-ion-conducting polymer database was constructed in our previous work<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. The experimental properties of various compounds were collected from Wikipedia (<ext-link xlink:href="https://ja.wikipedia.org/" ext-link-type="uri">https://ja.wikipedia.org/</ext-link>), Wikidata (<ext-link xlink:href="https://www.wikidata.org/" ext-link-type="uri">https://www.wikidata.org/</ext-link>), Computational Chemistry Comparison and Benchmark DataBase (CCCBDB, <ext-link xlink:href="https://cccbdb.nist.gov/" ext-link-type="uri">https://cccbdb.nist.gov/</ext-link>), chemical suppliers, and the literature. All data were collected and rechecked by the authors.</p></sec><sec id="Sec11"><title>Computer</title><p id="Par26">Data processing and machine learning were conducted on a desktop computer (Intel Core i9-9900K CPU @ 3.60 GHz, 32 GB memory, GeForce RTX 2080 graphical processing unit, and Ubuntu 16.04 operating system).</p></sec><sec id="Sec12"><title>Graph preparation from text</title><p id="Par27">Text information about post-treatment of PEDOT-PSS was collected and converted to graphs by the following procedures (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). We extracted the related experimental procedures from original articles (mainly experimental sections) and summarized them as text. The text was written in a set format for ease of machine learning (e.g., avoiding inconsistent spelling). In the future, automatic text collection (probably by machine learning) will be examined, to reduce the cost and to eliminate the human nature of data preparation. On the other hand, we note that the system can be robust against orthographical variances and different text expressions owing to the language-understanding deep-learning model<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> (as long as sufficient data are given).</p><p id="Par28">In the text, compounds were expressed as their IDs, such as C0001 and C0123, and their structure information was recorded in a compound database. In the database, the compound ID and simplified molecular input line entry system (SMILES) expressions were associated (about 50 types of chemicals). Numeric values were standardized as <italic>z</italic>-scores by each unit (e.g., siemens per centimeter and degrees Celsius, Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). The text was parsed automatically by an open module (StanfordNLP 0.2.0)<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> to construct dependency trees of words. Nodes of less important words and symbols (e.g., “at”, “were”, “and”, “was”, “by”, and “to”) were removed after parsing. Nodes of conductivity values (<italic>y</italic>) in graphs were replaced with “__unknown__” to prepare the inputs (<italic>x</italic>) for machine learning. An open-source library (NetworkX 2.4, <ext-link xlink:href="https://networkx.github.io/" ext-link-type="uri">https://networkx.github.io/</ext-link>) was used to generate graphs.</p></sec><sec id="Sec13"><title>Graph databases for multitask learning</title><p id="Par29">For multitask learning, all databases, typically written as tables, were converted to graphs by Python scripts if necessary. In the graphs, the relationships among the factors were connected by edges (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1d</xref>). As a common rule, a numeric parameter was connected in the order “parameter name”—“value”—“[unit]” to describe a property of a target node. Numeric values were standardized as the z-score by each unit (e.g., degrees Celsius and siemens per centimeter). For multitask learning, a graph database of PEDOT-PSS was reconstructed manually (apart from the text database described in the previous section) because the automatic parser made graphs according to a different rule (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). A free graph editor (yEd 3.19) was used to draw graphs in the “graphml” format, which was compatible with the NetworkX library.</p><p id="Par30">An integrated compound database was made by combining the compound information in each database and a chemical supplier’s catalog (Tokyo Chemical Industry Co.). A total of over 29,000 chemical structures were recorded. The integrated database was used for the multitask learning experiments (i.e., except for the automatic text parsing experiment in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">8</xref>). The prediction score can decrease slightly when a larger compound database is used (e.g., compare <italic>R</italic><sup>2</sup> scores in Figs. <xref rid="Fig3" ref-type="fig">3c</xref> and <xref rid="Fig4" ref-type="fig">4b</xref>) because of the larger loss of compound information after converting into 64-dimensional numeric vectors with larger data. Therefore, the compressing algorithms should be improved in future research.</p></sec><sec id="Sec14"><title>Converting graphs to numeric matrices and vectors</title><p id="Par31">For machine learning, graphs were converted into adjacency matrices and numeric vectors (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4a</xref>). Adjacency matrices, expressing the node connections, were simply calculated by a function of NetworkX. A matrix, <italic>D</italic>, was prepared according to the following steps: (1) assign unique IDs to each node; and (2) determine <italic>D</italic><sub>ij</sub>, which is 1 if nodes <italic>i</italic> and <italic>j</italic> were connected, and otherwise is 0.</p><p id="Par32">Each node content was converted to 64-dimensional numeric arrays (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4b</xref>). Other information, such as images and sounds, can be also treated by implementing additional processing scripts (see below). The first four-dimensional arrays of the vectors were headers to distinguish the node types. Three types of different random numeric arrays were assigned using an embedding function of Chainer 7.2.0, an open library for deep learning. The remaining 60-dimensional arrays were prepared by three different algorithms according to node types.</p><p id="Par33"><italic>Numeric nodes</italic>: When the node represents a number, the corresponding 60-dimensional numeric array will be a repeat of the value. For instance, an array of (0.5, 0.5,..., 0.5) is set for the value node of 0.5.</p><p id="Par34"><italic>Text nodes</italic>: To process text nodes, a natural language recognition model (BERT<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, pretrained model of “uncased_L-24_H-1024_A-16”, accessible at <ext-link xlink:href="https://github.com/tensorflow/models/tree/master/official/nlp/bert" ext-link-type="uri">https://github.com/tensorflow/models/tree/master/official/nlp/bert</ext-link>) was employed. The model could calculate 768-dimensional numeric vectors of the corresponding words, phrases, and text. Similar text expressions or meanings were converted to similar vectors by BERT<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The 768-dimensional vectors were compressed to 60-dimensional numeric arrays using a principal component analysis (PCA) algorithm, implemented with an open-source library (scikit-learn 0.22.2).</p><p id="Par35"><italic>Compound nodes</italic>: Organic compounds were recorded as unique IDs (e.g., C0023) in graph nodes. By referencing a compound database, their structure information, expressed by 60-dimensional numeric arrays, was loaded. In the compound database, molecular structures were recorded as SMILES expressed by character strings. Their chemical features were calculated using an open module of chemistry (RDKit 2019.03.2, <ext-link xlink:href="https://www.rdkit.org/" ext-link-type="uri">https://www.rdkit.org/</ext-link>), to obtain extended-connectivity fingerprints of the 2048-bit data. The binary data were split every 4 bits (e.g., (0011), (0000), (1111), …) and converted to corresponding integers (e.g., 512-dimensional array of (3, 0, 15, …)). Finally, a 60-dimensional array was prepared followed by PCA compression and standardization.</p><p id="Par36"><italic>Other nodes</italic>: In this study, only numeric, text, and molecular structure nodes were implemented in the graphs. In the future, other information, such as inorganic crystal structures, images, sounds, and spectra, will be processed by adding appropriate scripts to express them as vectors.</p></sec><sec id="Sec15"><title>Dataset preparation</title><p id="Par37">Target values (<italic>y</italic>) on the nodes in graphs were replaced with a keyword “__unknown__” to prepare problems (<italic>x</italic>) automatically (underlines are added to distinguish from the word “unknown”). If one graph has more than two target values (e.g., melting point and density), the replacement and problem generation were done individually to generate multiple problems; there were not multiple “__unknown__” nodes in one graph. The numeric nodes of the material properties were set as the target values (<italic>y</italic>) for prediction. For inverse problem solving in the last section of the main manuscript, all nodes (numbers, chemicals, and text) in all graphs were set as problems.</p><p id="Par38">Unless noted otherwise, the graph data were split into train and test datasets randomly (splitting ratio of 0 to 0.9). The train dataset was used to train a graph neural network and the test dataset was used only for prediction.</p></sec><sec id="Sec16"><title>Machine learning</title><p id="Par39">The prepared datasets were trained with a neural network (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">5</xref>). The Chainer library was used to script the model, which had a graph neural network layer to recognize graphs and three dense layers to calculate the final outputs. The graph neural network was prepared based on an open-source library in Chainer-chemistry 0.7.0. The Implemented function of the gated graph neural network was used. Only the input part of the function was modified to input the adjacency matrices and node vectors described above, whereas the original version was customized to input only the connection of atoms in molecules.</p><p id="Par40">The neural network was trained to reduce the mean square errors between the predicted and actual values. Minibatch sizes of 32 and 128 were selected for only PEDOT-PSS learning and multitask learning, respectively. Training was repeated with 100 epochs with the Adam optimizer<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. The dimension of output values was 1 for the normal prediction mode of numeric nodes.</p><p id="Par41">The model was constructed with a 64-dimensional output to solve inverse problems in the last section of the main manuscript. The last 60-dimensional vectors were used for prediction (i.e., the first four-dimensional arrays were used only to distinguish node types). The predicted vectors were compared with the word (or compound) list in the integrated databases. Ones giving the highest cosine similarity with the predicted vectors were extracted as the prediction result. In the future, direct outputs of words and compounds may be achieved using autoencoders or similar techniques<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Numeric nodes were predicted by averaging the predicted 60-dimensional vectors. All hyperparameters were optimized manually. Automatic parameter tuning will be tested in future research with higher computing power (e.g., multiple GPUs).</p></sec><sec id="Sec17"><title>Prediction by conventional models as the control experiments</title><p id="Par42"><italic>Language model to predict conductivity directly from text</italic> (r<italic>elated to</italic> Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">5</xref>): In Fig. <xref rid="Fig3" ref-type="fig">3</xref>, conductivity was predicted via graph structures, which were converted from texts. On the other hand, conventional recurrent neural networks, such as long short-term memory (LSTM)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, can treat text information directly. As a control experiment, conductivity was predicted from the texts. The conductivity values in the texts were replaced with “__unknown__” to make problems (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">5b</xref>). After converting words into embedding vectors, the word inputted to a LSTM layer (which outputs 16-dimensional latent vectors, implemented by Keras 2.3.1). Conductivity was calculated via a dense layer without activation functions.</p><p id="Par43"><italic>Random forest regressors to predict chemical properties</italic> (<italic>related to</italic> Fig. <xref rid="Fig5" ref-type="fig">5</xref><italic>and</italic> Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">14</xref>): As the control to the multitask training, machine learning was conducted in a conventional way with a Wikipedia database. Random forest was selected as a conventional yet robust prediction algorithm<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. First, compound information was converted to 60-dimensional arrays through the same process as the graph approach. Then, individual random forest regressors (by scikit-learn) were introduced and trained to predict each chemical property recorded in the database (absolute standard enthalpy of formation, boiling temperature, decomposition temperature, density, flash temperature, ionization energy, melting enthalpy, melting temperature, refractive index, vapor pressure, and pKa) from the 60-dimensional arrays. Train and test datasets were prepared randomly with a splitting ratio of 9/1.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>This work was partially supported by Grants-in-Aid for Scientific Research (Nos. 17H03072, 18K19120, 18H05515, 20H05298, and 19K15638) from MEXT, Japan. The work was also partially supported by a research grant from the Center for Data Science, Waseda University, and Information Services International-Dentsu, Ltd. The work was also partially supported by the Research Institute for Science and Engineering, Waseda University.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>K.O. and K.H. conceived the project. K.H. conducted experiments and wrote the manuscript. All authors analyzed the data and contributed to the discussion.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>Databases used for the analyses are available at <ext-link xlink:href="https://github.com/KanHatakeyama/Integrating-multiple-materials-science-projects/" ext-link-type="uri">https://github.com/KanHatakeyama/Integrating-multiple-materials-science-projects/</ext-link> (<ext-link xlink:href="10.5281/zenodo.3910817" ext-link-type="doi">https://doi.org/10.5281/zenodo.3910817</ext-link>). All related data that support the findings of this study are available from the corresponding authors upon reasonable request. Original data for Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref> are provided as an additional supplementary file (Supplementary Data <xref ref-type="supplementary-material" rid="MOESM3">1</xref>).</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>Source codes (i.e., prediction of conductivity from texts, multitask learning, inverse problems, and control experiments) and databases used for the analyses are available at <ext-link xlink:href="https://github.com/KanHatakeyama/Integrating-multiple-materials-science-projects/" ext-link-type="uri">https://github.com/KanHatakeyama/Integrating-multiple-materials-science-projects/</ext-link> (<ext-link xlink:href="10.5281/zenodo.3910817" ext-link-type="doi">https://doi.org/10.5281/zenodo.3910817</ext-link>).</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par44">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>G</given-names></name><name><surname>Hey</surname><given-names>T</given-names></name><name><surname>Szalay</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Computer science. Beyond the data deluge</article-title><source>Science</source><year>2009</year><volume>323</volume><fpage>1297</fpage><lpage>1298</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1MXjt1emtLs%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.1170411</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonelli</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Data—from objects to assets</article-title><source>Nature</source><year>2019</year><volume>574</volume><fpage>317</fpage><lpage>320</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvFOitLnF</pub-id><pub-id pub-id-type="doi">10.1038/d41586-019-03062-w</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXht1WlurzP</pub-id><pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramprasad</surname><given-names>R</given-names></name><name><surname>Batra</surname><given-names>R</given-names></name><name><surname>Pilania</surname><given-names>G</given-names></name><name><surname>Mannodi-Kanakkithodi</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Machine learning in materials informatics: recent applications and prospects</article-title><source>Npj Comput. Mater.</source><year>2017</year><volume>3</volume><fpage>54</fpage><pub-id pub-id-type="doi">10.1038/s41524-017-0056-5</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hill</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Materials science with large-scale data and informatics: unlocking new opportunities</article-title><source>MRS Bull.</source><year>2016</year><volume>41</volume><fpage>399</fpage><lpage>409</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XnvVCkur0%3D</pub-id><pub-id pub-id-type="doi">10.1557/mrs.2016.93</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hatakeyama-Sato</surname><given-names>K</given-names></name><name><surname>Tezuka</surname><given-names>T</given-names></name><name><surname>Umeki</surname><given-names>M</given-names></name><name><surname>Oyaizu</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">AI-assisted exploration of superionic glass-type Li(+) conductors with aromatic structures</article-title><source>J. Am. Chem. Soc.</source><year>2020</year><volume>142</volume><fpage>3301</fpage><lpage>3305</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXpt1Wjsg%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/jacs.9b11442</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Chandrasekaran</surname><given-names>A</given-names></name><name><surname>Huan</surname><given-names>TD</given-names></name><name><surname>Das</surname><given-names>D</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Polymer genome: a data-powered polymer informatics platform for property predictions</article-title><source>J. Phys. Chem. C</source><year>2018</year><volume>122</volume><fpage>17575</fpage><lpage>17585</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtlaktLnL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpcc.8b02913</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamada</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Predicting materials properties with little data using shotgun transfer learning</article-title><source>ACS Cent. Sci</source><year>2019</year><volume>5</volume><fpage>1717</fpage><lpage>1730</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvVGms77L</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00804</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nakada</surname><given-names>G</given-names></name><name><surname>Igarashi</surname><given-names>Y</given-names></name><name><surname>Imai</surname><given-names>H</given-names></name><name><surname>Oaki</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Materials-informatics-assisted high-yield synthesis of 2D nanomaterials through exfoliation</article-title><source>Adv. Theory Simul.</source><year>2019</year><volume>2</volume><fpage>1800180</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXnt1Sgtbc%3D</pub-id><pub-id pub-id-type="doi">10.1002/adts.201800180</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>K</given-names></name><name><surname>Obuchi</surname><given-names>Y</given-names></name><name><surname>Chikayama</surname><given-names>E</given-names></name><name><surname>Date</surname><given-names>Y</given-names></name><name><surname>Kikuchi</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Exploratory machine-learned theoretical chemical shifts can closely predict metabolic mixture signals</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>8213</fpage><lpage>8220</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhs1OlsbnI</pub-id><pub-id pub-id-type="doi">10.1039/C8SC03628D</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-Bombarelli</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach</article-title><source>Nat. Mater.</source><year>2016</year><volume>15</volume><fpage>1120</fpage><lpage>1127</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtlSgt7%2FP</pub-id><pub-id pub-id-type="doi">10.1038/nmat4717</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Granda</surname><given-names>JM</given-names></name><name><surname>Donina</surname><given-names>L</given-names></name><name><surname>Dragone</surname><given-names>V</given-names></name><name><surname>Long</surname><given-names>DL</given-names></name><name><surname>Cronin</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Controlling an organic synthesis robot with machine learning to search for new reactivity</article-title><source>Nature</source><year>2018</year><volume>559</volume><fpage>377</fpage><lpage>381</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtlClsb%2FJ</pub-id><pub-id pub-id-type="doi">10.1038/s41586-018-0307-8</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez-Bombarelli</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Automatic chemical design using a data-driven continuous representation of molecules</article-title><source>ACS Cent. Sci.</source><year>2018</year><volume>4</volume><fpage>268</fpage><lpage>276</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXntlWquw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">A machine learning approach to zeolite synthesis enabled by automatic literature data extraction</article-title><source>ACS Cent. Sci.</source><year>2019</year><volume>5</volume><fpage>892</fpage><lpage>899</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXns1Kjt7s%3D</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00193</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hiszpanski</surname><given-names>AM</given-names></name><etal/></person-group><article-title xml:lang="en">Nanomaterial synthesis insights from machine learning of scientific articles by extracting, structuring, and visualizing knowledge</article-title><source>J. Chem. Inf. Model.</source><year>2020</year><pub-id pub-id-type="doi">10.1021/acs.jcim.0c00199</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aykol</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Network analysis of synthesizable materials discovery</article-title><source>Nat. Commun.</source><year>2019</year><volume>10</volume><pub-id pub-id-type="doi">10.1038/s41467-019-10030-5</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mrdjenovich</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Propnet: a knowledge graph for materials science</article-title><source>Matter</source><year>2020</year><volume>2</volume><fpage>464</fpage><lpage>480</lpage><pub-id pub-id-type="doi">10.1016/j.matt.2019.11.013</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tshitoyan</surname><given-names>V</given-names></name><etal/></person-group><article-title xml:lang="en">Unsupervised word embeddings capture latent knowledge from materials science literature</article-title><source>Nature</source><year>2019</year><volume>571</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtlamurrK</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1335-8</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Towards artificial general intelligence with hybrid Tianjic chip architecture</article-title><source>Nature</source><year>2019</year><volume>572</volume><fpage>106</fpage><lpage>111</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsFShu7bF</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1424-8</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramachandram</surname><given-names>D</given-names></name><name><surname>Taylor</surname><given-names>GW</given-names></name></person-group><article-title xml:lang="en">Deep multimodal learning: a survey on recent advances and trends</article-title><source>IEEE Sign. Process. Mag.</source><year>2017</year><volume>34</volume><fpage>96</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1109/MSP.2017.2738401</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>GH</given-names></name><name><surname>Shao</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Pipe</surname><given-names>KP</given-names></name></person-group><article-title xml:lang="en">Engineered doping of organic semiconductors for enhanced thermoelectric efficiency</article-title><source>Nat. Mater.</source><year>2013</year><volume>12</volume><fpage>719</fpage><lpage>723</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3sXntVaqs7c%3D</pub-id><pub-id pub-id-type="doi">10.1038/nmat3635</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xia</surname><given-names>Y</given-names></name><name><surname>Sun</surname><given-names>K</given-names></name><name><surname>Ouyang</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Solution-processed metallic conducting polymer films as transparent electrode of optoelectronic devices</article-title><source>Adv. Mater.</source><year>2012</year><volume>24</volume><fpage>2436</fpage><lpage>2440</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38Xlt1Knur8%3D</pub-id><pub-id pub-id-type="doi">10.1002/adma.201104795</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Bießmann, L. et al. Highly conducting, transparent PEDOT:PSS polymer electrodes from post-treatment with weak and strong acids. <italic>Adv. Electron. Mater</italic>. <bold>5</bold>, <ext-link xlink:href="10.1002/aelm.201800654" ext-link-type="doi">https://doi.org/10.1002/aelm.201800654</ext-link> (2019).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roch</surname><given-names>LM</given-names></name><etal/></person-group><article-title xml:lang="en">From absorption spectra to charge transfer in nanoaggregates of oligomers with machine learning</article-title><source>ACS Nano</source><year>2020</year><pub-id pub-id-type="doi">10.1021/acsnano.0c00384</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Muckley, E. S., Collins, L., Srijanto, B. R. &amp; Ivanov, I. N. Machine learning-enabled correlation and modeling of multimodal response of thin film to environment on macro and nanoscale using “Lab-on-a-Crystal”. <italic>Adv. Funct. Mater</italic>. <bold>30</bold>, <ext-link xlink:href="10.1002/adfm.201908010" ext-link-type="doi">https://doi.org/10.1002/adfm.201908010</ext-link> (2020).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanaka</surname><given-names>F</given-names></name><name><surname>Sato</surname><given-names>H</given-names></name><name><surname>Yoshii</surname><given-names>N</given-names></name><name><surname>Matsui</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Materials informatics for process and material co-optimization</article-title><source>IEEE Trans. Semicond. Manuf.</source><year>2019</year><volume>32</volume><fpage>444</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1109/TSM.2019.2943162</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Materials informatics: a journey towards material design and synthesis</article-title><source>Dalton Trans.</source><year>2016</year><volume>45</volume><fpage>10497</fpage><lpage>10499</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XpsVOmtb0%3D</pub-id><pub-id pub-id-type="doi">10.1039/C6DT01501H</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Zhou, J. et al. Graph neural networks: a review of methods and applications. <ext-link xlink:href="https://arxiv.org/abs/1812.08434" ext-link-type="uri">https://arxiv.org/abs/1812.08434</ext-link> (2018).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Qi, P., Dozat, T., Zhang, Y. &amp; Manning, C. D. Universal Dependency Parsing from Scratch. <italic>Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies</italic>, 160–170 (Publisher is Association for Computational Linguistics, Brussels, Belgium, 2018). <ext-link xlink:href="10.18653/v1/K18-2001" ext-link-type="doi">https://doi.org/10.18653/v1/K18-2001</ext-link>.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanov, K. BERT: pre-training of deep bidirectional transformers for language understanding. <ext-link xlink:href="https://arxiv.org/abs/1810.04805" ext-link-type="uri">https://arxiv.org/abs/1810.04805</ext-link> (2019).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>D</given-names></name><name><surname>Hahn</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Extended-connectivity fingerprints</article-title><source>J. Chem. Inf. Model.</source><year>2010</year><volume>50</volume><fpage>742</fpage><lpage>754</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3cXlt1Onsbg%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci100050t</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Li, Y., Tarlow, D., Brockschmidt, M. &amp; Zemel, R. Gated graph sequence neural networks. <ext-link xlink:href="https://arxiv.org/abs/1511.05493" ext-link-type="uri">https://arxiv.org/abs/1511.05493</ext-link> (2017).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">McInnes, L., Healy, J. &amp; Melville, J. UMAP: uniform manifold approximation and projection for dimension reduction. <ext-link xlink:href="https://arxiv.org/abs/1802.03426" ext-link-type="uri">https://arxiv.org/abs/1802.03426</ext-link> (2018).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Koh, P. W. &amp; Liang, P. Understanding Black-box Predictions via Influence Functions. <ext-link xlink:href="https://arxiv.org/abs/1703.04730" ext-link-type="uri">https://arxiv.org/abs/1703.04730</ext-link> (2017).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jinich</surname><given-names>A</given-names></name><name><surname>Sanchez-Lengeling</surname><given-names>B</given-names></name><name><surname>Ren</surname><given-names>H</given-names></name><name><surname>Harman</surname><given-names>R</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">A mixed quantum chemistry/machine learning approach for the fast and accurate prediction of biochemical redox potentials and its large-scale application to 315000 redox reactions</article-title><source>ACS Cent. Sci</source><year>2019</year><volume>5</volume><fpage>1199</fpage><lpage>1210</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtFWisb%2FN</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00297</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Socher, R. et al. Zero-shot learning through cross-modal transfer. <ext-link xlink:href="https://arxiv.org/abs/1301.3666" ext-link-type="uri">https://arxiv.org/abs/1301.3666</ext-link> (2013).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. <ext-link xlink:href="https://arxiv.org/abs/1412.6980" ext-link-type="uri">https://arxiv.org/abs/1412.6980</ext-link> (2014).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Sak, H., Senior, A. &amp; Beaufays, F. Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition. <ext-link xlink:href="https://arxiv.org/abs/1402.1128" ext-link-type="uri">https://arxiv.org/abs/1402.1128</ext-link> (2014).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec18"><title>Supplementary information</title><p id="Par45"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/43246_2020_52_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/43246_2020_52_MOESM2_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Description of Additional Supplementary Files</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM3" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/43246_2020_52_MOESM3_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Data 1</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM4" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/43246_2020_52_MOESM4_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Peer Review File</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p><bold>Supplementary information</bold> is available for this paper at <ext-link xlink:href="10.1038/s43246-020-00052-8" ext-link-type="doi">https://doi.org/10.1038/s43246-020-00052-8</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">Communications Materials</facet-value></facet><facet name="year"><facet-value count="1">2020</facet-value></facet><facet name="country"><facet-value count="1">Japan</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
