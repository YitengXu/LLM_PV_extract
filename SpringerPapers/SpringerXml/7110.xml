<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-021-00656-9</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-021-00656-9</article-id><article-id pub-id-type="manuscript">656</article-id><article-id pub-id-type="doi">10.1038/s41524-021-00656-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/166</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6551-9810</contrib-id><name><surname>Liang</surname><given-names>Qiaohao</given-names></name><address><email>hqliang@mit.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs41524021006569_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Gongora</surname><given-names>Aldair E.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Ren</surname><given-names>Zekun</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" id="Au4"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9753-6802</contrib-id><name><surname>Tiihonen</surname><given-names>Armi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" id="Au5"><name><surname>Liu</surname><given-names>Zhe</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author" id="Au6"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6179-1390</contrib-id><name><surname>Sun</surname><given-names>Shijing</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au7"><name><surname>Deneault</surname><given-names>James R.</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" id="Au8"><name><surname>Bash</surname><given-names>Daniil</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au9"><name><surname>Mekki-Berrada</surname><given-names>Flore</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" id="Au10"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8990-8802</contrib-id><name><surname>Khan</surname><given-names>Saif A.</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" id="Au11"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-1270-9047</contrib-id><name><surname>Hippalgaonkar</surname><given-names>Kedar</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au12"><name><surname>Maruyama</surname><given-names>Benji</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" id="Au13"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2379-2018</contrib-id><name><surname>Brown</surname><given-names>Keith A.</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au14"><name><surname>Fisher III</surname><given-names>John</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au15"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8345-4937</contrib-id><name><surname>Buonassisi</surname><given-names>Tonio</given-names></name><address><email>buonassisi@mit.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs41524021006569_cor15">r</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution content-type="org-name">Massachusetts Institute of Technology</institution></institution-wrap><addr-line content-type="city">Cambridge</addr-line><addr-line content-type="state">MA</addr-line><country country="US">United States</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.189504.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 7558</institution-id><institution content-type="org-name">Boston University</institution></institution-wrap><addr-line content-type="city">Boston</addr-line><addr-line content-type="state">MA</addr-line><country country="US">United States</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.429485.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 0442 4521</institution-id><institution content-type="org-name">Singapore-MIT Alliance for Research and Technology</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.417730.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 0543 4035</institution-id><institution content-type="org-name">Air Force Research Laboratory</institution></institution-wrap><addr-line content-type="city">Dayton</addr-line><addr-line content-type="state">Ohio</addr-line><country country="US">United States</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.185448.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0637 0221</institution-id><institution content-type="org-division">Agency for Science</institution><institution content-type="org-name">Technology and Research (A*STAR)</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution content-type="org-name">National University of Singapore</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.5373.2</institution-id><institution-id institution-id-type="ISNI">0000000108389418</institution-id><institution content-type="org-name">Aalto University</institution></institution-wrap><addr-line content-type="city">Espoo</addr-line><country country="FI">Finland</country></aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.440588.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 0307 1240</institution-id><institution content-type="org-name">Northwestern Polytechincal University (NPU)</institution></institution-wrap><addr-line content-type="city">Xi’an</addr-line><addr-line content-type="state">Shaanxi</addr-line><country country="CN">P.R. China</country></aff></contrib-group><author-notes><corresp id="IDs41524021006569_cor1"><label>a</label><email>hqliang@mit.edu</email></corresp><corresp id="IDs41524021006569_cor15"><label>r</label><email>buonassisi@mit.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>18</day><month>11</month><year>2021</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2021</year></pub-date><volume>7</volume><issue seq="188">1</issue><elocation-id>188</elocation-id><history><date date-type="registration"><day>26</day><month>10</month><year>2021</year></date><date date-type="received"><day>16</day><month>5</month><year>2021</year></date><date date-type="accepted"><day>12</day><month>10</month><year>2021</year></date><date date-type="online"><day>18</day><month>11</month><year>2021</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2021</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Bayesian optimization (BO) has been leveraged for guiding autonomous and high-throughput experiments in materials science. However, few have evaluated the efficiency of BO across a broad range of experimental materials domains. In this work, we quantify the performance of BO with a collection of surrogate model and acquisition function pairs across five diverse experimental materials systems. By defining acceleration and enhancement metrics for materials optimization objectives, we find that surrogate models such as Gaussian Process (GP) with anisotropic kernels and Random Forest (RF) have comparable performance in BO, and both outperform the commonly used GP with isotropic kernels. GP with anisotropic kernels has demonstrated the most robustness, yet RF is a close alternative and warrants more consideration because it is free from distribution assumptions, has smaller time complexity, and requires less effort in initial hyperparameter selection. We also raise awareness about the benefits of using GP with anisotropic kernels in future materials optimization campaigns.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>TOTAL S.A. research grant funded through MITei</institution></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>National Science Foundation (NSF)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100000001</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">CMMI-1661412</award-id><award-id award-type="FundRef grant">CMMI-1661412</award-id><award-id award-type="FundRef grant">CBET-1605547</award-id><principal-award-recipient><name><surname>Gongora</surname><given-names>Aldair E.</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Brown</surname><given-names>Keith A.</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Buonassisi</surname><given-names>Tonio</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Google</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100006785</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>Boston University (BU)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100007161</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>Singapore-MIT Alliance for Research and Technology Centre (SMART)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100001474</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>Total</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100007185</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>United States Department of Defense | Defense Advanced Research Projects Agency (DARPA)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100000185</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">HR001118C0036</award-id><award-id award-type="FundRef grant">HR001118C0036</award-id><award-id award-type="FundRef grant">HR001118C0036</award-id><principal-award-recipient><name><surname>Tiihonen</surname><given-names>Armi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Sun</surname><given-names>Shijing</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Buonassisi</surname><given-names>Tonio</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Skolkovo Institute of Science and Technology (Skoltech)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100007455</institution-id></institution-wrap></funding-source></award-group><award-group><funding-source><institution-wrap><institution>United States Department of Defense | United States Air Force | AFMC | Air Force Office of Scientific Research (AF Office of Scientific Research)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100000181</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">19RHCOR089</award-id><award-id award-type="FundRef grant">19RHCOR089</award-id><principal-award-recipient><name><surname>Deneault</surname><given-names>James R.</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Maruyama</surname><given-names>Benji</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>Agency for Science, Technology and Research (A*STAR)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100001348</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">A1898b0043</award-id><award-id award-type="FundRef grant">A1898b0043</award-id><award-id award-type="FundRef grant">A1898b0043</award-id><award-id award-type="FundRef grant">A1898b0043</award-id><principal-award-recipient><name><surname>Bash</surname><given-names>Daniil</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Mekki-Berrada</surname><given-names>Flore</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Khan</surname><given-names>Saif A.</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Hippalgaonkar</surname><given-names>Kedar</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>188</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>26</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2021_Article_656.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Autonomous experimental systems have recently emerged as the frontier for accelerated materials research. These systems excel at optimizing materials objectives, e.g. environmental stability of solar cells or toughness of 3D printed mechanical structures, that are typically costly, slow, or difficult to simulate and experimentally evaluate. While autonomous experimental systems are often associated with high sample synthesis rates via high-throughput experiments (HTE), they may also utilize closed-loop feedback from machine learning (ML) during materials property optimization. The latter has motivated the integration of advanced lab automation components with ML algorithms. Specifically, active learning<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup> algorithms have traditionally been applied to minimizing total experiment costs while maximizing machine learning model accuracy through hyperparameter tuning. Their primary utility for materials science research, where experiments remain relatively costly, lies in an iterative formulation that proposes targeted experiments with regard to a specific design objective based on prior experimental observations. Bayesian optimization (BO)<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref></sup>, one class of active learning methods, utilizes a surrogate model to approximate a mapping from experiment parameters to an objective criterion, and provides optimal experiment selection when combined with an acquisition function. BO has been shown to be a data-efficient closed-loop active learning method for navigating complex design spaces<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref></sup>. Consequently, it has become an appealing methodology for accelerated materials research and optimizing material properties<sup><xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR22">22</xref></sup> beyond state-of-the-art.</p><p id="Par3">The materials science community has seen successful demonstrations in performing materials optimization via autonomous experiments guided by BO and its variants<sup><xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR23">23</xref>–<xref ref-type="bibr" rid="CR27">27</xref></sup>. Naturally, previous work emphasized the ability to achieve materials optimization with fewer experimental iterations. There have been very few quantitative analyses of the acceleration or enhancement resulting from applying BO algorithms and discussions on the sensitivity of BO performance to surrogate model and acquisition function selection. Rohr et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, Graff et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, and Gongora et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> have evaluated the performance of BO using multiple surrogate models and acquisition functions within specific electrocatalyst, ligand, and mechanical structures design spaces, respectively. However, comprehensive benchmarking of the performance of BO algorithms across a broad array of experimental materials systems, as we present here, has not been done. Although one could test BO across various analytical functions or emulated materials design spaces<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, empirical performance evaluation on a broader collection of experimental materials science data is still necessary to provide practical guidelines. Optimization algorithms need systematic and comprehensive benchmarks to evaluate their performance, and the lack of these could significantly slow down advanced algorithm development, eventually posing obstacles for building fully autonomous platforms. Presented below, the benchmarking framework, practical performance metrics, datasets collected from realistic noisy experiments, and insights derived from a side-by-side comparison of BO algorithms will allow researchers to evaluate and select their optimization algorithm before deploying it on autonomous research platforms. Our work provides comprehensive benchmarks for optimization algorithms specifically developed for autonomous and high-throughput experimental materials research. Ideally, it provides insight for designing and deploying Bayesian optimization algorithms that suit the sample generation rate of future autonomous platforms and tackle materials optimization in more complex design spaces.</p><p id="Par4">In this work, we benchmark the performance of BO across five different experimental materials science datasets, optimizing properties of carbon nanotube-polymer blends, silver nanoparticles, lead-halide perovskites, and additively manufactured polymer structures and shapes. We utilize a pool-based active learning framework to approximate experimental materials optimization processes. We also adapt metrics such as enhancement factor and acceleration factor to quantitatively compare performances of BO algorithms against that of a random sampling baseline. We observe that when utilizing the same acquisition functions, BO with Random Forest (RF)<sup><xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR33">33</xref></sup> as a surrogate model has comparable performance to BO with Gaussian Process (GP)<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> with automatic relevance detection (ARD)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> that has an anisotropic kernel. They also both outperform commonly used BO with GP without ARD. Our discussion on surrogate models’ differences in their implicit distributional assumptions, time complexities, hyperparameter tuning, and the benefits of using GP with anisotropic kernels yield deeper insights regarding surrogate model selection for materials optimization campaigns. We also offer open-source implementation of benchmarking code and datasets to support the future development of such algorithms in the field.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Experimental materials datasets</title><p id="Par5">As seen in Table <xref rid="Tab1" ref-type="table">1</xref>, we have assembled a list of five materials datasets with varying sizes, dimensions <italic>n</italic><sub>dim</sub>, and materials systems. These diverse datasets are generated from autonomous experimental studies conducted by collaborators, and facilitate BO performance analysis across a broad range of materials. They contain three to five independent input features, one property as materials optimization objective, and contain from a few tens to hundreds of data points. Based on their optimization objectives, the design space input features in the datasets range from materials compositions to synthesis processing parameters, as seen in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1–5</xref>. For consistency, each dataset has its optimization problem formulated as global minimization.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Description of experimental materials science datasets.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th><p>Domain</p></th><th><p>Synthesis</p></th><th><p>Size</p></th><th><p><italic>n</italic><sub>dim</sub></p></th><th><p>Optimization Objective</p></th></tr></thead><tbody><tr><td><p>P3HT/CNT<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></p></td><td><p>Composite blends</p></td><td><p>Drop casting</p></td><td><p>178</p></td><td><p>5</p></td><td><p>Electrical conductivity</p></td></tr><tr><td><p>AgNP<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>Silver nanoparticles</p></td><td><p>Flow synthesis</p></td><td><p>164</p></td><td><p>5</p></td><td><p>Absorbance spectrum score</p></td></tr><tr><td><p>Perovskite<sup><xref ref-type="bibr" rid="CR23">23</xref></sup></p></td><td><p>Thin film perovskite</p></td><td><p>Spin coating</p></td><td><p>94</p></td><td><p>3</p></td><td><p>Stability score</p></td></tr><tr><td><p>Crossed barrel<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></p></td><td><p>3D printed structure</p></td><td><p>3D printing</p></td><td><p>600</p></td><td><p>4</p></td><td><p>Mechanical toughness</p></td></tr><tr><td><p>AutoAM<sup><xref ref-type="bibr" rid="CR55">55</xref></sup></p></td><td><p>Materials manufacturing</p></td><td><p>3D printing</p></td><td><p>100</p></td><td><p>4</p></td><td><p>Shape score</p></td></tr></tbody></table></table-wrap></p><p id="Par6">It should be noted that while all datasets were gathered from relatively high-throughput experimental systems, P3HT/CNT, AgNP, Perovskite, and AutoAM had BO guiding the selection of subsequent experiments partially through the materials optimization campaigns. Across the datasets, the differences in the distribution of objective values can be observed in Fig. <xref rid="Fig1" ref-type="fig">1(a)</xref> and the objective values are normalized for comparison purposes; the differences in the distribution of sampled data points in its respective materials design space can be seen in Fig. <xref rid="Fig1" ref-type="fig">1(b)</xref>. The five materials datasets in the current study are available in the following GitHub repository<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Experimental materials dataset design space manifold complexity visualization.</title><p><bold>a</bold> Histogram of objective values normalized to zero-mean without loss of generality. <bold>b</bold> Input feature space, i.e. design space, visualization after dimensionality reduction to 3D via principal component analysis (PCA). The colors of each point in the datasets indicate its value. PCA was performed to reduce the dimension of each dataset to three for visualization, and the three axes shown are the top three principal component directions of each dataset.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_656_Fig1_HTML.png"/></fig></p></sec><sec id="Sec4"><title>Bayesian optimization: surrogate models and acquisition functions</title><p id="Par7">Bayesian optimization (BO)<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR5">5</xref></sup> aims to solve the problem of finding a global optimum (min or max) of an unknown objective function <italic>g</italic>: <bold>x</bold><sup>*</sup> = arg <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\min }_{{{{\bf{x}}}}}g({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq1.gif"/></alternatives></inline-formula> where <bold>x</bold> ∈ <italic>X</italic> and <italic>X</italic> is a domain of interest in <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:msup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>dim</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{R}}}}}^{{n}_{{{\text{dim}}}}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq2.gif"/></alternatives></inline-formula>. BO holds the assumption that this black-box function <italic>g</italic> can be evaluated at any <bold>x</bold> ∈ <italic>X</italic> and the responses are noisy point-wise observations (<bold>x</bold>, <italic>y</italic>), where <italic>E</italic>[<italic>y</italic>∣<italic>g</italic>(<bold>x</bold>)] = <italic>g</italic>(<bold>x</bold>). The surrogate model <italic>f</italic> is probabilistic and consists of a prior distribution that approximates the unknown objective function <italic>g</italic>, and is sequentially updated with collected data to yield a Bayesian posterior belief of <italic>g</italic>. Decision policies aimed to find the optimum in fewer experiments are implemented in acquisition functions, which can use the mean and variance predicted at any <bold>x</bold> ∈ <italic>X</italic> in the posterior to select the next observation to be performed.</p><p id="Par8">The BO algorithm is comprised of both a surrogate model and an acquisition function. The surrogate models considered in this study are random forest (RF)<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, Gaussian process (GP) regression<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>, and GP with automatic relevance detection (ARD)<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>.<list list-type="order"><list-item><p id="Par9">To approximate the experience of a researcher with little prior knowledge of a materials design space, for RF, we have hyperparameters applicable across all five datasets without loss of generality: <italic>n</italic><sub>tree</sub> = 100 and bootstrap = True. Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">1</xref> shows that <italic>n</italic><sub>tree</sub> = 100 is a suitable hyperparameter for RF surrogate models when applied to the five datasets.</p></list-item><list-item><p id="Par10">For hyperparameters of GP, we choose kernels from Matérn52, Matérn32, Matérn12, radial basis function (RBF), and multilayer perceptron (MLP). The initial lengthscale for each kernel was set to unit length.</p></list-item><list-item><p id="Par11">For hyperparameters of GP ARD, we not only have the above kernel choices from GP, but also use ARD, which allows GP to keep anisotropic kernels. The kernel function of GP then has individual characteristic lengthscales <italic>l</italic><sub><italic>j</italic></sub><sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup> for each of the input feature dimensions <italic>j</italic>.</p></list-item></list></p><p id="Par12">As an example, in dimension <italic>j</italic>, Matérn52 kernel function between two points <bold>p</bold>, <bold>q</bold> in design space would be<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msqrt><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>5</mml:mn><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:msubsup><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msqrt><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k({{{{\bf{p}}}}}_{j},{{{{\bf{q}}}}}_{j})={\sigma }_{0}^{2}\cdot (1+\frac{\sqrt{5}r}{{l}_{j}}+\frac{5{r}^{2}}{3{l}_{j}^{2}})\exp (-\frac{\sqrt{5}r}{{l}_{j}})$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ1.gif"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r=\sqrt{{({p}_{j}-{q}_{j})}^{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq3.gif"/></alternatives></inline-formula>, <italic>σ</italic> is the standard deviation and <italic>l</italic><sub><italic>j</italic></sub> is the characteristic length scale. These characteristic length scales can be used to estimate the distance moved along <italic>j</italic><sup>th</sup> dimension from the input values in the design space before the change of objective values become uncorrelated with this feature. <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{{l}_{j}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq4.gif"/></alternatives></inline-formula> is thus useful in understanding the sensitivity of objective value to input feature <italic>j</italic>.</p><p id="Par13">We then pair the selected surrogate model with one of three acquisition functions, including expected improvement (EI), probability of improvement (PI), and lower confidence bound (LCB) <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mrow><mml:msub><mml:mrow><mml:mstyle><mml:mtext>LCB</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mbox{LCB}}}}_{\overline{\lambda }}({{{\bf{x}}}})=-\hat{\mu }({{{\bf{x}}}})+\overline{\lambda }\hat{\sigma }({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq5.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq7.gif"/></alternatives></inline-formula> are the mean and standard deviation estimated by surrogate model while <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{\lambda }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq8.gif"/></alternatives></inline-formula> is an adjustable ratio between exploitation and exploration.</p><p id="Par14">In addition, these surrogate models, their hyperparameters, and acquisition functions were chosen because they represent the majority of off-the-shelf options accessible, and are ones that have been widely applied to materials optimization campaigns in the field. Our study provides a comprehensive test across the five datasets in order to reflect how each BO algorithm, resulting from the pairing above, performs across many different materials science design spaces. GP and RF were also selected as examples to specifically illustrate how the differences in implicit distributional assumptions of surrogate models could affect their predictions of the mean and standard deviation when selecting subsequent experiments and performance in BO.</p></sec><sec id="Sec5"><title>Pool-based active learning benchmarking framework</title><p id="Par15">Within each respective experimental dataset, the set of data points form a discrete representation of ground truth in the materials design space. Figure <xref rid="Fig2" ref-type="fig">2</xref> shows the pool-based active learning benchmarking framework we use to simulate materials optimization campaigns guided by BO algorithms in each materials system.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Benchmarking framework including a simulation of BO performing closed-loop optimization with alternating inference and planning stages.</title><p><italic>X</italic> is the iteratively collected sequence of experimental data (<bold>x</bold>, <italic>y</italic>) during the optimization campaign. <italic>D</italic> is the original pool or total undiscovered set of data from which the next experiments are selected. <italic>f</italic> is the surrogate model used to estimate mean <inline-formula id="IEq9"><alternatives><mml:math id="IEq9_Math"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq9.gif"/></alternatives></inline-formula> and standard deviation <inline-formula id="IEq10"><alternatives><mml:math id="IEq10_Math"><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq10.gif"/></alternatives></inline-formula>, which parameterize the acquisition function <italic>α</italic> to select next experiment <bold>x</bold><sup>*</sup> to be evaluated.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_656_Fig2_HTML.png"/></fig></p><p id="Par16">The framework has the following properties:<list list-type="order"><list-item><p id="Par17">It has the traits of an active learning study as it contains a machine learning model that is iteratively refined through subsequent experimental observation selection based on information from previously explored data points. The framework is also adapted for BO, and emphasizes the optimization of materials objectives over building an accurate regression model in design space.</p></list-item><list-item><p id="Par18">It is derived from pool-based active learning. Besides the randomly selected initial experiments, the subsequent experimental observations are selected from the total pool of undiscovered data points (<italic>x</italic>, <italic>y</italic>) ∈ <italic>D</italic>, whose input features <bold>x</bold> are all made available for evaluation by the acquisition functions. The ground truth in the materials design space was represented with a fixed number of discrete data points to resembles studies that have a known total number of experimental conditions to select from due to their equipment resolution limitation. We chose such representation over a continuous emulator for the following reasons and concerns: <list list-type="order"><list-item><p id="Par19">In real research scenarios, materials design spaces are not completely continuous due to noise and limitation in the resolution of equipment apparatus and experiment design.</p></list-item><list-item><p id="Par20">Because many materials datasets do not cover their design space evenly with at high resolution, the fitted ground truth model would have greater variance in regions that were loosely covered by the training experimental dataset. As a result, even if we don’t consider overfitting, the continuous emulator could have varied accuracy across its design space compared to real experimental ground truth, greatly affecting optimization results.</p></list-item><list-item><p id="Par21">To emulate materials design spaces, selecting of models such as GP introduces smoothness assumptions into the design space, and thus during the benchmarking process could give great advantages to BO algorithms with GP surrogate models sharing similar gaussianity assumptions. In Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">2</xref> - <xref ref-type="supplementary-material" rid="MOESM1">3</xref>, we show how such induced bias from different ground truth models affects the evaluation of the performance of BO.</p></list-item></list></p></list-item><list-item><p id="Par22">At each learning cycle of the framework, instead of selecting a larger batch, only one new experiment is obtained. In our retrospective study, a batch size of 1 was most applicable across five materials studies with varying dimensions and dataset sizes and allowed us to directly compare the impact of surrogate model and acquisition function selection while keeping the same batch size. In real experimental setups, the exact tradeoff between batch size and cost of experiment parallelization should be determined by researchers and their equipment apparatus limitations.</p></list-item></list></p><p id="Par23">Each BO algorithm is evaluated for 50 ensembles with 50 independent random seeds governing the initialization of experiments. The aggregated performances of the BO algorithms derived from 50 averaged runs resulting from 10 random five-fold splits using the 50 original ensembles, is compared against a statistical random search baseline, and we can quantitatively evaluate its performance via active learning metrics defined in the sections below. A detailed description of the framework and the calculation of statistical random baselines can be seen in the Methods section. The simulated materials optimization campaigns were conducted on the Boston University Shared Computing Cluster (SCC) and MIT Supercloud<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>, enabling the parallel execution of multiple optimization campaigns on individual computing nodes.</p></sec><sec id="Sec6"><title>Observation of performance through case study on Crossed barrel dataset</title><p id="Par24">While the five datasets covered a breadth of materials domains, the relative performances of tested BO algorithms were observed to be quite consistent. The benchmarking results are thus showcased using the Crossed barrel dataset<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, which was collected by grid sampling the design space through a robotic experimental system while optimizing the toughness of 3D printed crossed barrel structures. For the full combinatorial study including all types of GP kernels and acquisition functions, please kindly refer to Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">5–9</xref> besides Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>The aggregated performance of BO algorithms on the Crossed barrel dataset.</title><p>Performance is measured by <bold>a</bold> Top% vs. learning cycle <italic>i</italic> against a random baseline, <bold>b</bold> Enhancement factor EF and <bold>c</bold> Acceleration factor AF, respectively. The algorithms with GP ARD as a surrogate model are labeled in red, RF in blue, and GP in green; higher color saturation is correlated with better performance. Variation at each learning cycle is visualized by plotting the median as well as shaded regions representing the 5<sup>th</sup> to 95<sup>th</sup> percentile of the aggregated 50-run ensembles. The acquisition functions used are EI, PI, and LCB<inline-formula id="IEq11"><alternatives><mml:math id="IEq11_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{\lambda }}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq11.gif"/></alternatives></inline-formula><bold>d</bold> Jaccard similarity index calculated between the optimization campaign sequences of BO algorithms RF: LCB<inline-formula id="IEq12"><alternatives><mml:math id="IEq12_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq12.gif"/></alternatives></inline-formula> and GP ARD: LCB<inline-formula id="IEq13"><alternatives><mml:math id="IEq13_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq13.gif"/></alternatives></inline-formula>. The median, 5<sup>th</sup>, 95<sup>th</sup> percentile of the 50-run ensemble are shown respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_656_Fig3_HTML.png"/></fig></p><p id="Par25">As for the performance metric, we use<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:mi>%</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle><mml:mtext>number of top candidates discovered</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>number of total top candidates</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/></mml:mrow></mml:mfrac><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{Top}}} \% (i)=\frac{{{\mbox{number of top candidates discovered}}}}{{{\mbox{number of total top candidates}}}\,}\in [0,1]$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ2.gif"/></alternatives></disp-formula>to show the fraction of the crossed barrel structures with top 5% toughness that have been discovered by cycle <italic>i</italic> = 1, 2, 3, …, <italic>N</italic>. Top% describes how quickly can a BO-guided autonomous experimental system could identify multiple top candidates in a materials design space. Keeping multiple well-performing candidates allows one to not only observe regions in design space that frequently yield high-performing samples but also have backup options for further evaluation should the most optimal candidate fail in subsequent evaluations. There are research objectives related to finding any good materials candidate, yet in those cases, random selection could outperform optimization algorithms due to luck in a simple design space. Our objective of finding multiple or all top-tier candidates is more applicable to experimental materials optimization scenarios and suitable for demonstrating the true efficacy and impact of BO.</p><p id="Par26">Figure <xref rid="Fig3" ref-type="fig">3</xref> (a) illustrates learning rates based on Top% metric and the following are observed:<list list-type="order"><list-item><p id="Par27">RF initially excels at lower learning cycles, while GP with ARD takes the lead after Top% = 0.46. Under the same acquisition function, performance of RF as a surrogate model is often on par, if not slightly worse, when compared to the performance of GP with ARD.</p></list-item><list-item><p id="Par28">Both GP with ARD and RF outclass GP without ARD.</p></list-item><list-item><p id="Par29">LCB<inline-formula id="IEq14"><alternatives><mml:math id="IEq14_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq14.gif"/></alternatives></inline-formula> typically outperform other LCB<inline-formula id="IEq15"><alternatives><mml:math id="IEq15_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{\lambda }}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq15.gif"/></alternatives></inline-formula> acquisition functions that are biased towards overly exploration or exploitation as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">5</xref>–<xref ref-type="supplementary-material" rid="MOESM1">9</xref>. These results enhance prior beliefs on acquisition strategy selection originated from theoretical studies<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup> and thus emphasize the importance of acquisition strategies that balance exploration and exploitation for future studies. LCB<inline-formula id="IEq16"><alternatives><mml:math id="IEq16_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq16.gif"/></alternatives></inline-formula> at times even outperformed EI, which is a very popular acquisition function in many previous materials optimization studies but has also been known to make excessive greedy decisions<sup><xref ref-type="bibr" rid="CR41">41</xref>–<xref ref-type="bibr" rid="CR43">43</xref></sup>. The performance of BO algorithms using the probability of improvement (PI) as acquisition function has also been evaluated, but its performance was quite consistently worse than EI and therefore not the focus of discussion; this observation can be partially attributed to PI only focusing on how likely is an improvement occurs at next experiment, but not considering how much improvement could be made during the evaluation.</p></list-item></list></p><p id="Par30">When trying to further compare the BO algorithms with different surrogate models in this work, we would like to keep the acquisition function consistent. The same acquisition function LCB<inline-formula id="IEq17"><alternatives><mml:math id="IEq17_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq17_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq17.gif"/></alternatives></inline-formula> was thus used as a representative acquisition function for surrogate model comparisons below because it has shown a decent balance of exploration and exploitation based on its benchmarking results.</p><p id="Par31">We would like to highlight the relative performances of BO algorithms that utilize surrogate models GP ARD (Matérn52 kernel), RF, and GP (Matérn52 kernel). To quantify the relative performance, we set Top% = 0.8 as a realistic goal to indicate we have identified 80% of the structures with top 5% toughness (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). For surrogate models paired with LCB<inline-formula id="IEq18"><alternatives><mml:math id="IEq18_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq18_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq18.gif"/></alternatives></inline-formula>, we see that GP with ARD and RF reach that goal by evaluating approximately 75 and 85 candidates out of the total of 600, whereas GP without ARD needs about 170 samples out of 600. Top% rises initially as slowly as the random baseline because the surrogate models suffer from high variance in prediction, having only been trained with small datasets; Top% ramps up very quickly as the model learns to become more accurate in identifying general regions of interest to explore; the rate of learning eventually slows down at high learning cycles because the local exploitation for the global optimum has exhausted most if not all top 5% toughness candidates, and the algorithms therefore switch to exploring sub-optimal regions. Therefore, it can be assumed that the most valuable regions to examine performance is before each curve reaches Top% = 0.8 and Top% = 0.8 can be used as a realistic optimization goal.</p><p id="Par32">To quantify the acceleration of discovery from BO, we adapt two other metrics similar to the ones from Rohr et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Both compared to a statistical random baseline,</p><p id="Par33">Enhancement Factor (EF)<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>EF</mml:mtext></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:msub><mml:mrow><mml:mi>%</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>BO</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:msub><mml:mrow><mml:mi>%</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>random</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{EF}}}(i)=\frac{{{\mbox{Top}}}{ \% }_{{{\mbox{BO}}}}(i)}{{{\mbox{Top}}}{ \% }_{{{\mbox{random}}}}(i)}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ3.gif"/></alternatives></disp-formula>shows how much improvement in a metric one would receive at cycle <italic>i</italic>, and Acceleration Factor (AF)<disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>AF</mml:mtext></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:mi>%</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>BO</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>random</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{AF}}}({{\mbox{Top}}} \% =a)=\frac{{i}_{{{\mbox{BO}}}}}{{i}_{{{\mbox{random}}}}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ4.gif"/></alternatives></disp-formula>is the ratio of cycle numbers showing how much faster one could reach a specific value Top%(<italic>i</italic><sub>BO</sub>) = Top%(<italic>i</italic><sub>random</sub>) = <italic>a</italic> ∈ [0, 1]. The aggregated performance of BO algorithms is further quantified via EF and AF curves in Fig. <xref rid="Fig3" ref-type="fig">3(b, c)</xref>: starting off with small EFs or AFs before the surrogate model gains more accuracy; reaching absolute EF<sub>max</sub> and AF<sub>max</sub> of up to 8 × . Eventually, the learning algorithms show diminishing returns from an information gain perspective as we progress deeper into our optimization campaigns during pool-based active learning. We observe that for the two BO algorithms both with the same acquisition function LCB<inline-formula id="IEq19"><alternatives><mml:math id="IEq19_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq19_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq19.gif"/></alternatives></inline-formula> but different surrogate models GP ARD and RF, they reach EF<sub>max</sub> at different learning cycles and AF<sub>max</sub> at different Top%, both corresponding to the switch of best-performing algorithm around Top% = 0.46. RF: LCB<inline-formula id="IEq20"><alternatives><mml:math id="IEq20_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq20_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq20.gif"/></alternatives></inline-formula> clearly excels at lower learning cycles, yet GP ARD: LCB<inline-formula id="IEq21"><alternatives><mml:math id="IEq21_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq21_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq21.gif"/></alternatives></inline-formula> takes the lead and would reach Top% = 0.8 with fewer experiments. Therefore, these results objectively show that optimal BO algorithm selection varies with the assigned experiment budget and specific optimization task<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par34">Since we identified two BO algorithms, RF: LCB<inline-formula id="IEq22"><alternatives><mml:math id="IEq22_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq22_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq22.gif"/></alternatives></inline-formula> and GP ARD: LCB<inline-formula id="IEq23"><alternatives><mml:math id="IEq23_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq23_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq23.gif"/></alternatives></inline-formula>, to have comparable performance, we wanted to further investigate how similar their optimization paths were in the design space when starting from the same initial experiments. In Fig. <xref rid="Fig3" ref-type="fig">3(d)</xref>, we use the Jaccard similarity index to quantify the similarity in optimizations paths. Jaccard similarity, <inline-formula id="IEq24"><alternatives><mml:math id="IEq24_Math"><mml:mrow><mml:mi>J</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∣</mml:mo><mml:mi>A</mml:mi><mml:mo>∩</mml:mo><mml:mi>B</mml:mi><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mo>∣</mml:mo><mml:mi>A</mml:mi><mml:mo>∪</mml:mo><mml:mi>B</mml:mi><mml:mo>∣</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="IEq24_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J=\frac{| A\cap B| }{| A\cup B| }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq24.gif"/></alternatives></inline-formula>, is the size of the intersection divided by the size of the union of two finite sample sets; specifically in our benchmarking study, using the same 50-ensemble runs that generated Fig. <xref rid="Fig3" ref-type="fig">3(a)</xref>, we can calculate Jaccard similarity value <italic>J</italic>(<italic>i</italic>) at each learning cycle <italic>i</italic>, where <italic>A</italic>(<italic>i</italic>) is the set of data points sequential collected at each learning cycle during an optimization path guided by BO algorithm GP ARD: LCB<inline-formula id="IEq25"><alternatives><mml:math id="IEq25_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq25_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq25.gif"/></alternatives></inline-formula>, and <italic>B</italic>(<italic>i</italic>) is that of using RF: LCB<inline-formula id="IEq26"><alternatives><mml:math id="IEq26_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq26_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq26.gif"/></alternatives></inline-formula>. As baselines, we have also drawn what the Jaccard similarity value would look like between two optimization paths that begin with the same initial experiments and statistically have the least overlap or most overlap. When <italic>i</italic> = 1 or 2, the same initial experiments are given to the two BO algorithms, and <italic>J</italic> = 1. When 2 &lt; <italic>i</italic> &lt; 18, we can see that the Jaccard similarity value drops as quickly as the statistically least overlapping paths, indicating that despite the fact that GP with ARD and RF were trained on the same initial experiments at the onset, they follow very different paths in the materials design space. This behavior indicates that, despite achieving comparable performance, they exploit the underlying physics differently by virtue of the choice of experiments.</p><p id="Par35">When <italic>i</italic> ≥ 18, the general trend is that <italic>J</italic> increases with <italic>i</italic>, indicating that the paths chosen by the two algorithms gradually start to have some overlap as they move towards finding crossed barrels structures with high toughness. Recall both algorithms reached Top% = 0.8 between 75 to 85 learning cycles in Fig. <xref rid="Fig3" ref-type="fig">3(a)</xref>, and between those learning cycles, we observe that <italic>J</italic> is approximately between 0.27–0.33, still considerably far from <italic>J</italic> = 1. This observation shows that while both algorithms have comparable performance in the task of finding crossed barrel structures with good toughness, due to their different choice of surrogate models, their paths towards discovering optimum can differ considerably.</p><p id="Par36">In addition, the Jaccard similarity value does not increase monotonically, and a significant drop can be seen in <italic>J</italic> such as one around <italic>i</italic> = 50, which coincides with the learning cycles where GP ARD: LCB<inline-formula id="IEq27"><alternatives><mml:math id="IEq27_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq27_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq27.gif"/></alternatives></inline-formula> overtook RF : LCB<inline-formula id="IEq28"><alternatives><mml:math id="IEq28_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq28_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq28.gif"/></alternatives></inline-formula> as best performing algorithm in Fig. <xref rid="Fig3" ref-type="fig">3(a)</xref>. Since the two algorithms used the same acquisition function, this observation shows that while in general the optimization paths of the two algorithms have more overlap over time, occasional divergent paths still take place because the two algorithms have a considerable difference in gathered data used to learn their surrogate models and how their surrogate models predict mean and standard deviation. GP ARD : LCB<inline-formula id="IEq29"><alternatives><mml:math id="IEq29_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq29_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq29.gif"/></alternatives></inline-formula> and RF : LCB<inline-formula id="IEq30"><alternatives><mml:math id="IEq30_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq30_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq30.gif"/></alternatives></inline-formula> started at the same two initial experiments and use the same acquisition function, and the only difference is the surrogate model used. Thus, the divergence and convergence in optimization paths can be again primarily attributed to GP ARD and RF exploiting underlying physics of crossed barrel structure differently. Figure <xref rid="Fig3" ref-type="fig">3(d)</xref> highlights the impact of different surrogate model selection beyond final performance, and to provide better guidelines to future research, inspires us to further investigate the role of surrogate models.</p></sec><sec id="Sec7"><title>Comparison of performance across datasets</title><p id="Par37">To further assess the performance of BO, similar optimization campaigns were conducted for the P3HT/CNT, AgNP, AM ARES, and Perovskite datasets. Across most, if not all, of the investigated datasets, it was observed quite consistently that the performance of BO algorithms using GP with ARD and RF as surrogate models were comparable, and both outperform those using GP without ARD in most datasets. To illustrate, in Fig. <xref rid="Fig4" ref-type="fig">4</xref>, we show such relative performance using normalized EF<sub>max</sub> of BO algorithms same acquisition function LCB<inline-formula id="IEq31"><alternatives><mml:math id="IEq31_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq31_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq31.gif"/></alternatives></inline-formula> but with different surrogate models across all five datasets. In addition to the observation on relative performance, we also observe that BO algorithms with RF and GP ARD as a surrogate models also have plenty of overlap between their 5<sup>th</sup> to 95<sup>th</sup> percentile across five datasets, further indicating their similarity in performance. We also observe the variance of EF<sub>max</sub> for RF is on average lower than those for GPs. This phenomenon can be attributed to RF being an ensemble model, where the high variances from many single decision trees are mitigated through aggregation, resulting in a model with relatively low bias and medium variance<sup><xref ref-type="bibr" rid="CR32">32</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>. GP with anisotropic kernels (GP ARD) is thus shown to be a great surrogate model across most materials domains, with RF being a close second, and both proving to be robust models for future optimization campaigns.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Normalized EF<sub>max</sub> demonstrated by BO algorithms having GP without ARD, GP with ARD, and RF as surrogate models and all using LCB<inline-formula id="IEq32"><alternatives><mml:math id="IEq32_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq32_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq32.gif"/></alternatives></inline-formula> as acquisition function.</title><p>In each dataset, the BO algorithm with the largest EF<sub>max</sub> had its EF scaled to 1, and the other two BO algorithms showing lower EF<sub>max</sub> were correspondingly scaled, resulting in five sets of column plots. For each algorithm applied across datasets, the median of EF<sub>max</sub> is shown by the barplots, and its 5<sup>th</sup> and 95<sup>th</sup> percentile are shown by respective floating bars.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_656_Fig4_HTML.png"/></fig></p><p id="Par38">Notably, EF<sub>max</sub> of the other four datasets were in the 2 × to 4 × range as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">4</xref>, which is noticeably lower than the EF<sub>max</sub> of the crossed barrel dataset in Fig. <xref rid="Fig3" ref-type="fig">3(b)</xref>. The difference in the absolute EF<sub>max</sub> can be attributed to the data collection methodology of the individual datasets. While the crossed barrel dataset was collected using a grid sampling approach, the other four studies were collected along the path of a BO-guided materials optimization campaign. Therefore, these four datasets were smaller in size and possessed an intrinsic enhancement and acceleration within their datasets. As a result, it is reasonable that these datasets demonstrate lower EFs, AFs during benchmarking. Noticeably, the Perovskite dataset had the most intrinsic acceleration because its next experimental choice was guided by BO infused with probabilistic constraints generated from DFT proxy calculations of the environmental stability<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> of perovskite. As a result, the optimization sequence to be chosen in that study is already narrowed down to a more efficient path from initial experiments to final optimum, making the random baseline to appear arbitrarily much worse. Another interesting observation is how the performance of BO with GP without ARD (isotropic kernels) as a surrogate model catches up with those of BO with GP ARD and RF in Perovskite and AutoAM dataset where the design space has an already “easier" path towards the optimum. That is, when materials design space is relatively simple, GP without ARD can serve as an equally good surrogate model in BO compared to GP ARD and RF. Despite the differences described above, we observe that absolute EF<sub>max</sub> &gt; 1 across five datasets, indicating that performance enhancements of BO over a random baseline still exists even in such uneven search spaces. The results again show that BO is a very effective tool for experimental selection in materials science.</p><p id="Par39">The hypothesis that the lower EF<sub>max</sub> are caused by intrinsic acceleration and enhancement resulting from the dataset collection process can be verified by collecting a subset from the uniform grid sampled crossed barrel dataset. This subset is collected by running BO algorithm GP: EI until all candidates with top 5% toughness are found, representing an “easier" path towards optimums, and therefore carries intrinsic enhancement and acceleration. We run the same benchmarking framework on this subset, and observe that EF<sub>max</sub> is reduced, as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">4</xref>.</p></sec></sec><sec id="Sec8" sec-type="discussion"><title>Discussion</title><p id="Par40">In this section, we further compare GP ARD, RF, and GP as surrogate models in BO under the context of autonomous and high-throughput materials optimization.</p><p id="Par41">BO algorithms with GP-type surrogate models have been extensively used in many published materials studies and have shown to be robust models suitable for most optimization problems in materials science based on our benchmarking results in Fig. <xref rid="Fig3" ref-type="fig">3</xref> and Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">4</xref>. Meanwhile, RF is a close second alternative to GP in BO for future HT materials optimization campaigns when considering the factors below. To briefly summarize, RF is free from distribution assumptions in comparison to GP-type models. In general, it is quicker to train due to smaller time complexity, and requires less effort in initial hyperparameter selection. The lack of extrapolation power in RF can also be partially mitigated via initial sampling strategies.</p><p id="Par42">We first highlight the difference between GP and RF during the prediction of mean and standard deviation, where GPs rely on heavy distributional assumptions while RF is distribution-free. GP, whether anisotropic or isotropic, is essentially a distribution over a materials design space such that any finite selection of data points in this design space results in a multivariate Gaussian density over any point of interest. For the selection of a new data point as the next experiment, its predicted mean and standard deviation are all part of such a gaussian distribution constructed from previous experiments. Therefore, the predicted means and standard deviations of GPs from their posteriors carry gaussianity assumptions and can be interpreted as statistical predictions based on prior information. Meanwhile, an RF is an ensemble of decision trees that have slight variation due to bootstrapping. For RF, prediction of objective value and the standard deviation at a new point in materials space is an aggregated result, namely averaging<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> the values from all its decision trees’ respective predictions. Compared to those of GPs, the predicted means and standard deviations of RFs do not have distributional assumptions, and can be interpreted as empirical estimates. If rarely the ground truth of a materials design space indeed satisfied the gaussianity assumptions, then GP type surrogate models could have an advantage over RF in BO as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">2</xref>-<xref ref-type="supplementary-material" rid="MOESM1">3</xref>. However, commonly seen phase changes and exponential relations from thermodynamics often introduce measurements with piece-wise constants or with orders of magnitude changes within neighboring regions of materials design space. Whether these are new findings or outliers, they should be of specific interest to experimentalists. These results are typically smoothed out in the GP surrogate model to satisfy its distributional assumptions. The decision trees of RF would be able to capture these points more accurately and reflect their influences on future predictions. While both RF and GP are both suitable surrogate models, we would like to highlight their fundamental differences when fitting materials domain with unknown distributional assumption.</p><p id="Par43">We next discuss the difference in time complexities of GP and RF as surrogate models. Across five datasets in this study, starting from the same initial experiments and using the same acquisition function LCB<inline-formula id="IEq33"><alternatives><mml:math id="IEq33_Math"><mml:msub><mml:mrow/><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub></mml:math><tex-math id="IEq33_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\overline{2}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq33.gif"/></alternatives></inline-formula>, the ratio of average running time to finish benchmarking framework between the three surrogate models is <italic>t</italic><sub>RF</sub>: <italic>t</italic><sub>GP</sub>: <italic>t</italic><sub>GP ARD</sub> = 1: 1.14: 1.32. For the fitting, we have time complexities <inline-formula id="IEq34"><alternatives><mml:math id="IEq34_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>RF</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">O</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>n</mml:mi><mml:mo>)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>dim</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>tree</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>GP</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">O</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>2</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>dim</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:math><tex-math id="IEq34_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${t}_{{{\text{RF}}}}={{{\mathcal{O}}}}(nlog(n)\cdot {n}_{{{\text{dim}}}}\cdot {n}_{{{\text{tree}}}}) &lt; {t}_{{{\text{GP}}}}={{{\mathcal{O}}}}({n}^{{{\text{3}}}}+{n}^{{{\text{2}}}}\cdot {n}_{{{\text{dim}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq34.gif"/></alternatives></inline-formula><sup><xref ref-type="bibr" rid="CR45">45</xref>–<xref ref-type="bibr" rid="CR47">47</xref></sup>, where <italic>n</italic> is the number of training data, <italic>n</italic><sub>dim</sub> is the design space dimension, <italic>n</italic><sub>tree</sub> is the number of decision trees kept in the RF model. The higher computational complexity of the GP model is mostly due to the process of calculating the inverse of an <italic>n</italic> by <italic>n</italic> matrix during its training process, and keeping anisotropic kernels has added extra computational time. In our study, the datasets are relatively small in size <italic>n</italic>, and therefore the time complexity <inline-formula id="IEq35"><alternatives><mml:math id="IEq35_Math"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">O</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>3</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:math><tex-math id="IEq35_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathcal{O}}}}({n}^{{{\text{3}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq35.gif"/></alternatives></inline-formula> of GP was less troublesome while that of RF is mostly dominated by <inline-formula id="IEq36"><alternatives><mml:math id="IEq36_Math"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">O</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>dim</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>tree</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math><tex-math id="IEq36_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathcal{O}}}}({n}_{{{\text{dim}}}}\cdot {n}_{{{\text{tree}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq36.gif"/></alternatives></inline-formula>. However, if our datasets had sizes of order 10<sup>5</sup> or 10<sup>6</sup>, the amount of computational resources to run BO algorithms with GP-type surrogate models could quickly become intractable due to cubic complexity to <italic>n</italic> and a significantly larger difference in computation speed between RF and GPs would be easily noticeable. As a result, despite being a better performing surrogate model type, GP could be less preferred compared to RF in real-time optimization problems when there is a time limit for selecting the next set of conditions<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. For HT materials research, with increased application of automation, time used in generating samples will eventually match with the time used in suggesting new experiments. Thus, if we aim to have a fast and seamless feedback loop between running BO and performing high-throughput materials experiments, then RF could have a potential advantage over GP-type surrogate models when considering the tradeoff between performance and time complexity.</p><p id="Par44">We last discuss the effort required hyperparameter tuning of a surrogate model during optimization. While RF has potentially more hyperparameters such as <italic>n</italic><sub>tree</sub>, max depth, and max split to select, it is less penalized for sub-optimal choice of hyperparameters compared to GP. In this study, across five datasets, as long as sufficient <italic>n</italic><sub>tree</sub> were used in RF, its regression accuracy is comparable to that of RF with larger <italic>n</italic><sub>tree</sub> as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">1</xref>. Other hyperparameters of RF such as max depth or a minimum number of samples for leaf node either have had less of an impact or are too arbitrary to decide at the start of BO campaign in a specific materials domain. Meanwhile, besides the implicit distribution assumption of using a GP type surrogate model, a kernel (covariance function) of GP specifies a specific smoothness prior on the domain. Choosing a kernel that is incompatible with the unknown domain manifold could significantly slow down optimization conversion due to loss of generalization. For example, the Matérn52 kernel analytically requires the fitted GP to be 2 times differentiable in the mean-square sense<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, which can be difficult to verify for unknown materials design spaces. Selecting such a kernel could introduce extra domain smoothness assumptions to an unfamiliar design space, as we often have limited data to make confident distribution assumptions of the domain at optimization onset. Instead of devoting a nontrivial experimental budget to finding the best kernel for GP using adaptive kernels<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, automating kernel selection<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> or keeping a library of kernels available via online learning, RF is an easier off-the-shelf option that allows one to make fewer structural assumptions about unfamiliar materials domains. If a GP-type surrogate model is still preferred, a Multilayer Perceptron (MLP) kernel<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> mimicking smoothness assumption-free neural networks would be suggested as it has comparable performance to other kernels as seen in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">5</xref>-<xref ref-type="supplementary-material" rid="MOESM1">9</xref>.</p><p id="Par45">Admittedly, our benchmarking framework might have given RF a slight advantage by discretizing the materials domain through actively acquiring a new data point at each cycle and limiting the choice of next experiments within the pool of undiscovered data points. However, the crossed barrel dataset has a sampling density, size, and range within its design space sufficient to cover its manifold complexity. A drawback of RF is that it performs poorly in extrapolation beyond the search space covered by training data, yet in the context of materials optimization campaigns, this disadvantage can be mitigated by clever design of initial experiments, namely using sampling strategies like Latin hypercube sampling (LHS). In this way, we can not only preserve the pseudo-random nature of selecting initial experiments but also cover a wider range of data in each dimension so that the RF surrogate model would not have to often extrapolate to completely unknown regions. We thus believe that when paired with the intuitive tuning of LCB’s weights to adjust exploration and exploitation, RF warrants more consideration as an alternative to GP ARD as a surrogate model in BO for general materials optimization campaigns at early stages.</p><p id="Par46">We would like to lastly raise awareness about the benefits of using GP with anisotropic kernels over GP with isotropic kernels in future materials optimization campaigns. As mentioned earlier, ARD allows us to utilize individual lengthscales for each input dimension <italic>j</italic> in the kernel function of GP, which are subsequently optimized along with learning cycles. These lengthscales in an anisotropic kernel provide a “weight" for measuring the relative relevancy of each feature to predicting the objective, i.e. understanding the sensitivity of objective value to each input feature dimension. The reason GP without ARD shows worse performance is as follows: it will have a single lengthscale in an isotropic kernel as a scaling parameter controlling GP’s kernel function, which is at odds with the fact that each input feature has its distinct contribution to the objective. Depending on how different each feature is in nature, range, and units, e.g. solvent composition vs. printing speed, using the same lengthscale in the kernel function for each feature dimension could provide unreliable predictive results. The materials optimization objective naturally has different sensitivities to each input variable, and thus it is rationale then, that the “lengthscale" parameter inside the GP kernel should be independent. In Fig. <xref rid="Fig4" ref-type="fig">4</xref>, the noticeable improvements of using an anisotropic kernel can be seen in the relative lower performance of GP without ARD compared to that of GP with ARD. While data normalization can partially alleviate the problem, how it is conducted is highly subject to a researcher’s choice, and therefore we would like to raise awareness of the benefits of using GP with anisotropic kernels.</p><p id="Par47">In addition, the lengthscales from the kernels of GP with ARD provides us with more useful information about the input features. These lengthscale values have been used for removing irrelevant inputs<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>, where high <italic>l</italic><sub><italic>j</italic></sub> values imply low relevancy input feature <italic>j</italic>. In the context of materials optimization, we find the following use of ARD especially useful: ARD could identify a few directions in the input space with specially high “relevance.” This means that if we train GP with ARD on input data with their original units and without normalization, once we extract the length scale of each feature <italic>l</italic><sub><italic>j</italic></sub>, our GP model in theory should not be able to accurately extrapolate more than <italic>l</italic><sub><italic>j</italic></sub> units away from collected observations in <italic>j</italic><sup>th</sup> dimension. Thus, <italic>l</italic><sub><italic>j</italic></sub> suggests the range of next experiments to be performed in the <italic>j</italic><sup>th</sup> dimension of the materials design space. It also infers a suitable sampling density in each dimension in the experimental setting. When a particular input feature dimension has a relative small <italic>l</italic><sub><italic>j</italic></sub> or large <inline-formula id="IEq37"><alternatives><mml:math id="IEq37_Math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq37_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{{l}_{j}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq37.gif"/></alternatives></inline-formula>, it means that for a small change in objective value, we would have a relatively large change in the location within this input feature dimension; thus, the sampling density or resolution in this dimension should be high enough to capture such sensitivity. Previous studies have considered using information extracted from these length scales for even more advanced analysis and variable selection<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. At the expense of computation time tolerable in the context of materials optimization campaigns, an anisotropic kernel provides not only a better generalizable GP model but also useful information in analyzing input feature relevancy at each learning cycle. For the above mentioned reasons, it would be great practice for researchers to emphasize their use of GP with anisotropic kernels over GP with isotropic kernels as surrogate models during materials optimization campaigns.</p><p id="Par48">In conclusion, we benchmarked the performance of BO algorithms across five different experimental materials science domains. We utilize a pool-based active learning framework to approximate experimental materials optimization processes, and adapted active learning metrics to quantitatively evaluate the enhancement and acceleration of BO for common research objectives. We demonstrate that when paired with the same acquisition functions, RF as a surrogate model can compete with GP with ARD, and both outperform GP without ARD. In the context of autonomous and high-throughput experimental materials research, GP with anisotropic kernel has shown to be more robust as a surrogate model across most design spaces, yet RF also warrants more consideration because of it being free from distribution assumptions, having lower time complexities, and requiring less effort in initial hyperparameter selection. In addition, we raise awareness about the benefits of using GP with anisotropic kernels over GP with isotropic kernels in future materials optimization campaigns. We provide practical guidelines on surrogate model selection for materials optimization campaigns, and also offer open-source implementation of benchmarking code and datasets to support future algorithmic development.</p><p id="Par49">Establishing benchmarks for active learning algorithms like BO across a broad scope of materials systems is only a starting point. Our observations demonstrate how the choice of active learning algorithms has to adapt to their applications in materials science, motivating more efficient ML-guided closed-loop experimentation, and will likely directly result in a larger number of successful optimization of materials with record-breaking properties. The impact of this work can be extended to not only other materials systems, but also a broader scope of scientific studies utilizing closed-loop and high-throughput research platforms. Through our benchmarking effort, we hope to share our insights with the field of accelerated materials discovery and motivate a closer collaboration between ML and physical science communities.</p></sec><sec id="Sec9" sec-type="methods"><title>Methods</title><sec id="Sec10"><title>Prediction by surrogate models and acquisition functions</title><p id="Par50">In order to estimate the mean <inline-formula id="IEq38"><alternatives><mml:math id="IEq38_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq38_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }({{{{\bf{x}}}}}_{* })$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq38.gif"/></alternatives></inline-formula> and standard deviation <inline-formula id="IEq39"><alternatives><mml:math id="IEq39_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq39_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }({{{{\bf{x}}}}}_{* })$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq39.gif"/></alternatives></inline-formula> of predicted objective value at a previously undiscovered observation <bold>x﻿</bold><sub>*</sub> in design space:</p><p id="Par51">For a Gaussian process (GP), it assumes a prior over the design space that is constructed from already collected observations (x<sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>), <italic>i</italic> = 1, 2, . . . , <italic>n</italic>. This prior is the source of implicit distributional assumptions, and when an undiscovered new observation (<inline-formula id="IEq40"><alternatives><mml:math id="IEq40_Math"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq40_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{x}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq40.gif"/></alternatives></inline-formula>, <inline-formula id="IEq41"><alternatives><mml:math id="IEq41_Math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq41_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{y}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq41.gif"/></alternatives></inline-formula>) is being considered during a noisy setting (<italic>σ</italic> = 0.01), the joint distribution between the objective values of collected data <inline-formula id="IEq42"><alternatives><mml:math id="IEq42_Math"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mtext>n</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:msup></mml:math><tex-math id="IEq42_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\bf{y}}}}\in {{{{\mathcal{R}}}}}^{{{\text{n}}}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq42.gif"/></alternatives></inline-formula> and <inline-formula id="IEq43"><alternatives><mml:math id="IEq43_Math"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq43_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{y}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq43.gif"/></alternatives></inline-formula> is<disp-formula id="Equ5"><label>5</label><alternatives><mml:math id="Equ5_Math"><mml:mrow><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi mathvariant="bold">y</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced><mml:mo>~</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:mtd><mml:mtd columnalign="left"><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mtd><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left[\begin{array}{l}{{{\bf{y}}}}\\ {y}_{* }\end{array}\right] \sim {{{\mathcal{N}}}}\left(0,\left[\begin{array}{ll}K+{\sigma }^{2}I&amp;{K}_{* }^{T}\\ {K}_{* }&amp;{K}_{* * }\end{array}\right]\right).$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par52"><italic>K</italic> is the covariance matrix of the input features <italic>X</italic> = {<bold>x</bold><sub><italic>i</italic></sub>∣<italic>i</italic> = 1, 2, . . . , <italic>n</italic>}; <inline-formula id="IEq44"><alternatives><mml:math id="IEq44_Math"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq44_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{K}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq44.gif"/></alternatives></inline-formula> is the covariance between the collected data and new input feature <inline-formula id="IEq45"><alternatives><mml:math id="IEq45_Math"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq45_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathbf{x}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq45.gif"/></alternatives></inline-formula>; <inline-formula id="IEq46"><alternatives><mml:math id="IEq46_Math"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq46_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{K}}_{\ast \ast}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq46.gif"/></alternatives></inline-formula> is the covariance between the new data. For each of the covariance matrices, <italic>K</italic><sub><italic>p</italic><italic>q</italic></sub> = <italic>k</italic>(<bold>x</bold><sub><italic>p</italic></sub>, <bold>x</bold><sub><italic>q</italic></sub>), where <italic>k</italic> is the kernel function, whether isotropic or anisotropic, used in GP. Then from the posterior, we have estimates <inline-formula id="IEq47"><alternatives><mml:math id="IEq47_Math"><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:math><tex-math id="IEq47_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{K}}_\ast$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq47.gif"/></alternatives></inline-formula><disp-formula id="Equ6"><label>6</label><alternatives><mml:math id="Equ6_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }({{{\bf{x}}}})={y}_{* }={K}_{* }{[K+{\sigma }^{2}I]}^{-1}{{{\bf{y}}}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ6.gif"/></alternatives></disp-formula>and covariance matrix<disp-formula id="Equ7"><label>7</label><alternatives><mml:math id="Equ7_Math"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>I</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$cov({y}_{* })={K}_{* * }-{K}_{* }{[K+{\sigma }^{2}I]}^{-1}{K}_{* }^{T}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ7.gif"/></alternatives></disp-formula></p><p id="Par53">The standard deviation value <inline-formula id="IEq48"><alternatives><mml:math id="IEq48_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq48_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq48.gif"/></alternatives></inline-formula> can be obtained from the diagonal elements of this covariance matrix.</p><p id="Par54">For a random forest (RF), let <inline-formula id="IEq49"><alternatives><mml:math id="IEq49_Math"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq49_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{h}}_{k}({{{{\bf{x}}}}}_{* })$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq49.gif"/></alternatives></inline-formula> denote the prediction of objective value from the <italic>k</italic><sup>th</sup> decision tree in the forest, <italic>k</italic> = 1, 2, . . . , <italic>n</italic><sub>tree</sub>, then<disp-formula id="Equ8"><label>8</label><alternatives><mml:math id="Equ8_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tree</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo mathsize="big"> ∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">tree</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat \mu ({{\mathbf{x}}_*}) = \dfrac{1}{{{n_{{\rm{tree}}}}}}\sum\limits_{k = 1}^{{n_{{\rm{tree}}}}} {{{\hat h}_{\rm{k}}}} ({{\mathbf{x}}_*})$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ8.gif"/></alternatives></disp-formula>and<disp-formula id="Equ9"><label>9</label><alternatives><mml:math id="Equ9_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>tree</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>tree</mml:mtext></mml:mstyle></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt></mml:mrow></mml:math><tex-math id="Equ9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }({{{{\bf{x}}}}}_{* })=\sqrt{\frac{\mathop{\sum }\limits_{k = 1}^{{n}_{{{\text{tree}}}}}{({\hat{h}}_{k}({{{{\bf{x}}}}}_{* })-\hat{\mu }({{{{\bf{x}}}}}_{* }))}^{2}}{{n}_{{{\text{tree}}}}}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ9.gif"/></alternatives></disp-formula>The median or other variations could also be used in future studies to aggregate the predictions for potential improvement in robustness<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>.</p><p id="Par55">We tested three acquisition functions in our study, including expected improvement (EI), probability of improvement (PI), and lower confidence bound (LCB).<disp-formula id="Equ10"><label>10</label><alternatives><mml:math id="Equ10_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>EI</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>best</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{EI}}}\,({{{\bf{x}}}})=({y}_{{{\text{best}}}}-\hat{\mu }({{{\bf{x}}}})-\xi )\cdot {{\Phi }}(Z)+\hat{\sigma }({{{\bf{x}}}})\varphi (Z)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ10.gif"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><mml:math id="Equ11_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>PI</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Φ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{PI}}}\,({{{\bf{x}}}})={{\Phi }}(Z)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ11.gif"/></alternatives></disp-formula>where<disp-formula id="Equ12"><label>12</label><alternatives><mml:math id="Equ12_Math"><mml:mrow><mml:mi>Z</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mstyle><mml:mtext>best</mml:mtext></mml:mstyle></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Z=\frac{{y}_{{{\text{best}}}}-\hat{\mu }({{{\bf{x}}}})-\xi }{\hat{\sigma }({{{\bf{x}}}})}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ12.gif"/></alternatives></disp-formula><inline-formula id="IEq50"><alternatives><mml:math id="IEq50_Math"><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq50_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq50.gif"/></alternatives></inline-formula> and <inline-formula id="IEq51"><alternatives><mml:math id="IEq51_Math"><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover></mml:math><tex-math id="IEq51_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq51.gif"/></alternatives></inline-formula> are estimated mean and standard deviation by surrogate model; <italic>y</italic><sub>best</sub> is best discovered objective value within all collected values so far; <italic>ξ</italic> = 0.01 is jitter value that can slightly control exploration and exploitation; Φ and <italic>φ</italic> are the cumulative density function and probability density function of a normal distribution.<disp-formula id="Equ13"><label>13</label><alternatives><mml:math id="Equ13_Math"><mml:mrow><mml:msub><mml:mrow><mml:mstyle><mml:mtext>LCB</mml:mtext></mml:mstyle></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mbox{LCB}}}}_{\overline{\lambda }}({{{\bf{x}}}})=-\overline{\lambda }\hat{\mu }({{{\bf{x}}}})+\hat{\sigma }({{{\bf{x}}}})$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ13.gif"/></alternatives></disp-formula>where <inline-formula id="IEq52"><alternatives><mml:math id="IEq52_Math"><mml:mover accent="true"><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mo accent="true">¯</mml:mo></mml:mover></mml:math><tex-math id="IEq52_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline{\lambda }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq52.gif"/></alternatives></inline-formula> is a adjustable ratio between exploitation and exploration.</p></sec><sec id="Sec11"><title>Pool-based active learning framework</title><p id="Par56">As seen in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, to approximate early-stage exploration during each optimization campaign, <italic>n</italic> = 2 initial experiments are drawn randomly with no replacement from original pool <italic>D</italic> = {(<bold>x</bold><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>)∣<italic>i</italic> = 1, 2, …, <italic>N</italic>} and add to collection <italic>X</italic> = {(<bold>x</bold><sub><italic>i</italic></sub>, <italic>y</italic><sub><italic>i</italic></sub>)∣<italic>i</italic> = 1, 2, …, <italic>n</italic>}. During the planning stage, surrogate model <italic>f</italic> is used to estimate the mean <inline-formula id="IEq53"><alternatives><mml:math id="IEq53_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq53_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\mu }({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq53.gif"/></alternatives></inline-formula> and standard deviation <inline-formula id="IEq54"><alternatives><mml:math id="IEq54_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq54_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{\sigma }({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq54.gif"/></alternatives></inline-formula>. We then evaluate the acquisition function values <inline-formula id="IEq55"><alternatives><mml:math id="IEq55_Math"><mml:mrow><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mo>̂</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq55_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha (\hat{\mu }({{{\bf{x}}}}),\hat{\sigma }({{{\bf{x}}}}))$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq55.gif"/></alternatives></inline-formula> for each remaining experimental action <bold>x</bold> ∈ <italic>D</italic> in parallel. At each cycle, action <bold>x</bold><sup>*</sup> = arg <inline-formula id="IEq56"><alternatives><mml:math id="IEq56_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq56_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\max }_{{{{\bf{x}}}}}\alpha ({{{\bf{x}}}})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq56.gif"/></alternatives></inline-formula> will be selected as the next experiment. During the inference stage, after selecting action <bold>x</bold><sup>*</sup>, the corresponding sample observation <italic>y</italic><sup>*</sup> is obtained, and (<bold>x</bold><sup>*</sup>, <italic>y</italic><sup>*</sup>) is added to <italic>X</italic> and removed from set <italic>D</italic>. The new observation (<bold>x</bold><sup>*</sup>, <italic>y</italic><sup>*</sup>) is incorporated into the surrogate model. The sequential alternation between planning and inference is repeated until undiscovered data points run out.</p></sec><sec id="Sec12"><title>Statistical baselines</title><p id="Par57">In Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig4" ref-type="fig">4</xref>, we have introduced some statistical baselines when benchmarking the performance of BO algorithms with a pool-based active learning framework.</p><p id="Par58">For the random baseline in Fig. <xref rid="Fig3" ref-type="fig">3</xref>(a), assuming a total pool of <italic>N</italic> data points and the number of good materials candidates <italic>M</italic> = 0.05<italic>N</italic>, at cycle <italic>i</italic> = 1, expected probability of finding a good candidate is <italic>P</italic>(1) = 0.05 and expected value of <inline-formula id="IEq57"><alternatives><mml:math id="IEq57_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mi>%</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>⋅</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.0016</mml:mn></mml:mrow></mml:math><tex-math id="IEq57_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{Top}}}\, \% (1)=\frac{1\cdot P(1)}{M}=0.0016$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_IEq57.gif"/></alternatives></inline-formula>.</p><p id="Par59">Then at cycle <italic>i</italic> = 2, 3, . . . , <italic>N</italic>, there is<disp-formula id="Equ14"><label>14</label><alternatives><mml:math id="Equ14_Math"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>M</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{r}P(i)=\frac{M-\mathop{\sum }\nolimits_{n = 1}^{i-1}P(n)}{N-i}\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ14.gif"/></alternatives></disp-formula>and<disp-formula id="Equ15"><label>15</label><alternatives><mml:math id="Equ15_Math"><mml:mrow><mml:mspace width="0.25em"/><mml:mstyle><mml:mtext>Top</mml:mtext></mml:mstyle><mml:mspace width="0.25em"/><mml:mi>%</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\,{{\mbox{Top}}}\, \% (i)=\frac{\mathop{\sum }\nolimits_{n = 1}^{i}P(n)}{M}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ15.gif"/></alternatives></disp-formula></p><p id="Par60">In Fig. <xref rid="Fig3" ref-type="fig">3(d)</xref>, between two optimization paths starting with the same two initial data points:<list list-type="order"><list-item><p id="Par61">The statistically most overlap happens when two paths are identical, resulting in <italic>J</italic>(<italic>i</italic>) = 1, <italic>i</italic> = 1, 2, . . . , <italic>N</italic>;</p></list-item><list-item><p id="Par62">The statistically least overlap happens when the two follow drastically different paths until they run out of data points undiscovered by both algorithms, resulting in<disp-formula id="Equ16"><label>16</label><alternatives><mml:math id="Equ16_Math"><mml:mrow><mml:mi>J</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mtd><mml:mtd columnalign="left"><mml:mn>3</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd columnalign="left"><mml:mfrac><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mi>N</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="Equ16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J(i)=\left\{\begin{array}{ll}1&amp;1\le x\le 2\\ \frac{1}{i-1}&amp;3\le i\le \frac{N}{2}+1\\ \frac{2i-N}{N}&amp;\frac{N}{2}+2\le i\le N\end{array}\right.$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_656_Article_Equ16.gif"/></alternatives></disp-formula></p></list-item></list></p></sec></sec></body><back><ack><title>Acknowledgements</title><p>Q.L. acknowledges generous funding from TOTAL S.A. research grant funded through MITei for supporting his research. A.E.G., K.A.B. thank Google LLC, the Boston University Dean’s Catalyst Award, The Boston University Rafik B. Hariri Institute for Computing and Computational Science and Engineering, and NSF (CMMI-1661412) for support in this work and studies generating crossed barrel dataset. A.T., Z.L., S.S., T.B. acknowledge support from DARPA under Contract No. HR001118C0036, TOTAL S.A. research grant funded through MITei, US National Science Foundation grant CBET-1605547, and the Skoltech NGP program for research generating Perovskite dataset. Z.R. and T.B. are supported by the National Research Foundation, Prime Minister’s Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) program through the Singapore Massachusetts Institute of Technology (MIT) Alliance for Research and Technology’s Low Energy Electronic Systems research program. J.D., B.M. thank AFOSR Grant 19RHCOR089 for supporting their work in generating the AutoAM dataset. D.B., K.H. acknowledge funding from the Accelerated Materials Development for Manufacturing Program at A*STAR via the AME Programmatic Fund by the Agency for Science, Technology, and Research under Grant No. A1898b0043 and A*STAR Graduate Academy’s SINGA programme for producing P3HT/CNT dataset. F.M.B., S.K. acknowledge support from the Accelerated Materials Development for Manufacturing Program at A*STAR via the AME Programmatic Fund by the Agency for Science, Technology and Research under Grant No. A1898b0043.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>Q.L., A.E.G., Z.R., J.F., T.B. conceived this study. Q.L. implemented a pool-based active learning framework. A.E.G., Q.L., and Z.R. performed computation of approximated optimization campaigns across five experimental datasets. Q.L. and A.E.G. wrote the paper. A.E.G. and K.A.B. provided the Crossed barrel dataset and contributed to the discussion, revision, and editing of the manuscript. A.T., Z.L., S.S., and T.B. provided the Perovskite dataset contributed to discussion, revision, and editing of the paper. F.M.B., Z.R., and S.K. provided the AgNP dataset before the publication of its experimental study and contributed to the revision of the paper. J.D. and B.M. provided the AutoAM dataset before the publication of its experimental study and contributed to the revision of the paper. D.B. and K.H. provided P3HT/CNT dataset before the publication of its experimental study and contributed to the revision of the paper. S.K., K.H., B.M., K.A.B., J.F., and T.B. supervised the research.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The five experimental datasets in the current study is available for open access at the following GitHub repository<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>: <ext-link xlink:href="https://github.com/PV-Lab/Benchmarking" ext-link-type="uri">https://github.com/PV-Lab/Benchmarking</ext-link>.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The code for pool-based active learning framework and visualization in the current study are available in the following GitHub repository<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>: <ext-link xlink:href="https://github.com/PV-Lab/Benchmarking" ext-link-type="uri">https://github.com/PV-Lab/Benchmarking</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par63">The authors Z.R., Z.L., D.B., K.H., T.B. declare general IP in the area of applied machine learning, and are associated with start-up efforts (xinterra<sup>™</sup>) to accelerate materials development using applied machine learning. The other authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Settles, B. <italic>Active learning literature survey</italic> (University of Wisconsin-Madison Department of Computer Sciences, 2009).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>DA</given-names></name><name><surname>Ghahramani</surname><given-names>Z</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><article-title xml:lang="en">Active learning with statistical models</article-title><source>J. Artif. Intell. Res</source><year>1996</year><volume>4</volume><fpage>129</fpage><lpage>145</lpage></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shahriari</surname><given-names>B</given-names></name><name><surname>Swersky</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Adams</surname><given-names>RP</given-names></name><name><surname>De Freitas</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Taking the human out of the loop: a review of bayesian optimization</article-title><source>Proc. IEEE</source><year>2015</year><volume>104</volume><fpage>148</fpage><lpage>175</lpage></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rasmussen</surname><given-names>CE</given-names></name><name><surname>Nickisch</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Gaussian processes for machine learning (gpml) toolbox</article-title><source>J. Mach. Learn. Res.</source><year>2010</year><volume>11</volume><fpage>3011</fpage><lpage>3015</lpage></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Frazier, P. I. A tutorial on bayesian optimization. <italic>arXiv Preprint at</italic><ext-link xlink:href="https://arxiv.org/abs/1807.02811" ext-link-type="uri">https://arxiv.org/abs/1807.02811</ext-link> (2018).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Springenberg, J. T., Klein, A., Falkner, S. &amp; Hutter, F. <italic>Bayesian optimization with robust bayesian neural networks.</italic> In <italic>Advances in Neural Information Processing Systems</italic> <bold>29</bold>, 4134–4142 (2016).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Brochu, E., Cora, V. M. &amp; De Freitas, N. A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning. <italic>arXiv Preprint at</italic><ext-link xlink:href="https://arxiv.org/abs/1012.2599" ext-link-type="uri">https://arxiv.org/abs/1012.2599</ext-link> (2010).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Frazier, P. I. &amp; Wang, J. Bayesian optimization for materials design. In <italic>Information Science for Materials Discovery and Design</italic>, 45–75 (Springer, 2016).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Eriksson, D., Pearce, M., Gardner, J. R., Turner, R. &amp; Poloczek, M. Scalable global optimization via local bayesian optimization. In <italic>Advances in Neural Information Processing Systems</italic> <bold>32,</bold> 5496–5507 (2019).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Wang, Z., Li, C., Jegelka, S. &amp; Kohli, P. Batched high-dimensional bayesian optimization via structural kernel learning. In <italic>Int. J. Mach. Learn</italic>., 3656–3664 (PMLR, 2017).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Solomou</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Multi-objective bayesian materials discovery: application on the discovery of precipitation strengthened niti shape memory alloys through micromechanical modeling</article-title><source>Mater. Des.</source><year>2018</year><volume>160</volume><fpage>810</fpage><lpage>827</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhvFyktbvO</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamawaki</surname><given-names>M</given-names></name><name><surname>Ohnishi</surname><given-names>M</given-names></name><name><surname>Ju</surname><given-names>S</given-names></name><name><surname>Shiomi</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Multifunctional structural design of graphene thermoelectrics by bayesian optimization</article-title><source>Sci. Adv.</source><year>2018</year><volume>4</volume><fpage>eaar4192</fpage></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassman</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Active learning for accelerated design of layered materials</article-title><source>npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>1</fpage><lpage>9</lpage></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouet-Leduc</surname><given-names>B</given-names></name><name><surname>Barros</surname><given-names>K</given-names></name><name><surname>Lookman</surname><given-names>T</given-names></name><name><surname>Humphreys</surname><given-names>CJ</given-names></name></person-group><article-title xml:lang="en">Optimisation of gan leds and the reduction of efficiency droop using active machine learning</article-title><source>Sci. Rep.</source><year>2016</year><volume>6</volume><fpage>1</fpage><lpage>6</lpage></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerated search for batio3-based piezoelectrics with vertical morphotropic phase boundary using bayesian learning</article-title><source>Proc. Natl. Acad. Sci.</source><year>2016</year><volume>113</volume><fpage>13301</fpage><lpage>13306</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28Xhsl2gsbjM</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Efficient closed-loop maximization of carbon nanotube growth rate using bayesian optimization</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXlt1Kgt7Y%3D</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>BP</given-names></name><etal/></person-group><article-title xml:lang="en">Self-driving laboratory for accelerated discovery of thin-film materials</article-title><source>Sci. Adv.</source><year>2020</year><volume>6</volume><fpage>eaaz8867</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFGhsL3J</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Eyke, N. S., Koscher, B. A. &amp; Jensen, K. F. Toward machine learning-enhanced high-throughput experimentation. <italic>Trends Chem</italic>. (2021).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Roch</surname><given-names>LM</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Next-generation experimentation with self-driving laboratories</article-title><source>Trends Chem.</source><year>2019</year><volume>1</volume><fpage>282</fpage><lpage>291</lpage></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments</article-title><source>Sci. Adv.</source><year>2018</year><volume>4</volume><fpage>eaaq1566</fpage></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikolaev</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">Autonomy in materials research: a case study in carbon nanotube growth</article-title><source>npj Comput. Mater.</source><year>2016</year><volume>2</volume><fpage>1</fpage><lpage>6</lpage></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herbol</surname><given-names>HC</given-names></name><name><surname>Hu</surname><given-names>W</given-names></name><name><surname>Frazier</surname><given-names>P</given-names></name><name><surname>Clancy</surname><given-names>P</given-names></name><name><surname>Poloczek</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Efficient search of compositional space for hybrid organic–inorganic perovskites via bayesian optimization</article-title><source>npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhslKiurbM</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">A data fusion approach to optimize compositional stability of halide perovskites</article-title><source>Matter</source><year>2021</year><volume>4</volume><fpage>1305</fpage><lpage>1322</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhvVemsLvJ</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gongora</surname><given-names>AE</given-names></name><etal/></person-group><article-title xml:lang="en">A bayesian experimental autonomous researcher for mechanical design</article-title><source>Sci. Adv.</source><year>2020</year><volume>6</volume><fpage>eaaz1708</fpage></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Roch</surname><given-names>LM</given-names></name><name><surname>Kreisbeck</surname><given-names>C</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Phoenics: a bayesian optimizer for chemistry</article-title><source>ACS Cent. Sci.</source><year>2018</year><volume>4</volume><fpage>1134</fpage><lpage>1145</lpage></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gongora</surname><given-names>AE</given-names></name><etal/></person-group><article-title xml:lang="en">Using simulation to accelerate autonomous experimentation: a case study using mechanics</article-title><source>iScience</source><year>2021</year><volume>24</volume><fpage>102262</fpage></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langner</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Beyond ternary opv: high-throughput experimentation and self-driving laboratories optimize multicomponent systems</article-title><source>Adv. Mater.</source><year>2020</year><volume>32</volume><fpage>1907801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXivFygsro%3D</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohr</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Benchmarking the acceleration of materials discovery by sequential learning</article-title><source>Chem. Sci.</source><year>2020</year><volume>11</volume><fpage>2696</fpage><lpage>2706</lpage></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Graff, D. E., Shakhnovich, E. I. &amp; Coley, C. W. Accelerating high-throughput virtual screening through molecular pool-based active learning. <italic>Chem. Sci</italic>. (2021).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Hase, F. et al. Olympus: a benchmarking framework for noisy optimization and experiment planning. <italic>Mach. learn.: Sci. Technol</italic>. (2021).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Scikit-learn: machine learning in python</article-title><source>J. Mach. Learn. Res.</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Random forests</article-title><source>Mach. Learn.</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Wiener</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Classification and regression by randomforest</article-title><source>R news</source><year>2002</year><volume>2</volume><fpage>18</fpage><lpage>22</lpage></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Neal, R. M. <italic>Bayesian learning for neural networks</italic>, vol. 118 (Springer Science &amp; Business Media, 2012).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Liang, Q. Benchmarking the performance of bayesian optimization across multiple experimental materials science domains. at <ext-link xlink:href="https://github.com/PV-Lab/Benchmarking" ext-link-type="uri">https://github.com/PV-Lab/Benchmarking</ext-link> (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">GPy. <italic>GPy: A gaussian process framework in python</italic>. at <ext-link xlink:href="http://github.com/SheffieldML/GPy" ext-link-type="uri">http://github.com/SheffieldML/GPy</ext-link> (2012).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Reuther, A. et al. Interactive supercomputing on 40,000 cores for machine learning and data analysis. In <italic>2018 IEEE High Performance extreme Computing Conference (HPEC)</italic>, 1–6 (IEEE, 2018).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivas</surname><given-names>N</given-names></name><name><surname>Krause</surname><given-names>A</given-names></name><name><surname>Kakade</surname><given-names>SM</given-names></name><name><surname>Seeger</surname><given-names>MW</given-names></name></person-group><article-title xml:lang="en">Information-theoretic regret bounds for gaussian process optimization in the bandit setting</article-title><source>IEEE Trans. Inf. Theory</source><year>2012</year><volume>58</volume><fpage>3250</fpage><lpage>3265</lpage></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Snoek, J., Larochelle, H. &amp; Adams, R. P. <italic>Practical bayesian optimization of machine learning algorithms.</italic> In <italic>Advances in Neural Information Processing Systems</italic> <bold>25</bold> (2012).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Wu, J. &amp; Frazier, P. <italic>Practical two-step lookahead bayesian optimization</italic>. In <italic>Advances in Neural Information Processing Systems</italic>, <bold>32</bold>, 9813–9823 (2019).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryzhov</surname><given-names>IO</given-names></name></person-group><article-title xml:lang="en">On the convergence rates of expected improvement methods</article-title><source>Oper. Res.</source><year>2016</year><volume>64</volume><fpage>1515</fpage><lpage>1528</lpage></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Hennig, P. &amp; Schuler, C. J. Entropy search for information-efficient global optimization. <italic>J. Mach. Learn. Res</italic> <bold>13</bold> (2012).</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Frazier, P. I. <italic>Bayesian optimization</italic> (INFORMS, 2018).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roy</surname><given-names>M-H</given-names></name><name><surname>Larocque</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Robustness of random forests for regression</article-title><source>J. Nonparametric Stat</source><year>2012</year><volume>24</volume><fpage>993</fpage><lpage>1006</lpage></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Snelson, E. &amp; Ghahramani, Z. <italic>Sparse gaussian processes using pseudo-inputs</italic>. In <italic>Advances in Neural Information Processing Systems</italic> <bold>18</bold>, 1259–1266 (2006).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Snelson, E. L. <italic>Flexible and efficient Gaussian process models for machine learning</italic> (University College London, 2007).</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Snelson, E. &amp; Ghahramani, Z. Local and global sparse gaussian process approximations. In <italic>Artificial Intelligence and Statistics</italic>, 524–531 (2007).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Candelieri</surname><given-names>A</given-names></name><name><surname>Perego</surname><given-names>R</given-names></name><name><surname>Archetti</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Bayesian optimization of pump operations in water distribution systems</article-title><source>J. Glob. Optim</source><year>2018</year><volume>71</volume><fpage>213</fpage><lpage>235</lpage></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Wilson, A. &amp; Adams, R. Gaussian process kernels for pattern discovery and extrapolation. In <italic>International Conference on Machine Learning</italic>, 1067–1075 (2013).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Schlessinger, L., Malkomes, G. &amp; Garnett, R. Automated model search using bayesian optimization and genetic programming. In <italic>Workshop on Meta-Learning at Advances in Neural Information Processing Systems</italic> (2019).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Cho, Y. <italic>Kernel methods for deep learning</italic> (University of California, San Diego, 2012).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Paananen, T., Piironen, J., Andersen, M. R. &amp; Vehtari, A. Variable selection for gaussian processes via sensitivity analysis of the posterior predictive distribution. In <italic>International Conference on Artificial Intelligence and Statistics</italic>, 1743–1752 (PMLR, 2019).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Bash, D. et al. Multi-fidelity high-throughput optimization of electrical conductivity in p3ht-cnt composites. <italic>Adv. Funct. Mater</italic>. 2102606 (2021).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mekki-Berrada</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Two-step machine learning enables optimized nanoparticle synthesis</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>10</lpage></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Deneault, J. R. et al. Toward autonomous additive manufacturing: Bayesian optimization on a 3d printer. <italic>MRS Bull</italic>. 1–10 (2021).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec13"><title>Supplementary information</title><p id="Par64"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2021_656_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-021-00656-9" ext-link-type="doi">https://doi.org/10.1038/s41524-021-00656-9</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2021</facet-value></facet><facet name="country"><facet-value count="1">China</facet-value><facet-value count="1">Finland</facet-value><facet-value count="1">Singapore</facet-value><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
