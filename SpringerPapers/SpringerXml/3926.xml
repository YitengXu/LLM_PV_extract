<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1007/s11431-022-2368-y</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">11431</journal-id><journal-id journal-id-type="doi">10.1007/11431.1869-1900</journal-id><journal-title-group><journal-title>Science China Technological Sciences</journal-title><abbrev-journal-title abbrev-type="publisher">Sci. China Technol. Sci.</abbrev-journal-title></journal-title-group><issn pub-type="ppub">1674-7321</issn><issn pub-type="epub">1869-1900</issn><publisher><publisher-name>Science China Press</publisher-name><publisher-loc>Beijing</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s11431-022-2368-y</article-id><article-id pub-id-type="manuscript">2368</article-id><article-id pub-id-type="doi">10.1007/s11431-022-2368-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Interactive method research of dual mode information coordination integration for astronaut gesture and eye movement signals based on hybrid model</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Zhuang</surname><given-names>HongChao</given-names></name><address><email>zhuanghongchao_hit@163.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs114310222368y_cor1">a</xref></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Xia</surname><given-names>YiLu</given-names></name><address><email>xiayilu97@126.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs114310222368y_cor2">b</xref></contrib><contrib contrib-type="author"><name><surname>Wang</surname><given-names>Ning</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>WeiHua</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author"><name><surname>Dong</surname><given-names>Lei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Bo</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.449573.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 0604 9956</institution-id><institution content-type="org-division">School of Mechanical Engineering</institution><institution content-type="org-name">Tianjin University of Technology and Education</institution></institution-wrap><addr-line content-type="postcode">300222</addr-line><addr-line content-type="city">Tianjin</addr-line><country country="CN">China</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.449573.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 0604 9956</institution-id><institution content-type="org-division">School of Information Technology Engineering</institution><institution content-type="org-name">Tianjin University of Technology and Education</institution></institution-wrap><addr-line content-type="postcode">300222</addr-line><addr-line content-type="city">Tianjin</addr-line><country country="CN">China</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.19373.3f</institution-id><institution-id institution-id-type="ISNI">0000 0001 0193 3564</institution-id><institution content-type="org-division">School of Automotive Engineering</institution><institution content-type="org-name">Harbin Institute of Technology (Weihai)</institution></institution-wrap><addr-line content-type="postcode">264209</addr-line><addr-line content-type="city">Weihai</addr-line><country country="CN">China</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.464215.0</institution-id><institution-id institution-id-type="ISNI">0000 0001 0243 138X</institution-id><institution content-type="org-name">Tianjin Institute of Aerospace Mechanical and Electrical Equipment</institution></institution-wrap><addr-line content-type="postcode">300458</addr-line><addr-line content-type="city">Tianjin</addr-line><country country="CN">China</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.19373.3f</institution-id><institution-id institution-id-type="ISNI">0000 0001 0193 3564</institution-id><institution content-type="org-division">School of Mechatronics Engineering</institution><institution content-type="org-name">Harbin Institute of Technology</institution></institution-wrap><addr-line content-type="postcode">150000</addr-line><addr-line content-type="city">Harbin</addr-line><country country="CN">China</country></aff></contrib-group><author-notes><corresp id="IDs114310222368y_cor1"><label>a</label><email>zhuanghongchao_hit@163.com</email></corresp><corresp id="IDs114310222368y_cor2"><label>b</label><email>xiayilu97@126.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>9</day><month>5</month><year>2023</year></pub-date><pub-date date-type="pub" publication-format="print"><month>6</month><year>2023</year></pub-date><volume>66</volume><issue seq="13">6</issue><issue-title>Special Topic: New Properties and Emerging Applications of Room Temperature Liquid Metals</issue-title><fpage>1717</fpage><lpage>1733</lpage><history><date date-type="registration"><day>11</day><month>5</month><year>2022</year></date><date date-type="received"><day>27</day><month>11</month><year>2022</year></date><date date-type="accepted"><day>2</day><month>3</month><year>2023</year></date><date date-type="online"><day>9</day><month>5</month><year>2023</year></date></history><permissions><copyright-statement content-type="compact">Â© Science China Press 2023</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Science China Press</copyright-holder></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p>The lightweight human-robot interaction model with high real-time, high accuracy, and strong anti-interference capability can be better applied to future lunar surface exploration and construction work. Based on the feature information inputted from the monocular camera, the signal acquisition and processing fusion of the astronaut gesture and eye-movement modal interaction can be performed. Compared with the single mode, the human-robot interaction model of bimodal collaboration can achieve the issuance of complex interactive commands more efficiently. The optimization of the target detection model is executed by inserting attention into YOLOv4 and filtering image motion blur. The central coordinates of pupils are identified by the neural network to realize the human-robot interaction in the eye movement mode. The fusion between the astronaut gesture signal and eye movement signal is performed at the end of the collaborative model to achieve complex command interactions based on a lightweight model. The dataset used in the network training is enhanced and extended to simulate the realistic lunar space interaction environment. The human-robot interaction effects of complex commands in the single mode are compared with those of complex commands in the bimodal collaboration. The experimental results show that the concatenated interaction model of the astronaut gesture and eye movement signals can excavate the bimodal interaction signal better, discriminate the complex interaction commands more quickly, and has stronger signal anti-interference capability based on its stronger feature information mining ability. Compared with the command interaction realized by using the single gesture modal signal and the single eye movement modal signal, the interaction model of bimodal collaboration is shorter about 79% to 91% of the time under the single mode interaction. Regardless of the influence of any image interference item, the overall judgment accuracy of the proposed model can be maintained at about 83% to 97%. The effectiveness of the proposed method is verified.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>human-robot interaction</kwd><kwd>gesture and eye movement</kwd><kwd>hybrid model</kwd><kwd>YOLOv4</kwd><kwd>CBAM</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Science China Press, co-published with Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>12</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>24</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Science China Press</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>5</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>11</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/11431_2022_Article_2368.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-month</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-day</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Engineering, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>false</meta-value></custom-meta></custom-meta-group></article-meta></front><back><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>L</given-names></name><name><surname>Gao</surname><given-names>H B</given-names></name><name><surname>Deng</surname><given-names>Z Q</given-names></name><etal/></person-group><article-title xml:lang="en">Three-layer intelligence of planetary exploration wheeled mobile robots: Robint, virtint, and humint</article-title><source>Sci China Tech Sci</source><year>2015</year><volume>58</volume><fpage>1299</fpage><lpage>1317</lpage><pub-id pub-id-type="doi">10.1007/s11431-015-5853-9</pub-id></mixed-citation></ref><ref id="CR2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Power consumption characteristics research on mobile system of electrically driven large-load-ratio six-legged robot</article-title><source>Chin J Mech Eng</source><year>2023</year><volume>36</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1186/s10033-023-00848-y</pub-id></mixed-citation></ref><ref id="CR3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Zhuang</surname><given-names>H C</given-names></name><name><surname>Gao</surname><given-names>H B</given-names></name><etal/></person-group><article-title xml:lang="en">Static force analysis of foot of electrically driven heavy-duty six-legged robot under tripod gait</article-title><source>Chin J Mech Eng</source><year>2018</year><volume>31</volume><fpage>63</fpage><pub-id pub-id-type="doi">10.1186/s10033-018-0263-0</pub-id></mixed-citation></ref><ref id="CR4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>H C</given-names></name><name><surname>Gao</surname><given-names>H B</given-names></name><name><surname>Deng</surname><given-names>Z Q</given-names></name></person-group><article-title xml:lang="en">Gait planning research for an electrically driven large-load-ratio six-legged robot</article-title><source>Appl Sci</source><year>2017</year><volume>7</volume><fpage>296</fpage><pub-id pub-id-type="doi">10.3390/app7030296</pub-id></mixed-citation></ref><ref id="CR5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>H C</given-names></name><name><surname>Gao</surname><given-names>H B</given-names></name><name><surname>Deng</surname><given-names>Z Q</given-names></name><etal/></person-group><article-title xml:lang="en">A review of heavy-duty legged robots</article-title><source>Sci China Tech Sci</source><year>2014</year><volume>57</volume><fpage>298</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1007/s11431-013-5443-7</pub-id></mixed-citation></ref><ref id="CR6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>G</given-names></name><name><surname>Shi</surname><given-names>Z C</given-names></name><name><surname>Shang</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Precise monocular vision-based pose measurement system for lunar surface sampling manipulator</article-title><source>Sci China Tech Sci</source><year>2019</year><volume>62</volume><fpage>1783</fpage><lpage>1794</lpage><pub-id pub-id-type="doi">10.1007/s11431-019-9518-8</pub-id></mixed-citation></ref><ref id="CR7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cordes</surname><given-names>F</given-names></name><name><surname>Ahrns</surname><given-names>I</given-names></name><name><surname>Bartsch</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">LUNARES: Lunar crater exploration with heterogeneous multi robot systems</article-title><source>Intel Serv Robotics</source><year>2011</year><volume>4</volume><fpage>61</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1007/s11370-010-0081-4</pub-id></mixed-citation></ref><ref id="CR8"><label>8</label><mixed-citation publication-type="other">Dunker P A, Lewinger W A, Hunt A J, et al. A biologically inspired robot for lunar <italic>in-situ</italic> resource utilization. In: Proceedings of 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems. Louis, 2009. 5039â5044</mixed-citation></ref><ref id="CR9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>RodrÃ­guez-MartÃ­nez</surname><given-names>D</given-names></name><name><surname>Van Winnendael</surname><given-names>M</given-names></name><name><surname>Yoshida</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">High-speed mobility on planetary surfaces: A technical review</article-title><source>J Field Robotics</source><year>2019</year><volume>36</volume><fpage>1436</fpage><lpage>1455</lpage><pub-id pub-id-type="doi">10.1002/rob.21912</pub-id></mixed-citation></ref><ref id="CR10"><label>10</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Che</surname><given-names>X</given-names></name><name><surname>Nemchin</surname><given-names>A</given-names></name><name><surname>Liu</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Age and composition of young basalts on the Moon, measured from samples returned by Changâe-5</article-title><source>Science</source><year>2021</year><volume>374</volume><fpage>887</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1126/science.abl7957</pub-id></mixed-citation></ref><ref id="CR11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazur</surname><given-names>J E</given-names></name><name><surname>Crain</surname><given-names>W R</given-names></name><name><surname>Looper</surname><given-names>M D</given-names></name><etal/></person-group><article-title xml:lang="en">New measurements of total ionizing dose in the lunar environment</article-title><source>Space Weather</source><year>2011</year><volume>9</volume><fpage>S07002</fpage><pub-id pub-id-type="doi">10.1029/2010SW000641</pub-id></mixed-citation></ref><ref id="CR12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Lin</surname><given-names>H</given-names></name><name><surname>Wen</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Construction of a virtual lunar environment platform</article-title><source>Int J Digital Earth</source><year>2013</year><volume>6</volume><fpage>469</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1080/17538947.2011.628415</pub-id></mixed-citation></ref><ref id="CR13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>R</given-names></name><name><surname>Yuan</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">A 2-year locomotive exploration and scientific investigation of the lunar farside by the Yutu-2 rover</article-title><source>Sci Robot</source><year>2022</year><volume>7</volume><fpage>1</fpage><lpage>3</lpage><pub-id pub-id-type="doi">10.1126/scirobotics.abj6660</pub-id></mixed-citation></ref><ref id="CR14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>HorÃ¡nyi</surname><given-names>M</given-names></name><name><surname>Sternovsky</surname><given-names>Z</given-names></name><name><surname>Lankton</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">The lunar dust experiment (LDEX) onboard the lunar atmosphere and dust environment explorer (LADEE) mission</article-title><source>Space Sci Rev</source><year>2014</year><volume>185</volume><fpage>93</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1007/s11214-014-0118-7</pub-id></mixed-citation></ref><ref id="CR15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perzanowski</surname><given-names>D</given-names></name><name><surname>Schultz</surname><given-names>A C</given-names></name><name><surname>Adams</surname><given-names>W</given-names></name><etal/></person-group><article-title xml:lang="en">Building a multimodal human-robot interface</article-title><source>IEEE Intell Syst</source><year>2001</year><volume>16</volume><fpage>16</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1109/MIS.2001.1183338</pub-id></mixed-citation></ref><ref id="CR16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Yoon</surname><given-names>W C</given-names></name></person-group><article-title xml:lang="en">Generating task-oriented interactions of service robots</article-title><source>IEEE Trans Syst Man Cybern Syst</source><year>2014</year><volume>44</volume><fpage>981</fpage><lpage>994</lpage><pub-id pub-id-type="doi">10.1109/TSMC.2014.2298214</pub-id></mixed-citation></ref><ref id="CR17"><label>17</label><mixed-citation publication-type="other">Csapo A, Gilmartin E, Grizou J, et al. Multimodal conversational interaction with a humanoid robot. In: Proceedings of 2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom). Kosice, 2012. 667â672</mixed-citation></ref><ref id="CR18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hong</surname><given-names>A</given-names></name><name><surname>Lunscher</surname><given-names>N</given-names></name><name><surname>Hu</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">A multimodal emotional humanrobot interaction architecture for social robots engaged in bidirectional communication</article-title><source>IEEE Trans Cybern</source><year>2021</year><volume>51</volume><fpage>5954</fpage><lpage>5968</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2020.2974688</pub-id></mixed-citation></ref><ref id="CR19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iba</surname><given-names>S</given-names></name><name><surname>Paredis</surname><given-names>C J J</given-names></name><name><surname>Khosla</surname><given-names>P K</given-names></name></person-group><article-title xml:lang="en">Interactive multimodal robot programming</article-title><source>Int J Robotics Res</source><year>2005</year><volume>24</volume><fpage>83</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1177/0278364904049250</pub-id></mixed-citation></ref><ref id="CR20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arkin</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>D</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Multimodal estimation and communication of latent semantic knowledge for robust execution of robot instructions</article-title><source>Int J Robotics Res</source><year>2020</year><volume>39</volume><fpage>1279</fpage><lpage>1304</lpage><pub-id pub-id-type="doi">10.1177/0278364920917755</pub-id></mixed-citation></ref><ref id="CR21"><label>21</label><mixed-citation publication-type="other">Kryuchkov B, Syrkin L, Usov V, et al. Using augmentative and alternative communication for human-robot interaction during maintaining habitability of a lunar base. In: Proceedings of International Conference on Interactive Collaborative Robotics. St. Petersburg, 2017. 95â104</mixed-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="other">Fong T, Scholtz J, Shah J A, et al. A preliminary study of peer-to-peer human-robot interaction. In: Proceedings of 2006 IEEE International Conference on Systems, Man and Cybernetics. Taipei, 2006. 3198â3203</mixed-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="other">Wibirama S, Murnani S, Setiawan N A. Spontaneous gaze gesture interaction in the presence of noises and various types of eye movements. In: Proceedings of ACM Symposium on Eye Tracking Research and Applications. Stuttgart, 2020. 1â5</mixed-citation></ref><ref id="CR24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujii</surname><given-names>K</given-names></name><name><surname>Gras</surname><given-names>G</given-names></name><name><surname>Salerno</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Gaze gesture based human robot interaction for laparoscopic surgery</article-title><source>Med Image Anal</source><year>2018</year><volume>44</volume><fpage>196</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.media.2017.11.011</pub-id></mixed-citation></ref><ref id="CR25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nickel</surname><given-names>K</given-names></name><name><surname>Stiefelhagen</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Visual recognition of pointing gestures for human-robot interaction</article-title><source>Image Vision Computing</source><year>2007</year><volume>25</volume><fpage>1875</fpage><lpage>1884</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2005.12.020</pub-id></mixed-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="other">Meena R, Jokinen K, Wilcock G. Integration of gestures and speech in human-robot interaction. In: Proceedings of 2012 IEEE 3rd International Conference on Cognitive Infocommunications (CogInfoCom). Kosice, 2012. 673â678</mixed-citation></ref><ref id="CR27"><label>27</label><mixed-citation publication-type="other">Liu Z T, Pan F F, Wu M, et al. A multimodal emotional communication based humans-robots interaction system. In: Proceedings of 35th Chinese Control Conference (CCC). Chengdu, 2016. 6363â6368</mixed-citation></ref><ref id="CR28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">An EEG/EMG/EOG-based multimodal human-machine interface to real-time control of a soft robot hand</article-title><source>Front Neurorobot</source><year>2019</year><volume>13</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.3389/fnins.2019.00001</pub-id></mixed-citation></ref><ref id="CR29"><label>29</label><mixed-citation publication-type="other">Li Z, Jarvis R. Visual interpretation of natural pointing gestures in 3D space for human-robot interaction. In: Proceedings of 11th International Conference on Control Automation Robotics &amp; Vision. Singapore, 2010. 2513â2518</mixed-citation></ref><ref id="CR30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>P J</given-names></name><name><surname>Sun</surname><given-names>Z Z</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">An overview of the mission and technical characteristics of Changeâ4 Lunar Probe</article-title><source>Sci China Tech Sci</source><year>2017</year><volume>60</volume><fpage>658</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1007/s11431-016-9034-6</pub-id></mixed-citation></ref><ref id="CR31"><label>31</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>P J</given-names></name><name><surname>Sun</surname><given-names>Z Z</given-names></name><name><surname>Rao</surname><given-names>W</given-names></name><etal/></person-group><article-title xml:lang="en">Mission overview and key technologies of the first Mars probe of China</article-title><source>Sci China Tech Sci</source><year>2017</year><volume>60</volume><fpage>649</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.1007/s11431-016-9035-5</pub-id></mixed-citation></ref><ref id="CR32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>H Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>L P</given-names></name><etal/></person-group><article-title xml:lang="en"><italic>In-situ</italic> lunar dust deposition amount induced by lander landing in ChangâE-3 mission</article-title><source>Sci China Tech Sci</source><year>2020</year><volume>63</volume><fpage>520</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1007/s11431-019-1434-y</pub-id></mixed-citation></ref><ref id="CR33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Face mask wearing detection algorithm based on improved YOLO-v4</article-title><source>Sensors</source><year>2021</year><volume>21</volume><fpage>3263</fpage><pub-id pub-id-type="doi">10.3390/s21093263</pub-id></mixed-citation></ref><ref id="CR34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>F</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Real-time railroad track components inspection based on the improved YOLOv4 framework</article-title><source>Automation Construction</source><year>2021</year><volume>125</volume><fpage>103596</fpage><pub-id pub-id-type="doi">10.1016/j.autcon.2021.103596</pub-id></mixed-citation></ref><ref id="CR35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dewi</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>R C</given-names></name><name><surname>Jiang</surname><given-names>X</given-names></name><etal/></person-group><article-title xml:lang="en">Deep convolutional neural network for enhancing traffic sign recognition developed on YOLOv4</article-title><source>Multimed Tools Appl</source><year>2022</year><volume>81</volume><fpage>37821</fpage><lpage>37845</lpage><pub-id pub-id-type="doi">10.1007/s11042-022-12962-5</pub-id></mixed-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Kaliuzhnyi</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">YOLOX-SAR: High-precision object detection system based on visible and infrared sensors for SAR remote sensing</article-title><source>IEEE Sens J</source><year>2022</year><volume>22</volume><fpage>17243</fpage><lpage>17253</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2022.3186889</pub-id></mixed-citation></ref><ref id="CR37"><label>37</label><mixed-citation publication-type="other">Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module. In Proceedings of 2018 European conference on computer vision (ECCV). Munich, 2018. 3â19</mixed-citation></ref><ref id="CR38"><label>38</label><mixed-citation publication-type="other">Kim H M, Kim J H, Park K R, et al. Small object detection using prediction head and attention. In: Proceedings of 2022 International Conference on Electronics, Information, and Communication (ICEIC). Jeju, 2022. 1â4</mixed-citation></ref><ref id="CR39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S H</given-names></name><name><surname>Fernandes</surname><given-names>S L</given-names></name><name><surname>Zhu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">AVNC: Attention-based VGG-style network for COVID-19 diagnosis by CBAM</article-title><source>IEEE Sens J</source><year>2021</year><volume>22</volume><fpage>17431</fpage><lpage>17438</lpage><pub-id pub-id-type="doi">10.1109/JSEN.2021.3062442</pub-id></mixed-citation></ref><ref id="CR40"><label>40</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>H</given-names></name><name><surname>Xia</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>N</given-names></name><etal/></person-group><article-title xml:lang="en">High inclusiveness and accuracy motion blur real-time gesture recognition based on YOLOv4 model combined attention mechanism and DeblurGanv2</article-title><source>Appl Sci</source><year>2021</year><volume>11</volume><fpage>9982</fpage><pub-id pub-id-type="doi">10.3390/app11219982</pub-id></mixed-citation></ref><ref id="CR41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Tang</surname><given-names>Z</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><etal/></person-group><article-title xml:lang="en">Motion blur removal for UAV-based wind turbine blade images using synthetic datasets</article-title><source>Remote Sens</source><year>2022</year><volume>14</volume><fpage>87</fpage><pub-id pub-id-type="doi">10.3390/rs14010087</pub-id></mixed-citation></ref><ref id="CR42"><label>42</label><mixed-citation publication-type="other">Tomosada H, Kudo T, Fujisawa T, et al. GAN-based image deblurring using DCT discriminator. In: Proceedings of 25th International Conference on Pattern Recognition (ICPR). Milan, 2021. 3675â3681</mixed-citation></ref><ref id="CR43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>BÄiaÈu</surname><given-names>A M</given-names></name><name><surname>Dumitrescu</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Contributions to driver fatigue detection based on eye-tracking</article-title><source>Int J Circuits Syst Signal Processing</source><year>2021</year><volume>15</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.46300/9106.2021.15.1</pub-id></mixed-citation></ref><ref id="CR44"><label>44</label><mixed-citation publication-type="other">Li C M, Qi Z L, Nan J, et al. Human face detection algorithm via Haar cascade classifier combined with three additional classifiers. In: Proceedings of the 13th IEEE International Conference on Electronic Measurement and Instruments (ICEMI). Yangzhou, 2017. 483â487</mixed-citation></ref><ref id="CR45"><label>45</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>H</given-names></name><name><surname>Hsieh</surname><given-names>S S</given-names></name><name><surname>Holmes David</surname><given-names>R I</given-names></name><etal/></person-group><article-title xml:lang="en">An interactive eye-tracking system for measuring radiologistsâ visual fixations in volumetric CT images: Implementation and initial eye-tracking accuracy validation</article-title><source>Med Phys</source><year>2021</year><volume>48</volume><fpage>6710</fpage><lpage>6723</lpage><pub-id pub-id-type="doi">10.1002/mp.15219</pub-id></mixed-citation></ref><ref id="CR46"><label>46</label><mixed-citation publication-type="other">Saleh N, Tarek A. Vision-based communication system for patients with amyotrophic lateral sclerosis. In: Proceedings of the 3rd Novel Intelligent and Leading Emerging Sciences Conference (NILES). Giza, 2021. 19â22</mixed-citation></ref><ref id="CR47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Eye detection using discriminatory Haar features and a new efficient SVM</article-title><source>Image Vision Computing</source><year>2015</year><volume>33</volume><fpage>68</fpage><lpage>77</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2014.10.007</pub-id></mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><p>This work was supported by the National Natural Science Foundation of China (Grant No. 51505335), the Industry University Cooperation Collaborative Education Project of the Department of Higher Education of the Chinese Ministry of Education (Grant No. 202102517001), the Tianjin Postgraduate Scientific Research Innovation Project (Special Project of Intelligent Network Vehicle Connection) (Grant No. 2021YJSO2S33), the Tianjin Postgraduate Scientific Research Innovation Project (Grant No. 2021YJSS216), and the Doctor Startup Project of Tianjin University of Technology and Education (Grant No. KYQD 1806).</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Engineering</facet-value><facet-value count="1">Engineering, general</facet-value></facet><facet name="keyword"><facet-value count="1">CBAM</facet-value><facet-value count="1">gesture and eye movement</facet-value><facet-value count="1">human-robot interaction</facet-value><facet-value count="1">hybrid model</facet-value><facet-value count="1">YOLOv4</facet-value></facet><facet name="pub"><facet-value count="1">Science China Technological Sciences</facet-value></facet><facet name="year"><facet-value count="1">2023</facet-value></facet><facet name="country"><facet-value count="1">China</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
