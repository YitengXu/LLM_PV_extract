<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-020-00406-3</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-020-00406-3</article-id><article-id pub-id-type="manuscript">406</article-id><article-id pub-id-type="doi">10.1038/s41524-020-00406-3</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1035</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Benchmarking materials property prediction methods: the Matbench test set and Automatminer reference algorithm</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8567-1879</contrib-id><name><surname>Dunn</surname><given-names>Alexander</given-names></name><address><email>ardunn@lbl.gov</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs41524020004063_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3418-8809</contrib-id><name><surname>Wang</surname><given-names>Qi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Ganose</surname><given-names>Alex</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Dopp</surname><given-names>Daniel</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au5"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5893-9967</contrib-id><name><surname>Jain</surname><given-names>Anubhav</given-names></name><address><email>ajain@lbl.gov</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs41524020004063_cor5">e</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.184769.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2231 4551</institution-id><institution content-type="org-division">Energy Technologies Area</institution><institution content-type="org-name">Lawrence Berkeley National Laboratory</institution></institution-wrap><addr-line content-type="street">1 Cyclotron Road</addr-line><addr-line content-type="postcode">94720</addr-line><addr-line content-type="city">Berkeley</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.47840.3f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2181 7878</institution-id><institution content-type="org-division">Department of Materials Science and Engineering</institution><institution content-type="org-name">University of California</institution></institution-wrap><addr-line content-type="street">210 Hearst Mining Building</addr-line><addr-line content-type="postcode">94720</addr-line><addr-line content-type="city">Berkeley</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.266539.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8438</institution-id><institution content-type="org-division">Department of Computer Science</institution><institution content-type="org-name">University of Kentucky</institution></institution-wrap><addr-line content-type="street">329 Rose St.</addr-line><addr-line content-type="postcode">40506</addr-line><addr-line content-type="city">Lexington</addr-line><addr-line content-type="state">KY</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs41524020004063_cor1"><label>a</label><email>ardunn@lbl.gov</email></corresp><corresp id="IDs41524020004063_cor5"><label>e</label><email>ajain@lbl.gov</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>15</day><month>9</month><year>2020</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2020</year></pub-date><volume>6</volume><issue seq="138">1</issue><elocation-id>138</elocation-id><history><date date-type="registration"><day>19</day><month>8</month><year>2020</year></date><date date-type="received"><day>5</day><month>5</month><year>2020</year></date><date date-type="accepted"><day>14</day><month>8</month><year>2020</year></date><date date-type="online"><day>15</day><month>9</month><year>2020</year></date></history><pub-history><event event-type="Correction"><event-desc>An amendment to this paper has been published and can be accessed via a link at the top of the paper.</event-desc><date><day>16</day><month>10</month><year>2020</year></date></event></pub-history><permissions><copyright-statement content-type="compact">© This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply 2020</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><related-article related-article-type="correction-forward" ext-link-type="doi" xlink:href="10.1038/s41524-020-00433-0"/><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">We present a benchmark test suite and an automated machine learning procedure for evaluating supervised machine learning (ML) models for predicting properties of inorganic bulk materials. The test suite, Matbench, is a set of 13 ML tasks that range in size from 312 to 132k samples and contain data from 10 density functional theory-derived and experimental sources. Tasks include predicting optical, thermal, electronic, thermodynamic, tensile, and elastic properties given a material’s composition and/or crystal structure. The reference algorithm, Automatminer, is a highly-extensible, fully automated ML pipeline for predicting materials properties from materials primitives (such as composition and crystal structure) without user intervention or hyperparameter tuning. We test Automatminer on the Matbench test suite and compare its predictive power with state-of-the-art crystal graph neural networks and a traditional descriptor-based Random Forest model. We find Automatminer achieves the best performance on 8 of 13 tasks in the benchmark. We also show our test suite is capable of exposing predictive advantages of each algorithm—namely, that crystal graph methods appear to outperform traditional machine learning methods given ~10<sup>4</sup> or greater data points. We encourage evaluating materials ML algorithms on the Matbench benchmark and comparing them against the latest version of Automatminer.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>DOE | SC | Basic Energy Sciences (BES)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100006151</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">DE-AC02-05CH11231</award-id><award-id award-type="FundRef grant">DE-AC02-05CH11231</award-id><award-id award-type="FundRef grant">DE-AC02-05CH11231</award-id><award-id award-type="FundRef grant">DE-AC02-05CH11231</award-id><award-id award-type="FundRef grant">DE-AC02-05CH11231</award-id><principal-award-recipient><name><surname>Dunn</surname><given-names>Alexander</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Wang</surname><given-names>Qi</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Ganose</surname><given-names>Alex</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Dopp</surname><given-names>Daniel</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Jain</surname><given-names>Anubhav</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Research</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>159</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>8</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>19</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2020_Article_406.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta><notes notes-type="CrossLinking"><sec><p>A correction to this article is available online at <ext-link xlink:href="https://doi.org/10.1038/s41524-020-00433-0" ext-link-type="uri">https://doi.org/10.1038/s41524-020-00433-0</ext-link>.</p></sec></notes></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">New functional materials are vital for making fundamental advances across scientific domains, including computing and energy conversion. However, most materials are brought to commercialization primarily by direct experimental investigation, an approach typically limited by 20+ year design processes, constraints in the number of chemical systems that can be investigated, and the limits of a particular researcher’s intuition. By leveraging materials big data and advances in machine learning (ML), the emerging field of materials informatics has demonstrated massive potential as a catalyst for materials development, alongside ab initio techniques such as high-throughput density functional theory<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup> (DFT). For example, by using support vector machines to search a space of more than 118k candidate crystal structures, Tehrani et al.<sup><xref ref-type="bibr" rid="CR3">3</xref></sup> identified, synthesized, and experimentally validated two superhard carbides. In another study, Cooper et al.<sup><xref ref-type="bibr" rid="CR4">4</xref></sup> applied natural language processing (NLP) techniques to assemble 9k photovoltaic candidates from scientific literature; equipped with algorithmic structure-property encodings and a design-to-device data mining workflow, they identified and experimentally realized a high-performing panchromatic absorption dye. These examples are but two of many. The sheer investigative volume and potential research impact of materials data mining has helped brand it as materials 4.0<sup><xref ref-type="bibr" rid="CR5">5</xref></sup> or the 4th paradigm<sup><xref ref-type="bibr" rid="CR6">6</xref></sup> of materials research.</p><p id="Par3">However, the growing role of ML in materials design exposes weaknesses in the materials data mining pipeline: first, there is no systematic method for comparing and selecting materials ML models. Comparing newly published models to existing techniques is crucial for rational ML model design and advancement of the field. Other fields of applied ML have seen rapid advancement in recent years in large part due to the creation and use of standardized community benchmarks such as ImageNet<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> (20,000+ citations) for image classification and the Stanford Question Answering Dataset<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> (1400+ citations) for NLP. While there are commonly used datasets for materials problems, e.g., Castelli et al.’s investigation of cubic perovskites<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, it is uncommon for two algorithms to be tested against the same dataset and with the same data cleaning procedures. Methods for estimating generalization error (e.g., the train/test split) also vary significantly. Typically, either the predictive error is averaged over a set of cross-validation folds (CV score)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> or a hold-out test set is used, with the specifics of the split procedure varying between studies. Furthermore, if a model’s hyperparameters are tuned to directly optimize one of these metrics, equivalent to trying many models and only reporting the best one, they may significantly misrepresent the true generalization error<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup> (model selection bias). Arbitrary choice of hold-out set can also bias a comparison in favor of one model over another (sample selection bias)<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref></sup>. Thus, the materials informatics community lacks a standard benchmarking method for critically evaluating models. If models cannot be accurately compared, ML studies are difficult to reproduce and innovation suffers.</p><p id="Par4">Moreover, the breadth of materials ML tasks is so large that many models must still be designed and tuned by hand. Although hand-tuned descriptors and ML algorithms can fulfill the urgent need<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> for accurate predictions at low computational cost, their design is relatively expensive in terms of human (researcher) time and expertise. The recent explosion<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> of descriptors and models has given practitioners a paradox-of-choice, as selecting the optimal descriptors and model for a given task is nontrivial. The consequences of this paradox-of-choice can be that researchers select suboptimal models or spend researcher time retuning existing models for new applications. Simply, the usability of materials ML pipelines must be improved<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Thus, an automatic algorithm—which requires no expert domain knowledge to operate yet utilizes knowledge from published literature—could be of great use in prototyping, validating, and analyzing high-fidelity models.</p><p id="Par5">Given the above considerations, a benchmark consisting of the following two parts is needed: (1) a robust set of materials ML tasks and (2) an automatic reference model. The ML tasks must mitigate arbitrarily favoring one model over another. Furthermore, the MoleculeNet<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> benchmark (molecular machine learning) has previously demonstrated that a diverse test suite of ML tasks, rather than a single test, is appropriate for nuanced comparisons of chemical ML methods. The ML tasks should contain a variety of datasets such that domain-specific algorithms can compare on specific datasets and general-purpose algorithms can compare across multiple relevant tasks. The second part, the reference algorithm, may serve multiple purposes. First, it might provide a community standard—or baseline—which future innovation in materials ML should aim to surpass. Second, it can act as an entry point into materials informatics for non-domain specialists since it only requires a dataset as input. Finally, it can help to determine which descriptors in the literature are most applicable to a given task or set of tasks.</p><p id="Par6">In this paper, we introduce both these developments—a benchmark test set and a reference algorithm—for application to inorganic, solid state materials property prediction tasks. Matbench, the test suite, is a collection of 13 materials science-specific data mining tasks curated to reflect the diversity of modern materials data. Containing both traditional small materials datasets of only a few hundred samples and large datasets of &gt;10<sup>5</sup> samples from simulation-derived databases, Matbench provides a consistent nested cross-validation<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> (NCV) method for estimating regression and classification errors on a range of mechanical, electronic, and thermodynamic material properties. Automatminer, the reference algorithm, is a general-purpose and fully automated machine learning pipeline. In contrast to other published models that are trained to predict a specific property, Automatminer is capable of predicting any materials property given materials primitives (e.g., chemical composition) as input when provided with a suitable training dataset. It does this by performing a procedure similar to a human researcher: by generating descriptors using Matminer’s library<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> of published materials-specific featurizations, performing feature reduction and data preprocessing, and determining the best machine learning model by internally testing various possibilities on validation data. We test Automatminer on the test suite in order to establish baseline performance, and we present a comparison of Automatminer with published ML methods. Finally, we demonstrate our benchmark capable of distinguishing predictive strengths and weaknesses among ML techniques. We expect both Matbench and Automatminer to evolve over time, although the current versions of these tools are ready for immediate use. As evidence of its usefulness, Kabiraj et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> have recently used Automatminer in their research on 2D ferromagnets.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Matbench test suite v0.1</title><p id="Par7">The Matbench test suite v0.1 contains 13 supervised ML tasks from 10 datasets. Matbench’s data are sourced from various subdisciplines of materials science, such as experimental mechanical properties (alloy strength), computed elastic properties, computed and experimental electronic properties, optical and phonon properties, and thermodynamic stabilities for crystals, 2D materials, and disordered metals. The number of samples in each task ranges from 312 to 132,752, representing both relatively scarce experimental materials properties and comparatively abundant properties such as DFT-GGA<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> formation energies. Each task is a self-contained dataset containing a single material primitive as input (either composition or composition plus crystal structure) and target property as output for each sample. To help enforce homogeneity, datasets are precleaned to remove unphysical computed data and task-irrelevant experimental data (see Methods for more details); thus, as opposed to many raw datasets, structured online databases, or a recent materials benchmarking effort by Clement et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>, Matbench’s tasks have already had their data cleaned for input into ML pipelines. We recommend the datasets be used as-is for consistent comparisons between models. To mitigate model and sample selection biases, each task uses a consistent nested cross-validation<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> procedure for error estimation (see Methods). The distribution of datasets with respect to application type, sample count, type of input data, and type of output data is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>; detailed notes on each task can be found in Table <xref rid="Tab1" ref-type="table">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Categorical dataset distribution of the 13 machine learning tasks in the Matbench test suite v0.1.</title><p>Methods of categorization are listed on the left: Application describes the ML target property of the task as it relates to materials, Num. samples describes the number of samples in each task, Input Type describes the materials primitives that serve as input for each task, and Task Type designates the supervised ML task type. Numbers in the bars represent the number of tasks fitting the descriptor above it (e.g., there are 10 regression tasks).</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2020_406_Fig1_HTML.png"/></fig><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>The dataset test suite.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Target property (unit)</p></th><th><p>Task type</p></th><th><p>Data source</p></th><th><p>Samples</p></th><th><p>Structure available</p></th><th><p>Method</p></th></tr></thead><tbody><tr><td><p>Bulk modulus (GPa)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR45">45</xref></sup></p></td><td><p>10,987</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Shear modulus (GPa)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR45">45</xref></sup></p></td><td><p>10,987</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Band gap (eV)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>106,113</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Metallicity (binary)</p></td><td><p>Classification</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>106,113</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Band gap (eV)</p></td><td><p>Regression</p></td><td><p>Zhuo et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup></p></td><td><p>4604</p></td><td><p>No</p></td><td><p>Experiment</p></td></tr><tr><td><p>Metallicity (binary)</p></td><td><p>Classification</p></td><td><p>Zhuo et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup></p></td><td><p>4921</p></td><td><p>No</p></td><td><p>Experiment</p></td></tr><tr><td><p>Bulk metallic glass formation (binary)</p></td><td><p>Classification</p></td><td><p>Landolt-Bornstein Handbook<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup></p></td><td><p>5680</p></td><td><p>No</p></td><td><p>Experiment</p></td></tr><tr><td><p>Refractive index (no unit)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup></p></td><td><p>4764</p></td><td><p>Yes</p></td><td><p>DFPT-GGA</p></td></tr><tr><td><p>Formation energy (eV/atom)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>132,752</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Formation energy of Perovskite cell (eV)</p></td><td><p>Regression</p></td><td><p>Castelli et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></p></td><td><p>18,928</p></td><td><p>Yes</p></td><td><p>DFT-GGA</p></td></tr><tr><td><p>Freq. at last phonon PhDOS peak (cm<sup>−1</sup>)</p></td><td><p>Regression</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup></p></td><td><p>1296</p></td><td><p>Yes</p></td><td><p>DFPT-GGA</p></td></tr><tr><td><p>Exfoliation energy (meV/atom)</p></td><td><p>Regression</p></td><td><p>JARVIS DFT 2D<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></p></td><td><p>636</p></td><td><p>Yes</p></td><td><p>DFT-vDW-DF</p></td></tr><tr><td><p>Steel yield strength (MPa)</p></td><td><p>Regression</p></td><td><p>Citrine Informatics<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></p></td><td><p>312</p></td><td><p>No</p></td><td><p>Experiment</p></td></tr></tbody></table><table-wrap-foot><p>The test suite contains 13 separate ML tasks spread across 10 datasets. The test suite’s datasets are diversified across multiple metrics, including target property, number of samples (representing several orders of magnitude), and method for determining the target property.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec4"><title>Automatminer reference algorithm</title><p id="Par8">At a high level, an Automatminer pipeline can be considered a black box that performs many of the steps typically performed by trained researchers (feature extraction, feature reduction, model selection, hyperparameter tuning). Given only a training dataset, and without further researcher intervention or hyperparameter tuning, Automatminer produces a machine learning model that accepts materials compositions and/or crystal structures and returns predictions. Automatminer can create persistent end-to-end pipelines containing all internal training data, configuration, and the best-found model—allowing the final models to be further inspected, shared, and reproduced.</p><p id="Par9">As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the Automatminer pipeline is composed of four general stages. Although the specific details may depend on the particular Automatminer configuration or preset chosen, the following provides a high-level overview of each stage. In the first stage, autofeaturization, Automatminer generates features using Matminer’s featurizer library<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. To check whether a given featurizer is able to produce valid (i.e., not null) descriptors for most of the input data, Automatminer uses a computationally efficient precheck functionality that ensures the featurizer is valid for a threshold percentage (90%) of materials input objects. Should the featurizer fail the precheck, it is not used for feature generation. An example of an invalid behavior would be trying to apply a featurizer that is not parameterized for noble gases to crystals or compounds containing those elements. Automatminer next applies each valid featurizer in an error-tolerant fashion, expanding a material primitive into potentially many thousands of features derived from published literature. The next step in the pipeline is the cleaning stage. This prepares the feature matrix for ML by handling errors (e.g., imputing unknown values) and encoding categorical features. The third stage uses one or more dimensionality reduction algorithms (e.g., based on Pearson correlation coefficients<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> or principal component analysis<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>) to sequentially reduce the feature vector dimension by, for example, removing redundant or linearly dependent sets of features. Similar multi-layer feature reduction was previously shown by Liu et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> to significantly improve ML performance across materials prediction tasks; the exact sequence of dimensionality reduction algorithms is determined by the pipeline preset or by the user. In the Express preset used throughout this work, the sequence includes feature selection via Pearson correlation and tree-ensemble feature importance (details in Methods). Finally, an AutoML stage prototypes and validates internal ML pipelines, which are entirely agnostic to materials inputs. These internal pipelines as implemented in the TPOT (Tree-based Pipeline Optimization Tool) library<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> are directed graphs (trees) with data transformations (operators) representing the nodes of each tree. TPOT’s operators can represent any data transformation, including scaling or normalization, dimensionality reduction (e.g., PCA), and various ML estimators and classifiers (e.g., regularized regression, support vector regression, ensemble models, and boosted models). A full list of the TPOT operator space is presented in Supplementary Tables <xref ref-type="supplementary-material" rid="MOESM1">2</xref> and <xref ref-type="supplementary-material" rid="MOESM1">3</xref>. When training data are input to a TPOT pipeline, the data propagate through each operator until an internal validation loss is computed. TPOT’s training begins with a random population of pipelines constructed from a predefined pool of operators. TPOT equates the internal validation loss of an individual pipeline with its fitness for inference and utilizes genetic programming to iteratively evolve more performant pipelines. Over the course of TPOT’s training, increasingly fit pipelines are selected for inference. More details are available in Methods and TPOT’s original publication<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>The AutoML + Matminer (Automatminer) pipeline.</title><p>The pipeline can be applied to composition-only datasets, structure datasets, and datasets containing electronic bandstructure information. Once fit, the pipeline accepts one or more materials primitives and returns a prediction of a materials property. During autofeaturization, the input dataset is populated with potentially relevant features using the Matminer library. Next, data cleaning and feature reduction stages prepare the feature matrices for input to an AutoML search algorithm. During training, the final stage searches ML pipelines for optimal configurations; during prediction, the best ML pipeline (according to internal validation score) is used to make predictions.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2020_406_Fig2_HTML.png"/></fig></p><p id="Par10">Each stage of Automatminer can be extensively customized to facilitate end-user needs; for example, pipelines can retain custom features, use single models instead of AutoML, and fine tune feature selection hyperparameters. However, preconfigured pipeline presets are available based on memory, CPU, and time constraints, and no user customization is required to train or predict using materials data when using these presets. In this work, we report results generated using the Express preset, which is designed to run with a maximum AutoML training time of 24 h.</p><p id="Par11">We evaluate Automatminer on the Matbench test suite and provide comparisons with alternative algorithms in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. The evaluation is performed using a five-fold Nested Cross Validation (NCV) procedure. In contrast to relying on a single train-test split, in the five-fold NCV procedure, five different train-test sets are created. For each of the five train-test sets, a machine learning model is fit using only the training data and evaluated on the test data. Note that this implies that even for a single type of model (e.g., Automatminer or CGCNN<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>), a slightly different model will be trained for each of the five splits since the training data differs between splits. The errors from the five different overall runs are averaged to give the overall score. Note that within each of the five runs of this outer loop, the training data portion is generally split using an inner cross validation that is used for model selection within the training data, hence the name Nested Cross Validation (in our procedure, an algorithm can make use of the training data, however, it chooses). One advantage of 5-fold nested CV over a traditional train-test split is that each sample in the overall dataset is present as training in four of the splits and as test in one of the splits.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Comparison of machine learning algorithm accuracies on the Matbench v0.1 test suite.</title><p>See Table <xref rid="Tab1" ref-type="table">1</xref> for more details of the test sets. Numbers on each square represent either the mean average error (regression) or mean ROC-AUC (classification) of a five-fold nested cross validation (NCV), except for Best Literature scores. Best Literature scores were taken from published literature models<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup> evaluated on similar tasks or datasets, often subsets of those in Matbench, and do not use NCV. Colors represent prediction quality (analogous to relative error) with respect to either the dataset target mean average deviation (MAD) for regression or the high/low limits of ROC-AUC (0.5 is equivalent to random, 1.0 is best) for classification; blue and red represent high and low prediction qualities, respectively, with respect to these baselines. Accordingly, red-hued columns indicate more difficult ML tasks where no algorithm has high predictive accuracy when compared to predicting the mean. Red-hued rows therefore indicate poorly performing algorithms across multiple tasks. The best score for each task is outlined with a black box (The Best Literature scores are excluded because they do not use the same testing protocol). To account for variance from choice of NCV split, multiple scores may be outlined if within 1% of the true best score. A comparison with a pure Random Forest (RF) model using Magpie<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and SineCoulombMatrix<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> features is provided for reference. Dummy predictor results are also shown for each task. All Automatminer, CGCNN, MEGNet, and RF results were generated using the same NCV test procedure on identical train/test folds; all featurizer (descriptor) fitting, hyperparameter optimization, internal validation, and model selection were done on the training set only. A full breakdown of all error estimation procedures can be found in Methods.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2020_406_Fig3_HTML.png"/></fig></p><p id="Par12">For all tasks, the Automatminer Express preset configuration is used in this work. The Express preset only implements featurizers from Matminer that are broadly applicable (tend to produce valid feature values for almost all compositions and/or crystal structures), are computationally efficient (&lt;2 s/sample), and can be trivially transformed from matrices to vectors for each sample. Express feature reduction typically retains between 20 and 200 features based on a feature importance threshold from a Random Forest<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> model. The reduced number of features allows for accelerated evolution of the TPOT genetic algorithm within the Express training time limit of 24 h. Further details can be found in the Methods, Supplementary Tables <xref ref-type="supplementary-material" rid="MOESM1">1</xref>–<xref ref-type="supplementary-material" rid="MOESM1">3</xref>, and <xref ref-type="supplementary-material" rid="MOESM1">Supplementary Notes</xref><xref ref-type="supplementary-material" rid="MOESM1">1</xref> and <xref ref-type="supplementary-material" rid="MOESM1">2</xref>. While other presets are available in Automatminer, we have found that the Express preset generally retains 95% or more of the accuracy of more expensive presets on multiple data-scarce tasks (bulk metallic glass classification, experimental band gap regression/classification, exfoliation energy regression) at less than 50% of the computational cost to reach reasonable AutoML convergence. We emphasize that the Automatminer Express preset is a single configuration capable of fitting on all Matbench tasks with no additional input or configuration. We do not modify this preset for different tasks.</p><p id="Par13">Four alternative algorithms are used for comparison. To simulate a control, a Dummy model predicts the mean of the training set (regression) or randomly selects a label in proportion to the distribution of the training set (classification). As a second baseline representing commonly used methods, we employ a Random Forest<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> model (RF) using Magpie elemental statistics<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and Sine Coulomb Matrix<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> (if structures are present in the dataset) to predict each property. Finally, for tasks containing relaxed structures, we also test against CGCNN<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and MEGNet<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, two graph-network algorithms for general-purpose property prediction. It must be emphasized that a goal of Matbench is to minimize arbitrary biases when comparing models. Therefore, the four alternatives and Automatminer all underwent identical error estimation procedures (NCV on identical folds) for each task.</p><p id="Par14">For some Matbench tasks, we were able to find published scores of researcher-optimized machine learning models, which we label as the Best Literature score. However, it should be noted that although these studies report the same error metric (MAE) using similar datasets, the scores do not use identical datasets (e.g., using different data filtering algorithms to remove erroneous or unreliable data points) or the same error estimation procedure (e.g., they do not use nested cross validation and may use different proportions of train and test). Therefore, these scores cannot be directly compared to the algorithms listed above.</p><p id="Par15">All models outperform Dummy on all tasks: the Dummy comparison exhibits errors between 68 and 299% higher than the best model for any task. We next examine which algorithms perform best, with best taken to include scores within 1% of the best NCV score (we find the standard deviation between folds for the same model is typically between 0.5 and 5%). The Automatminer Express preset has a best NCV score (lowest mean average error, MAE or highest receiver operating characteristic area under curve, ROC-AUC) on 8 of 13 tasks. In particular, Automatminer equals or outperforms the RF pipeline on all tasks except predicting formation energies across the Materials Project. Among the nine structure tasks only, Automatminer and MEGNet both have best scores on four tasks each. CGCNN is the highest performer only for the Materials Project band gap regression task; yet, across the six tasks with more than <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^4$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2020_406_Article_IEq1.gif"/></alternatives></inline-formula> samples, the MEGNet and CGCNN scores are generally quite close.</p><p id="Par16">Notably, we also find Automatminer has similar errors to scores taken from literature. Although these results are taken directly from published reports which use similar—but not identical—datasets and a variety of non-NCV error procedures, it is notable that Automatminer can automatically generate models of roughly similar quality to tediously hand-optimized models. This suggests that similar results as those obtained in the literature can be obtained from a fully automated ML pipeline that requires no researcher tuning or intuition.</p><p id="Par17">Next, we examine how the performance of the various machine learning algorithms varies with the size of the training dataset without regard to the specific task. To do this, we normalize the errors on the various tasks by dividing the mean average error (MAE) by the mean average deviation (MAD) in the dataset. With this normalization, a model that always predicts the average of the dataset will have an error of exactly 1.0. Using least-squares linear regression, we find noticeable inverse trends in the MAE/MAD relative error (Fig. <xref rid="Fig4" ref-type="fig">4</xref>) with respect to the log of dataset size. Interestingly, irrespective of the target property, the rates of improvement with increasing dataset size (slope of the lines) are vastly different between algorithms. In Fig. <xref rid="Fig4" ref-type="fig">4a</xref>, we plot the trend for structure-based regression tasks only. The graph-network models CGCNN and MEGNet have relatively high errors on tasks with small datasets but improve rapidly as the task’s dataset size increases. In contrast, the descriptor-based Automatminer and RF models have lower errors on small datasets, but their rates of improvement are far shallower, and they lose their small data advantage as the data size passes <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mrow><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$10^4$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2020_406_Article_IEq2.gif"/></alternatives></inline-formula> samples. Both graph neural network approaches have similarly high rates of improvement, which may indicate that the underlying ML algorithms are able to leverage information from large datasets more efficiently than traditional ML (RF) or AutoML. This finding corroborates Schmidt et al.’s prediction<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> that universal graph neural networks<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup> will dominate the state-of-the-art on large (&gt;10<sup>5</sup> samples) materials datasets.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Trends in relative predictive accuracy for all algorithms on the Matbench v0.1 dataset.</title><p><bold>a</bold> Results from the eight Matbench v0.1 regression tasks with crystal structure. Algorithms are segregated by color. For each task-algorithm pair, the mean MAE of the nested CV test folds is divided by the dataset mean average deviation to get the relative error. A relative error of zero represents perfect predictive performance; a relative error of 1.0 is equivalent to predicting the mean of the dataset (as in the Dummy Predictor). The plot is agnostic to target property. A least-squares linear regression line of the same color as the scatter points was fit for each algorithm. Multiple tasks have an identical dataset size but differ in their relative errors (e.g., log<sub>10</sub><italic>K</italic> and log<sub>10</sub><italic>G</italic>). <bold>b</bold> Results for all regression tasks (including those lacking crystal structure data as input) and only showing the two algorithms valid for all such tasks.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2020_406_Fig4_HTML.png"/></fig></p><p id="Par18">In Fig. <xref rid="Fig4" ref-type="fig">4b</xref>, we compare Automatminer against the Random Forest model since these two models are able to make predictions on all regression tasks (both composition-only as well as composition plus structure tasks). In <xref rid="Fig4" ref-type="fig">4b</xref>, AutoML’s advantage over more conventional techniques narrows as the number of samples increases. Near 10<sup>5</sup> samples, the AutoML advantage is essentially lost. This phenomenon can be partially explained from the 24-h training time limitation of the Automatminer Express preset. Although the exact pipeline used by the RF model exists in the Express model space, the long training time of each ML pipeline reduces the AutoML search efficiency. Given enough time and computational resources to internally validate and improve its model, it is highly probable the Automatminer Express preset will either find a model equivalent to or superior to the RF model. However, simple ML models (such as the RF we tested) can equal or outperform our AutoML approach if the AutoML search is inefficient in finding the optimal model.</p><p id="Par19">All algorithms exhibit a noisy yet universal trend, which decreases the relative errors as the dataset size increases, even though the underlying task is also changing with size. Such a trend corroborates Zhang and Ling’s observations<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> based on a survey of materials ML data in published literature, which suggests the relationship between error (constructed using literature CV data and scaled by range rather than mean average deviation) and dataset size can be fit with a decreasing power law. This trend identified by Zhang and Ling is similar to that found in the more structured results we present. However, we additionally find that the rate of improvement differs substantially between more conventional machine learning approaches versus the graph neural network approaches. Furthermore, despite these overall trends, it is clear that the details of the underlying task do matter. For example, the two graph networks (CGCNN and MEGNet) appear to far outperform the two traditional ML algorithms (Random Forest and Automatminer) on the two formation energy prediction tasks. However, they do not outperform the traditional algorithms by as much on the band gap regression task, despite the large-data domain that graph networks excel in. Similarly, while Automatminer outperforms the graph networks on most small datasets, MEGNet decisively outperforms Automatminer for the PhDOS task. The predictive advantage may lie in MEGNet’s specific architecture and implementation rather than an inherent advantage of crystal graph neural networks, given CGCNN has higher error than both Automatminer and MEGNet for the PhDOS task.</p></sec></sec><sec id="Sec5" sec-type="discussion"><title>Discussion</title><p id="Par20">The reference algorithm and test suite presented above encompass a benchmark that can be used to accelerate development of supervised learning tasks in materials science. Automatminer provides an extensible and universal platform for automated model selection, while Matbench defines a consistent test procedure for unbiased model comparison. Together, Automatminer + Matbench define a performance baseline for machine learning models aiming to predict materials properties from composition or crystal structure. In this section, we address limitations and extensions of both the reference algorithm and the test suite.</p><p id="Par21">Although the Express preset was used to demonstrate Automatminer’s performance, the Automatminer pipeline is fully configurable at each stage. To reduce the complexity of developing end-to-end materials ML pipelines, Automatminer provides other preset configurations for varying CPU capabilities, time requirements, and objectives. Each preset defines a specific balance between computational cost and comprehensiveness of ML search. For example, the Debug preset employs only a single computationally inexpensive featurizer (Magpie featurizer<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>) and a heavily restricted AutoML model space restricted to a two minute training time; similarly, the Debug_single preset only uses a single predictor (Random Forest) in place of an AutoML algorithm. Other presets exist, which expand on the Express featurizer set using more expensive featurization and longer AutoML optimization times. Generally, we observe diminishing returns on performance with more expensive presets; minor improvements in performance require significant increases in computational time. This is particularly noticeable on small datasets where many ML pipelines can be attempted within the time restriction. For instance, in classifying experimental metallicities, the Express preset improves ROC-AUC a negligible ~0.2% (0.919) on average over Debug (0.917), with the Heavy (most expensive) preset improving only another 0.6% (0.925). Further details on the comparison of presets can be found in the Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2</xref> and Supplementary Note <xref ref-type="supplementary-material" rid="MOESM1">1</xref>.</p><p id="Par22">Automatminer may be further improved by including more descriptor techniques in its featurizer sets, especially if those featurizers provide information-dense features at low computational cost. For example, Automatminer does not implement any features for determining 2nd-nearest neighbor coordination, an important structural motif representing medium-range order. Lack of relevant featurizers may also explain the graph networks’ advantages in predicting certain thermodynamic properties. Due to the ability of crystal graph networks to effectively convolve site/bond data, they may more accurately represent 3D chemo-spatial information than traditional descriptors. Future Automatminer development might benefit from using the chemo-spatial data (hidden-layer embeddings) from crystal graph networks as input via transfer learning; similarly, graph-composition networks such as RooSt<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, which have demonstrated success in learning hidden representations from stoichiometry alone, may serve as a valuable improvement on Automatminer’s current featurizer set. Adding such descriptors to Automatminer is well within its current capabilities, since Automatminer is extensible (with respect to featurizers) by design.</p><p id="Par23">With respect to machine learning models searched by the AutoML library, we find that the majority of AutoML training on materials ML tasks find tree-ensemble methods perform better than the other models in the search space such as <italic>k</italic>-nearest neighbors, logistic regression, and elastic net regression. On small datasets, we observe tree-ensembles have sufficient model complexity to model material-property relationships more faithfully than regularized linear methods or logistic regression. However, the dominance of tree-ensembles is in part an artifact of the relatively small model search space of Automatminer, which at present does not include nonlinear support vector machine kernels or neural networks. Models with higher complexity, such as deep neural networks, may also improve Automatminer’s performance on large datasets. Thus, the AutoML search can be improved by expanding the model space at increasing computational cost. However, regardless of the predefined model space or feature set construction, thoughtfully engineered models such as graph networks or other concepts will likely be able to exceed the baseline AutoML model’s performance. An AutoML algorithm is best suited for the rapid prototyping of more complex human-tuned models rather than the replacement of architectures designed with human expertise.</p><p id="Par24">In the Matbench benchmark, we use NCV as a one-size-fits-all tool for evaluation, but it is also conceivable that domain-specific methods better estimate the generalization error than NCV. Ren et al.<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> use grouped CV to estimate the error of their models for classifying bulk metallic glasses outside of the chemical systems contained in the training set. The rationale behind grouped CV is that the testing procedure should mimic the real-world application. In the case of bulk metallic glass study, the intended goal of the algorithm was to make predictions in chemical systems where no data points were yet present. However, a randomized train/test split would likely result in selecting some data points from all chemical systems for the training and testing data. Instead, grouped CV will first separate data points by chemical space, and then select an entire chemical space to fall into either the test or training set. This ensures that testing is conducted on chemical spaces for which there is no training data within that chemical space.</p><p id="Par25">Yet, using grouped CV requires a well-defined manner for grouping the data. In the case of bulk metallic glasses, chemical systems are easily identified as natural groups since the goal is to predict data for entirely unexplored chemical systems. For other materials ML tasks, features for grouping may be hidden in subtle structural motifs or nuances of electronic configuration. Leave-one-cluster-out CV (LOCO-CV)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> is one potential variant of grouped CV that aims to automate grouping by <italic>k</italic>-means clustering. However, the groups are determined by the choice of input features, which poses two fundamental problems with this technique. First, researchers employing different input features will end up with different definitions of groups and thus different testing procedures; this could be corrected if the features used for the grouping procedure were standardized (even if a different set of input features was used for prediction). Second, the input features may not properly capture the most physically relevant grouping; for example, if all input features are based on composition, but the most natural grouping is by a structural feature such as crystal type, then the resulting groups will have less value. Thus, for now it is largely up to researchers to determine the need for using grouped CV and to determine the best grouping strategy. Other strategies<sup><xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR35">35</xref></sup> to predict outlier data in the test set may also prove useful.</p><p id="Par26">An improved benchmark could use a specific, distinct error estimation procedure for every task; such a procedure can be determined by domain experts to most accurately represent the real-world use of the algorithm. The ideal benchmark would therefore be a consensus of community tasks, each with an error estimation procedure customized to most accurately reflect the algorithm’s true error rate in that particular subfield. We chose NCV as a standard error estimator because there are few such well-agreed-upon procedures for existing materials datasets. Future versions of the benchmark may include error estimation procedures other than NCV.</p><p id="Par27">Matbench is not intended to be a final benchmark but a versioned resource that will grow with the field. The ever-increasing volume of data generated from advances in high-throughput experimentation and computation may enable future ML algorithms to predict classes of materials properties that are presently sparse. For example, ab initio defect calculations are presently expensive, but an investigation by Emery and Wolverton<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> has demonstrated DFT can generate defect data in promising quantities for future mainstream statistical learning. Advances in high-throughput experimental techniques (such as automated experimentation) also have the possibility to vastly increase the size and scope of materials data; for instance, a recent study<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> was able to capture UV-Vis spectroscopy data for more than 179,000 metal oxides. A benchmark must evolve to represent these advancements in materials data production. We expect Matbench to be an evolving representation of materials property prediction tasks, and updated versions of Matbench will be released to reflect emerging areas of research. In a similar fashion, Automatminer is designed to be extensible toward emerging techniques for generating descriptors from compositions, crystal structures, and electronic band structures. As more research is released for converting materials objects to machine-learnable descriptors, we intend on incorporating this knowledge into Automatminer’s architecture.</p><p id="Par28">In conclusion, we presented Matbench v0.1, a set of ML tasks aimed at standardizing comparisons of materials property prediction algorithms. We also introduced Automatminer, a fully automated pipeline for predicting materials properties, which we used to set a baseline across the task set. Using Matbench, we compared Automatminer with crystal graph neural network models, a traditional Random Forest model, and a Dummy control model. We find Automatminer’s auto-generated models outperform or equal the RF model across all but one task and are more accurate than crystal graph networks on most tasks with ~10<sup>4</sup> points or fewer. However, crystal graph networks appear to learn better on tasks with larger datasets. Automatminer can be used outside of benchmarking to make predictions automatically and seed research for more specialized, hand-tuned models. We encourage evaluating ML algorithms on the Matbench benchmark and comparing with the latest version of Automatminer.</p></sec><sec id="Sec6" sec-type="methods"><title>Methods</title><sec id="Sec7"><title>Matbench dataset generation and cleaning</title><p id="Par29">Raw data for Matbench v0.1 were obtained by downloading from the original sources. Tabular versions of some datasets are available online through Matminer’s dataset retrieval tools. These datasets contain metadata and auxiliary data. In contrast, the final Matbench datasets are curated tasks containing only the materials input objects and target variables, with all extraneous data removed. Unphysical (e.g., negative DFT elastic moduli), highly uncommon or unrepresentative samples (e.g., solid state noble gases) were removed according to a specific per-task procedure. Table <xref rid="Tab2" ref-type="table">2</xref> describes the resources and steps needed to recreate each dataset from the original source or Matminer version.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Procedures and sources for creating datasets in Matbench v0.1.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Task name</p></th><th><p>Target property (unit)</p></th><th><p>Original source</p></th><th><p>Matminer source dataset</p></th><th><p>Additional modifications</p></th></tr></thead><tbody><tr><td><p>log_kvrh</p></td><td><p>Bulk modulus (GPa)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR45">45</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>1,2,3,6,7</p></td></tr><tr><td><p>log_gvrh</p></td><td><p>Shear modulus (GPa)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>–<xref ref-type="bibr" rid="CR45">45</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>1,2,3,6,7</p></td></tr><tr><td><p>mp_gap</p></td><td><p>Band gap (eV)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>1,6,7</p></td></tr><tr><td><p>mp_is_metal</p></td><td><p>Metallicity (binary)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>1,6,7</p></td></tr><tr><td><p>expt_gap</p></td><td><p>Band gap (eV)</p></td><td><p>Zhuo et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup></p></td><td><p>expt_gap</p></td><td><p>8,9,10</p></td></tr><tr><td><p>expt_is_metal</p></td><td><p>Metallicity (binary)</p></td><td><p>Zhuo et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup></p></td><td><p>expt_gap</p></td><td><p>8,10,11</p></td></tr><tr><td><p>glass</p></td><td><p>Bulk metallic glass formation (binary)</p></td><td><p>Landolt-Bornstein Handbook<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup></p></td><td><p>glass_ternary_landolt</p></td><td><p>8,12</p></td></tr><tr><td><p>dielectric</p></td><td><p>Refractive index (no unit)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR48">48</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>1,4,6,7</p></td></tr><tr><td><p>mp_e_form</p></td><td><p>Formation energy (eV/atom)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup></p></td><td><p>None<sup>a</sup></p></td><td><p>5,6,7</p></td></tr><tr><td><p>perovskites</p></td><td><p>Formation energy per Perovskite cell (eV)</p></td><td><p>Castelli et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></p></td><td><p>castelli_perovskites</p></td><td><p>7</p></td></tr><tr><td><p>phonons</p></td><td><p>Freq. at last phonon PhDOS peak (cm<sup>−1</sup>)</p></td><td><p>Materials Project<sup><xref ref-type="bibr" rid="CR43">43</xref>,<xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR49">49</xref></sup></p></td><td><p>phonon_dielectric_mp</p></td><td><p>1,7</p></td></tr><tr><td><p>jdft2d</p></td><td><p>Exfoliation energy (meV/atom)</p></td><td><p>JARVIS DFT 2D<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></p></td><td><p>jarvis_dft_2d</p></td><td><p>7</p></td></tr><tr><td><p>steels</p></td><td><p>Steel yield strength (MPa)</p></td><td><p>Citrine Informatics<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></p></td><td><p>steel_strength</p></td><td><p>8</p></td></tr></tbody></table><table-wrap-foot><p>Original Source denotes the original work that produced the raw data, which needs not be in tabular form. Matminer source datasets are tabular versions of this raw data, which can be retrieved with Matminer and may apply additional postprocessing or filtering to the original source data. More information on these datasets can be found on Matminer’s dataset summary page and in the Matminer source code. Additional modifications are enumerated.</p><p><sup>a</sup>Generated using the Materials Project API<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> on 4/12/2019.</p><p>(1) Remove entries having a formation energy or energy above the convex hull more than 150 meV.</p><p>(2) Remove entries having <italic>G</italic><sub>Voigt</sub>, <italic>G</italic><sub>Reuss</sub>, <italic>G</italic><sub>VRH</sub>, <italic>K</italic><sub>Voigt</sub>, <italic>K</italic><sub>Reuss</sub>, or <italic>K</italic><sub>VRH</sub> less than or equal to zero.</p><p>(3) Remove entries failing <italic>G</italic><sub>Reuss</sub> ≤ <italic>G</italic><sub>VRH</sub> ≤ <italic>G</italic><sub>Voigt</sub> or <italic>K</italic><sub>Reuss</sub> ≤ <italic>K</italic><sub>VRH</sub> ≤ <italic>K</italic><sub>Voigt</sub></p><p>(4) Remove entry with refractive index less than 1.</p><p>(5) Remove entries having formation energies greater than 3.0 eV. This operation removes ~1500 1-dimensional crystal structures likely resulting from mis-converged DFT structure optimizations of Half-Heuslers present in the Materials Project database as of the generation date.</p><p>(6) Remove entries containing noble gases.</p><p>(7) Remove all columns except structure and the target variable.</p><p>(8) Remove all columns except composition and the target variable.</p><p>(9) Filter according to unique compositions by ensuring no composition has conflicting metallicity.</p><p>(10) Correct erroneous GaAs<sub>0.1</sub>P<sub>0.9</sub> composition from Zhou et al.<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> originally aggregated from Kiselyova et al.<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>.</p><p>(11) Filter according to unique compositions by removing compositions with a range of reported band gap values of more than 0.1 eV. For each remaining composition, select the value closest to the mean of that composition’s reported values.</p><p>(12) Filter according to unique compositions, removing compositions with any conflicting bulk metallic glass formation classifications.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec8"><title>Evaluation of ML algorithms on Matbench benchmark</title><p id="Par30">Five-fold nested cross validation was used to evaluate each algorithm on every task of the benchmark. The outer test loop of the cross validation used uniformly randomized splits generated with scikit-learn<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> K-Fold (random seed 18012019). The splits were identical for each algorithm. Classification tasks used stratified cross validation generated with StratifiedKFold (random seed 18012019) to more accurately represent classification performance with unbalanced numbers of each class label. Within each of the five splits, 80% training + validation data are given to the algorithm to optimize the model internally, and the remaining 20% is used for testing. After predicting on each of the five 20% test splits, the error or AUC is averaged over the five folds. The internal validation and model selection process is dependent on the algorithm.</p><p id="Par31">It is worthwhile to quickly enumerate the limitations of NCV and justify its use. First, NCV is computationally expensive. For <italic>k-</italic>fold NCV, the traditional hold-out tuning/validation/test procedure must be repeated <italic>k</italic> times. NCV also depends on the choice of internal learning procedure for each fold, an aspect which mimics the selection process used by other resampling methods; thus, even when the test sets are fixed, repeating identical procedures can produce error estimates with high variance<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. Several alternative schemes have been proposed which preserve NCV’s advantages while attempting to mitigate issues from increased variability and computational cost. One potential improvement is repeated NCV; but even this approach demonstrates large variation of loss estimates across nested CV runs and is even more computationally expensive than NCV<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. A promising alternative proposes a smooth analytical alternative to NCV, which would reduce the NCV’s computational intensity<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. This analytical alternative also reduces the variability introduced by learning set choice using weights determined after the outer CV loop has been fixed. Yet, the analytical alternative relies on critical assumptions, which do not hold for particular models such as support vector machines with noisy observations. Therefore, at this time, NCV is an adequate method for evaluating and comparing models using the Matbench benchmark.</p></sec><sec id="Sec9"><title>Machine learning pipelines</title><p id="Par32">The descriptor-based RF and Automatminer models use Matminer<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> to generate all descriptors and have identical data cleaning procedures. The Random Forest model uses the SineCoulombMatrix<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> featurizer for tasks containing structure and mean, average deviation, range, and max/min statistics on elemental Magpie<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> features (implemented as the Magpie preset for the ElementProperty featurizer) for all tasks containing chemical compositions. To handle missing features, the RF pipeline drops features with more than 1% missing values. Remaining samples having missing features are imputed using the mean of the known data. Categorical features were encoded using one-hot encoding. The Random Forest model itself consisted of 500 estimators and no max depth, meaning nodes are expanded until all leaves are pure or contain less than two samples.</p><p id="Par33">Automatminer v1.0.3.20191111 was used for all Automatminer benchmarks. Features were generated according to Automatminer’s autofeaturizer Express preset, and a full list of featurizers is available in the Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>. The number of features was reduced (prior to input to TPOT’s AutoML pipelines) using two sequential methods. First, for every set of features cross correlated by absolute Pearson coefficients of <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mrow><mml:mfenced close="∣" open="∣"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfenced><mml:mo>≥</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left| R \right| \ge 0.95$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2020_406_Article_IEq3.gif"/></alternatives></inline-formula>, only the feature with greatest <italic>R</italic><sup>2</sup> with respect to the target was retained. Next, a Random Forest ensemble method identifies relevant features by capturing at least 99% of the Gini importance<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Finally, TPOT v.10.1 was used to train and internally validate (five-fold CV within the training data) competing ML pipelines before selecting the model used to make test predictions. TPOT uses an evolutionary algorithm to optimize the hyperparameters in a given model space. In this context, algorithms (e.g., support vector machines, gradient boosted trees) are integrated into their existing hyperparameter grids such that the algorithms are treated essentially as special hyperparameters. Internal TPOT pre and postprocessing steps (such as normalization) are also included in the model space. Rather than determining a set number of generations to evolve the model population, the Automatminer Express preset sets TPOT to evaluate the maximum number of generations of 100 individual pipelines each within 24 h given a maximum evaluation time of 20 min per individual. Individuals were trained and evaluated with 10× parallelism using the n_jobs Automatminer preset configuration option. A full table of the Automatminer-TPOT model space is described in the Supplementary Tables <xref ref-type="supplementary-material" rid="MOESM1">2</xref> and <xref ref-type="supplementary-material" rid="MOESM1">3</xref>.</p><p id="Par34">CGCNN and MEGNet models were trained and optimized by splitting the training portion of each outer NCV fold into 75% train and 25% validation portions. Thus, the overall split for each fold is 60% training, 20% validation, and 20% test. Each model is trained in epochs of 128-structure batches by optimizing according to mean squared error loss (regression) or binary cross-entropy (classification). After each epoch, the validation loss is computed with the same scoring functions as the final evaluation: MAE for regression or ROC-AUC for classification (made negative so that higher loss represents worse performance). To prevent overfitting, the training is stopped early when the validation loss does not improve over a period of at least 500 epochs. A full table of hyperparameters for each algorithm is provided in Supplementary Tables <xref ref-type="supplementary-material" rid="MOESM1">4</xref>–<xref ref-type="supplementary-material" rid="MOESM1">7</xref>.</p><p id="Par35">Each model’s training, validation, and evaluation for each NCV fold were performed on separate groups of compute nodes. Each fold of the RF model and Automatminer were trained and evaluated on a single 24-core Intel Xeon E5-2670 v3 with 64GB RAM (LR4 node). All CGCNN and MEGNet training was performed using one NVIDIA 1080Ti GPU using CUDA (accompanied by two Intel Xeon E5-2623 CPUs with 60GB RAM). Workflows were set up and executed using the FireWorks<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> software package. Timing data for all model training are shown in Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>This work was intellectually led and funded by the United States Department of Energy, Office of Basic Energy Sciences, Early Career Research Program, which provided funding for A.D., Q.W., A.G., D.D., and A.J. Lawrence Berkeley National Laboratory is funded by the DOE under award DE-AC02-05CH11231. This research used the Lawrencium computational cluster resource provided by the IT Division at the Lawrence Berkeley National Laboratory (Supported by the Director, Office of Science, Office of Basic Energy Sciences, of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231). This research used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility operated under Contract No. DE-AC02-05CH11231. We thank Samy Cherfaoui of the Electrical Engineering and Computer Science Department at the University of California, Berkeley for code contributions. We also thank Patrick Huck for assistance in hosting the data on the MPContribs platform through the Materials Project.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>A.D., Q.W., and A.J. conceived the project. A.D., Q.W., D.D., and A.J. designed the dataset test suite. A.D., Q.W., and A.G. implemented the Automatminer codebase. A.D. and Q.W. performed the benchmarking tests. A.D. prepared the manuscript. A.J. supervised the project. All authors reviewed, edited, and approved the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>Instructions for downloading and using the Matbench benchmark can be viewed on the official documentation (<ext-link xlink:href="https://hackingmaterials.lbl.gov/automatminer/datasets.html" ext-link-type="uri">https://hackingmaterials.lbl.gov/automatminer/datasets.html</ext-link>). The datasets can also be interactively viewed and examined on the Materials Project MPContribs-ML platform (<ext-link xlink:href="https://ml.materialsproject.org" ext-link-type="uri">https://ml.materialsproject.org</ext-link>) as serialized tabular data. The code for retrieving and loading the Matbench datasets can be found in the dataset_retrieval module of the Matminer code (<ext-link xlink:href="https://github.com/hackingmaterials/matminer" ext-link-type="uri">https://github.com/hackingmaterials/matminer</ext-link>). We also encourage readers to suggest modifications to the Matbench dataset test suite on the help forum (<ext-link xlink:href="https://matsci.org/c/matminer" ext-link-type="uri">https://matsci.org/c/matminer</ext-link>).</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>All versions of the Automatminer code are open source via a BSD-style license and are available through the online repository (<ext-link xlink:href="https://github.com/hackingmaterials/automatminer" ext-link-type="uri">https://github.com/hackingmaterials/automatminer</ext-link>). We note that all the code for running the specific tests in this paper is also present in a subpackage of this repository: (<ext-link xlink:href="https://github.com/hackingmaterials/automatminer/tree/master/automatminer_dev" ext-link-type="uri">https://github.com/hackingmaterials/automatminer/tree/master/automatminer_dev</ext-link>).</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par36">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>W</given-names></name><name><surname>Sham</surname><given-names>LJ</given-names></name></person-group><article-title xml:lang="en">Self-consistent equations including exchange and correlation effects</article-title><source>Phys. Rev.</source><year>1965</year><volume>140</volume><fpage>A1133</fpage><lpage>A1138</lpage></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hohenberg</surname><given-names>P</given-names></name><name><surname>Kohn</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Inhomogeneous electron gas</article-title><source>Phys. Rev.</source><year>1964</year><volume>136</volume><fpage>B864</fpage><lpage>B871</lpage></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri Tehrani</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning directed search for ultraincompressible, superhard materials</article-title><source>J. Am. Chem. Soc.</source><year>2018</year><volume>140</volume><fpage>9844</fpage><lpage>9853</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtlWrtrrM</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>CB</given-names></name><etal/></person-group><article-title xml:lang="en">Design-to-device approach affords panchromatic co-sensitized solar cells</article-title><source>Adv. Energy Mater.</source><year>2019</year><volume>9</volume><fpage>1802820</fpage></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jose</surname><given-names>R</given-names></name><name><surname>Ramakrishna</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Materials 4.0: materials big data enabled materials discovery</article-title><source>Appl. Mater. Today</source><year>2018</year><volume>10</volume><fpage>127</fpage><lpage>132</lpage></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Perspective: materials informatics and big data: realization of the “fourth paradigm” of science in materials science</article-title><source>APL Mater.</source><year>2016</year><volume>4</volume><fpage>053208</fpage></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Deng, J. et al. ImageNet: A large-scale hierarchical image database. In <italic>2009 IEEE Conference on Computer Vision and Pattern Recognition</italic> 248–255 (IEEE, 2009).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Rajpurkar, P., Zhang, J., Lopyrev, K. &amp; Liang, P. SQuAD: 100,000+ Questions for Machine Comprehension of Text. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1606.05250" ext-link-type="uri">https://arxiv.org/abs/1606.05250</ext-link> (2016).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname><given-names>IE</given-names></name><etal/></person-group><article-title xml:lang="en">New cubic perovskites for one- and two-photon water splitting using the computational materials repository</article-title><source>Energy Environ. Sci.</source><year>2012</year><volume>5</volume><fpage>9034</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38Xhtlyqs7vL</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Hastie, T., Tibshirani, R. &amp; Friedman, J. H. (eds) in <italic>The elements of statistical learning: data mining, inference, and prediction</italic> 2nd edn., Chapter 7, pp. 241–249 (Springer, 2009).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cawley</surname><given-names>GC</given-names></name><name><surname>Talbot</surname><given-names>NLC</given-names></name></person-group><article-title xml:lang="en">On over-fitting in model selection and subsequent selection bias in performance evaluation</article-title><source>J. Mach. Learn. Res.</source><year>2010</year><volume>11</volume><fpage>2079</fpage><lpage>2107</lpage></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heckman</surname><given-names>JJ</given-names></name></person-group><article-title xml:lang="en">Sample selection bias as a specification error</article-title><source>Econometrica</source><year>1979</year><volume>47</volume><fpage>153</fpage></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Alexander J. Smola, Arthur Gretton, Karsten M. Borgwardt &amp; Bernhard Scholkopf. Correcting sample selection bias by unlabeled data. In <italic>NIPS’06 Proc. 19th International Conference on Neural Information Processing Systems</italic> 601–608 (2006).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Miroslav Dud ́ık, Robert E. Schapire &amp; Steven J. Phillips. Correcting sample selection bias in maximum entropy density estimation. In <italic>NIPS’05 Proc. 18th International Conference on Neural Information Processing Systems</italic> 323–330 (2005).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Ju</surname><given-names>W</given-names></name><name><surname>Shi</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Materials discovery and design using machine learning</article-title><source>J. Materiomics</source><year>2017</year><volume>3</volume><fpage>159</fpage><lpage>177</lpage></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>J</given-names></name><name><surname>Marques</surname><given-names>MRG</given-names></name><name><surname>Botti</surname><given-names>S</given-names></name><name><surname>Marques</surname><given-names>MAL</given-names></name></person-group><article-title xml:lang="en">Recent advances and applications of machine learning in solid-state materials science</article-title><source>Npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>83</fpage></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">MoleculeNet: a benchmark for molecular machine learning</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>513</fpage><lpage>530</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhslChtrbO</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stone</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Cross-validatory choice and assessment of statistical predictions</article-title><source>J. R. Stat. Soc. Ser. B Methodol.</source><year>1974</year><volume>36</volume><fpage>111</fpage><lpage>147</lpage></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Matminer: an open source toolkit for materials data mining</article-title><source>Comput. Mater. Sci.</source><year>2018</year><volume>152</volume><fpage>60</fpage><lpage>69</lpage></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kabiraj</surname><given-names>A</given-names></name><name><surname>Kumar</surname><given-names>M</given-names></name><name><surname>Mahapatra</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">High-throughput discovery of high Curie point two-dimensional ferromagnetic materials</article-title><source>Npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>35</fpage></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdew</surname><given-names>JP</given-names></name><name><surname>Yue</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Accurate and simple density functional for the electronic exchange energy: Generalized gradient approximation</article-title><source>Phys. Rev. B</source><year>1986</year><volume>33</volume><fpage>8800</fpage><lpage>8802</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DC%2BC2sfgsFSksg%3D%3D</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clement</surname><given-names>CL</given-names></name><name><surname>Kauwe</surname><given-names>SK</given-names></name><name><surname>Sparks</surname><given-names>TD</given-names></name></person-group><article-title xml:lang="en">Benchmark AFLOW data sets for machine learning</article-title><source>Integr. Mater. Manuf. Innov.</source><year>2020</year><volume>9</volume><fpage>153</fpage><lpage>156</lpage></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Freedman, D., Pisani, R. &amp; Purves, R. <italic>Statistics (international student edition)</italic> 4th edn. (W. W. Norton &amp; Company, 2007).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hotelling</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Analysis of a complex of statistical variables into principal components</article-title><source>J. Edu. Psychol.</source><year>1933</year><volume>24</volume><fpage>417</fpage><lpage>441</lpage></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Olson, R. S. et al. In <italic>Applications of Evolutionary Computation</italic> (eds Squillero, G. &amp; Burelli, P.) vol. 9597 pp. 123–137 (Springer International Publishing, 2016).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>145301</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSnu7c%3D</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Random forests</article-title><source>Mach. Learn</source><year>2001</year><volume>45</volume><fpage>5</fpage><lpage>32</lpage></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name><name><surname>Wolverton</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">A general-purpose machine learning framework for predicting properties of inorganic materials</article-title><source>Npj Comput. Mater.</source><year>2016</year><volume>2</volume><fpage>16028</fpage></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faber</surname><given-names>F</given-names></name><name><surname>Lindmaa</surname><given-names>A</given-names></name><name><surname>von Lilienfeld</surname><given-names>OA</given-names></name><name><surname>Armiento</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Crystal structure representations for machine learning models of formation energies</article-title><source>Int. J. Quantum Chem.</source><year>2015</year><volume>115</volume><fpage>1094</fpage><lpage>1101</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXms1ylt7k%3D</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Graph networks as a universal machine learning framework for molecules and crystals</article-title><source>Chem. Mater.</source><year>2019</year><volume>31</volume><fpage>3564</fpage><lpage>3572</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXntFaqt7g%3D</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Ling</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">A strategy to apply machine learning to small datasets in materials science</article-title><source>Npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>25</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXitl2gur3O</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Goodall, R. E. A. &amp; Lee, A. A. Predicting materials properties without crystal structure: Deep representation learning from stoichiometry. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1910.00617" ext-link-type="uri">https://arxiv.org/abs/1910.00617</ext-link> (2019).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ren</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerated discovery of metallic glasses through iteration of machine learning and high-throughput experiments</article-title><source>Sci. Adv.</source><year>2018</year><volume>4</volume></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredig</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Can machine learning identify the next high-temperature superconductor? Examining extrapolation performance for materials discovery</article-title><source>Mol. Syst. Des. Eng.</source><year>2018</year><volume>3</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhsFaqs7vE</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Evaluating explorative prediction power of machine learning algorithms for materials discovery using k-fold forward cross-validation</article-title><source>Comput. Mater. Sci.</source><year>2020</year><volume>171</volume><fpage>109203</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhslSisLvM</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emery</surname><given-names>AA</given-names></name><name><surname>Wolverton</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">High-throughput DFT calculations of formation energy, stability and oxygen vacancy formation energy of ABO3 perovskites</article-title><source>Sci. Data</source><year>2017</year><volume>4</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs1Kmsr%2FP</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>HS</given-names></name><name><surname>Soedarmadji</surname><given-names>E</given-names></name><name><surname>Newhouse</surname><given-names>PF</given-names></name><name><surname>Guevarra</surname><given-names>Dan</given-names></name><name><surname>Gregoire</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Synthesis, optical imaging, and absorption spectroscopy data for 179072 metal oxides</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Scikit-learn: machine learning in python</article-title><source>J. Mach. Learn. Res.</source><year>2011</year><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernau</surname><given-names>C</given-names></name><name><surname>Augustin</surname><given-names>T</given-names></name><name><surname>Boulesteix</surname><given-names>A-L</given-names></name></person-group><article-title xml:lang="en">Correcting the optimal resampling-based error rate by estimating the error rate of wrapper algorithms: estimating the error rate of wrapper algorithms</article-title><source>Biometrics</source><year>2013</year><volume>69</volume><fpage>693</fpage><lpage>702</lpage></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krstajic</surname><given-names>D</given-names></name><name><surname>Buturovic</surname><given-names>LJ</given-names></name><name><surname>Leahy</surname><given-names>DE</given-names></name><name><surname>Thomas</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Cross-validation pitfalls when selecting and assessing regression and classification models</article-title><source>J. Cheminforma.</source><year>2014</year><volume>6</volume></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Breiman, L., Friedman, J. H., Olshen, R. A. &amp; Stone, C. J. In <italic>Classification And Regression Trees</italic> 1st edn. (eds Kimmel J. &amp; Cava, A.) Ch. 5 (Chapman &amp; Hall/CRC, 1984).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">FireWorks: a dynamic workflow system designed for high-throughput applications: FireWorks: a dynamic workflow system designed for high-throughput applications</article-title><source>Concurr. Comput. Pract. Exp.</source><year>2015</year><volume>27</volume><fpage>5037</fpage><lpage>5059</lpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Commentary: The Materials Project: a materials genome approach to accelerating materials innovation</article-title><source>APL Mater.</source><year>2013</year><volume>1</volume><fpage>011002</fpage></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ong</surname><given-names>SP</given-names></name><etal/></person-group><article-title xml:lang="en">The Materials Application Programming Interface (API): a simple, flexible and efficient API for materials data based on REpresentational State Transfer (REST) principles</article-title><source>Comput. Mater. Sci.</source><year>2015</year><volume>97</volume><fpage>209</fpage><lpage>215</lpage></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jong</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Charting the complete elastic properties of inorganic crystalline compounds</article-title><source>Sci. Data</source><year>2015</year><volume>2</volume></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuo</surname><given-names>Y</given-names></name><name><surname>Mansouri Tehrani</surname><given-names>A</given-names></name><name><surname>Brgoch</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Predicting the band gaps of inorganic solids by machine learning</article-title><source>J. Phys. Chem. Lett.</source><year>2018</year><volume>9</volume><fpage>1668</fpage><lpage>1673</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXktlGitbs%3D</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Kawazoe, Y., Yu, J.-Z., Tsai, A.-P. &amp; Masumoto, T. <italic>Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys</italic> (Springer, 1997).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petousis</surname><given-names>I</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput screening of inorganic compounds for the discovery of novel dielectric and optical materials</article-title><source>Sci. Data</source><year>2017</year><volume>4</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhvFehtLs%3D</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petretto</surname><given-names>G</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput density-functional perturbation theory phonons for inorganic materials</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXosFyqt7g%3D</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>Kalish</surname><given-names>I</given-names></name><name><surname>Beams</surname><given-names>R</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">High-throughput identification and characterization of two-dimensional materials using density functional theory</article-title><source>Sci. Rep.</source><year>2017</year><volume>7</volume></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Conduit, G. &amp; Bajaj, S. Mechanical properties of some steels. <ext-link xlink:href="https://citrination.com/datasets/153092/" ext-link-type="uri">https://citrination.com/datasets/153092/</ext-link> (2017).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiselyova</surname><given-names>NN</given-names></name><name><surname>Dudarev</surname><given-names>VA</given-names></name><name><surname>Korzhuyev</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Database on the bandgap of inorganic substances and materials</article-title><source>Inorg. Mater. Appl. Res.</source><year>2016</year><volume>7</volume><fpage>34</fpage><lpage>39</lpage></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>DeCost</surname><given-names>B</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Machine learning with force-field-inspired descriptors for materials: fast screening and mapping energy landscape</article-title><source>Phys. Rev. Mater.</source><year>2018</year><volume>2</volume><fpage>083801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltVKltb0%3D</pub-id></mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec10"><title>Supplementary information</title><p id="Par37"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2020_406_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p><bold>Supplementary information</bold> is available for this paper at <ext-link xlink:href="10.1038/s41524-020-00406-3" ext-link-type="doi">https://doi.org/10.1038/s41524-020-00406-3</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2020</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
