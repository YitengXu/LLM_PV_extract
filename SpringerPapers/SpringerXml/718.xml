<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1186/s13640-018-0258-x</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" specific-use="web-only" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">13640</journal-id><journal-id journal-id-type="doi">10.1007/13640.1687-5281</journal-id><journal-title-group><journal-title>EURASIP Journal on Image and Video Processing</journal-title><abbrev-journal-title abbrev-type="publisher">J Image Video Proc.</abbrev-journal-title></journal-title-group><issn pub-type="epub">1687-5281</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s13640-018-0258-x</article-id><article-id pub-id-type="manuscript">258</article-id><article-id pub-id-type="doi">10.1186/s13640-018-0258-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group><subj-group subj-group-type="article-collection" specific-use="Regular"><subject>Real-time Image and Video Processing in Embedded Systems for Smart Surveillance Applications</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Color sensors and their applications based on real-time color image segmentation for cyber physical systems</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><name><surname>Xiong</surname><given-names>Neal N.</given-names></name><address><email>xiongnaixue@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Shen</surname><given-names>Yang</given-names></name><address><email>shenyang1808@163.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Yang</surname><given-names>Kangye</given-names></name><address><email>Kally_Yang@126.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Lee</surname><given-names>Changhoon</given-names></name><address><email>changhoonlee08@gmail.com</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au5"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6358-9420</contrib-id><name><surname>Wu</surname><given-names>Chunxue</given-names></name><address><email>Tyfond2@126.com</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs136400180258x_cor5">e</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0004 1761 2484</institution-id><institution-id institution-id-type="GRID">grid.33763.32</institution-id><institution content-type="org-division">School of Computer Science and Technology</institution><institution content-type="org-name">Tianjin University</institution></institution-wrap><addr-line content-type="postcode">300350</addr-line><addr-line content-type="city">Tianjin</addr-line><country country="CN">China</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9188 055X</institution-id><institution-id institution-id-type="GRID">grid.267139.8</institution-id><institution content-type="org-division">School of Optical-Electrical and Computer Engineering</institution><institution content-type="org-name">University of Shanghai for Science and Technology</institution></institution-wrap><addr-line content-type="postcode">200093</addr-line><addr-line content-type="city">Shanghai</addr-line><country country="CN">China</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ISNI">0000 0000 9760 4919</institution-id><institution-id institution-id-type="GRID">grid.412485.e</institution-id><institution content-type="org-division">Department of Computer Science and Engineering</institution><institution content-type="org-name">Seoul National University of Science and Technology(SeoulTech)</institution></institution-wrap><addr-line content-type="city">Seoul</addr-line><country country="KR">Korea</country></aff></contrib-group><author-notes><corresp id="IDs136400180258x_cor5"><label>e</label><email>Tyfond2@126.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>4</day><month>4</month><year>2018</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2018</year></pub-date><volume>2018</volume><issue seq="23">1</issue><elocation-id>23</elocation-id><history><date date-type="registration"><day>27</day><month>2</month><year>2018</year></date><date date-type="received"><day>30</day><month>10</month><year>2017</year></date><date date-type="accepted"><day>26</day><month>2</month><year>2018</year></date><date date-type="online"><day>4</day><month>4</month><year>2018</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2018</copyright-statement><copyright-year>2018</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p>Color information plays an important role in the color image segmentation and real-time color sensor, which affects the result of video image segmentation and correct real-time temperature value. In this paper, a novel real-time color image segmentation method is proposed, which is based on color similarity in RGB color space. According to the color and luminance information in RGB color space, the dominant color is determined at first, and then color similarity can be calculated with the proposed calculation method of color component, which creates a color-class map. Next, the information of the corresponding color-class map is utilized to classify the pixels. Due to the characteristic that thermal inks feature color values that change in real time as the temperature changes, the segmentation results of thermal ink can be used as a real-time color sensor. Then, we also propose a method of color correction and light source compensation for the sake of potential inaccuracy of its measures. We discuss the proposed segmentation method application combining with color sensor (thermal ink) in real-time color image segmentation for Cyber physical system (CPS) by the application in fire detection and summarize a new method in identifying fire in a video based on these characteristics. The experiments showed that the proposed method in vision-based fire detection and identification in videos was effective; the results were accurate and can be used in real-time analysis.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Video image segmentation</kwd><kwd>Color sensor</kwd><kwd>Color similarity</kwd><kwd>Thermal ink</kwd><kwd>Real-time systems</kwd><kwd>Cyber physical system</kwd><kwd>Fire detection and identification</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>Shanghai Universities Distinguished Professor Foundation (Eastern scholar)</institution></institution-wrap></funding-source><award-id award-type="FundRef grant">10-15-302-014</award-id></award-group><award-group><funding-source><institution-wrap><institution>Youth Science Foundation of Jiangxi Province</institution></institution-wrap></funding-source><award-id award-type="FundRef grant">20122BAB211022</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2018</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2018</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2018</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>2</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>27</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-collection-editor</meta-name><meta-value>Seungmin Rho,Alexander J. Aved,Yu Chen,Guillermo Botella</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/13640_2018_Article_258.pdf</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Signal,Image and Speech Processing</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Image Processing and Computer Vision</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Biometrics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Pattern Recognition</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p>Cyber physical systems (CPS), as a computing process and the physical process of unity, is integrated computing, communication, and control in one of the next generation of intelligent systems. It interacts with the physical process through the human-computer interaction interface and uses the network space to manipulate a physical entity in a remote, reliable, real-time, secure, and cooperative way.</p><p>CPS includes future ubiquitous environment awareness, embedded computing, network communication, and network control system engineering that enable physical system with computing, communication, precise control, remote collaboration, and autonomous capabilities. It focuses on computing resources and physical resources in close integration and coordination, mainly for some intelligent systems such as robots, and intelligent navigation. At present, the information physics system is still a relatively new research field.</p><p>With the continuous development of computer technology, network technology, and mathematical theory, the research of digital image processing in real-time systems which has become an important component of CPS has been widely applied in various fields, such as biomedicine, satellite remote sensing, and image communication. As a part of the image processing, image segmentation plays an important role. Image segmentation in real-time systems is a technology and process of which divides image into a number of specific and unique section and extracts the interested section. Video image segmentation is an important issue in the field of computer vision and also a classic puzzle [<xref ref-type="bibr" rid="CR1">1</xref>, <xref ref-type="bibr" rid="CR2">2</xref>]. Its researches have been applied in face identification system, fingerprint identification system, fire detection and identification system, machine vision, and medical imaging, and so on. As is known to all, CT (computed tomography) is widely used in hospitals, which uses the result of image segmentation to help diagnose patients effectively and rapidly in real time. Now, face identification system and machine vision are the most concern of scholars.</p><p>There are many video image segmentation algorithms of different applications. But so far, there is no uniform solution or standard for video image segmentation, and there is also no complete theory for guidance on how to select the appropriate segmentation method based on image characteristics. Under normal conditions, in order to more effectively solve a specific problem in the field of image segmentation, it is combined with the knowledge of the relevant areas.</p><p>According to the image gray level, image segmentation can be divided into gray scale image segmentation and color image segmentation. Compared to the gray scale images, color images include not only the brightness but also the color information, such as hue and saturation. In many cases, we can not extract the target information from the image by simply using the gray information while the human eye can recognize thousands of colors, so we can quickly obtain the segmentation with color information [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>]. Therefore, it is essential to study the color image segmentation which has broad prospects.</p><p>There are many splitting methods in color image segmentation, such as histogram threshold-based method, region-based methods, and fuzzy clustering segmentation and edge detection methods [<xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR8">8</xref>]. These methods also combine different color spaces according to the needs of segmentation [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. Therefore, in the process of color image segmentation, we should firstly determine the color space, then select the appropriate segmentation method.</p><p>The image is very vulnerable to the effects of light and noise, so not only the noise but also the light changes should be taken into consideration when segmenting. Image color appearance would change when the light was changed. It will lead to inaccuracy of segmentation by only using the color information, regardless of the brightness information. In order to obtain the good segmentation, it uses the color information and the brightness information concurrently. In this paper, a novel image segmentation method is proposed which can segment the foreground and background in RGB color space by using the color information and the brightness information. The segmentation result of this method is better.</p><p>With modern industrial production develops toward the high speed and automatic direction, color recognition has been widely used in various industrial detection and automatic control field. And the work of color identification which is led by the human eye in the long-term production has been replaced by more and more color sensors. Color sensor detects color with comparison the object color with the reference color, and if they are consistent in a certain error range, then output the detection results. Color sensor can be applied in many fields, such as monitoring the production process and product quality in the industry [<xref ref-type="bibr" rid="CR11">11</xref>]; the realization of the true color copy without affected by environmental temperature, humidity, paper and toner influence in the electronic reproduction aspects; a disease indicator to study a sickness in the Medical; and automatic control in detection two adjacent label colors of a paper and automatically count the number of all sorts of color by auto-counter in the commodity packaging [<xref ref-type="bibr" rid="CR12">12</xref>]. There are many kinds of color sensors so far. The typical color sensor is TCS230 color sensor [<xref ref-type="bibr" rid="CR13">13</xref>], the latest color sensor of TAOS Company. It can recognize and detect colors and has many good new features in comparison with other color sensors. It is adequate for colorimeter measurement applications, such as medical diagnosis, color printing, computer color monitor calibration, and cosmetics, paint, textile and the process control of printing materials.</p><p>According to the working principle of color sensor and image segmentation method, we will design a similar color sensor function by using the thermal ink characteristic in this paper. The characteristic of thermal ink is that its color value will change in real time as the temperature changes. Therefore, the segmentation results of thermal ink can be used as a real-time color sensor. We get the color information from the segmentation area (thermal ink); through the ink color value of correction and the comparison of the standard color, the right color value is concluded, and finally, temperature is output by the relevance of the thermal ink color and temperature. This design can be used for measuring the indoor, outdoor temperature, and food labels, etc. It can also be used to control the temperature of greenhouse plants. We identify the change of temperature through real-time monitoring of the color change, thereby adjust the temperature to obtain a better yield. This method is real time and fast and without multiple sensor nodes.</p><p>The rest of the paper is organized as follows. Section <xref rid="Sec2" ref-type="sec">2</xref> analyzes the color space, video image capture, recognition and segmentation, existing algorithms, and color sensor. Section <xref rid="Sec12" ref-type="sec">3</xref> describes the proposed method and color correction. Section <xref rid="Sec19" ref-type="sec">4</xref> analyzes the experimental results of the proposed method and put forward its application. Section <xref rid="Sec22" ref-type="sec">5</xref> draws the conclusion and finally gives the suggestions.</p></sec><sec id="Sec2"><title>The analysis work</title><sec id="Sec3"><title>Color space</title><p>In the color image segmentation, the first step is to choose a color space. The color model we know contains RGB, HSI, HSV, CMYK, CIE, YUV, and so on. RGB model is the most commonly used for hardware color model while the HSI model is the most commonly used color model for color processing. They are often used in image processing technology [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>].</p><p>RGB space is represented by the three primary colors of red, green, blue; other colors are made up with the three primary colors. The RGB model is represented by the Cartesian coordinate system, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The three axes stand for R, G, B, respectively, and every point in the three-dimensional space means the three components of brightness value. The brightness value is between one and zero.
<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>RGB color model</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig1_HTML.gif" id="MO1"/></fig></p><p>In Fig. <xref rid="Fig1" ref-type="fig">1</xref>, the origin is black, which value is (0,0,0); while the farthest vertex with a value of (1,1,1) from the origin is white. The straight line between black and white called gray line means that the gray value changes from black to white. The remaining three corners represent the complementary color of the three primary colors - yellow, cyan, magenta.</p><p>The three components in the RGB color space, which is highly relevant. And it will be changed accordingly as long as the brightness is changed. RGB is a non-uniform color space, so the perception of differences (color) between the two colors cannot stand fort the distance that between two points in the color space. Thus, the RGB color space is often converted to the other color spaces, such as HSI, HSV, the CIE, and Lab, by using linear or nonlinear transform in image processing. However, the original image we have collected usually is the RGB space, color space conversion will increase the amount of computation. And there are many segmentation methods using RGB color space, for example, license location [<xref ref-type="bibr" rid="CR16">16</xref>] gets the license plate area accurately by calculating the contrast in the RGB components, reducing the calculated amount.</p><p>HSI color model is put forward by Munsell, which is suitable for human visual characteristics. The H (hue) means the different colors, S (saturation) means the depth of color, and I (brightness) mean the light and shade of color. This model has two important characteristics: (1) I component has nothing to do with the color information of the image and (2) H and S component are closely linked to the feelings. They are suitable for image processing with the visual system to perceive the color characteristics, and we often take advantage of the H component to segment the color image. The model shows in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.
<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>HSI color model</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig2_HTML.gif" id="MO2"/></fig></p><p>To deal with the image in the HSI space, image must be converted to the HSI mode. The conversion formula (geometric derivation method) as follows Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>): 
<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mtable><mml:mtr><mml:mtd><mml:mi>H</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfenced close="" open="" separators=""><mml:mrow><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>G</mml:mi><mml:mo>≥</mml:mo><mml:mi>B</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>G</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>B</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>when</mml:mtext><mml:mspace width="1em"/><mml:mi>θ</mml:mi><mml:mspace width="0.3em"/><mml:mo>=</mml:mo><mml:mspace width="0.3em"/><mml:mover><mml:mrow><mml:mo>cos</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mover><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msqrt><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo><mml:mo>(</mml:mo><mml:mi>G</mml:mi><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>I</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>S</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mo>min</mml:mo><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo>min</mml:mo><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mfrac><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{aligned} H &amp; =\left.\begin{cases} \theta, &amp; G \geq B \\ 2\pi - \theta, &amp; G &lt; B \end{cases}\right.\\ &amp;\text{when}~ \theta\,=\,\cos^{-1}\left(\frac{(R-G)+(R-B)}{2\sqrt{(R-B)(G-B)+(R-G)^{2}}}\right). \\ I &amp; =\frac{R+G+B}{3}, \\ S &amp; =1-\frac{3\min(R,G,B)}{R+G+B}=1-\frac{\min(R,G,B)}{I}. \end{aligned}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ1.gif"/></alternatives></disp-formula></p><p>In the conversion Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>), transformation from the RGB model to the HSI model needs more computation. When brightness was zero, saturation was meaningless and when the saturation was zero, hue made no sense. In the conversion, the hue will generate a singularity that cannot be eliminated [<xref ref-type="bibr" rid="CR17">17</xref>]. The singularity may lead to the discontinuous of the nearby tonal value in value, which will ignore the low saturation pixels in the image processing and lead to the incorrect segmentation [<xref ref-type="bibr" rid="CR18">18</xref>]. As is known to us, HSI is suitable for human visual characteristics. Therefore, many scholars have put forth a lot of research for color image segmentation in the HSI model. Reference [<xref ref-type="bibr" rid="CR19">19</xref>] used the saturation and brightness information of HSI model to get texture image segmentation, which is a combination of fractal theory and BP neural network.</p></sec><sec id="Sec4"><title>Video image capture</title><p>Generally, there are two ways in video image capture: (1) the use of video capture card with the SDK development tools. This method relies on the Video capture card and the type of camera, not flexible and universal and (2) the use of Microsoft’s Windows operating system and VFW (Video For Window) software Development Kit of Visual C++. It is a pure software way to realize the collection of video streaming, input, and output. This method does not depend on the type of vision sensors, with better flexibility and versatility [<xref ref-type="bibr" rid="CR20">20</xref>, <xref ref-type="bibr" rid="CR21">21</xref>].</p><p>This paper uses OpenCV’s CVCAM technology to realize the collection of video stream of visual sensor, processing, and playback (display) at the same time, and realize the file streaming reading, processing, and broadcasting (display). 
<list list-type="bullet"><list-item><p>The introduction of the open source Computer Vision Library (OpenCV)</p><p>OpenCV is an open source computer vision library that was funded by Intel, composed of a series of C functions and the C++ class, and provides easy-to-use computer vision framework and rich library. The functions include the field of image processing, computer vision, pattern recognition, and artificial intelligence. With the realization of image processing, signal processing, structure analysis, motion detection, camera calibration, computer graphics, 3D reconstruction, and machine learning, a large number of generic algorithms have higher efficiency.</p></list-item><list-item><p>OpenCV library has the following advantages:</p><p><list list-type="order"><list-item><p>The cross-platform: Windows, Linux, Mac OS, iOS, Android, independent of the operating system, hardware and graphics manager;</p></list-item><list-item><p>Free: open source, does not matter if for business applications or for non-commercial applications;</p></list-item><list-item><p>The high speed: uses the C/C++, suitable for the development of real-time applications;</p></list-item><list-item><p>Easy to use: has a general image/video to load and a save and retrieve module;</p></list-item><list-item><p>Flexible: has good scalability, with low-level and high-level application development kit.</p></list-item></list></p></list-item><list-item><p>OpenCV 1.0 version consists of the following six modules:</p><p><list list-type="order"><list-item><p>The CXCORE module: basic data structures and algorithmsfunction;</p></list-item><list-item><p>The CV module: main OpenCV functions;</p></list-item><list-item><p>CVAUX module: experimental auxiliary functions;</p></list-item><list-item><p>The HighGUI module: graphics interface functions;</p></list-item><list-item><p>The ML module: machine learning function;</p></list-item><list-item><p>CVCAM modules: camera interface function.</p></list-item></list></p></list-item></list></p><p>Because the OpenCV library functions by optimizing the C code, not only is the code simple and efficient but also can make full use of the advantages of multi-core processors. Therefore, this paper uses Visual C++ development environment and OpenCV technology for video image capture, processing and display.</p></sec><sec id="Sec5"><title>Video recognition</title><p>Video recognition mainly includes three links: front-end video information collection and transmission, video retrieval, and back-end analysis processing. Video recognition requires front-end video capture camera to provide a clear and stable video signal as video signal quality will directly affect the effect of video identification, then through embedded intelligent analysis module to detect, analyze, identify the video screen, and filter out interference, then make targets and track marks to the video screen in abnormal situations. In which, the intelligent video analysis module is based on the principles of artificial intelligence and pattern recognition algorithms. Its researches have been applied in fire recognition system [<xref ref-type="bibr" rid="CR22">22</xref>].</p><p>Segment algorithms of a flame object is a key problem in fire recognition based on video sequences applications and have a direct impaction improving fire recognition accuracy [<xref ref-type="bibr" rid="CR23">23</xref>]. In segmentation of flame object, its procedure is precisely based on analyzing fire image characteristic. This paper introduces a new segmentation method of a flame goal based on threshold value of the area using digital image processing technology and pattern recognition technology. Further, it can judge whether fire occurs from the characteristic information such as the fire color, spreading area, the similarity change, and fire smoke. Experiments prove that the method has better robustness. It can segment the image of flame effectively from a sequence of images and reduce the false and missing alarms of the fire surveillance system. So it is very effective to the complex large outdoors occasion.</p><p>Using video recognition technology, through effective analysis of surveillance video images of discrimination, may well detect a fire and treated as early as possible to reduce the economic losses, safeguard people’s life, and property safety! Either economically or technically video, fire recognition technology has a distinct advantage. It will also be an important research direction for future identification of fire.</p><p>Currently, due to different research directions of hardware devices, video fire recognition technology is divided into the following several research ideas: only analysis of static characteristics of the flame, such as the shape, color, and texture of the flame, analysis of the dynamic characteristics such as similarity, spread trend, edge changes, the whole mobile, layered changes, or in the process of dynamic analysis with some simple area characterized criterion [<xref ref-type="bibr" rid="CR24">24</xref>]. Dynamic characteristics are focused on by comparing two or more adjacent images in the video to judge the fire flame. An analysis of the properties of a single image of flame is relatively lacking; static characteristics are focused on single picture by precise analysis of the geometric properties of flame to arrive at a determination result. This analysis is faster, but ignoring the analysis of trend of the flame between several consecutive frame pictures; judgment result is inevitable errors.</p><p>In order to improve the defects and based on the analysis of the fire and the image features, this paper proposes a new segmentation method of flame goal based on threshold value of the area. The method can not only remove noise but also rapidly and accurately extract the target object. Further, it can judge whether fire occurs from the characteristic information, such as the fire flame color, spreading area and the similarity change, and fire smoke. Experimental results show that the method greatly improves the reliability of the fire judging and accuracy and reduces the false alarm and the omission of the fire recognition, shortening the recognition time of fire.</p></sec><sec id="Sec6"><title>Video segmentation</title><p>The so-called video segmentation is to separate the object or objects in video sequences that are important or people are interested in (Video Object; VO) from the background, or that is to draw respectively consistent attributes of each area and, at the same time, to distinguish the background and foreground regions. Video images can be regarded as a kind of 3D image. In other words, the video image is composed of a series of time-continuous 2D images. From the perspective of spatial segmentation, video image segmentation is mainly the use of both the spatial and temporal information to pick out the independent motion regions of the video image in a frame by frame detection [<xref ref-type="bibr" rid="CR25">25</xref>]. Video segmentation is the premise and foundation of other video image processing, such as video coding, video retrieval, and video database operation. The segmentation quality has a direct impact on the work of the late. So, the research of video segmentation technology is important and challenging.</p><p>The main purpose of video segmentation is to segment the moving foreground that people are interested in from the background. At present, there are many splitting methods in video segmentation, such as image difference method, time difference method, and optical flow method. Image difference method is the use of the original image and the reconstructed background image to make differences to realize video segmentation. Time difference method is based on the different images, introducing the relationship between hot and cold time-space domain. Optical flow method is based on the moving object optical flow characteristics with time’s change to efficiently extract and track the moving object [<xref ref-type="bibr" rid="CR26">26</xref>]. Comparing these methods, image difference method with low computational complexity, less affected by the light and low requirement to the hardware, detected better in most cases. The key of image difference method is how to reconstruct a complete video image background. Background reconstruction method mentioned in the literature requires at least 25 video images of unified coordinate pixel values to reconstruct the background image. This method takes a long time and is not conducive to the implementation of segmentation. Since each frame video image of moving foreground region in the same coordinate point have different gray value in general, i.e., frame difference should be a large difference in the foreground area than in the stationary background area. Therefore, by calculating the gray scale value between successive frames can be obtained foreground motion region.</p><p>At present, the general steps of video segmentation are the following: first, the original video image data is simplified and eliminated the noise in order to facilitate the segmentation, which can be accomplished by low-pass filtering, median filtering, and morphological filtering; next, extract the features of the video image, which including color, texture, motion, frame difference, and so on; then, based on certain standards of uniformity, determine the split decision according to the feature extraction to classify the video image, and finally, the post-treatment to achieve filtering noise and accurately extract the boundary, getting accurate segmentation results.</p></sec><sec id="Sec7"><title>The analysis of segmentation algorithms</title><p>Threshold segmentation method [<xref ref-type="bibr" rid="CR27">27</xref>] is one of the most commonly used parallel regional technologies; it is one of the largest number used in image segmentation. Actually threshold segmentation method is that transform image <italic>G</italic> to the output image <italic>F</italic> as follow: 
<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mi>F</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mi>T</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>G</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>T.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ F(i,j)=\begin{cases} 1, &amp; G(i,j) \geq T \\ 0, &amp; G(i,j) &lt; T. \end{cases}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ2.gif"/></alternatives></disp-formula></p><p><italic>T</italic> is the threshold value. If it is the object, then image element <italic>G</italic>(<italic>i</italic>,<italic>j</italic>)=1 or image element <italic>G</italic>(<italic>i</italic>,<italic>j</italic>)=0. Thus, the key of threshold segmentation algorithm is to determine the threshold value. When threshold is determined, we compare the threshold with the gray value of the pixel and divide every pixel concurrently; segmentation result will output the image area directly. Threshold segmentation has the advantage of simple calculation, high efficiency operation, and high speed. It has been widely used in applications that focus on operation efficiency, such as hardware implementation. Scholars have studied all kinds of threshold processing technologies, including global threshold value, adaptive threshold value, and the best threshold value.</p><p>In the color image segmentation, we also consider the color value of pixels, i.e., the color information and brightness, which influence the segmentation result. And many scholars have made a lot of research of this problem.</p><p>Cheng and Quan [<xref ref-type="bibr" rid="CR18">18</xref>] puts forward a model color image background difference method based on HSI. According to chromaticity (H), saturation (S), and brightness (I), independent characteristics of the HSI model, it creates the brightness information by the H component and the S component and extracts the precise prospects with using a dynamic threshold of the brightness information. The change of the light will influence accuracy of detection of moving objects, so this paper eliminates it with HSI. The results show that this method is robust for noise and light changes and can well solve the problems of brightness changes. This method can well solve the influence of light, but it would increase the amount of computation when the color space was transformed to HSI space.</p><p>Huang et al. [<xref ref-type="bibr" rid="CR28">28</xref>] describes an algorithm in traffic sign segmentation. It considers the influence of light and the transformations in the color space and analysis of a lot of traffic sign pictures and researches the relationship between the color pixels in the RGB color space; the paper puts forward a traffic sign segmentation method based on an RGB model. This method can be very good in dealing with traffic sign segmentation of the impact of the noise and light; the segmentation result is precise and can be real-time processed, but it needs a lot of research of traffic to get the experience threshold.</p><p>In this paper, we seriously discussed the influence factors of the image segmentation, including light, noise, and color space. An algorithm of color image segmentation base on color similarity in the RGB color space is presented; we calculate the pixels’ similarity by color similarity and form classification map, and obtain the segmentation finally.</p></sec><sec id="Sec8"><title>Color sensor and color correction</title><sec id="Sec9"><title>Color sensor</title><p>Color has always played an important role in our life and production activities. The color of an object contains a lot of information, so it is easily affected by many factors, such as radiation light and reflections, light source azimuth, observation orientation, and the performance of the sensor [<xref ref-type="bibr" rid="CR29">29</xref>]; the change of any parameter will lead to a change in the observed color.</p><p>The standard method of color measurement is that measures the sample tristimulus values by making use of spectrophotometric color measurement instrument and obtains the color of the sample. At present, there are two basic types sensor based on the principle of all kinds of color identification: 
<list list-type="bullet"><list-item><p>RGB color sensor (red, green, blue) mainly detects tristimulus values;</p></list-item><list-item><p>Chromatic aberration sensor detects the chromatic aberration of the object to be tested and the standard color. This kind of device contains diffuse type, beam type, and optical fiber type, and is encapsulated in various metals and polycarbonate shells.</p></list-item></list></p><p>RGB color sensor has two kinds of measurement modes: one is to analyze the proportion of red, green, blue. No matter how detection distance changes, it just only cause the change of light intensity but not the proportion of the three kinds of color light. Therefore, it can be used even in the target mechanical vibration occasions. The other mode is to use the reflected light intensity of the primary colors of red, green, and blue to detect. It can detect the tiny color discrimination, but the sensor will be affected by the impact of the target mechanical position. Most RGB color sensors have a guide function that makes it very easy to set up. This kind of sensor mostly has a built-in chart and a threshold value which can determine the operating characteristics. It can more accurately measured color using panchromatic color sensitive devices and means of correlation analysis. Typically, in order to obtain the color tristimulus values, it requires at least three photodiodes as well as three corresponding filters [<xref ref-type="bibr" rid="CR30">30</xref>], so the structure and circuits are complicated.</p></sec><sec id="Sec10"><title>Partial color detection</title><p>In the color sensor, the main point is how to detect a color. We know that there is a disparity between the real color of the object surface and the acquisition image color by imaging device. This is a partial color, which is caused by the surrounding environment, such as light and noise. And the degree of color cast has a deal with the color temperature of the outside light. Color temperature [<xref ref-type="bibr" rid="CR31">31</xref>] to the color of the light source is the description of a color measurement. When a light color from a light source and the radiation color of a black body in a certain temperature phase is the same, we call it light color temperature.</p><p>Under the different light sources, such as natural light, tungsten filament lamp, and halogen lamp, the same kind of color is not the same. The difference is caused by different sources of the “color temperature.” Generally, the image color shows slanting blue when the light color temperature is higher. And the image color shows slanting red when the light color temperature is lower. So how to make the collected images to correctly reflect the real color is a key of research.</p><p>Before correcting the color, we should know if the image exists a partial color and how to detect it and its degree. At present, there are some representative partial color detection methods, including histogram statistics [<xref ref-type="bibr" rid="CR32">32</xref>], gray balance method [<xref ref-type="bibr" rid="CR33">33</xref>], and white balance method [<xref ref-type="bibr" rid="CR33">33</xref>]. They can detect images whether there are partial colors.</p><p>Histogram statistics can show the whole color performance of the image. It will give the average brightness of three channel of RGB color space. We can judge whether the color of initial image is partial by the average brightness of R, G, and B channels. If the brightness of any component is the highest value, then the whole image color will be the color of this component representative. That is, if the brightness value of component G is the biggest, the whole image displays red. But the cause of the partial color is complex for different applications, so this method is difficult to get comprehensive and accurate judgment.</p><p>Gray balance method assumes the mean of the R, G, and B is equal in the whole image, which embodies as neutral “ash.” It uses statistics to average the brightness of every channel, converts it into Lab color space, obtains the homogeneous Lab coordinates relatively, calculates the color lengths to the neutral point, and judges whether there is partial color. But when the environment is lighter or darker, or the color of the image is more single, the mean of the R, G, and B is not equal.</p><p>White balance method deals with the existing mirror reflection image; it considers that the specular part of the mirror or the white area reflection can reflect the light color of light source. We count the max brightness value of every channel, convert it into Lab color space, obtain the homogeneous Lab coordinates relatively, calculate the color lengths to the neutral point, and judge whether there is partial color. But the result is distorted when the shooting objects has no white or specular part.</p><p>All these methods are just only suitable for a certain scope but not all. Therefore, it is limited just to the average image color or brightness max value to measure partial color degree. So, people develop other detection methods for well detection.</p></sec><sec id="Sec11"><title>Color correction</title><p>After color cast detection, the next step is color correction. Color correction is how to describe object intrinsic color under different lighting conditions, and it has been applied in medical image, remote sensing images, mural images, licenses, and many other images. There are some classic methods for color correction, such as gray world color correction [<xref ref-type="bibr" rid="CR34">34</xref>] and perfect reflection color correction [<xref ref-type="bibr" rid="CR35">35</xref>].</p><p>Gray world color correction meets a hypothesis of the film image which is colorful, namely the statistics mean value of every channel should be equal and the color shows gray scale. We calculate the mean average of the filmed image, keep component G unchanged, and let the mean value of component R and B as the basis of color correction. But this method cannot be used in an image with a large single color.</p><p>Perfect reflection color correction. The object itself has no color; it shows color through a different wavelength of light absorption, reflection, and projection. If the object is white, all the light is reflected. The white object or area is called the perfect reflector. Perfect reflection theory is based on the hypothesis that it consider the perfect reflector as a standard white in an image. No matter what light it is, a white object, the R, G, and B of its image are of great value. Based on the perfect reflector, it corrects other colors.</p><p>The two kinds of color correction method are suitable for most color corrections, and the calculation is relatively simple, but sometimes can not come back to the real object color.</p><p>With various application scenarios of color correction, many scholars have proposed novel methods for color correction. Luz et al. propose a method based on Markov Random Field (MRF) which is used to represent the relationship between color depleted and color image to enhances the color of the image for the application of underwater image [<xref ref-type="bibr" rid="CR36">36</xref>]. The parameters of the MRF model are learned from the training data and then the most likely color distribution for each pixel in the given color-depleted image is inferred by using belief propagation (BP). This allows the system to adapt the color restoration algorithm to the current environmental conditions and also to the task requirements. Colin et al. propose a method for correcting the color of multiview video sets as a preprocessing step to compression [<xref ref-type="bibr" rid="CR37">37</xref>]. Distinguished from a previous work, where one of the captured views is used as the color reference, they correct all views to match the average color of the set of views. Block-based disparity estimation is used to find matching points between all views in the video set, and the average color is calculated for these matching points. A least-squares regression is performed for each view to find a function that will make the view most closely match the average color. Rizzi et al. propose a new algorithm for digital images unsupervised enhancement with simultaneous global and local effects, called ACE for Automatic Color Equalization [<xref ref-type="bibr" rid="CR38">38</xref>]. It is based on a computational model of the human visual system that merges the two basic “Gray World” and “White Patch” global equalization mechanisms. Similar with the human visual system, ACE adapts to a wide range of lighting conditions and effectively extracts visual information from the environment. It has shown promising results in achieving different equalization tasks, e.g., performing color and lightness constancy, realizing image dynamic data driven stretching, and controlling the contrast. Yoon et al. use the temporal difference ratio of HSV color channels to compensate of color distortion between consecutive frames [<xref ref-type="bibr" rid="CR39">39</xref>]. Experimental results show that the proposed method can be applied to consumer video surveillance systems for removing atmospheric artifacts without color distortion.</p></sec></sec></sec><sec id="Sec12" sec-type="methods"><title>Methods</title><p>In this section, we firstly introduce the calculation method of the color similarity traditionally and put forward an improve method for this method, then give the way of extraction of flame target and judgment of fire. Finally, the paper will describe the realization of the proposed algorithm. We also describe the measures for the fill light of the image and color correction and draw the correction model.</p><sec id="Sec13"><title>The calculation of the color similarity</title><p>We introduce a scale invariance and semantics of mathematical model, called <italic>SIMILATION</italic> [<xref ref-type="bibr" rid="CR40">40</xref>], which calculates color similarity. Given a set, the <italic>SIMILATION</italic> is defined as the <italic>harmonic mean</italic> and <italic>arithmetic mean</italic> of the proportion of the set. The <italic>harmonic mean</italic> (3), the <italic>arithmetic mean</italic> (4), and the <italic>SIMILATION</italic> (5) are defined as follows [<xref ref-type="bibr" rid="CR40">40</xref>]: 
<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mi mathvariant="normal">harmonic</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">mean</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathrm{harmonic~mean} = \frac{n}{\frac{1}{V_{1}}+\frac{1}{V_{2}}+\frac{1}{V_{3}}+\cdots+\frac{1}{V_{n}}}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ3.gif"/></alternatives></disp-formula></p><p><disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mi mathvariant="normal">arithmetric</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">mean</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \mathrm{arithmetric~mean} = \frac{V_{1}+V_{2}+V_{3}+\cdots+V_{n}}{n}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ4.gif"/></alternatives></disp-formula></p><p><disp-formula id="Equ5"><label>5</label><alternatives><mml:math id="Equ5_Math"><mml:mtable><mml:mtr><mml:mtd><mml:mtext mathvariant="italic">SIMILATION</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">harmonic</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">mean</mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">arithmetric</mml:mi><mml:mspace width="1em"/><mml:mtext mathvariant="italic">mean</mml:mtext></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mfrac><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:mo>⋯</mml:mo><mml:mspace width="0.3em"/><mml:mo>+</mml:mo><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mspace width="0.3em"/><mml:mo>×</mml:mo><mml:mspace width="0.3em"/><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mo>⋯</mml:mo><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ {{} \begin{aligned} &amp;SIMILATION = \frac{\mathrm{harmonic~mean}}{\mathrm{arithmetric~mean}} \\ &amp; = \frac{\frac{n}{\frac{1}{V_{1}}+\frac{1}{V_{2}}+\frac{1}{V_{3}}+ \cdots+\frac{1}{V_{n}}}}{\frac{V_{1}+V_{2}+V_{3}+\cdots+V_{n}}{n}} \\ &amp; = \frac{n^{2}}{\left(V_{1}\,+\,V_{2}\,+\,V_{3}\,+\,\cdots\,+\,V_{n}\right) \!\times\! \left(\frac{1}{V_{1}}+\frac{1}{V_{2}}+\frac{1}{V_{3}}+\cdots+\frac{1}{V_{n}}\right)} \end{aligned}}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ5.gif"/></alternatives></disp-formula></p><p>In the concept, <italic>SIMILATION</italic> represents the similarity level of a set of values, and its range is from the positive infinitesimal to one. When the value of <italic>SIMILATION</italic> equals one, it means that each value of the set is equal. When the value of <italic>SIMILATION</italic> is positive infinitesimal (note: according to the Eqs. (<xref rid="Equ3" ref-type="disp-formula">3</xref>) and (<xref rid="Equ4" ref-type="disp-formula">4</xref>), <italic>SIMILATION</italic> value could not be 0), it means that each value of the set is variety. So, we can describe the similarity of a set of values by <italic>SIMILATION</italic>, because a low similarity is equivalent to a high diversity.</p><p>The <italic>SIMILATION</italic> is scale invariance, and it reflects the diversity from the proportional relationship. In Table <xref rid="Tab1" ref-type="table">1</xref>, the <italic>SIMILATION</italic> still is a constant, as long as the proportional relationship between the data of a set has not changed. The standard deviation is a description of a set of data similarity. Although the ratio between data is invariant, the standard deviation has changed. More specifically, scaling of R, G, B values simultaneously with the same degree is equivalent to brightness changing in color spaces. Thus, the scale invariant property is fit for achieving brightness invariance in color segmentation.
<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Scale Invariance of <italic>SIMILATION</italic></p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Value set</p></th><th align="left"><p><italic>SIMILATION</italic></p></th><th align="left"><p>Standard deviation</p></th></tr></thead><tbody><tr><td align="left"><p>2,4,6</p></td><td align="left"><p>0.8182</p></td><td align="left"><p>1.1</p></td></tr><tr><td align="left"><p>20,40,60</p></td><td align="left"><p>0.8182</p></td><td align="left"><p>11</p></td></tr><tr><td align="left"><p>200,400,600</p></td><td align="left"><p>0.8182</p></td><td align="left"><p>110</p></td></tr></tbody></table></table-wrap></p><p>The color similarity between two colors (<italic>R</italic><sub>1</sub>,<italic>G</italic><sub>1</sub>,<italic>B</italic><sub>1</sub>) and (<italic>R</italic><sub>2</sub>,<italic>G</italic><sub>2</sub>,<italic>B</italic><sub>2</sub>) is measured as below: 
<list list-type="order"><list-item><p>Compute (<italic>R</italic><sub>0</sub>,<italic>G</italic><sub>0</sub>,<italic>B</italic><sub>0</sub>) as shown in Eq. (<xref rid="Equ6" ref-type="disp-formula">6</xref>).</p></list-item><list-item><p>Substitute (<italic>V</italic><sub>1</sub>,<italic>V</italic><sub>2</sub>,<italic>V</italic><sub>3</sub>) with (<italic>R</italic><sub>0</sub>,<italic>G</italic><sub>0</sub>,<italic>B</italic><sub>0</sub>) as in Eq. (<xref rid="Equ4" ref-type="disp-formula">4</xref>) to calculate the <italic>SIMILATION</italic>.</p></list-item></list></p><p><disp-formula id="Equ6"><label>6</label><alternatives><mml:math id="Equ6_Math"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mi>.</mml:mi></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ (R_{0},G_{0},B_{0})=\left(\frac{R_{1}}{R_{2}},\frac{G_{1}}{G_{2}},\frac{B_{1}}{B_{2}}\right).  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ6.gif"/></alternatives></disp-formula></p><p>The Eq. (<xref rid="Equ5" ref-type="disp-formula">5</xref>) shows that any one could not be zero, so any component of the two sets of color does not equal to zero in the Eq. (<xref rid="Equ6" ref-type="disp-formula">6</xref>). Therefore, this measure could not deal with some color value which equal to 0, such as (255,0,0). In the coordinate system of RGB model, there is a lot of color value, for example, yellow (255,255,0) or black (0,0,0).</p><p>In the RGB color space, when the color shows red, it declares the component red is a bigger number than the other components relatively. While the color shows yellow, it declares the component red is a smaller number than the other components relatively and the gap of the other two components is small. Thus, we modify the color similarity method as follows: 
<list list-type="order"><list-item><p>Determine a reference color according to a certain rule (it will be described in the next section), the component value of this color does not contain a value of 0;</p></list-item><list-item><p>Firstly, check the consulted color by comparing the three components for 0, if it does not, please calculate the color similarity with the <italic>SIMILATION</italic>;</p></list-item><list-item><p>If the three components contain 0, we calculate the color similarity as follows: 
<list list-type="order"><list-item><p>Only one component equals to 0, such as (<italic>R</italic>,<italic>G</italic>,0). Check the value of (<italic>R</italic>−<italic>B</italic>) for a positive number; if the value is a positive number, it indicates that the color is rendered as red; otherwise, it is rendered as green. Similarly, other color combinations can also be calculated on the basis of the method.</p></list-item><list-item><p>Only two components equal to 0, such as (<italic>R</italic>,0,0). The (<italic>R</italic>,0,0) means the color is rendered as red. Similarly, the colors (0,<italic>G</italic>,0) and (0,0,<italic>B</italic>) are rendered as green and blue respectively.</p></list-item><list-item><p>The black (0,0,0) remains without any further processing.</p></list-item></list></p></list-item></list></p><p>Finally, the results are compared with the reference color; if they belong to the same color, then the two colors are similar.</p><p>In Table <xref rid="Tab2" ref-type="table">2</xref>, the <italic>SIMILATION</italic> measures the similarity of the two sets of color well. The first row shows that the <italic>SIMILATION</italic> equals to one, only the brightness is different in two colors, and it is equal in the value of two sets of <italic>R</italic><sub>0</sub>,<italic>G</italic><sub>0</sub>,<italic>B</italic><sub>0</sub>. On the other hand, if two colors are not equal in hue (the brightness is different), we also calculate their similarity coefficient by <italic>SIMILATION</italic>, (i.e., either second or third row of Table <xref rid="Tab2" ref-type="table">2</xref>); the similarity of the two colors is 90%. The fourth line of Table <xref rid="Tab2" ref-type="table">2</xref>, although we could not directly calculate the similarity by <italic>SIMILATION</italic>, we draw the similarity of two colors information by comparing the color components. The rest of the lines in Table <xref rid="Tab2" ref-type="table">2</xref> describe the coefficient of similarity of a pixel value and other pixel values.
<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Brightness and color similarities</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>(<italic>R</italic><sub>2</sub>,<italic>G</italic><sub>2</sub>,<italic>B</italic><sub>2</sub>)</p></th><th align="left"><p>(<italic>R</italic><sub>1</sub>,<italic>G</italic><sub>1</sub>,<italic>B</italic><sub>1</sub>)</p></th><th align="left"><p>(<italic>R</italic><sub>0</sub>,<italic>G</italic><sub>0</sub>,<italic>B</italic><sub>0</sub>)</p></th><th align="left"><p>Color similarity (%)</p></th></tr></thead><tbody><tr><td align="left"><p>10,10,100</p></td><td align="left"><p>20,20,200</p></td><td align="left"><p>2,2,2</p></td><td align="left"><p>100</p></td></tr><tr><td align="left"><p>10,10,100</p></td><td align="left"><p>20,150,240</p></td><td align="left"><p>2,15,2.4</p></td><td align="left"><p>47.17</p></td></tr><tr><td align="left"><p>20,20,200</p></td><td align="left"><p>20,150,240</p></td><td align="left"><p>1,7.5,1.2</p></td><td align="left"><p>47.17</p></td></tr><tr><td align="left"><p>20,150,240</p></td><td align="left"><p>0,150,240</p></td><td align="left"><p>0,1,1</p></td><td align="left"><p>Similar</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>140,140,10</p></td><td align="left"><p>7,1,1</p></td><td align="left"><p>46.67</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>70,140,10</p></td><td align="left"><p>3.5,1,1</p></td><td align="left"><p>71.59</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>70,140,70</p></td><td align="left"><p>3.5,1,7</p></td><td align="left"><p>54.78</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>10,220,240</p></td><td align="left"><p>0.5,1.6,24</p></td><td align="left"><p>20.57</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>240,10,100</p></td><td align="left"><p>12,0.07,10</p></td><td align="left"><p>2.87</p></td></tr><tr><td align="left"><p>20,140,10</p></td><td align="left"><p>240,10,200</p></td><td align="left"><p>12,0.07,20</p></td><td align="left"><p>2.12</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec14"><title>Extraction of flame target</title><p>Extraction and segmentation of the flame object is the key technology of fire recognition; the accuracy of flame segmentation and extraction is prerequisite to improve accuracy and robustness of the whole detection system. In the ideal image, one can use hollow out method combined with an edge tracking technology to design the algorithm to achieve, but in the practical engineering application, where there are a lot of noise in the captured image, the existing edge detection algorithm, usually with the aid of Roberts Cross [<xref ref-type="bibr" rid="CR41">41</xref>], Prewitt, and Sobel edge detection operators, is according to the gray value jump or not to detect the image edge, and these methods to measure outline are usually irregular and edge discontinuity, will cost a lot of time to refine the outline and to connect these discontinuous outline, which cannot be allowed in the practical application.</p><p>This paper proposes a flame target contour extraction algorithm based on area threshold. The algorithm idea is that at first use, the difference method judges whether there is a target object, and if so, get the area of the target object and the image of the region through a 2D maximum entropy threshold binarization processing, which can get the block of the connected regions in the image. These regions are part of some object, while the others are noise, then put each connected white area as a set, and for a concrete analysis of each set, eliminate the noise and get the outline of the object; the algorithm process is as follows:</p><p>A reference image is <italic>f</italic><sub>0</sub>(<italic>x</italic>,<italic>y</italic>), sequence image of digital image is <italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>,<italic>y</italic>),<italic>i</italic>=0,1,2,⋯,<italic>N</italic>. (<italic>x</italic>,<italic>y</italic>) is the coordinates of the pixel in the each image. <italic>N</italic> is the number of frames in consecutive image sequences. 
<disp-formula id="Equ7"><label>7</label><alternatives><mml:math id="Equ7_Math"><mml:mtable><mml:mtr><mml:mtd><mml:mtable><mml:mtr><mml:mtd><mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>&lt;</mml:mo><mml:mtext mathvariant="italic">Th</mml:mtext><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mtext>no fire</mml:mtext></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd><mml:mtd><mml:mi>Δ</mml:mi><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>≥</mml:mo><mml:mtext mathvariant="italic">Th</mml:mtext><mml:mn>1</mml:mn><mml:mspace width="1em"/><mml:mtext>on fire</mml:mtext><mml:mi>.</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ {\displaystyle \begin{array}{cc}\varDelta {f}_i\left(x,y\right)&amp; ={f}_i\left(x,y\right)-{f}_0\left(x,y\right)\\ {}=\left\{\begin{array}{ll}0,\kern1em &amp; \varDelta {f}_i\left(x,y\right)&lt; Th1\kern1em \mathrm{no}\ \mathrm{fire}\\ {}0,\kern1em &amp; \varDelta {f}_i\left(x,y\right)\ge Th1\kern1em \mathrm{on}\ \mathrm{fire}.\end{array}\right.\end{array}} $$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ7.gif"/></alternatives></disp-formula></p><p><italic>Δ</italic><italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>,<italic>y</italic>) is the difference of the two images and <italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>,<italic>y</italic>) is a current image; <italic>f</italic><sub>0</sub>(<italic>x</italic>,<italic>y</italic>) is a reference image. In order to highlight the target (fire), <italic>T</italic><italic>h</italic>1 selects the 2D maximum entropy threshold of the image; it can separate the target and the surrounding background points as far as possible, to facilitate the next step for extraction of the flame and to eliminate the noise points. 
<list list-type="order"><list-item><p>Scan <italic>Δ</italic><italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>,<italic>y</italic>) binary image, all white pixels in this binary image will be added to the linked list that take PixelLink as the head node.</p></list-item><list-item><p>To classify the pixels in the PixelLink list, produce a set corresponding to each connected regions (for each set to create a linked list). Begin from a certain point, plus the similar neighboring points forming a region. The similarity criterion can be in gray scale, color, and shape or other characteristics. The test of similarity can be determined by the threshold. It means that start from the point that meet the detection standards, growing area in all directions; if the proximal point meet the detection criterion, add it into the small area, and when the new points are merged, repeat the process to a new region, Until there is no acceptable adjacent point, generation process will to come to an end.</p></list-item><list-item><p>Calculate the area of each connected region, which represented a list, then select the appropriate value of area as threshold for image filtering. The connected regions that exceed the area threshold will remain intact, and the smaller ones as noise are eliminated.</p></list-item><list-item><p>Using the method of hollow out can get a single pixel width continuous contour of the object, and there will be no outline of the cross. Suppose <italic>m</italic> is the target contour in the image <italic>f</italic><sub><italic>i</italic></sub>(<italic>x</italic>,<italic>y</italic>), denoted as <italic>A</italic><sub><italic>i</italic>,1</sub>,<italic>A</italic><sub><italic>i</italic>,2</sub>,⋯,<italic>A</italic><sub><italic>i</italic>,<italic>m</italic></sub>.</p></list-item></list></p><p>After finding flame-suspicious areas, then according to the fire’s features such as color of fire, the size of spread area, similarity, and smoke, make a judgment, to further test whether the suspicious area is the flame.</p></sec><sec id="Sec15"><title>The steps of proposed algorithm</title><p>In this paper, in order to reduce the calculation amount color space conversion, we choose the RGB model. The proposed method that based on the RGB model of color image segmentation is shown in Table <xref rid="Tab3" ref-type="table">3</xref>.
<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Color image segmentation algorithm</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Algorithm 1: Segmentation algorithm</p></th></tr></thead><tbody><tr><td align="left"><p>1. <italic>R</italic><sub><italic>i</italic></sub>= Read(ColorsImage)</p></td></tr><tr><td align="left"><p>2. <italic>B</italic><sub><italic>i</italic></sub>= RGBColorSpace()</p></td></tr><tr><td align="left"><p>3. <italic>D</italic><sub><italic>i</italic></sub>= Determined(DominantColors)</p></td></tr><tr><td align="left"><p>4. <italic>C</italic><sub><italic>i</italic></sub>= Calculate(ColorSimilarity)</p></td></tr><tr><td align="left"><p>5. <italic>G</italic><sub><italic>i</italic></sub>= Calculate(Brightness)</p></td></tr><tr><td align="left"><p>6. <italic>P</italic><sub><italic>i</italic></sub>= Devide(Pixels)</p></td></tr><tr><td align="left"><p>7. <italic>O</italic><sub><italic>i</italic></sub>= Output(ImageResults)</p></td></tr><tr><td align="left"><p>8. end</p></td></tr></tbody></table></table-wrap></p><p>The process is done in the following steps: 
<list list-type="order"><list-item><p>Given a color image (it is RGB space), determine the dominant color and quantity.</p><p>The dominant color (i.e., the reference color) is determined on the basis of segmentation need. If we just split the foreground and background, then we need to choose two dominant colors; or determine a dominant color if we only need to split the image of a region of color consistency, such as leaves or traffic signs. This paper focuses on the segmentation of foreground and background, so two dominant colors are enough.</p><p>Read a color image (the size of the image size is <italic>m</italic>×<italic>n</italic>×3); the color space is RGB space. Calculate the probability of each color in this image.</p><p>We know that the foreground color and background of every image are made up of a lot of the same or similar color. In the RGB space, every color is composed of components R, G, and B. Let the number of each appearing color as a function value and RGB component as a variable and find the two of the largest probability of the appearing color as the dominant color. Calculation is as follows: 
<list list-type="order"><list-item><p>Scan the image according to row <italic>m</italic>, save the color value that is firstly scanned with the format (<italic>R</italic>,<italic>G</italic>,<italic>B</italic>) and set the number as 1.</p></list-item><list-item><p>Continue to scan, compare the color value that meets with the saved color value, and test the RGB components for equality. If equal, add number one; or save it and set the number as 1.</p><p>We can get the number of each color in the image in accordance with the above approach and would determine the dominant color by comparing the number of every color. The dominant color is the reference of the <italic>SIMILATION</italic>, so any component of the dominant color cannot be zero, and if its value is zero, plus one.</p></list-item></list></p></list-item><list-item><p>Calculate the <italic>SIMILATION</italic> value and form color information map.</p><p>After we have determined the dominant colors (two), we calculate the similarity between each color and the two dominants respectively by the modified computing method. There are two cases: 
<list list-type="order"><list-item><p>When every RGB component is not zero, the <italic>SIMILATION</italic> that we calculate has two values, which stand for the similarity with the two dominant colors respectively. Comparing the two similarity coefficients, the similarity coefficient that is bigger will divide into the collection of the dominant color.</p></list-item><list-item><p>If any one of RGB component is zero, we will judge the similarity of the color component of the final show between each color and the two dominant colors and divide into the corresponding collection when they are similarity. Ultimately, a color-class map is formed.</p></list-item></list></p></list-item><list-item><p>Divide the image pixels and output results.</p><p>Pixels are divided into the one collection of these two types based on pixel color and color information map, so foreground and background are segmented. The extracted section will be clear and the boundaries will not be fuzzy, if the colors of the image are obvious. However, the division of pixels refers to the standard of the color similarity measure, which will lead to inaccurate segmentation in some images. For example, some sections belonging to the background may be divided into the foreground region while others belonging to the foreground is divided into the background region. Therefore, it needs other ways to divide the foreground or background for the poor segmentation results.</p></list-item></list></p></sec><sec id="Sec16"><title>The step of judgment of fire</title><p>The fire in the formation process, follow certain characteristics and laws.Familiar with the fire formation rules, select and use some unique characteristics of fire flame, it is a vital role to identify fire as soon as possible. In general, the fire in the video has the following characteristics.</p><p>For the continuous acquisition of the two images, there must be some regional similarity between the previous frame and the next frame, but this does not cause a similar region turn into a complete overlap. This feature is particularly obvious at the beginning of the fire. The gray of the flame core is greater than the other parts in the image. And after the infrared attenuation, in the video signal, the performance mode of the interference signal is mainly fixed fast moving spot, and the large area of infrared illumination changes.</p><p>Therefore, in recognition of the flame, firstly, the flame can be divide into interference pattern and non-interference patterns, and then in the non-interference image pattern to identify the characteristics of the flame to judge is the flame or not. Thus, the video fire recognition includes two aspects, the flame object extraction and according to the flame color and size and other characteristics to judge. The main process is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.
<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>The flow chart of segmentation algorithm</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig3_HTML.gif" id="MO3"/></fig></p><p><list list-type="order"><list-item><p>The judgment of the fire colors</p><p>In a real-world environment, a fire may have two possible phenomena: the initial stage had a lot of smoke and the direct flame. In these two phenomena, the color model is completely different and should be dealt separately. 
<list list-type="order"><list-item><p>The flame model</p><p>Generally, the distribution of the color between the flame and the illuminator is different. To flame, from the flame core to the outer flame, a general trend is to move its color from white to red, according to the characteristics. You can identify the flame.</p></list-item><list-item><p>The smoke model</p><p>In reality, the fire does not necessarily have to produce flame, it can be a billowing smoke. Therefore, smoke is also one of the main characteristics of fire, but smoke is not like flame that have attributes which is relatively easy to distinguish with other objects.But this is not to say that the smoke would have no effect in a fire recognition; on the contrary, the features of smoke can be combined and be of good use that can greatly improve the accuracy of the system alarms, which naturally improves the usability of the system.</p><p>At the time of the fire, the most obvious characteristic of smoke is that the area is constantly expanding. Therefore, according to the color and shape features of a smoke, one can identify a suspected smoke area. Given the following definition: 
<disp-formula id="Equ8"><label>8</label><alternatives><mml:math id="Equ8_Math"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="Equ8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ r=\frac{R}{R+G+B}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ8.gif"/></alternatives></disp-formula></p><p><disp-formula id="Equ9"><label>9</label><alternatives><mml:math id="Equ9_Math"><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>G</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="Equ9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ g=\frac{G}{R+G+B}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ9.gif"/></alternatives></disp-formula></p><p><disp-formula id="Equ10"><label>10</label><alternatives><mml:math id="Equ10_Math"><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mn>0.30</mml:mn><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:mn>0.59</mml:mn><mml:mi>G</mml:mi><mml:mo>+</mml:mo><mml:mn>0.11</mml:mn><mml:mi>B</mml:mi></mml:math><tex-math id="Equ10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ Y=0.30R+0.59G+0.11B  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ10.gif"/></alternatives></disp-formula></p><p>In the above equations, R, G, and B are the original pixels. After extracting a lot of information about the characteristics of smoke, statistical analysis showed that the following condition is satisfied: 
<disp-formula id="Equ11"><label>11</label><alternatives><mml:math id="Equ11_Math"><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mn>0.304264</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.335354</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0.318907</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>g</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.337374</mml:mn><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>r</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>g.</mml:mi><mml:mspace width="1em"/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><tex-math id="Equ11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \begin{cases} 0.304264&lt;r&lt;0.335354, \\ 0.318907&lt;g&lt;0.337374, \\ r&lt;g. \end{cases}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ11.gif"/></alternatives></disp-formula></p><p>It is a flame region. Calculate the area of the region, then extract the adjacent image, using the same method for processing and analysis. Finally, compare the area of the two processed images, if the area is, change the alarm.</p></list-item></list></p></list-item><list-item><p>The judgment of fire area feature</p><p>Flame appears as a bright region in the image acquisition, but if only on the basis of this bright region one can determine fire, it is easy to take some light, sunlight, and other high-temperature objects mistakenly as fire. When a fire happened in the early stage, there is a very significant feature is that of spread, and performance in the area is the area of the expanded, so is the size of the same suspicious areas of adjacent frames, which means the size of the area will change. To further verify that the suspicious region is a flame, calculate for the object contour of five consecutive frames separately. <italic>f</italic><sub><italic>i</italic>+1</sub>(<italic>x,y</italic>),<italic>f</italic><sub><italic>i</italic>+2</sub>(<italic>x,y</italic>),<italic>f</italic><sub><italic>i</italic>+3</sub>(<italic>x,y</italic>),<italic>f</italic><sub><italic>i</italic>+4</sub>(<italic>x,y</italic>),<italic>f</italic><sub><italic>i</italic>+5</sub>(<italic>x,y</italic>) and then calculate the area difference and overlap degree of the area between <italic>A</italic><sub><italic>i,p</italic></sub>, <italic>P</italic> and suspicious region <italic>A</italic><sub><italic>i</italic>,1</sub>, and area, denoted as <italic>Δ</italic><sub><italic>i,p</italic></sub> and <italic>ε</italic>,<italic>ε</italic><sub><italic>i,p</italic></sub>, respectively. 
<disp-formula id="Equ12"><label>12</label><alternatives><mml:math id="Equ12_Math"><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="Equ12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \Delta_{i,p}=\sum_{(x,y)\in A_{n}}^{}A_{i+p}(x,y)-A_{i}(x,y)  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ12.gif"/></alternatives></disp-formula></p><p><disp-formula id="Equ13"><label>13</label><alternatives><mml:math id="Equ13_Math"><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>⋂</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo><mml:mo>∈</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow/></mml:munderover><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="Equ13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ \varepsilon_{i,p}=\frac{\sum_{(x,y)\in A_{n}}^{}A_{i+p}(x,y)\bigcap A_{i}(x,y)}{\sum_{(x,y)\in A_{n}}^{}A_{i+p}(x,y)}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ13.gif"/></alternatives></disp-formula></p><p>In the above equations, <italic>p</italic>=<italic>i</italic>+1,⋯,<italic>i</italic>+5. Use statistics for the mean of each <italic>Δ</italic><sub><italic>i,p</italic></sub> and <italic>ε</italic><sub><italic>i,p</italic></sub> respectively, denoted as <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mi>Δ</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>Δ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\Delta =\sum _{p=1}^{5}\Delta _{i,p}/5$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq1.gif"/></alternatives></inline-formula> and <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mi>ε</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>ε</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:mn>5</mml:mn></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\varepsilon =\sum _{p=1}^{5}\varepsilon _{i,p}/5$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq2.gif"/></alternatives></inline-formula>, then given the threshold <italic>Th</italic>2 and <italic>Th</italic>3, if <italic>Δ</italic>≥<italic>Th</italic>2 and 1&gt;<italic>ε</italic>≥<italic>Th</italic>3, the change area is a suspicious fire area.</p></list-item></list></p></sec><sec id="Sec17"><title>The measure of fill light</title><p>When without or short of natural light, light output from the thermal ink of the drawing board area can not move toward the camera, so it is difficult to capture the image of this area. If more shades are placed next to the drawing board, clearer images can be obtained on a rainy day or evening. Light source compensation is applicable to many situations, such as VIN recognition and mixed color detection system of glass bottles. The light source in this article is in connection with thermal ink, i.e., the light source refers to light intensity of the normal fluorescent light. It is shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.
<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Schematic diagram of compensation light</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig4_HTML.gif" id="MO4"/></fig></p><p>Color block means thermal ink (herein referred to as the sensing zone) and the center part is the standard color area. The sensing area is surrounded by the same fluorescent lamps; power is provided by solar cells and windmills. The camera is unable to capture where fluorescent lights are, and the intensity of the fluorescent light can not cause a color change of the thermal ink. Therefore, the intensity of fluorescent lights should be determined by several tests. In daytime, solar panels convert light to electricity; wind power also generates electricity, which is stored in the battery. The battery becomes a power supply to the fluorescent lamps, which can be lit continuously. It is an environmentally friendly way, because both solar energy and wind power are renewable, with neither pollution nor drying up.</p></sec><sec id="Sec18"><title>The method of color correction</title><p>The color value would be changed by light, temperature, and any other environmental factors, so the image should be color corrected after shooting to get accurate color values. In this paper, we will discuss the correction of standard color and thermal ink color. 
<list list-type="order"><list-item><p>The correction of standard color</p><p>The standard color is mainly influenced by light and has nothing with temperature. But the color will change after a long time, this is a common problem. We do not consider it.</p><p>Assuming that the size of standard color card is <italic>N</italic> pixels and the value of every pixel <italic>P</italic><sub><italic>i</italic></sub> is <italic>f</italic>(<italic>i</italic>)=(<italic>R</italic><sub><italic>i</italic></sub>,<italic>G</italic><sub><italic>i</italic></sub>,<italic>B</italic><sub><italic>i</italic></sub>),<italic>i</italic>=1,2,⋯,<italic>N</italic>, so the average color intensity of the whole standard color card is <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mi>I</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>N</mml:mi></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$I=\sum _{i=1}^{N}f(i)/N$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq3.gif"/></alternatives></inline-formula>. Maybe the value of some pixels in the standard color card is too big or too small by the influence of light. These pixel points are not so many but it will affect the overall color intensity values. So, we calculate an accurate color intensity value as follows: 
<list list-type="order"><list-item><p>For every color intensity value <italic>f</italic>(<italic>i</italic>), we judged this condition |<italic>f</italic>(<italic>i</italic>)−<italic>I</italic>|≥<italic>β</italic><italic>I</italic>, where <italic>β</italic> is a coefficient.</p></list-item><list-item><p>If (a) is true, the “white spots” (a pixel point whose color value is too big or too small) plus 1 and the total number of pixels minus 1.</p></list-item><list-item><p>The color intensity values of the white points after scanning complete <italic>f</italic>(<italic>j</italic>)=(<italic>R</italic><sub><italic>j</italic></sub>,<italic>G</italic><sub><italic>j</italic></sub>,<italic>B</italic><sub><italic>j</italic></sub>),<italic>j</italic>=1,2,⋯,<italic>n</italic>, <italic>n</italic> is the sum of the white points, <italic>f</italic>(<italic>i</italic>) the average value of the color intensity of the white point.</p></list-item><list-item><p>The final color intensity <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mi>Ī</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>j</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\bar {I}=\frac {\sum _{i=1}^{N}f(i)-\sum _{j=1}^{n}f(j)}{N-n}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq4.gif"/></alternatives></inline-formula>.</p></list-item></list></p><p>We can infer the model of calculating the average intensity value of the colors in the standard color card by a different light <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>Ī</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(L,\bar {I})$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq5.gif"/></alternatives></inline-formula> and the average value of the color intensity in the standard color card <italic>f</italic>(<italic>I</italic>), so the influence of light is <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>Ī</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$f(L,\bar {I})-f(I)$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq6.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p>The correction of thermal ink</p><p>The size of the thermal ink sensing area is thought as <italic>M</italic> pixels, the value of each pixel point <italic>P</italic><sub><italic>i</italic></sub> is <italic>g</italic>(<italic>i</italic>)=(<italic>R</italic><sub><italic>i</italic></sub>,<italic>G</italic><sub><italic>i</italic></sub>,<italic>B</italic><sub><italic>i</italic></sub>),<italic>i</italic>=1,2,⋯,<italic>M</italic>. So, the average value of the color intensity in the entire sensing area <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mi>g</mml:mi><mml:mo>(</mml:mo><mml:mi>i</mml:mi><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mi>M</mml:mi></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$R=\sum _{i=1}^{M}g(i)/M$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq7.gif"/></alternatives></inline-formula>, which is the color intensity value of the thermal ink region affected by light. We know that an essential color change of the thermal ink is a temperature change of environment. Thermal ink features non-correspondent value of color intensity and temperature, which is not a linear relation. When temperature changes in a specific period, the value of color intensity does not increase linearly, nor increase steadily, so it is necessary to identify a functional relation between temperature and the value of color intensity. In general, <italic>T</italic>=<italic>f</italic>(<italic>R</italic>) can stand for the relation between temperature and the value of color intensity, but it is an exceptional. Figure <xref rid="Fig5" ref-type="fig">5</xref> shows that the curve is not a straight line but segmentations.
<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>A char of function <italic>T</italic>=<italic>f</italic>(<italic>R</italic>)</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig5_HTML.gif" id="MO5"/></fig></p><p>It indicates that the change of color intensity and temperature is not the same. The function <italic>T</italic>=<italic>f</italic>(<italic>R</italic>) is adjusted as below: 
<disp-formula id="Equ14"><label>14</label><alternatives><mml:math id="Equ14_Math"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="" open="{" separators=""><mml:mrow><mml:mtable class="array" columnalign="left"><mml:mtr><mml:mtd><mml:mi>∂</mml:mi><mml:mn>1</mml:mn><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>∂</mml:mi><mml:mn>2</mml:mn><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mo>,</mml:mo><mml:mspace width="1em"/></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>∂</mml:mi><mml:mn>3</mml:mn><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo><mml:mi>.</mml:mi><mml:mspace width="1em"/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><tex-math id="Equ14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document} $$ T=\begin{cases} \partial 1 f(R), \\ \partial 2 f(R), \\ \partial 3 f(R). \end{cases}  $$ \end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_Equ14.gif"/></alternatives></disp-formula></p><p><italic>∂</italic>1 is a relation coefficient of the temperature and the color intensity value from temperature 0 to <italic>T</italic><sub><italic>i</italic></sub>, so <italic>∂</italic>2 from temperature <italic>T</italic><sub><italic>i</italic></sub> to <italic>T</italic><sub><italic>n</italic>−<italic>i</italic></sub> and <italic>∂</italic>3 from temperature <italic>T</italic><sub><italic>n</italic>−<italic>i</italic></sub> to <italic>T</italic><sub><italic>n</italic></sub>.</p><p>The three coefficients should be obtained via experimental data. Since the value of color intensity is easily affected by temperature and light, the relationship of the three can be denoted as <italic>R</italic>=<italic>S</italic>(<italic>T</italic>,<italic>L</italic>), where <italic>T</italic> is symbolic of temperature, <italic>R</italic> typical value of color, and <italic>L</italic> the effect of light.</p></list-item><list-item><p>The calculation of temperature area</p><p>In order to calculate the value of sensing area, firstly, it should be borne in mind that standard color card and thermal ink are easily affected by light and temperature. The effect of light on standard color card and sensing area is the same as well as the few influences of temperature on a standard color card and, finally, the typical value of color intensity in the sensing area at a given temperature <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>,</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>,</mml:mo><mml:mi>Ī</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mi>I</mml:mi><mml:mo>)</mml:mo><mml:mo>)</mml:mo><mml:mo>)</mml:mo></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$(S(T,L)-(f(L,\bar {I})-f(I)))$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13640_2018_258_Article_IEq8.gif"/></alternatives></inline-formula>.</p></list-item></list></p></sec></sec><sec id="Sec19"><title>Results and discussion</title><sec id="Sec20"><title>The result of color similarity segmentation</title><p>In this experiment, we choose Microsoft visual studio 2010 platform and a software library Opencv (Open Source Computer Vision Library) for image segmentation. Microsoft visual studio 2010 is a development platform with a powerful function, which can improve the working efficiency and flexibility of the programmer and support for multiple development application. Opencv is a computer vision library based on open source issue of the cross-platform and run on several operating systems of Linux, Windows, and Mac OS. It is lightweight and has high efficiency that is composed of a series of C function and a small amount of C + + class structure. It also provides an interface for Python, Ruby, and MATLAB language to implement many generic algorithms of image processing and computer vision. We choose several color images to do an experiment, pictures a–d, the experimental results as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.
<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p><bold>a</bold>–<bold>d</bold> Fire segmentation results (from left to right is the original image, the foreground, and the background) will have a corresponding change; it can be easy for recognition and segmentation, so it can be used for fire protection systems in real-time analysis</p></caption><graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13640_2018_258_Fig6_HTML.gif" id="MO6"/></fig></p></sec><sec id="Sec21"><title>The results of fire segmentation</title><p>In this experiment, we simulate an indoor fire happening, and the camera to capture the video then have a series of processing. We choose several images in the same scene to do the experiment, pictures <xref rid="Fig6" ref-type="fig">a</xref>–<xref rid="Fig6" ref-type="fig">d</xref>, the experimental results as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.</p><p>Video image segmentation results (from left to right is original image, the foreground and the background), the background is almost the same. From the experimental results, these four sets of images can well separate foreground and background in accordance with the proposed algorithm. In Fig. <xref rid="Fig6" ref-type="fig">6</xref><xref rid="Fig6" ref-type="fig">a</xref>, <xref rid="Fig6" ref-type="fig">b</xref>, the foreground is smoke. The only difference is the area of the smoke is constantly expanding. Though in (<xref rid="Fig6" ref-type="fig">a</xref>) and (<xref rid="Fig6" ref-type="fig">b</xref>), a small portion of background is divided into the foreground, it does not affect the segmentation results. In Fig. <xref rid="Fig6" ref-type="fig">6</xref><xref rid="Fig6" ref-type="fig">c</xref>, <xref rid="Fig6" ref-type="fig">d</xref>, where the foreground is smoke and flame, it is easy to see the area of fire and smoke as both are changing from (<xref rid="Fig6" ref-type="fig">c</xref>) to (<xref rid="Fig6" ref-type="fig">d</xref>), the flame area was expanded and the smoke area was reduced. In Fig. <xref rid="Fig6" ref-type="fig">6</xref><xref rid="Fig6" ref-type="fig">d</xref>, the background is similar to the flame. But the flame is divided into the foreground well.</p></sec></sec><sec id="Sec22" sec-type="conclusion"><title>Conclusions</title><p>This work presents a new color image segmentation algorithm based on color similarity in real-time color image segmentation for cyber physical systems. We firstly determine the dominant color. And then, we use a mathematical model called <italic>SIMILATION</italic>, which takes the hue and the brightness into account at the same time to calculate the color similarity in the RGB color space. After that, we combine the proposed methods of calculation of the image color components to form a color map. Furthermore, pixels are divided based on color map and the segmentation is completed. Besides, this paper also discuss its application in fire detection and propose a new method in identify fire in video based on these characteristics. First, analyze the characteristics of colors of the fire regions and extract the potential fire regions. Then, analyze its features such as fire flame color, spreading area, the similarity change, and fire smoke. They are very commonly observed in fire; so, that achieves automatic feature extraction. The results were accurate and can be used in real-time analysis. The experimental results show that the proposed method has better robustness for the brightness variations and lower computational complexity in real-time systems. However, there is a lack in this algorithm; for instance, some section belonging to the background may be divided into the foreground region. Therefore, we should use other methods to deal with a certain type of video image for accurate segmentation.</p><p>In this paper, we have described the characteristics of thermal ink and fire, then do some experiments. Although the real-time system has not been coming out, we can use thermal ink characteristics and image processing technology to identify the temperature by the method in this paper and use fire characteristics and video processing technology to identify the fire. It can be applied in food temperature control and tag identification, fire detection, and identification system, etc.</p></sec></body><back><ack><title>Acknowledgements</title><sec><p>The authors would like to appreciate all anonymous reviewers for their insightful comments and constructive suggestions to polish this paper in high quality. This research was supported by Shanghai Universities Distinguished Professor Foundation (Eastern scholar) in 2014 with Project number 10-15-302-014 and the Youth Science Foundation of Jiangxi Province: Dependable and Automatic Resource Management Research in Cloud computing Networks (ID: 20122BAB211022).</p></sec><sec><title>Abbreviations</title><p>Not applicable.</p></sec><sec><title>Funding</title><p>Not applicable.</p></sec><sec><title>Availability of data and materials</title><p>Not applicable.</p></sec></ack><sec sec-type="author-contribution"><title>Authors’ contributions</title><p>For this article with several authors, CW and NX conceived and designed the experiments and the theory analysis. YS performed the experiments. KY analyzed the data. CL contributed reagents, materials, and analysis tools. NX wrote the paper. All authors agree with the above contribution details. All authors read and approved the final manuscript.</p></sec><sec sec-type="ethics-statement"><sec><title>Ethics approval and consent to participate</title><p>Not applicable.</p></sec><sec><title>Consent for publication</title><p>Not applicable.</p></sec><sec sec-type="COI-statement"><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec><sec><title>Publisher’s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pal</surname><given-names>NR</given-names></name><name><surname>Pal</surname><given-names>SK</given-names></name></person-group><article-title xml:lang="en">A review on image segmentation techniques</article-title><source>Pattern Recog</source><year>1993</year><volume>26</volume><issue>9</issue><fpage>1277</fpage><lpage>1294</lpage><pub-id pub-id-type="doi">10.1016/0031-3203(93)90135-J</pub-id></mixed-citation></ref><ref id="CR2"><label>2</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shapiro</surname><given-names>VA</given-names></name><name><surname>Veleva</surname><given-names>PK</given-names></name><name><surname>Sgurev</surname><given-names>VS</given-names></name></person-group><article-title xml:lang="en">An adaptive method for image thresholding.</article-title><source>Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol. III. Conference C: Image, Speech and Signal Analysis</source><year>1992</year><publisher-loc>The Hague</publisher-loc><publisher-name>IEEE</publisher-name><fpage>696</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1109/ICPR.1992.202082</pub-id></mixed-citation></ref><ref id="CR3"><label>3</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luong</surname><given-names>QT</given-names></name><collab>in eds. by</collab><name><surname>Chen</surname><given-names>CH</given-names></name><name><surname>Pau</surname><given-names>LF</given-names></name><name><surname>Wang</surname><given-names>PS</given-names></name></person-group><source>Color in Computer Vision</source><year>1993</year><publisher-loc>Singapore</publisher-loc><publisher-name>World Scientific</publisher-name><pub-id pub-id-type="doi">10.1142/9789814343138_0012</pub-id></mixed-citation></ref><ref id="CR4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trémeau</surname><given-names>A</given-names></name><name><surname>Tominaga</surname><given-names>S</given-names></name><name><surname>Plataniotis</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Color in image and video processing: most recent trends and future research directions</article-title><source>EURASIP J. Image Video Process</source><year>2008</year><volume>2008</volume><issue>1</issue><fpage>581371</fpage></mixed-citation></ref><ref id="CR5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>K</given-names></name><name><surname>Wu</surname><given-names>LJ</given-names></name><name><surname>Xu</surname><given-names>LH</given-names></name></person-group><article-title xml:lang="en">A survey on color image segmentation techniques</article-title><source>J. Image Graph.</source><year>2005</year><volume>10</volume><fpage>1</fpage><lpage>10</lpage></mixed-citation></ref><ref id="CR6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>A</given-names></name><name><surname>Aloimonos</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Active segmentation</article-title><source>Int. J. HR.</source><year>2009</year><volume>6</volume><issue>3</issue><fpage>361</fpage><lpage>386</lpage></mixed-citation></ref><ref id="CR7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>CH</given-names></name><name><surname>Chen</surname><given-names>CC</given-names></name></person-group><article-title xml:lang="en">Image segmentation based on edge detection and region growing for thinprep-cervical smear</article-title><source>Int. J. Pattern Recognit. Artif. Intell.</source><year>2010</year><volume>24</volume><issue>7</issue><fpage>1061</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1142/S0218001410008305</pub-id></mixed-citation></ref><ref id="CR8"><label>8</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chaira</surname><given-names>T</given-names></name><name><surname>Ray</surname><given-names>AK</given-names></name><name><surname>Salvetti</surname><given-names>O</given-names></name></person-group><article-title xml:lang="en">Intuitionistic fuzzy c means clustering in medical image segmentation</article-title><source>Proceedings of the Sixth International Conference on Advances in Pattern Recognition</source><year>2007</year><publisher-loc>Kolkata</publisher-loc><publisher-name>Springer Singapore</publisher-name><fpage>226</fpage><lpage>230</lpage></mixed-citation></ref><ref id="CR9"><label>9</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jay</surname><given-names>S</given-names></name><name><surname>Schmugge</surname><given-names>S</given-names></name><name><surname>Shin</surname><given-names>MC</given-names></name></person-group><article-title xml:lang="en">Effect of colorspace transformation, the illuminance component, and color modeling on skin detection</article-title><source>Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source><year>2004</year><publisher-loc>Washington</publisher-loc><publisher-name>IEEE</publisher-name><fpage>813</fpage><lpage>818</lpage></mixed-citation></ref><ref id="CR10"><label>10</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sang</surname><given-names>DT</given-names></name><name><surname>Woo</surname><given-names>DM</given-names></name><name><surname>Park</surname><given-names>DC</given-names></name><collab>in Eds. by</collab><name><surname>Lei</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>FL</given-names></name><name><surname>Deng</surname><given-names>H</given-names></name><name><surname>Miao</surname><given-names>D</given-names></name></person-group><source>Color Image Segmentation Using Centroid Neural Network</source><year>2012</year><publisher-loc>Germany</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-33478-8_73</pub-id></mixed-citation></ref><ref id="CR11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oestreich</surname><given-names>JW</given-names></name><name><surname>Tolley</surname><given-names>WK</given-names></name><name><surname>Rice</surname><given-names>DA</given-names></name></person-group><article-title xml:lang="en">The development of a color sensor system to measure mineral compositions</article-title><source>Miner. Eng.</source><year>1995</year><volume>8</volume><fpage>31</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/0892-6875(94)00100-Q</pub-id></mixed-citation></ref><ref id="CR12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">Research development of color sensor technique</article-title><source>J. Transducer Technol.</source><year>2003</year><volume>22</volume><fpage>1</fpage><lpage>4</lpage></mixed-citation></ref><ref id="CR13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Development of a new double-path color sensor based on TCS230</article-title><source>Appl. Electron. Tech.</source><year>2007</year><volume>8</volume><fpage>89</fpage><lpage>91</lpage></mixed-citation></ref><ref id="CR14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mignotte</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Segmentation by fusion of histogram-based k-means clusters in different color spaces</article-title><source>IEEE Trans. Image Process.</source><year>2008</year><volume>17</volume><fpage>780</fpage><lpage>787</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">2516602</pub-id><pub-id pub-id-type="doi">10.1109/TIP.2008.920761</pub-id></mixed-citation></ref><ref id="CR15"><label>15</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Ren</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name></person-group><source>A Color Image Segmentation Algorithm by Integrating Watershed with Region Merging</source><year>2012</year><publisher-loc>Germany</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-31900-6_22</pub-id></mixed-citation></ref><ref id="CR16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">A novel license plate location method on rgb color space. journal of image and graphics</article-title><source>J. Image Graph.</source><year>2010</year><volume>11</volume><fpage>1623</fpage><lpage>1628</lpage></mixed-citation></ref><ref id="CR17"><label>17</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chapron</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">A new chromatic edge detector used for color image segmentation</article-title><source>Proceedings of 11th IAPR International Conference on Pattern Recognition</source><year>1992</year><publisher-loc>Los Alamitos</publisher-loc><publisher-name>IEEE</publisher-name><fpage>311</fpage><lpage>314</lpage></mixed-citation></ref><ref id="CR18"><label>18</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Quan</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Color image background difference based on his model</article-title><source>J. Comput. Appl.</source><year>2009</year><volume>S1</volume><fpage>231</fpage><lpage>232235</lpage></mixed-citation></ref><ref id="CR19"><label>19</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Zhan</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">A segmentation method of color texture image</article-title><source>Chin. J. Comput.</source><year>2001</year><volume>9</volume><fpage>965</fpage><lpage>971</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">1868517</pub-id></mixed-citation></ref><ref id="CR20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gary</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">The opencv library</article-title><source>Dr. Dobb’s J. Softw. Tools Prof. Programmer</source><year>2000</year><volume>25</volume><fpage>120</fpage><lpage>123</lpage></mixed-citation></ref><ref id="CR21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pulli</surname><given-names>K</given-names></name><name><surname>Baksheev</surname><given-names>A</given-names></name><name><surname>Kornyakov</surname><given-names>K</given-names></name><name><surname>Kornyakov</surname><given-names>K</given-names></name><name><surname>Eruhimov</surname><given-names>V</given-names></name></person-group><article-title xml:lang="en">Real-time computer vision with opencv</article-title><source>Commun. ACM</source><year>2012</year><volume>55</volume><fpage>61</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1145/2184319.2184337</pub-id></mixed-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Phillips</surname><given-names>W</given-names></name><name><surname>Shah</surname><given-names>M</given-names></name><name><surname>Da Vitoria Lobo</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Flame recognition in video</article-title><source>Pattern Recogn. Lett.</source><year>2002</year><volume>23</volume><fpage>319</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/S0167-8655(01)00135-0</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0986.68875</pub-id></mixed-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>G</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Cao</surname><given-names>D</given-names></name><name><surname>Bao</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Flame recognition based on video image</article-title><source>Appl. Mech. Mater.</source><year>2014</year><volume>687-691</volume><fpage>3604</fpage><lpage>3607</lpage><pub-id pub-id-type="doi">10.4028/www.scientific.net/AMM.687-691.3604</pub-id></mixed-citation></ref><ref id="CR24"><label>24</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Y</given-names></name><name><surname>Chellappa</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Tracking a dynamic set of feature points</article-title><source>IEEE Trans. Image Process.</source><year>1995</year><volume>4</volume><fpage>1382</fpage><lpage>1395</lpage><pub-id pub-id-type="doi">10.1109/83.465103</pub-id></mixed-citation></ref><ref id="CR25"><label>25</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>Z</given-names></name><name><surname>Chou</surname><given-names>W</given-names></name><name><surname>zhong</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>CH</given-names></name></person-group><article-title xml:lang="en">Video segmentation using spatial and temporal statistical analysis method</article-title><source>Proceedings of 2000 IEEE International Conference on Multimedia and Expo</source><year>2000</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>1527</fpage><lpage>1530</lpage></mixed-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>G</given-names></name><name><surname>Salo</surname><given-names>M</given-names></name><name><surname>Rahtu</surname><given-names>E</given-names></name><name><surname>Pietikainen</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Automatic dynamic texture segmentation using local descriptors and optical flow</article-title><source>IEEE Trans. Image Process.</source><year>2012</year><volume>22</volume><fpage>326</fpage><lpage>339</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3017467</pub-id><pub-id pub-id-type="doi">10.1109/TIP.2012.2210234</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1373.94074</pub-id></mixed-citation></ref><ref id="CR27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobias</surname><given-names>OJ</given-names></name><name><surname>Seara</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Image segmentation by histogram threshing using fuzzy sets</article-title><source>IEEE Trans. Image Process.</source><year>2002</year><volume>11</volume><fpage>1457</fpage><lpage>1465</lpage><pub-id pub-id-type="doi">10.1109/TIP.2002.806231</pub-id></mixed-citation></ref><ref id="CR28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Traffic sign segment based on RGB vision model</article-title><source>Microelectron. Comput.</source><year>2004</year><volume>10</volume><fpage>147</fpage><lpage>148152</lpage></mixed-citation></ref><ref id="CR29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buluswar</surname><given-names>SD</given-names></name><name><surname>Draper</surname><given-names>BA</given-names></name></person-group><article-title xml:lang="en">Color machine vision for autonomous vehicles</article-title><source>Eng. Appl. Artif. Intell.</source><year>1998</year><volume>11</volume><fpage>245</fpage><lpage>256</lpage><pub-id pub-id-type="doi">10.1016/S0952-1976(97)00079-1</pub-id></mixed-citation></ref><ref id="CR30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stiebig</surname><given-names>H</given-names></name><name><surname>Knipp</surname><given-names>D</given-names></name><name><surname>Hapke</surname><given-names>P</given-names></name><name><surname>Finger</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Three color piiin-detector using microcrystalline silicon</article-title><source>J. Non-Cryst. Solids.</source><year>1998</year><volume>227-230</volume><fpage>1330</fpage><lpage>1334</lpage><pub-id pub-id-type="doi">10.1016/S0022-3093(98)00355-X</pub-id></mixed-citation></ref><ref id="CR31"><label>31</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>S</given-names></name></person-group><source>Colorimetry</source><year>1990</year><publisher-loc>Beijing</publisher-loc><publisher-name>Beijing Institute of Technology Press</publisher-name></mixed-citation></ref><ref id="CR32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Hao</surname><given-names>C</given-names></name><name><surname>Lei</surname><given-names>F</given-names></name><name><surname>Fan</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Automatic illuminations detection and color correction of image using chromatic histogram characters</article-title><source>J. Image Graph.</source><year>2003</year><volume>9</volume><fpage>1001</fpage><lpage>1007</lpage></mixed-citation></ref><ref id="CR33"><label>33</label><mixed-citation publication-type="other">Z Li, Color reconstruction theory and practice in color image. Master’s thesis, Wuhan University, Wuhan, China (2005).</mixed-citation></ref><ref id="CR34"><label>34</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gasparini</surname><given-names>F</given-names></name><name><surname>Schettini</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Color correction for digital photographs</article-title><source>Proceedings of 12th International Conference on Image Analysis and Processing</source><year>2003</year><publisher-loc>Mantova</publisher-loc><publisher-name>IEEE</publisher-name><fpage>646</fpage><lpage>651</lpage></mixed-citation></ref><ref id="CR35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gasparini</surname><given-names>F</given-names></name><name><surname>Schettini</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Color balancing of digital photos using simple image statistics</article-title><source>Pattern Recognit.</source><year>2004</year><volume>37</volume><fpage>1201</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2003.12.007</pub-id></mixed-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luz</surname><given-names>A</given-names></name><name><surname>Mendez</surname><given-names>T</given-names></name><name><surname>Dudek</surname><given-names>G</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Rangarajan</surname><given-names>A</given-names></name><name><surname>Vemuri</surname><given-names>B</given-names></name><name><surname>Yuille</surname><given-names>AL</given-names></name></person-group><article-title xml:lang="en">Color correction of underwater images for aquatic robot inspection eds. by</article-title><source>Energy Minimization Methods in Computer Vision and Pattern Recognition</source><year>2005</year><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><fpage>60</fpage><lpage>73</lpage></mixed-citation></ref><ref id="CR37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doutre</surname><given-names>C</given-names></name><name><surname>Nasiopoulos</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Color correction preprocessing for multiview video coding</article-title><source>IEEE Trans. Circ. Syst. Video Technol.</source><year>2009</year><volume>19</volume><issue>9</issue><fpage>1400</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2009.2022780</pub-id></mixed-citation></ref><ref id="CR38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rizzi</surname><given-names>A</given-names></name><name><surname>Gatta</surname><given-names>C</given-names></name><name><surname>Marini</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">A new algorithm for unsupervised global and local color correction</article-title><source>Pattern Recogn. Lett.</source><year>2003</year><volume>24</volume><issue>11</issue><fpage>1663</fpage><lpage>1677</lpage><pub-id pub-id-type="doi">10.1016/S0167-8655(02)00323-9</pub-id></mixed-citation></ref><ref id="CR39"><label>39</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoon</surname><given-names>I</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Hayes</surname><given-names>MH</given-names></name><name><surname>Paik</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Adaptive defogging with color correction in the HSV color space for consumer surveillance system</article-title><source>IEEE Trans. Consum. Electron.</source><year>2012</year><volume>58</volume><issue>1</issue><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1109/TCE.2012.6170062</pub-id></mixed-citation></ref><ref id="CR40"><label>40</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Color image segmentation based on color similarity.</article-title><source>Proceedings of 2009 International Conference on Computational Intelligence and Software Engineering</source><year>2009</year><publisher-loc>Wuhan</publisher-loc><publisher-name>IEEE</publisher-name><fpage>1</fpage><lpage>4</lpage></mixed-citation></ref><ref id="CR41"><label>41</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gong</surname><given-names>H</given-names></name><name><surname>Hao</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Robert edge detection algorithm based on GPU</article-title><source>J. Chem. Pharm. Res.</source><year>2014</year><volume>6</volume><fpage>1308</fpage><lpage>1314</lpage></mixed-citation></ref></ref-list></ref-list></back></article></records><facets><facet name="subject"><facet-value count="1">Biometrics</facet-value><facet-value count="1">Engineering</facet-value><facet-value count="1">Image Processing and Computer Vision</facet-value><facet-value count="1">Pattern Recognition</facet-value><facet-value count="1">Signal,Image and Speech Processing</facet-value></facet><facet name="keyword"><facet-value count="1">Color sensor</facet-value><facet-value count="1">Color similarity</facet-value><facet-value count="1">Cyber physical system</facet-value><facet-value count="1">Fire detection and identification</facet-value><facet-value count="1">Real-time systems</facet-value><facet-value count="1">Thermal ink</facet-value><facet-value count="1">Video image segmentation</facet-value></facet><facet name="pub"><facet-value count="1">EURASIP Journal on Image and Video Processing</facet-value></facet><facet name="year"><facet-value count="1">2018</facet-value></facet><facet name="country"><facet-value count="1">China</facet-value><facet-value count="1">South Korea</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
