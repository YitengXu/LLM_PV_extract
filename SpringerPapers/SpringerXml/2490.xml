<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-021-00564-y</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-021-00564-y</article-id><article-id pub-id-type="manuscript">564</article-id><article-id pub-id-type="doi">10.1038/s41524-021-00564-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/299/2736</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Predicting thermoelectric properties from chemical formula with explicitly identifying dopant effects</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9803-0782</contrib-id><name><surname>Na</surname><given-names>Gyoung S.</given-names></name><address><email>ngs0@krict.re.kr</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs4152402100564y_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Jang</surname><given-names>Seunghun</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7241-5342</contrib-id><name><surname>Chang</surname><given-names>Hyunju</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.29869.3c</institution-id><institution-id institution-id-type="ISNI">0000 0001 2296 8192</institution-id><institution content-type="org-name">Korea Research Institute of Chemical Technology</institution></institution-wrap><addr-line content-type="city">Daejeon</addr-line><country country="KR">South Korea</country></aff></contrib-group><author-notes><corresp id="IDs4152402100564y_cor1"><label>a</label><email>ngs0@krict.re.kr</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>14</day><month>7</month><year>2021</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2021</year></pub-date><volume>7</volume><issue seq="106">1</issue><elocation-id>106</elocation-id><history><date date-type="registration"><day>2</day><month>6</month><year>2021</year></date><date date-type="received"><day>18</day><month>3</month><year>2021</year></date><date date-type="accepted"><day>28</day><month>5</month><year>2021</year></date><date date-type="online"><day>14</day><month>7</month><year>2021</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2021</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Dopants play an important role in synthesizing materials to improve target materials properties or stabilize the materials. In particular, the dopants are essential to improve thermoelectic performances of the materials. However, existing machine learning methods cannot accurately predict the materials properties of doped materials due to severely nonlinear relations with their materials properties. Here, we propose a unified architecture of neural networks, called DopNet, to accurately predict the materials properties of the doped materials. DopNet identifies the effects of the dopants by explicitly and independently embedding the host materials and the dopants. In our evaluations, DopNet outperformed existing machine learning methods in predicting experimentally measured thermoelectric properties, and the error of DopNet in predicting a figure of merit (ZT) was 0.06 in mean absolute error. In particular, DopNet was significantly effective in an extrapolation problem that predicts ZTs of unknown materials, which is a key task to discover novel thermoelectric materials.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>Korea Research Institute of Chemical Technology (KRICT)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100003704</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">SI2151-10</award-id><award-id award-type="FundRef grant">SI2151-10</award-id><award-id award-type="FundRef grant">SI2151-10</award-id><principal-award-recipient><name><surname>Na</surname><given-names>Gyoung S.</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Jang</surname><given-names>Seunghun</given-names></name></principal-award-recipient><principal-award-recipient><name><surname>Chang</surname><given-names>Hyunju</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>106</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>2</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2021_Article_564.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">In physical science, various calculation methods to predict materials properties have been developed because the materials properties determine applications of materials<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>. However, extensive computation costs of the calculation methods frequently limit the applicability of them in practical applications<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. In particular, the conventional calculation methods are not applicable to the doped materials due to impractical computation costs caused by large cells of the doped materials<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. For this reason, most experiments to discover novel materials of desired thermoelectric properties have been conducted relying on the intuition of domain experts.</p><p id="Par3">With the rapidly growing public materials databases, machine learning began to be studied widely in physical science to efficiently predict the materials properties<sup><xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. In the early stage of materials machine learning, the materials were described as vector-shaped representations based on global characteristics of the materials or statistical information from atomic attributes. Then, conventional machine learning methods (e.g., Gaussian process regression<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>) were applied to predict materials properties based on these vector-shaped representations<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Recently, advanced machine learning methods that explore structural information of input data, as well as input features, have been studied in physical science to fully utilize structural information from the crystal structures. In particular, graph neural networks (GNNs)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> have been successfully applied to various scientific applications of physical science because the crystal structure is natively represented as a mathematical graph. In various chemical and physical applications, GNNs have achieved state-of-the-art performances beyond the conventional machine learning methods<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR13">13</xref>,<xref ref-type="bibr" rid="CR14">14</xref></sup>.</p><p id="Par4">Predicting the materials properties of the doped materials is the next challenge of machine learning in physical science. The doped materials are popular in real-world applications due to their superior performances and stability<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>. In particular, the doped materials are dominant in the thermoelectric materials due to their superior thermoelectric performances<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>. However, although machine learning has been successfully applied to various scientific applications in physical science, existing machine learning methods are not effective to predict the materials properties of the doped materials. There are three problems in predicting materials properties of the doped materials based on machine learning:<list list-type="bullet"><list-item><p id="Par5">Lack of information: the crystal structures of the doped materials are not available in most cases because impractical computation costs are required to calculate the crystal structures of the doped materials.</p></list-item><list-item><p id="Par6">Dopant effect vanishing: the chemical formula-based materials representations cannot precisely describe the effects of the dopants in the doped materials because numerical changes by the dopants are tiny in the materials representations due to small proportions of the dopants.</p></list-item><list-item><p id="Par7">Severely nonlinear relations: relations between the doped materials and their materials properties are severely nonlinear because the dopants sometimes dramatically change the materials properties of the host materials.</p></list-item></list>Therefore, to accurately predict the materials properties of the doped materials, we need a machine learning method to effectively approximate severely nonlinear functions from the chemical formulas with identifying the dopant effects.</p><p id="Par8">In this paper, we propose a unified architecture of neural networks, called DopNet, to accurately predict the materials properties of the doped materials. DopNet utilizes the chemical formula of the materials to predict materials properties without the crystal structures. To overcome the problems from the dopant effect vanishing and severely nonlinear relations, DopNet explicitly describes the host materials and the dopants. In Discussion Section, we will show that the doped materials can be clearly identified according to their materials properties by explicitly embedding the host materials and the dopants. Another benefit of DopNet is that it does not require additional information about the materials other than the chemical formulas of them. Hence, DopNet can be universally applied to both experimental and calculation materials databases.</p><p id="Par9">In our evaluations on a real-world materials dataset, DopNet achieved state-of-the-art accuracies in predicting experimentally measured thermoelectric properties of 573 different materials and conditions. Specifically, the prediction error of DopNet in predicting a figure of merit (ZT) was 0.06 in mean absolute error (MAE). In particular, DopNet was significantly more effective than the most popular gradient boosting tree regression (GBTR)<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> in predicting ZTs of completely unseen materials. The errors of GBTR and DopNet in predicting ZTs of these unseen materials were 0.41 and 0.13, respectively, and the performance improvement by DopNet is 68.29%. The significant improvement of DopNet in this extrapolation problem is noteworthy because an accurate prediction for unseen materials is a key task of inverse design. Although we focused on the prediction of the thermoelectric properties in this paper, DopNet can be generally applied to predict any materials properties of the doped materials. For the future works of machine learning in materials science, we publicly open the source code of DopNet at <ext-link xlink:href="https://github.com/ngs00/DopNet" ext-link-type="uri">https://github.com/ngs00/DopNet</ext-link>.</p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Doped materials in regression of materials property</title><p id="Par10">Usually, only a small amount of these dopants are added to the host materials, but the materials properties can be changed drastically<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup>. Hence, the doped material have severely nonlinear relations with their materials properties in local areas of materials space as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. For instance, the changes from (b) Mg<sub>2</sub>Si<sub>0.999</sub>Bi<sub>0.001</sub> to (a) Mg<sub>2</sub>Si<sub>0.9985</sub>Bi<sub>0.0015</sub> is tiny in the entire materials space, but ZT was significantly improved from 0.32 to 0.64<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Also, ZTs of (c)-(e) fluctuated from 0.54 to 1.45<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. However, existing machine learning methods are not suitable to approximate these severely nonlinear relations<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Although some machine learning methods to approximate the severely nonlinear functions were proposed<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, they require large-scale training datasets, which are impractical in experimental materials databases.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Mixed distribution of thermoelectirc materials and their ZTs.</title><p><bold>a</bold> Conceptual materials space proportional to ZT at 700 K. Five example doped materials denoted by (a)–(e) are presented to describe severely nonlinear relations between the doped materials and ZT. (a) Mg<sub>2</sub>Si<sub>0.9985</sub>Bi<sub>0.0015</sub> (ZT = 0.64); (b) Mg<sub>2</sub>Si<sub>0.999</sub>Bi<sub>0.001</sub> (ZT = 0.32); (c) Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn (ZT = 0.54). (d) Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn<sub>1.9998</sub>Sb<sub>0.002</sub> (ZT = 1.45); (e) Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn<sub>1.9994</sub>Sb<sub>0.006</sub> (ZT = 1.34). <bold>b</bold> Distribution of doped materials and their ZTs at 700 K in real-world dataset<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Each point indicate a material, and they were visualized in 2-dimensional space by t-SNE<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. For the five example material, ZTs at 700 K are presented in parentheses.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig1_HTML.png"/></fig></p><p id="Par11">This severely nonlinear relations between the thermoelectric materials and their ZTs are observed in real-world datasets. Figure <xref rid="Fig1" ref-type="fig">1</xref>b shows distribution of thermoelectric materials collected from a real-world database<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Each point is a material, and the colors of the points indicate the values of ZTs at 700 K. For the five example material, their ZTs at 700 K are presented in parentheses. The materials were visualized in 2-dimensional space by t-SNE<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. As shown in the figure, the doped materials are highly mixed in terms of their ZTs. As a result, this mixed distribution forms a severely nonlinear relation between the doped materials and their ZTs. It is consistent with our common sense in the conceptual materials space of Fig. <xref rid="Fig1" ref-type="fig">1</xref>a. Specifically, Mg<sub>2</sub>Si<sub>0.6</sub>Ge<sub>0.4</sub>Bi<sub>0.02</sub> and Mg<sub>2</sub>Si<sub>0.6</sub>Ge<sub>0.4</sub>Ag<sub>0.02</sub> are distributed almost the same region despite their completely different ZTs. That is, the effect of the dopants Bi<sub>0.02</sub> and Ag<sub>0.02</sub> are not identified. The similar problem was observed in the example materials Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn, Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn<sub>1.994</sub>Sb<sub>0.006</sub>, and Zr<sub>0.15</sub>Hf<sub>0.15</sub>Ti<sub>0.7</sub>NiSn. In the next section, we propose a neural network for the accurate prediction of thermoelectric properties by explicitly identifying the dopant effects in the doped materials.</p></sec><sec id="Sec4"><title>Architecture of DopNet</title><p id="Par12">In the existing machine learning methods, to predict materials properties from the chemical formulas, the materials are represented based on the statistical information from the elemental attributes of the atoms in the materials regardless of identifying the dopants<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. However, the atoms in the host material and the dopants are independently embedded in DopNet. This explicit embedding mechanism of DopNet improves the prediction performance for the doped materials by capturing the dopant effects, which are numerically tiny in the materials representations.</p><p id="Par13">DopNet consists of three parts: (1) host embedding networks to extract latent embeddings representing the host materials, (2) dopant embedding networks to generate latent embeddings of the dopants, and (3) dense network to predict target materials property from the embeddings of the host materials and the dopants. Figure <xref rid="Fig2" ref-type="fig">2</xref> illustrates the architecture and forward step of DopNet to predict target materials property <bold>y</bold> from the input chemical formula through four steps:<list list-type="bullet"><list-item><p id="Par14">The input chemical formula is decomposed into the host material and the dopant(s). Each atom in the material is classified as a dopant when their proportion is less than or equal to <italic>γ</italic>, where <italic>γ</italic> ≥ 0 is a pre-defined hyperparameter of DopNet. For instance, Zr<sub>0.5</sub>Hf<sub>0.5</sub>Sn<sub>1.998</sub>Sb<sub>0.002</sub> is decomposed into a host material Zr<sub>0.5</sub>Hf<sub>0.5</sub>Sn<sub>1.998</sub> and a dopant Sb<sub>0.002</sub> for a given <italic>γ</italic> = 0.1.</p></list-item><list-item><p id="Par15">The host material is described as a vector-shaped representation <bold>x</bold><sub><italic>h</italic></sub> based on statistical information from the elemental attributes of the constituent atoms. For the host feature vector <bold>x</bold><sub><italic>h</italic></sub>, an autoencoder <italic>g</italic><sub><italic>ψ</italic></sub>(<italic>h</italic><sub><italic>ϕ</italic></sub>(<bold>x</bold><sub><italic>h</italic></sub>))<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> is applied to generate a compact latent embedding of the host material. Then, the host embedding <bold>z</bold><sub><italic>h</italic></sub> is calculated by feeding the latent feature vector of the host material into the host embedding network.</p></list-item><list-item><p id="Par16">The feature vectors of the dopants are stored in a set <italic>S</italic><sub><italic>d</italic></sub> that can contain maximum <italic>K</italic> dopants, where the maximum number of dopants <italic>K</italic> is a hyperparameter of DopNet. In doped materials including <italic>M</italic> &lt; <italic>K</italic> dopants, the <italic>K</italic> − <italic>M</italic> undefined dopant feature vectors are set to zero vectors. Then, the dopants are embedded independently of the host material through the dopant embedding networks that share model parameters with each other. After the embedding process, the generated dopant embeddings are concatenated as a single vector representation <bold>z</bold><sub><italic>d</italic></sub>.</p></list-item><list-item><p id="Par17">The target property <bold>y</bold> is predicted through the dense network by feeding the final materials representation <bold>z</bold><sub><italic>h</italic></sub> ⊕ <bold>z</bold><sub><italic>d</italic></sub>.</p></list-item></list>The hyperparameter settings and network configurations of DopNet are provided in the method section, and the implementation details including selected elemental attributes are presented in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Architecture of DopNet and its forward process to predict target materials property <bold>y</bold> from the chemical formula Zr<sub>0.5</sub>Hf<sub>0.5</sub>Sn<sub>1.998</sub>Sb<sub>0.002</sub>.</title><p>The yellow circle and squircle in the network indicates artificial neuron with ReLU activation<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> and ReLU feedforward network, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig2_HTML.png"/></fig></p></sec><sec id="Sec5"><title>Prediction of thermoelectric properties</title><p id="Par18">About 70% of the primary energy is lost in the form of heat during the energy conversion process<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. To utilize the wasted energy, thermoelectric materials that convert heat into electricity by Seebeck effect<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> have been widely studied in physical science<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. The efficiency of the energy conversion process originated from the thermoelectric materials are typically given by a figure of merit (ZT) as:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:mi>Z</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>σ</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ZT=\frac{{S}^{2}\sigma T}{\kappa },$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ1.gif"/></alternatives></disp-formula>where <italic>S</italic> is Seebeck coefficient, <italic>σ</italic> is electrical conductivity, <italic>T</italic> is temperature, and <italic>κ</italic> is thermal conductivity.</p><p id="Par19">Despite the importance of the thermoelectric materials, machine learning to predict thermoelectric performances of the materials was hardly studied in physical science. The main obstacle is the lack of training datasets because simulation and calculation methods are not applicable to estimate the thermoelectric properties of the materials in most cases. However, although there is no dataset for the thermoelectric materials, Materials Research Laboratory (MRL) opened about 500 materials with their thermoelectric properties in their website<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, and we manually collected the chemical formulas of the thermoelectric materials and their thermoelectric properties at the website of MRL to predict the thermoelectric properties of the materials. In this paper, we refer this collected thermoelectric dataset to MRL dataset. This MRL dataset contains 573 thermoelectric materials from various combinations of the host materials and the dopants with several thermoelectric properties measured experimentally at 300 K, 400 K, and 700 K. The collected materials systems in the MRL dataset are summarized in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">2</xref>.</p><p id="Par20">In the evaluations, we predicted five thermoelectric properties: Seebeck coefficient, electrical conductivity, thermal conductivity, power factor, and ZT. For prediction, the chemical formulas of the materials were converted into the vector-shaped materials representations based on elemental attributes of the constituent atoms. The selected elemental attributes and the representation method of the materials are provided in Supplementary Note <xref ref-type="supplementary-material" rid="MOESM1">1</xref>. We compared the prediction performance of DopNet with support vector regression (SVR)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>, Gaussian process regression (GPR)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, gradient boosting tree regression (GBTR)<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, and deep neural network (DNN)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. SVR is effective to prevent overfitting due to its margin-based loss formulation. GPR is widely used in scientific applications due to its extrapolation capabilities<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. GBTR is the most popular method in scientific applications and achieved state-of-the-art performance in various applications<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. The prediction performances were measured by mean absolute error (MAE) and coefficient of determination (<italic>R</italic><sup>2</sup> score)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. All machine learning methods were evaluated with 3-fold cross-validation, and the evaluation was repeated 10 times. We reported the average of the prediction performances measured by the 10 times repetitions of the evaluations.</p><p id="Par21">Table <xref rid="Tab1" ref-type="table">1</xref> summarizes the evaluation results of SVR, GPR, GBTR, DNN, and DopNet on the MRL dataset. For all thermoelectric properties, DopNet showed the best prediction performances, as highlighted by the bold fonts. In particular, DopNet outperformed GBTR that showed state-of-the-art performances in various scientific applications<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>. Furthermore, DopNet achieved <italic>R</italic><sup>2</sup> score of 0.86 ± 0.02 in predicting ZT that determines the thermoelectric capability of the materials, and its prediction error for ZT was 0.06 ± 3.00e−3. Figure <xref rid="Fig3" ref-type="fig">3</xref> shows the prediction results of GPR, GBTR, and DopNet for power factor and ZT. Note that the prediction results of SVR are not presented due to its low <italic>R</italic><sup>2</sup> scores −4.10 ± 0.34 and 0.17 ± 0.01 in predicting power factor and ZT, respectively. Although GPR and GBTR well predicted the power factors of the materials, there are severe outliers in their prediction results as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>a. By contrast, the severe outliers were removed in the prediction results of DopNet. In predicting ZT, GPR and GBTR also well predicted the ZTs of the materials. However, they significantly underestimated the ZTs of the high-ZT materials, as highlighted in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b. By contrast, DopNet roughly predicted the ZTs of the high-ZT materials.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Prediction errors of SVR, GPR, GBTR, DNN, and DopNet.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Prediction Method</p></th><th><p>Seebeck coefficient</p></th><th><p>Electrical conductivity</p></th><th><p>Thermal conductivity</p></th><th><p>Power factor</p></th><th><p>ZT</p></th></tr></thead><tbody><tr><td><p>SVR</p></td><td><p>148.62 ± 1.75</p></td><td><p>1464.15 ± 193.08</p></td><td><p>2.56 ± 0.14</p></td><td><p>2.44e−3 ± 6.05e−5</p></td><td><p>0.16 ± 5.00e−3</p></td></tr><tr><td/><td><p>(−0.03 ± 0.04)</p></td><td><p>(−0.02 ± 0.00)</p></td><td><p>(−0.08 ± 0.01)</p></td><td><p>(−4.10 ± 0.34)</p></td><td><p>(0.17 ± 0.01)</p></td></tr><tr><td><p>GPR</p></td><td><p>148.62 ± 1.75</p></td><td><p>2160.61 ± 107.03</p></td><td><p>2.15 ± 0.06</p></td><td><p>8.63e−4 ± 1.94e−4</p></td><td><p>0.15 ± 6.00e−3</p></td></tr><tr><td/><td><p>(−0.03 ± 0.04)</p></td><td><p>(−0.01 ± 0.01)</p></td><td><p>(0.43 ± 0.04)</p></td><td><p>(−5.98 ± 7.23)</p></td><td><p>(0.47 ± 0.04)</p></td></tr><tr><td><p>GBTR</p></td><td><p>45.40 ± 1.42</p></td><td><p>795.96 ± 233.47</p></td><td><p>1.21 ± 0.09</p></td><td><p>3.05e−4 ± 1.55e−5</p></td><td><p>0.07 ± 3.00e−3</p></td></tr><tr><td/><td><p>(0.80 ± 0.02)</p></td><td><p>(0.57 ± 0.27)</p></td><td><p>(0.55 ± 0.09)</p></td><td><p>(0.74 ± 0.05)</p></td><td><p>(0.78 ± 0.03)</p></td></tr><tr><td><p>DNN</p></td><td><p>56.53 ± 2.51</p></td><td><p>1325.92 ± 197.95</p></td><td><p>1.27 ± 0.06</p></td><td><p>3.69e−4 ± 1.40e−5</p></td><td><p>0.09 ± 0.01</p></td></tr><tr><td/><td><p>(0.74 ± 0.03)</p></td><td><p>(0.03 ± 0.10)</p></td><td><p>(0.53 ± 0.06)</p></td><td><p>(0.69 ± 0.02)</p></td><td><p>(0.77 ± 0.02)</p></td></tr><tr><td><p>DopNet</p></td><td><p><bold>39.46</bold> <bold>±</bold> <bold>1.34</bold></p></td><td><p><bold>763.66</bold> <bold>±</bold> <bold>208.02</bold></p></td><td><p><bold>1.12</bold> <bold>±</bold> <bold>0.09</bold></p></td><td><p><bold>2.75e−4</bold> <bold>±</bold> <bold>1.15e−5</bold></p></td><td><p><bold>0.06</bold> <bold>±</bold> <bold>3.00e−3</bold></p></td></tr><tr><td/><td><p>(<bold>0.86</bold> <bold>±</bold> <bold>0.04</bold>)</p></td><td><p>(<bold>0.64</bold> <bold>±</bold> <bold>0.13</bold>)</p></td><td><p>(<bold>0.61</bold> <bold>±</bold> <bold>0.08</bold>)</p></td><td><p>(<bold>0.79</bold> <bold>±</bold> <bold>0.03</bold>)</p></td><td><p>(<bold>0.86</bold> <bold>±</bold> <bold>0.02</bold>)</p></td></tr></tbody></table><table-wrap-foot><p>The prediction errors measured by MAE are reported with their standard deviations. For each machine learning method, <italic>R</italic><sup>2</sup> scores are presented in the parenthesis below the prediction errors. The measured Seebeck coefficient, electrical conductivity, thermal conductivity, power factor, and ZT are distributed within [−752.00, 1235.00], [6.90e−5, 1.32e+5], [0.20, 48.70], [1.77e−10, 6.73e−3], and [6.76e−8, 1.60], respectively. The best prediction performance was highlighted by the bold font.</p></table-wrap-foot></table-wrap><fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Prediction results of the machine learning methods on the MRL dataset.</title><p><bold>a</bold> Scatter plots of the prediction results for the test materials in predicting power factor. <bold>b</bold> Scatter plots of the prediction results in predicting ZT. X and Y axes are measured and predicted materials properties, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig3_HTML.png"/></fig></p><p id="Par22">In addition to power factor and ZT, we also present the prediction results for the transport properties of the materials, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. GPR completely failed to predict the transport properties of the test materials, even though it roughly predicted ZTs. By contrast, GBTR and DopNet accurately predicted Seebeck coefficients of the materials. In particular, many outliers in the prediction results of GBTR were removed as shown in the prediction results of DopNet. However, GBTR and DopNet showed large prediction errors for the materials of the low electrical conductivities as shown in the yellow areas of Fig. <xref rid="Fig4" ref-type="fig">4</xref>b. This happened because the data with small target values are sometimes omitted in the training of ML algorithms. As future work, we can employ a weighted surrogate loss function to reduce the prediction errors for the materials of the low electrical conductivities. In the prediction results of thermal conductivity, GBTR and DopNet showed severe outliers marked by red circles in Fig. <xref rid="Fig4" ref-type="fig">4</xref>c. These data are Zn<sub>0.9975</sub>Al<sub>0.0025</sub>O at 300 K and 400 K. Experimentally, they have low electrical conductivities less than 3. However, similar materials, such as Zn<sub>0.995</sub>Al<sub>0.005</sub>O and Zn<sub>0.95</sub>Al<sub>0.05</sub>O, have very high electrical conductivities larger than 30. That is, GBTR and DopNet were failed to predict the electrical conductivities of Zn<sub>0.9975</sub>Al<sub>0.0025</sub>O at 300 K and 400 K because GBTR and DopNet were overfitted to Zn<sub>0.995</sub>Al<sub>0.005</sub>O and Zn<sub>0.95</sub>Al<sub>0.05</sub>O. This overfitting problem is common in ML and can be solved by collecting larger training datasets.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Regression results of the ML algorithms in predicting Seebeck coefficient, electrical conductivity, and thermal conductivity.</title><p><bold>a</bold> Scatter plots of the prediction results for the test materials in predicting Seebeck coefficient. <bold>b</bold> Scatter plots of the prediction results with large error regions for electrical conductivity. <bold>c</bold> Scatter plots of the prediction results with large error data for thermal conductivity. The prediction results of electrical conductivity were presented by log scale. Two marked materials (<bold>a</bold>) and (<bold>b</bold>) in the prediction results of thermal conductivity are Zn<sub>0.9975</sub>Al<sub>0.0025</sub>O at 300 K and 400 K, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig4_HTML.png"/></fig></p></sec><sec id="Sec6"><title>Prediction of high-ZT materials</title><p id="Par23">The ultimate goal of machine learning in materials science is to discover a novel material, which is called inverse design. For this purpose, an accurate prediction of the high-ZT materials is important because the goal of the inverse design for the thermoelectric materials is to discover a novel material with high ZT. To evaluate the effectiveness of the machine learning methods in the inverse design, we measured the prediction errors of the machine learning methods in predicting the high-ZT materials. Table <xref rid="Tab3" ref-type="table">2</xref> shows the predicted ZTs of GPR, GBTR, and DopNet for the top 10 high-ZT materials in the MRL dataset. As shown in the table, GPR and GBTR significantly underestimated ZT of the high-ZT materials. The prediction errors of GBTR are 0.12–0.81, and its MAE for the high-ZT materials was 0.45. By contrast, DopNet showed prediction errors lower than 0.5 for all materials, and its MAE for the high-ZT materials was 0.26. Hence, DopNet improved the prediction accuracy for the high-ZT materials by 42.22% compared to GBTR. This significant improvement by DopNet in predicting high ZT is important and has wide impacts because the accurate prediction of the high-ZT materials is a key task in the machine-based inverse design of the materials.</p></sec><sec id="Sec7"><title>ZT prediction of unseen materials from external databases</title><p id="Par24">Since the MRL dataset contains ZTs of the same materials for each temperature, some known materials with ZTs measured at different temperatures can be included in the training dataset. To evaluate the machine learning methods in the extrapolation problem, we measured the prediction accuracies of GBTR and DopNet for completely unseen thermoelectric materials. The test thermoelectric materials for this evaluation were collected in previous literature<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR41">41</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. ZTs of the collected test materials were measured at 700 K. The test thermoelectric materials for the evaluation can be categorized as:<list list-type="bullet"><list-item><p id="Par25">Known combination: the combinations of the host atoms and dopants were already provided in the MRL dataset, but the same doping concentrations were not given in the training.</p></list-item><list-item><p id="Par26">Unknown combination: both the combinations of the atoms and the doping concentrations are completely unseen in the MRL dataset.</p></list-item></list></p><p id="Par27">Table <xref rid="Tab2" ref-type="table">3</xref> summarizes the evaluation results. For the two test cases, GBTR showed relatively large errors in predicting ZTs of the test thermoelectric materials because the tree-based methods are not suitable for the extrapolation problems. In particular, GBTR couldn’t capture the changes by the different doping concentrations of Sb in Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>1−<italic>x</italic></sub>Sb<sub><italic>x</italic></sub>, and ZTs of these materials are predicted as the same value of 0.32. MAE of GBTR for the materials from the external databases was 0.41. By contrast, DopNet predicted ZT of the test materials more accurately, and the MAE of DopNet was 0.13. Furthermore, while GBTR failed to identify the dopant effects in Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>1−<italic>x</italic></sub>Sb<sub><italic>x</italic></sub> system, DopNet roughly predicted the order of ZTs in the Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>1−<italic>x</italic></sub>Sb<sub><italic>x</italic></sub> system. As shown in the prediction results in Table <xref rid="Tab2" ref-type="table">3</xref> and the MAE of DopNet, it can be used to roughly investigate the thermoelectric performances of new material before synthesizing it (Table <xref rid="Tab3" ref-type="table">2</xref>).<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Predicted ZTs of the machine learning algorithms for top 10 high-ZT materials in the MRL dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Chemical formula</p></th><th><p>Ground truth (= y)</p></th><th><p><italic>f</italic><sub><italic>G</italic><italic>P</italic></sub></p></th><th><p><italic>f</italic><sub><italic>G</italic><italic>B</italic></sub></p></th><th><p><italic>f</italic><sub><italic>D</italic><italic>N</italic></sub></p></th><th><p>∣<italic>y</italic> − <italic>f</italic><sub><italic>G</italic><italic>P</italic></sub>∣</p></th><th><p>∣<italic>y</italic> − <italic>f</italic><sub><italic>G</italic><italic>B</italic></sub>∣</p></th><th><p>∣<italic>y</italic> − <italic>f</italic><sub><italic>D</italic><italic>N</italic></sub>∣</p></th></tr></thead><tbody><tr><td><p>Pb<sub>0.96</sub>Sr<sub>0.4</sub>TeNa<sub>0.2</sub></p></td><td><p>1.60<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></p></td><td><p>0.55</p></td><td><p>0.79</p></td><td><p>1.43</p></td><td><p>1.05</p></td><td><p>0.81</p></td><td><p>0.17</p></td></tr><tr><td><p>Pb<sub>0.98</sub>Sr<sub>0.2</sub>TeNa<sub>0.1</sub></p></td><td><p>1.56<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></p></td><td><p>0.76</p></td><td><p>0.81</p></td><td><p>1.53</p></td><td><p>0.80</p></td><td><p>0.75</p></td><td><p>0.03</p></td></tr><tr><td><p>Na<sub>0.02</sub>PbTe<sub>0.85</sub>Se<sub>0.15</sub></p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>0.84</p></td><td><p>1.24</p></td><td><p>1.21</p></td><td><p>0.66</p></td><td><p>0.26</p></td><td><p>0.29</p></td></tr><tr><td><p>In<sub>0.25</sub>Co<sub>4</sub>Sb<sub>12</sub></p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR55">55</xref></sup></p></td><td><p>0.46</p></td><td><p>1.22</p></td><td><p>1.06</p></td><td><p>1.04</p></td><td><p>0.28</p></td><td><p>0.44</p></td></tr><tr><td><p>Zr<sub>0.5</sub>Hf<sub>0.5</sub>NiSn<sub>1.998</sub>Sb<sub>0.002</sub></p></td><td><p>1.45<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.85</p></td><td><p>0.98</p></td><td><p>1.35</p></td><td><p>0.60</p></td><td><p>0.47</p></td><td><p>0.10</p></td></tr><tr><td><p>Zr<sub>0.25</sub>Hf<sub>0.25</sub>Ti<sub>0.5</sub>NiSn</p></td><td><p>1.42<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.62</p></td><td><p>1.04</p></td><td><p>1.22</p></td><td><p>0.80</p></td><td><p>0.38</p></td><td><p>0.20</p></td></tr><tr><td><p>Tl<sub>0.02</sub>Pb<sub>0.98</sub>Te</p></td><td><p>1.39<sup><xref ref-type="bibr" rid="CR56">56</xref></sup></p></td><td><p>0.90</p></td><td><p>0.91</p></td><td><p>0.91</p></td><td><p>0.49</p></td><td><p>0.48</p></td><td><p>0.48</p></td></tr><tr><td><p>Na<sub>0.02</sub>PbTe<sub>0.75</sub>Se<sub>0.25</sub></p></td><td><p>1.39<sup><xref ref-type="bibr" rid="CR54">54</xref></sup></p></td><td><p>0.84</p></td><td><p>1.27</p></td><td><p>1.15</p></td><td><p>0.55</p></td><td><p>0.12</p></td><td><p>0.24</p></td></tr><tr><td><p>In<sub>0.2</sub>Co<sub>4</sub>Sb<sub>12</sub></p></td><td><p>1.39<sup><xref ref-type="bibr" rid="CR55">55</xref></sup></p></td><td><p>0.47</p></td><td><p>1.26</p></td><td><p>1.04</p></td><td><p>0.92</p></td><td><p>0.13</p></td><td><p>0.35</p></td></tr><tr><td><p>Ag<sub>0.15</sub>Sb<sub>0.15</sub>Te<sub>1.15</sub>Ge<sub>0.85</sub></p></td><td><p>1.38<sup><xref ref-type="bibr" rid="CR57">57</xref></sup></p></td><td><p>0.60</p></td><td><p>0.59</p></td><td><p>1.05</p></td><td><p>0.78</p></td><td><p>0.79</p></td><td><p>0.33</p></td></tr><tr><td><p>Average error (MAE)</p></td><td/><td/><td/><td/><td><p>0.77</p></td><td><p>0.45</p></td><td><p>0.26</p></td></tr></tbody></table><table-wrap-foot><p>ZTs of all materials were measured at 700 K. The predicted values of GPR, GBTR, and DopNet are denoted by <italic>f</italic><sub><italic>GP</italic></sub>, <italic>f</italic><sub><italic>GB</italic></sub>, and <italic>f</italic><sub><italic>DN</italic></sub>, respectively.</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p> Predicted ZTs of the machine learning algorithms for external test materials of two test cases.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Test case</p></th><th><p>Chemical formula</p></th><th><p>Temperature (K)</p></th><th><p>Ground truth (= y)</p></th><th><p><italic>f</italic><sub><italic>G</italic><italic>B</italic></sub></p></th><th><p><italic>f</italic><sub><italic>D</italic><italic>N</italic></sub></p></th></tr></thead><tbody><tr><td><p>Known combination</p></td><td><p>Na<sub>0.01</sub>Pb<sub>0.99</sub>Te</p></td><td><p>700</p></td><td><p>1.22<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></p></td><td><p>1.31</p></td><td><p>1.26</p></td></tr><tr><td/><td><p>Na<sub>0.02</sub>Pb<sub>0.98</sub>Te</p></td><td><p>700</p></td><td><p>1.37<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></p></td><td><p>1.32</p></td><td><p>1.29</p></td></tr><tr><td/><td><p>Na<sub>0.03</sub>Pb<sub>0.97</sub>Te</p></td><td><p>700</p></td><td><p>1.49<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></p></td><td><p>1.32</p></td><td><p>1.37</p></td></tr><tr><td><p>Unknown combination</p></td><td><p>Pb<sub>0.95</sub>Ce<sub>0.05</sub>Te</p></td><td><p>300</p></td><td><p>0.24<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.06</p></td><td><p>0.09</p></td></tr><tr><td/><td><p>Zn<sub>0.02</sub>PbTe</p></td><td><p>300</p></td><td><p>0.41<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.22</p></td><td><p>0.09</p></td></tr><tr><td/><td><p>Pb<sub>0.95</sub>Ce<sub>0.05</sub>Te</p></td><td><p>673</p></td><td><p>0.88<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>1.16</p></td><td><p>0.98</p></td></tr><tr><td/><td><p>Na<sub>0.025</sub>Mg<sub>0.03</sub>Pb<sub>0.95</sub>Te</p></td><td><p>700</p></td><td><p>1.07<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>1.29</p></td><td><p>1.22</p></td></tr><tr><td/><td><p>Na<sub>2</sub>TeSr<sub>0.01</sub>PbTe</p></td><td><p>700</p></td><td><p>1.24<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.43</p></td><td><p>1.32</p></td></tr><tr><td/><td><p>Mg<sub>3.05</sub>Nb<sub>0.15</sub>Sb<sub>1.5</sub>Bi<sub>0.49</sub>Te<sub>0.01</sub></p></td><td><p>673</p></td><td><p>1.57<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.40</p></td><td><p>1.66</p></td></tr><tr><td/><td><p>Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>0.998</sub>Sb<sub>0.002</sub></p></td><td><p>700</p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.32</p></td><td><p>1.27</p></td></tr><tr><td/><td><p>Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>0.996</sub>Sb<sub>0.004</sub></p></td><td><p>700</p></td><td><p>1.38<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.32</p></td><td><p>1.25</p></td></tr><tr><td/><td><p>Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn<sub>0.99</sub>Sb<sub>0.01</sub></p></td><td><p>700</p></td><td><p>1.21<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.41</p></td><td><p>1.24</p></td></tr><tr><td/><td><p>Zr<sub>0.25</sub>Hf<sub>0.25</sub>Te<sub>0.5</sub>NiSn</p></td><td><p>700</p></td><td><p>1.30<sup><xref ref-type="bibr" rid="CR22">22</xref></sup></p></td><td><p>0.32</p></td><td><p>1.27</p></td></tr><tr><td/><td><p>Mg<sub>3.2</sub>Sb<sub>1.5</sub>Bi<sub>0.49</sub>Te<sub>0.01</sub></p></td><td><p>716</p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.33</p></td><td><p>1.43</p></td></tr><tr><td/><td><p>PbTeCd<sub>0.02</sub></p></td><td><p>773</p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>1.19</p></td><td><p>1.31</p></td></tr><tr><td/><td><p>Tl<sub>0.02</sub>Pb<sub>0.98</sub>Te</p></td><td><p>800</p></td><td><p>1.50<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.93</p></td><td><p>1.36</p></td></tr><tr><td/><td><p>Ce<sub>0.1</sub>In<sub>0.1</sub>Tb<sub>0.2</sub>Co<sub>4</sub>Sb<sub>12</sub></p></td><td><p>800</p></td><td><p>1.34<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.64</p></td><td><p>0.95</p></td></tr><tr><td/><td><p>Ba<sub>0.06</sub>La<sub>0.05</sub>Tb<sub>0.02</sub>Co<sub>4</sub>Sb<sub>12</sub></p></td><td><p>850</p></td><td><p>1.28<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p>0.63</p></td><td><p>1.25</p></td></tr><tr><td><p>Average error (MAE)</p></td><td/><td/><td/><td><p>0.41</p></td><td><p>0.13</p></td></tr></tbody></table><table-wrap-foot><p>ZTs of the test materials were measured at 700 K. The predicted ZT of GBTR and DopNet are denoted by <italic>f</italic><sub><italic>GB</italic></sub> and <italic>f</italic><sub><italic>DN</italic></sub>, respectively.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec8"><title>Hyperparameter analysis</title><p id="Par28">Compared to conventional artificial neural networks, DopNet has two additional hyperparameters denoted by <italic>γ</italic> and <italic>K</italic>. The dopant threshold <italic>γ</italic> determines whether an atom in a given material is an atom in the host material or a dopant. If the proportion of the atom is less than or equal to <italic>γ</italic>, it is identified as a dopant. Another hyperparameter <italic>K</italic> defines the maximum number of dopants allowed in DopNet. However, <italic>K</italic> is automatically determined to cover all doped materials in the dataset for a given <italic>γ</italic>. Hence, we measured the prediction errors of DopNet as <italic>γ</italic> changes on the MRL dataset. Figure <xref rid="Fig5" ref-type="fig">5</xref>a shows the evaluation results of DopNet for different values of the dopant threshold <italic>γ</italic>. The prediction errors were measured by MAE. As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, DopNet achieved lower errors than GBTR for all values of <italic>γ</italic>. This evaluation result shows the robustness of DopNet for the hyperparameter <italic>γ</italic>.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Prediction errors of DopNet for different values of the hyperparameters.</title><p><bold>a</bold> Prediction errors for different dopant thresholds. <bold>b</bold> Prediction errors for different initial learning rates of the gradient method. <bold>c</bold> Prediction errors for different batch sizes.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig5_HTML.png"/></fig></p><p id="Par29">In addition to <italic>γ</italic>, there are two important hyperparameters in the training of deep neural networks, called initial learning rate and batch size. We also evaluated the prediction errors of DopNet for different initial learning rates and batch sizes in predicting ZT. As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b, DopNet showed smaller prediction errors with reasonable choices of the initial learning rates in { 5e−3, 1e−2, 5e−2, 1e−1}. However, the prediction error of DopNet was larger than the errors of GBTR for the initial learning rate of 5e−1 because the gradient descent method to train GBTR was not converged. For the different batch sizes, DopNet always outperformed GBTR as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>c. As a result, DopNet was robust to the changes in the dopant threshold (<italic>γ</italic>) and the batch size, and it will enhance the general applicability of DopNet to real-world applications.</p><p id="Par30">In a practical implementation, the dopant threshold <italic>γ</italic> is an important hyperparameter of DopNet because it determines the host materials and the additives in a given material. However, the dopant threshold should be selected in appropriate ranges. For a large dopant threshold, too many elements can be identified as the additives, and the host materials were not defined. To prevent this implementation issue, we propose a rule to select the dopant threshold. Let <italic>β</italic> is defined as the maximum value of the proportions of the elements in a chemical composition. In the implementation of DopNet, the dopant threshold should satisfy the following inequality for the chemical compositions <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:mi mathvariant="script">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{X}}=\{{{\bf{x}}}_{1},{{\bf{x}}}_{2},...\,,{{\bf{x}}}_{N}\}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_IEq1.gif"/></alternatives></inline-formula> in a given dataset, where <bold>x</bold><sub><italic>i</italic></sub> is a chemical composition.<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:mi>γ</mml:mi><mml:mspace width="0.25em"/><mml:mo>&lt;</mml:mo><mml:mspace width="0.25em"/><mml:mi>min</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma \,&lt;\,\min \{{\beta }_{1},{\beta }_{2},...\,,{\beta }_{N}\}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ2.gif"/></alternatives></disp-formula>If the dopant threshold does not satisfy the inequality in Eq. (<xref rid="Equ2" ref-type="disp-formula">2</xref>), some materials are recognized as having no host materials. Thus, the dopant threshold should be selected in <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>min</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mspace width="0.25em"/><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$(0,\min \{{\beta }_{1},{\beta }_{2},...\,,{\beta }_{N}\})$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_IEq2.gif"/></alternatives></inline-formula> to properly separate the materials into the hosts and the additives.</p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p id="Par31">The doped materials are common in thermoelectric materials. Since the materials can have completely different materials properties by a small amount of the dopants, the doped materials usually have mixed distribution for the materials properties, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. In particular, the thermoelectric properties of the materials can be dramatically changed by the dopants. For this reason, predicting the materials properties of the doped materials using machine learning algorithms is challenging because the relations between the doped materials and their materials properties are severely nonlinear. In this paper, we proposed a unified architecture of the neural networks, called DopNet, to accurately predict the thermoelectric properties of the doped materials. DopNet is designed to explicitly and independently representing the host materials and the dopants to identify the effects of the dopants in the entire materials, as illustrated by the architecture of DopNet in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. To the best of our knowledge, DopNet is the first machine learning algorithm to predict materials properties from the chemical formulas of the materials by identifying the dopant effects.</p><p id="Par32">We evaluated DopNet in predicting the five thermoelectric properties of various doped materials. For the evaluations, we manually collected the chemical formulas of 573 materials and their thermoelectric properties from the MRL database<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, DopNet outperformed state-of-the-art machine learning methods in predicting Seebeck coefficient, electrical conductivity, thermal conductivity, power factor, and ZT. In particular, DopNet achieved <italic>R</italic><sup>2</sup> scores of 0.79 and 0.86 in predicting power factor and ZT, respectively. Furthermore, DopNet was significantly more effective than GPR and GBTR in predicting the high-ZT materials. Specifically, the MAEs of GPR and GBTR for the top 10 high-ZT materials were 0.77 and 0.45, respectively. However, the MAE of DopNet was 0.26, and the improvement of DopNet is 42.22% compared to GBTR. This improvement of DopNet in predicting high-ZT materials is noteworthy because our ultimate goal is to discover a novel high-ZT material.</p><p id="Par33">Since the MRL dataset contains ZTs of the same material measured at different temperatures, some materials in the test dataset can be shown in the training dataset with different temperatures. To evaluate DopNet in predicting ZTs of completely unseen materials, we predicted ZTs of the materials collected from external databases as shown in Table <xref rid="Tab2" ref-type="table">3</xref>. In this experiment, GBTR was not effective to predict ZTs of the unseen materials, and the MAE of GBTR increased explosively from 0.07 for the test dataset of the MRL dataset to 0.41 for the completely unseen materials. The inaccurate prediction performance of GBTR in the extrapolation limits the applicability of GBTR to real-world applications despite its superior interpolation capabilities. This problem of GBTR in the extrapolation is unsolvable because the functions approximated the tree-based methods are defined only in the ranges of the training datasets. By contrast, the MAE of DopNet increased from 0.06 for the MRL dataset to 0.13 for the unseen materials. That is, DopNet was significantly more effective than the popular GBTR in the extrapolation problem. We will investigate the reason for the performance improvement of DopNet in the extrapolation problem in the next section.</p><p id="Par34">To clarify the reason for the performance improvement of DopNet, we investigated the embedding results of DopNet and compared the embedding results of DopNet with the embeddings of DNN. Note that we did not compare the embedding results with SVR, GPR, and GBTR because they do not generate the latent embeddings of the input data. To visualize the embedding results, t-SNE<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> was applied to the outputs of the last hidden layers of DNN and DopNet. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the visualization results of the embeddings generated by DNN and DopNet. In the figure, each point is the data (pair of material and temperature) in the MRL dataset, and the colors of the points indicate the measured ZTs. As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, the data is disorderly distributed in the initial stage. After 100 epochs of the training of DopNet, the data was roughly clustered into the low-ZT data and the high-ZT data. Finally, the data was arranged in a direction proportional to the measured ZTs. That is, DopNet generated a latent data representation that makes the regression problem easier. We also compared the embedding results of DopNet with the embeddings of DNN. As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b, DNN did not generate a proper data embedding that separates the input data according to their target values. For instance, despite the completely different ZTs of In<sub>0.</sub>2Co<sub>4</sub>Sb<sub>12</sub> and In<sub>0.05</sub>Co<sub>4</sub>Sb<sub>12</sub> at 700 K, they were embedded into the same area as shown in the embedding results of DNN. By contrast, they were separately embedded by DopNet. In addition to this case, DNN did not properly represent Zr<sub>0.25</sub>Hf<sub>0.25</sub>Ti<sub>0.5</sub>NiSn and Zr<sub>0.5</sub>Hf<sub>0.25</sub>NiSn. This embedding result of DNN shows that DNN cannot effectively identify the subtle changes by the dopants in the materials. However, DopNet properly represented the doped materials according to their ZTs because it was specifically designed to identify the dopant effects in the materials, as illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>2-dimensional visualization of embedding results of DNN and DopNet.</title><p><bold>a</bold> embedding results of DopNet for each epoch of the training. <bold>b</bold> embedding results comparison between FNN and DopNet. The embedding results were visualized by t-SNE<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> applied to the outputs of the last hidden layers of FNN and DopNet. Each point is the data (pair of material and temperature) in the MRL dataset, and the colors of the points indicate the values of ZTs. Two axes <italic>h</italic><sub>1</sub> and <italic>h</italic><sub>2</sub> indicate the first and second latent features calculated by t-SNE, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Fig6_HTML.png"/></fig></p><p id="Par35">From the embedding results, we can also rationalize the performance improvement of DopNet in predicting ZTs of the unseen materials, which is called the extrapolation problem. We can observe that the data is monotonically arranged according to ZTs in the embedding results of DopNet, as shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>b. That is, the linearity of the relation between the materials and their ZTs increased in the embedding space generated by DopNet. Hence, the relation to be approximated the prediction model was linearized by encoding the nonlinearity into the input data of the prediction model. The positive effects of improving the prediction accuracy in the extrapolation problems by the nonlinearity encoding were well investigated theoretically and experimentally in<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p><p id="Par36">Recently, Fan et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> calculated the electronic structure, electron relaxation time, and thermoelectric properties for Pb doped Mg<sub>2</sub>Si structure, and reported the calculated thermoelectric properties at different temperature. Also, Pőhls et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> tried to calculate the thermoelectric properties of RECuZnP<sub>2</sub> (RE = Pr, Nd, Er) using sophisticated calculation methods based on ab initio scattering and transport (AMSET) and compressive sensing lattice dynamics. According to their calculation results at 300 K, 400 K, and 700 K, MAE between the experimental and calculation ZTs are 0.12<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup>. Similarly, DopNet showed the error of 0.13 in predicting ZTs of the completely unseen materials as shown in Table <xref rid="Tab2" ref-type="table">3</xref>. As a result, DopNet achieved the comparable extrapolation capabilities with the traditional calculation methods even though DopNet does not require human labor and extensive computing resources.</p><p id="Par37">We also predicted ZTs of RECuZnP<sub>2</sub> (RE = Pr, Nd, Er) at 300 K, 400 K, 700 K. We omitted the Mg<sub>2</sub>Si structure because it was contained the training dataset of DopNet. By contrast, the RECuZnP<sub>2</sub> (RE = Pr, Nd, Er) system have never been shown in the training dataset. Moreover, Pr and Er have never been shown in any data in the training dataset. For the calculation results of<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, MAE was 0.12 for the RECuZnP<sub>2</sub> (RE = Pr, Nd, Er) system. However, DopNet achieved MAE of 0.06 ± 0.01 in 10 times repetitions of the training and prediction processes for the same materials systems. Furthermore, although the calculation methods should be manually modified to improve the prediction accuracies, the prediction accuracy of DopNet can be improved just by collecting more training data. Thus, DopNet can be used as a computation tool to discover novel thermoelectric materials.</p><p id="Par38">DopNet can provide a rapid prediction with reasonable prediction accuracies in discovering new thermoelectric materials. One of the most practical benefits of DopNet is that additional information generated by experimental analyses and density functional theory (DFT)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> calculations are not required. Thus, DopNet can be used for fast screening in large materials databases or combinatorially generated candidates. This screening method based on DopNet will significantly accelerate the process of discovering novel materials because we can sum up several candidate materials in thousands of materials from combinations of host materials, doped elements, and doping concentrations.</p><p id="Par39">For a trained DopNet <italic>f</italic><sub><italic>θ</italic></sub>(<bold>z</bold>; <bold><italic>θ</italic></bold><sup>*</sup>), the computational screening process based on DopNet can be conducted as the following three steps.<list list-type="bullet"><list-item><p id="Par40">Step 1: The chemical compositions of the candidate materials are generated combinatorially. For instance, the compositions of our target materials system Tl<sub><italic>a</italic></sub>Pb<sub><italic>b</italic></sub>Te<sub><italic>c</italic></sub> are generated combinatorially for all possible values of the proportions <italic>a</italic>, <italic>b</italic>, and <italic>c</italic>. Then, the combinatorially generated compositions are validated based on chemical rules, such as valency checking.</p></list-item><list-item><p id="Par41">Step 2: The trained DopNet predicts the target materials properties for the generated compositions. After the prediction, the compositions are sorted according to user-defined criteria.</p></list-item><list-item><p id="Par42">Step 3: For top <italic>k</italic> materials in the prediction results, domain experts synthesize the selected materials to validate their properties experimentally.</p></list-item></list>Usually, the first and second steps are finished within an hour, i.e., promising materials can be identified from thousands of candidate materials within an hour. Thus, experimenters can significantly reduce the time required to synthesize thousands of materials to the time required to synthesize only a few <italic>k</italic> materials.</p></sec><sec id="Sec10" sec-type="methods"><title>Methods</title><sec id="Sec11"><title>Forward process of DopNet</title><p id="Par43">Forward process of DopNet consists of host embedding, dopant embedding, and prediction. For a host feature vector <bold>x</bold><sub><italic>h</italic></sub>, a latent embedding of the host material is calculated via an the autoencoder and the host embedding network as:<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{z}}}_{h}={f}_{\omega }({u}_{\omega }({h}_{\phi }({{\bf{x}}}_{h}))),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ3.gif"/></alternatives></disp-formula>where <italic>h</italic><sub><italic>ϕ</italic></sub> is an encoder network of the autoencoder, and <italic>f</italic><sub><italic>ω</italic></sub> is the host embedding network. Simultaneously, the dopant embedding <bold>z</bold><sub><italic>d</italic></sub> is calculated from the set of dopant features <italic>S</italic><sub><italic>d</italic></sub> via dopant embedding networks as:<disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⊕</mml:mo><mml:mo>⋯</mml:mo><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{z}}}_{d}={f}_{\mu }({{\bf{x}}}_{{d}_{1}})\oplus {f}_{\mu }({{\bf{x}}}_{{d}_{2}})\oplus \cdots \oplus {f}_{\mu }({{\bf{x}}}_{{d}_{K}}),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ4.gif"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\bf{x}}}_{{d}_{i}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_IEq3.gif"/></alternatives></inline-formula> is a feature vector of <italic>i</italic>th dopant in the input material, <italic>f</italic><sub><italic>μ</italic></sub> is the dopant embedding network, and ⊕ indicates vector concatenation. After generating the latent embeddings of the host material and the dopants, the target materials property <bold>y</bold> is predicted via <italic>f</italic><sub><italic>θ</italic></sub> as:<disp-formula id="Equ5"><label>5</label><alternatives><mml:math id="Equ5_Math"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{y}}={f}_{\theta }({{\bf{z}}}_{h}\oplus {{\bf{z}}}_{d}).$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ5.gif"/></alternatives></disp-formula>By the independent embedding processes of the host material and the dopants in Eqs. (<xref rid="Equ3" ref-type="disp-formula">3</xref>) and (<xref rid="Equ4" ref-type="disp-formula">4</xref>), DopNet can easily capture the dopant effects in the entire materials. The forward step of DopNet is formally described in Algorithm1.<disp-formula id="Equa"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Figa_HTML.png"/></disp-formula></p></sec><sec id="Sec12"><title>Model parameter optimization</title><p id="Par44">The materials representations of the host materials are converted into the latent and compact embeddings via autoencoder in DopNet. In the training of DopNet, the autoencoder for the host material and the dopant embedding network are independently trained on the basis of the decomposed materials representations <bold>x</bold><sub><italic>h</italic></sub> and <italic>S</italic><sub><italic>d</italic></sub>. Autoencoders are designed to extract latent features of given data and trained in unsupervised manner by minimizing reconstruction loss. For a given training dataset <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{D}}=\{({{\bf{x}}}_{1},{{\bf{y}}}_{1}),({{\bf{x}}}_{2},{{\bf{y}}}_{2}),...,({{\bf{x}}}_{N},{{\bf{y}}}_{N})\}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_IEq4.gif"/></alternatives></inline-formula>, the training problem of autoencoders are defined by:<disp-formula id="Equ6"><label>6</label><alternatives><mml:math id="Equ6_Math"><mml:mrow><mml:msup><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>∣</mml:mo><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>h</mml:mi></mml:mrow><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:msubsup><mml:mrow><mml:mo>∣</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\phi }^{* },{\psi }^{* }={\arg \min }_{\phi ,\psi }\frac{1}{N}\mathop{\sum }\limits_{i=1}^{N}| | {{\bf{x}}}_{h,i}-{g}_{\psi }({h}_{\phi }({{\bf{x}}}_{h,i}))| {| }_{2}^{2},$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ6.gif"/></alternatives></disp-formula>where <italic>N</italic> is the number of materials in the training dataset. Note that the label data <bold>y</bold><sub><italic>i</italic></sub> is not used in the training of the autoencoder. In DopNet, there is no restriction in choosing autoencoder to embed the representations of the host materials. For instance, a probabilistic model of autoencoder<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> can be used in DopNet rather than the traditional autoencoder defined by Eq. (<xref rid="Equ6" ref-type="disp-formula">6</xref>).</p><p id="Par45">After the training of the autoencoder, the dopant embedding network <italic>f</italic><sub><italic>μ</italic></sub> and the dense network <italic>f</italic><sub><italic>θ</italic></sub> is simultaneously trained in supervised manner. For a trained autoencoder <italic>g</italic><sub><italic>ψ</italic></sub>(<italic>h</italic><sub><italic>ϕ</italic></sub>(<bold>x</bold><sub><italic>h</italic></sub>)), the host embedding network, the <italic>K</italic> dopant embedding networks, and the dense network are trained by directly minimizing the surrogate loss, such as MAE and root mean square error (RMSE). For instance, the training problem of the networks can be defined based on MAE as:<disp-formula id="Equ7"><label>7</label><alternatives><mml:math id="Equ7_Math"><mml:mrow><mml:msup><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>ω</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>arg</mml:mi><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mo>∣</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⊕</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\theta }^{* },{\omega }^{* },{\mu }^{* }={\arg \min }_{\theta ,\mu ,\omega }\frac{1}{N}\mathop{\sum }\limits_{i=1}^{N}| {{\bf{y}}}_{i}-{f}_{\theta }({{\bf{z}}}_{h,i}\oplus {{\bf{z}}}_{d,i})| ,$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2021_564_Article_Equ7.gif"/></alternatives></disp-formula>where <italic>θ</italic>, <italic>ω</italic>, and <italic>μ</italic> are the model parameters of the dense, host embedding, and dopant embedding networks, respectively. In the training of the neural networks in DopNet, the dropout technique<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> was applied to improve the generalization capability. Adam optimizer<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> and stochastic gradient descent method with learning rate decay were used to optimize the model parameters of the autoencoder and the other model parameters in DopNet, respectively. Algorithm2 formally describes the training process of DopNet based on the gradient descent methods.<disp-formula id="Equb"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2021_564_Figb_HTML.png"/></disp-formula></p></sec><sec id="Sec13"><title>Hyperparameter settings</title><p id="Par46">We applied greedy search with validation dataset to select hyperparameters of the machine learning methods. All hyperparameters were set as optimal values that minimize the prediction errors of the validation datasets. Two hyperparameters of SVR, called margin <italic>ϵ</italic> and regularization coefficient <italic>C</italic>, were selected within {0.01, 0.1, 0.5, 1.0} and {0.1, 0.2, 0.4}, respectively. For GBTR, maximum depth of tree and number of estimators were searched in {3, 4, 5, 6, 7, 8} and {100, 200, 300, 400}, respectively. The hyperparameters of DopNet were also selected manually. The selected value of the dopant threshold <italic>γ</italic> and the maximum number of dopants <italic>K</italic> were set to 5e−1 and 3, respectively. DopNet was trained by stochastic gradient descent (SGD)<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. The autoencoder of DopNet to embedded the host materials were trained by SGD with the initial learning rate 1e−3, the <italic>L</italic><sub>2</sub> regularization coefficient 1e−5, and the batch size 32. The other parts of DopNet (dopant embedding network and dense network) were also trained by SGD with the initial learning rate 1e−1, the <italic>L</italic><sub>2</sub> regularization 1e−7, and the batch size 32. For all experiments, the autoencoder of DopNet was defined as fc(256)-fc(64)-fc(256)-fc(<italic>n</italic><sub>1</sub>), where fc indicates fully-connected layer, the numbers in the fc are the number of output neurons, and <italic>n</italic><sub>1</sub> is dimensionality of the host feature <bold>x</bold><sub><italic>h</italic></sub>. The host and dopant embedding networks of DopNet were defined as simple dense networks with one fc(256). The dense network of DopNet to predict the target materials properties was implemented by three fully-connected layers as fc(512)-fc(16)-fc(1), and the dropout technique was applied to each fc layer.</p><p id="Par47">To convert the chemical formulas into the numerical feature vectors, intrinsic elemental features are assigned for each atom in the materials. These feature vectors of the atoms is converted into a feature vector of a material by calculating statistics of the atomic feature vectors. We calculated average, standard deviation, minimum value, and maximum value of the atomic feature vectors. Total 31 intrinsic elemental features were assigned for each atom, such as atomic number, atomic weight, and electronegativity. Hence, the materials were represented by 124 features from the mean, standard deviation, minimum, and maximum of the 31 atomic attributed of the constituent atoms. Finally, the feature vector of the materials were concatenated with the temperatures at which the thermoelectric properties of the materials were measured. The elemental features used in this paper are available in Python Mendeleev Package<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>This study was supported by a project from the Korea Research Institute of Chemical Technology (KRICT) [grant number: SI2151-10].</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>G.S.N. and H.J. supervised the research. G.S.N. and S.J. contributed to design of experiments and G.S.N. conducted experiments. G.S.N. and S.J. wrote the original manuscript and analyzed the results. G.S.N. and S.J. equally contributed this work. All the authors were involved in writing the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The MRL dataset used in the evaluations was manually collected from <ext-link xlink:href="http://www.mrl.ucsb.edu:8080/datamine/thermoelectric.jsp" ext-link-type="uri">http://www.mrl.ucsb.edu:8080/datamine/thermoelectric.jsp</ext-link>. The collected MRL dataset is available at <ext-link xlink:href="https://github.com/ngs00/DopNet" ext-link-type="uri">https://github.com/ngs00/DopNet</ext-link>.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The source code of DopNet and the experiment scripts are publicly available at <ext-link xlink:href="https://github.com/ngs00/DopNet" ext-link-type="uri">https://github.com/ngs00/DopNet</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par48">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>X-P</given-names></name><etal/></person-group><article-title xml:lang="en">Time-dependent density-functional theory molecular-dynamics study on amorphization of sc-sb-te alloy under optical excitation</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>31</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXms1yqt7o%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41524-020-0303-z</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsai</surname><given-names>Y-C</given-names></name><name><surname>Bayram</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Band alignments of ternary wurtzite and zincblende iii-nitrides investigated by hybrid density functional theory</article-title><source>ACS Omega</source><year>2020</year><volume>5</volume><fpage>3917</fpage><lpage>3923</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvVeqsr4%3D</pub-id><pub-id pub-id-type="doi">10.1021/acsomega.9b03353</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">First-principles calculation of metal-doped caalsin3: material design for new phosphors</article-title><source>RSC Adv.</source><year>2015</year><volume>5</volume><fpage>39319</fpage><lpage>39323</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXmslersL0%3D</pub-id><pub-id pub-id-type="doi">10.1039/C5RA04350F</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Umari</surname><given-names>P</given-names></name><name><surname>Mosconi</surname><given-names>E</given-names></name><name><surname>Angelis</surname><given-names>FD</given-names></name></person-group><article-title xml:lang="en">Relativistic GW calculations on CH<sub>3</sub>NH<sub>3</sub>PbI<sub>3</sub> and CH<sub>3</sub>NH<sub>3</sub>SnI<sub>3</sub> perovskites for solar cell applications</article-title><source>Sci. Rep.</source><year>2014</year><volume>4</volume><pub-id pub-id-type="doi">10.1038/srep04467</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXovVOisbw%3D</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Govoni</surname><given-names>M</given-names></name><name><surname>Galli</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Large scale gw calculations</article-title><source>J. Chem. Theory Comput.</source><year>2015</year><volume>11</volume><fpage>2680</fpage><lpage>2696</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXmvVyqsA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/ct500958p</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shim</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>E-K</given-names></name><name><surname>Lee</surname><given-names>YJ</given-names></name><name><surname>Nieminen</surname><given-names>RM</given-names></name></person-group><article-title xml:lang="en">Density-functional calculations of defect formation energies using supercell methods: defects in diamond</article-title><source>Phys. Rev. B</source><year>2005</year><volume>71</volume><fpage>035206</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.71.035206</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2MXhtVyqs78%3D</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuo</surname><given-names>Y</given-names></name><name><surname>Mansouri Tehrani</surname><given-names>A</given-names></name><name><surname>Brgoch</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Predicting the band gaps of inorganic solids by machine learning</article-title><source>J. Phys. Chem. Lett</source><year>2018</year><volume>9</volume><fpage>1668</fpage><lpage>1673</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXktlGitbs%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpclett.8b00124</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>145301</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSnu7c%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.145301</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Z-W</given-names></name><name><surname>del Cueto</surname><given-names>M</given-names></name><name><surname>Geng</surname><given-names>Y</given-names></name><name><surname>Troisi</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Effect of increasing the descriptor set on machine learning prediction of small molecule-based organic solar cells</article-title><source>Chem. Mater.</source><year>2020</year><volume>32</volume><fpage>7777</fpage><lpage>7787</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhs1Kiu7bO</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c02325</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Rasmussen, C. E. &amp; Williams, C. K. I. <italic>Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning)</italic> (The MIT Press, 2005).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Seko</surname><given-names>A</given-names></name><name><surname>Shitara</surname><given-names>K</given-names></name><name><surname>Nakayama</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>I</given-names></name></person-group><article-title xml:lang="en">Prediction model of band gap for inorganic compounds by combination of density functional theory calculations and machine learning techniques</article-title><source>Phys. Rev. B</source><year>2016</year><volume>93</volume><fpage>115104</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.93.115104</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsVShurjP</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Kipf, T. N. &amp; Welling, M. Semi-supervised classification with graph convolutional networks. In <italic>International Conference on Learning Representations (ICLR)</italic> (2017).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Moleculenet: a benchmark for molecular machine learning</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>513</fpage><lpage>530</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhslChtrbO</pub-id><pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morawietz</surname><given-names>T</given-names></name><name><surname>Artrith</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Machine learning-accelerated quantum mechanics-based atomistic simulations for industrial applications</article-title><source>J. Comput. Aided Mol. Des.</source><year>2020</year><volume>35</volume><fpage>557</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1007/s10822-020-00346-6</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitVagtLzN</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zitolo</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Identification of catalytic sites for oxygen reduction in iron- and nitrogen-doped graphene materials</article-title><source>Nat. Mater.</source><year>2015</year><volume>14</volume><fpage>937</fpage><lpage>942</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhtlSlsL%2FE</pub-id><pub-id pub-id-type="doi">10.1038/nmat4367</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shui</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Du</surname><given-names>F</given-names></name><name><surname>Dai</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">N-doped carbon nanomaterials are durable catalysts for oxygen reduction reaction in acidic fuel cells</article-title><source>Sci. Adv.</source><year>2015</year><volume>1</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1126/sciadv.1400129</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XnslOrs74%3D</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das Adhikari</surname><given-names>S</given-names></name><name><surname>Guria</surname><given-names>AK</given-names></name><name><surname>Pradhan</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Insights of doping and the photoluminescence properties of mn-doped perovskite nanocrystals</article-title><source>J. Phys. Chem. Lett.</source><year>2019</year><volume>10</volume><fpage>2250</fpage><lpage>2257</lpage><pub-id pub-id-type="doi">10.1021/acs.jpclett.9b00182</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXnsV2qtrc%3D</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Snyder</surname><given-names>GJ</given-names></name></person-group><article-title xml:lang="en">Band engineering of thermoelectric materials</article-title><source>Adv. Mater.</source><year>2012</year><volume>24</volume><fpage>6125</fpage><lpage>6135</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhsFans7vJ</pub-id><pub-id pub-id-type="doi">10.1002/adma.201202919</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Review of current high-zt thermoelectric materials</article-title><source>J. Mater. Sci.</source><year>2020</year><volume>55</volume><fpage>12642</fpage><lpage>12704</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXht1Cqt7zJ</pub-id><pub-id pub-id-type="doi">10.1007/s10853-020-04949-0</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Chen, T. &amp; Guestrin, C. Xgboost: a scalable tree boosting system. In <italic>Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</italic>. Association for Computing Machinery (2016).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bux</surname><given-names>SK</given-names></name><etal/></person-group><article-title xml:lang="en">Mechanochemical synthesis and thermoelectric properties of high quality magnesium silicide</article-title><source>J. Mater. Chem.</source><year>2011</year><volume>21</volume><fpage>12259</fpage><lpage>12266</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXpvFCrtLc%3D</pub-id><pub-id pub-id-type="doi">10.1039/c1jm10827a</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sakurada</surname><given-names>S</given-names></name><name><surname>Shutoh</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Effect of ti substitution on the thermoelectric properties of (zr,hf)nisn half-heusler compounds</article-title><source>Appl. Phys. Lett.</source><year>2005</year><volume>86</volume><fpage>082105</fpage><pub-id pub-id-type="doi">10.1063/1.1868063</pub-id><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2MXis1Wit7k%3D</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavanaei</surname><given-names>A</given-names></name><name><surname>Ghodrati</surname><given-names>M</given-names></name><name><surname>Kheradpisheh</surname><given-names>SR</given-names></name><name><surname>Masquelier</surname><given-names>T</given-names></name><name><surname>Maida</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep learning in spiking neural networks</article-title><source>Neural Netw.</source><year>2019</year><volume>111</volume><fpage>47</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2018.12.002</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bian</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name></person-group><article-title xml:lang="en">Neural network for nonsmooth, nonconvex constrained minimization via smooth approximation</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2014</year><volume>25</volume><fpage>545</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2013.2278427</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Weinberger, K. Q., Blitzer, J. &amp; Saul, L. K. Distance metric learning for large margin nearest neighbor classification. In <italic>Conference on Neural Information Processing Systems (NIPS)</italic> (MIT Press, 2009).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaultois</surname><given-names>MW</given-names></name><etal/></person-group><article-title xml:lang="en">Data-driven review of thermoelectric materials: performance and ressource considerations</article-title><source>Chem. Mater.</source><year>2013</year><volume>25</volume><fpage>2911</fpage><lpage>2920</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3sXntFWnsLc%3D</pub-id><pub-id pub-id-type="doi">10.1021/cm400893e</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Visualizing data using t-sne</article-title><source>J. Mach. Learn. Res.</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Baldi, P. Autoencoders, unsupervised learning and deep architectures. In <italic>Proceedings of the 2011 International Conference on Unsupervised and Transfer Learning Workshop - Volume 27</italic>, UTLW’11, 37-50 (JMLR.org, 2011).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forman</surname><given-names>C</given-names></name><name><surname>Muritala</surname><given-names>I</given-names></name><name><surname>Pardemann</surname><given-names>R</given-names></name><name><surname>Meyer</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Estimating the global waste heat potential</article-title><source>Renew. Sust. Energy Rev.</source><year>2016</year><volume>57</volume><fpage>1568</fpage><lpage>1579</lpage><pub-id pub-id-type="doi">10.1016/j.rser.2015.12.192</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seebeck</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Ueber die magnetische polarisation der metalle und erze durch temperatur-diferenz</article-title><source>Ann. Phys.</source><year>1826</year><volume>82</volume><fpage>133</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1002/andp.18260820202</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snyder</surname><given-names>GJ</given-names></name><name><surname>Toberer</surname><given-names>ES</given-names></name></person-group><article-title xml:lang="en">Complex thermoelectric materials</article-title><source>Nat. Mater.</source><year>2008</year><volume>7</volume><fpage>105</fpage><lpage>114</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXhtVGltbk%3D</pub-id><pub-id pub-id-type="doi">10.1038/nmat2090</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Julio Gutiérrez Moreno, J., Cao, J., Fronzi, M. &amp; Assadi, M.H.N. A review of recent progress in thermoelectric materials through computational methods. <italic>Mater. Renew. Sustain. Energy</italic><bold>9</bold>, 16 (2020).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Awad, M. &amp; Khanna, R. Support vector regression. <italic>Efficient Learning Machines.</italic> (Springer, 2015).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXht1WlurzP</pub-id><pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Wilson, A. G. &amp; Adams, R. P. Gaussian process kernels for pattern discovery and extrapolation. In <italic>Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28</italic>, ICML’13 (JMLR.org, 2013).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Accelerated discovery of stable spinels in energy systems via machine learning</article-title><source>Nano Energy</source><year>2021</year><volume>81</volume><fpage>105665</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXis1ShtbfE</pub-id><pub-id pub-id-type="doi">10.1016/j.nanoen.2020.105665</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sheridan</surname><given-names>RP</given-names></name><name><surname>Wang</surname><given-names>WM</given-names></name><name><surname>Liaw</surname><given-names>A</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Gifford</surname><given-names>EM</given-names></name></person-group><article-title xml:lang="en">Extreme gradient boosting as a method for quantitative structure-activity relationships</article-title><source>J. Chem. Inf. Model.</source><year>2016</year><volume>56</volume><fpage>2353</fpage><lpage>2360</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhvFCgs73E</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.6b00591</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Draper, N. R. &amp; Smith, H. <italic>Applied Regression Analysis, 3rd ed.</italic> (Wiley-Interscience, 1998).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rothschild</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Environment dominates over host genetics in shaping human gut microbiota</article-title><source>Nature</source><year>2018</year><volume>555</volume><fpage>210</fpage><lpage>215</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXjsFaitLg%3D</pub-id><pub-id pub-id-type="doi">10.1038/nature25973</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">A data-driven design for fault detection of wind turbines using random forests and xgboost</article-title><source>IEEE Acess</source><year>2018</year><volume>6</volume><fpage>21020</fpage><lpage>21031</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2018.2818678</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jood</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">Na doping in pbte: solubility, band convergence, phase boundary mapping, and thermoelectric properties</article-title><source>J. Am. Chem. Soc.</source><year>2020</year><volume>142</volume><fpage>15464</fpage><lpage>15475</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFGisrbO</pub-id><pub-id pub-id-type="doi">10.1021/jacs.0c07067</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasan</surname><given-names>MN</given-names></name><name><surname>Wahid</surname><given-names>H</given-names></name><name><surname>Nayan</surname><given-names>N</given-names></name><name><surname>Mohamed Ali</surname><given-names>MS</given-names></name></person-group><article-title xml:lang="en">Inorganic thermoelectric materials: a review</article-title><source>Int. J. Energy Res.</source><year>2020</year><volume>44</volume><fpage>6170</fpage><lpage>6222</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhtFKjsLjO</pub-id><pub-id pub-id-type="doi">10.1002/er.5313</pub-id></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Xu, K. et al. How neural networks extrapolate: From feedforward to graph neural networks. In <italic>International Conference on Learning Representations</italic> (2021).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>T</given-names></name><name><surname>Xie</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Oganov</surname><given-names>AR</given-names></name><name><surname>Cheng</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">First-principles study of thermoelectric properties of Mg<sub>2</sub>Si-Mg<sub>2</sub>2Pb semiconductor materials</article-title><source>RSC Adv.</source><year>2018</year><volume>8</volume><fpage>17168</fpage><lpage>17175</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXptFSrs70%3D</pub-id><pub-id pub-id-type="doi">10.1039/C8RA02436G</pub-id></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pőhls</surname><given-names>J-H</given-names></name><etal/></person-group><article-title xml:lang="en">Experimental validation of high thermoelectric performance in RECuZn<italic>P</italic><sub>2</sub> predicted by high-throughput dft calculations</article-title><source>Mater. Horiz.</source><year>2021</year><volume>8</volume><fpage>209</fpage><lpage>215</lpage><pub-id pub-id-type="doi">10.1039/D0MH01112F</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohn</surname><given-names>W</given-names></name><name><surname>Sham</surname><given-names>LJ</given-names></name></person-group><article-title xml:lang="en">Self-consistent equations including exchange and correlation effects</article-title><source>Phys. Rev.</source><year>1965</year><volume>140</volume><fpage>A1133</fpage><lpage>A1138</lpage><pub-id pub-id-type="doi">10.1103/PhysRev.140.A1133</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Kingma, D. P. &amp; Welling, M. Auto-encoding variational bayes. In International Conference on Learning Representations (ICLR) (2014).</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname><given-names>N</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Salakhutdinov</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Dropout: A simple way to prevent neural networks from overfitting</article-title><source>J. Mach. Learn. Res.</source><year>2014</year><volume>15</volume><fpage>1929</fpage><lpage>1958</lpage></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. L. Adam: A method for stochastic optimization.In <italic>International Conference on Learning Representations (ICLR)</italic> (2015).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Bottou, L. Large-scale machine learning with stochastic gradient descent. In <italic>COMPSTAT.</italic> (Physica-Verlag HD, 2010).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Python mendeleev package. <ext-link xlink:href="https://github.com/lmmentel/mendeleev" ext-link-type="uri">https://github.com/lmmentel/mendeleev</ext-link> (2020). Accessed 12 March 2021.</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Agarap, A. F. Deep learning using rectified linear units (ReLU). Preprint at <ext-link xlink:href="https://arxiv.org/abs/1803.08375" ext-link-type="uri">https://arxiv.org/abs/1803.08375</ext-link> (2018).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biswas</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">High-performance bulk thermoelectrics with all-scale hierarchical architectures</article-title><source>Nature</source><year>2012</year><volume>489</volume><fpage>414</fpage><lpage>418</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhtlyltrzN</pub-id><pub-id pub-id-type="doi">10.1038/nature11439</pub-id></mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pei</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Convergence of electronic bands for high performance bulk thermoelectrics</article-title><source>Nature</source><year>2011</year><volume>473</volume><fpage>66</fpage><lpage>69</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXls1Kgu7o%3D</pub-id><pub-id pub-id-type="doi">10.1038/nature09996</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Rosenfeld</surname><given-names>HD</given-names></name><name><surname>Subramanian</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Thermoelectric properties of indium-filled skutterudites</article-title><source>Chem. Mater.</source><year>2006</year><volume>18</volume><fpage>759</fpage><lpage>762</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD28XjsFCnsw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/cm052055b</pub-id></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heremans</surname><given-names>JP</given-names></name><etal/></person-group><article-title xml:lang="en">Enhancement of thermoelectric efficiency in pbte by distortion of the electronic density of states</article-title><source>Science</source><year>2008</year><volume>321</volume><fpage>554</fpage><lpage>557</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXoslGhtrg%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.1159725</pub-id></mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Skrabek, E. Properties of the general tags system. In <italic>CRC Handbook of Thermoelectrics</italic>, 267–275 (CRC Press,1995).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec14"><title>Supplementary information</title><p id="Par49"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2021_564_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-021-00564-y" ext-link-type="doi">https://doi.org/10.1038/s41524-021-00564-y</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2021</facet-value></facet><facet name="country"><facet-value count="1">South Korea</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
