<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41528-022-00137-z</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41528</journal-id><journal-id journal-id-type="doi">10.1038/41528.2397-4621</journal-id><journal-title-group><journal-title>npj Flexible Electronics</journal-title><abbrev-journal-title abbrev-type="publisher">npj Flex Electron</abbrev-journal-title></journal-title-group><issn pub-type="epub">2397-4621</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41528-022-00137-z</article-id><article-id pub-id-type="manuscript">137</article-id><article-id pub-id-type="doi">10.1038/s41528-022-00137-z</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/166/987</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/925/927/511</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Flexible computational photodetectors for self-powered activity sensing</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-3603-2453</contrib-id><name><surname>Zhang</surname><given-names>Dingtian</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7958-3699</contrib-id><name><surname>Fuentes-Hernandez</surname><given-names>Canek</given-names></name><address><email>c.fuentes@northeastern.edu</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="corresp" rid="IDs4152802200137z_cor2">b</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Vijayan</surname><given-names>Raaghesh</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Zhang</surname><given-names>Yang</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au5"><name><surname>Li</surname><given-names>Yunzhi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au6"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0360-1085</contrib-id><name><surname>Park</surname><given-names>Jung Wook</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au7"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5778-3251</contrib-id><name><surname>Wang</surname><given-names>Yiyang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au8"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8976-2041</contrib-id><name><surname>Zhao</surname><given-names>Yuhui</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au9"><name><surname>Arora</surname><given-names>Nivedita</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au10"><name><surname>Mirzazadeh</surname><given-names>Ali</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au11"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5403-0828</contrib-id><name><surname>Do</surname><given-names>Youngwook</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au12"><name><surname>Cheng</surname><given-names>Tingyu</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au13"><name><surname>Swaminathan</surname><given-names>Saiganesh</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" id="Au14"><name><surname>Starner</surname><given-names>Thad</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au15"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8193-2912</contrib-id><name><surname>Andrew</surname><given-names>Trisha L.</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" id="Au16"><name><surname>Abowd</surname><given-names>Gregory D.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.213917.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 4943</institution-id><institution content-type="org-division">School of Interactive Computing</institution><institution content-type="org-name">Georgia Institute of Technology</institution></institution-wrap><addr-line content-type="city">Atlanta</addr-line><addr-line content-type="state">GA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.213917.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 4943</institution-id><institution content-type="org-division">School of Electrical and Computer Engineering</institution><institution content-type="org-name">Georgia Institute of Technology</institution></institution-wrap><addr-line content-type="city">Atlanta</addr-line><addr-line content-type="state">GA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.261112.7</institution-id><institution-id institution-id-type="ISNI">0000 0001 2173 3359</institution-id><institution content-type="org-division">Department of Electrical and Computer Engineering</institution><institution content-type="org-name">Northeastern University</institution></institution-wrap><addr-line content-type="city">Boston</addr-line><addr-line content-type="state">MA</addr-line><country country="US">USA</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.266683.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2166 5835</institution-id><institution content-type="org-division">Department of Chemistry</institution><institution content-type="org-name">University of Massachusetts Amherst</institution></institution-wrap><addr-line content-type="city">Amherst</addr-line><addr-line content-type="state">MA</addr-line><country country="US">USA</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.19006.3e</institution-id><institution-id institution-id-type="ISNI">0000 0000 9632 6718</institution-id><institution content-type="org-division">Department of Electrical and Computer Engineering</institution><institution content-type="org-name">University of California</institution></institution-wrap><addr-line content-type="city">Los Angeles</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Human-Computer Interaction Institute</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.266683.f</institution-id><institution-id institution-id-type="ISNI">0000 0001 2166 5835</institution-id><institution content-type="org-division">Departments of Chemistry and Chemical Engineering</institution><institution content-type="org-name">University of Massachusetts Amherst</institution></institution-wrap><addr-line content-type="city">Amherst</addr-line><addr-line content-type="state">MA</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs4152802200137z_cor2"><label>b</label><email>c.fuentes@northeastern.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>27</day><month>1</month><year>2022</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2022</year></pub-date><volume>6</volume><issue seq="7">1</issue><elocation-id>7</elocation-id><history><date date-type="registration"><day>5</day><month>1</month><year>2022</year></date><date date-type="received"><day>10</day><month>7</month><year>2021</year></date><date date-type="accepted"><day>21</day><month>12</month><year>2021</year></date><date date-type="online"><day>27</day><month>1</month><year>2022</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2022</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Conventional vision-based systems, such as cameras, have demonstrated their enormous versatility in sensing human activities and developing interactive environments. However, these systems have long been criticized for incurring privacy, power, and latency issues due to their underlying structure of pixel-wise analog signal acquisition, computation, and communication. In this research, we overcome these limitations by introducing in-sensor analog computation through the distribution of interconnected photodetectors in space, having a weighted responsivity, to create what we call a computational photodetector. Computational photodetectors can be used to extract mid-level vision features as a single continuous analog signal measured via a two-pin connection. We develop computational photodetectors using thin and flexible low-noise organic photodiode arrays coupled with a self-powered wireless system to demonstrate a set of designs that capture position, orientation, direction, speed, and identification information, in a range of applications from explicit interactions on everyday surfaces to implicit activity detection.</p></abstract><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>7</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>5</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41528_2022_Article_137.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Optical and Electronic Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Polymer Sciences</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Electronics and Microelectronics, Instrumentation</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">The realization of ubiquitous interactive environments that enable human activity sensing in a smart home, hospital, and other privacy-sensitive environments<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup> or asset tracking in an unmanned retail store<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, require autonomous optical sensing technologies that are more sustainable, scalable, conformal, and privacy-preserving when compared to current camera-based approaches<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR7">7</xref></sup> and von Neumann architectures. New materials and device architectures are needed for such pervasive sensing technologies, as Mark Weiser put it, to disappear and weave into the fabric of everyday life<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>.</p><p id="Par3">Recent work of light-sensing surfaces has opened a new design space that integrates sensing and computation into everyday objects we interact with, enabling a myriad of applications for gestural input to activity recognition without physical contact<sup><xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup>. Flexible photodetectors based on emerging semiconductors<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, such as organic photodiodes (OPDs), are desirable to develop light-sensing surfaces because they can be manufactured over large areas<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, patterned into arbitrary shapes<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR17">17</xref></sup>, and with conformal form factors<sup><xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR25">25</xref></sup>. Despite progress, for light-sensing surfaces to be scalable, they need to maintain low power consumption and latency, regardless of the total area and the number of photosensitive elements on their surface. It is also important that they operate under a wide range of lighting conditions and preserve privacy.</p><p id="Par4">Some of these challenges can be addressed by in-sensor computation, i.e., by processing the sensory data in the analog domain prior to analog-digital conversion (ADC) and digital computation<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Low-level data aggregation or feature extraction performed in the analog domain can compress the high-dimensional raw data acquired from a large array of sensors into lower dimensions and reduce their inherent redundancies<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. In lieu of digitizing and outputting raw sensor signals, such designs also reduce unwanted capturing of private information. By inextricably integrating the sensing and computing components as ‘computational sensors’, we can more easily achieve low-latency, low power, and privacy-preserving sensory systems.</p><p id="Par5">In this paper, we present computational photodetectors and demonstrate applications in ubiquitous human activity sensing. Herein, a computational photodetector achieves in-sensor analog computation by combining the weighted output from a heterogeneous array of photodiodes distributed in space, into a two-pin readout for time-resolved differential measurement (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a), from which mid-level vision features that encode position, orientation, direction, speed, and identification information can be extracted. Key advantages of this approach are: power consumption and latency for acquiring and processing the signal from a computational photodetector are invariant to the pattern design, size, shape, number of photodiodes in the array, or their distribution in space, supporting large-scale sensing applications. Also, since no images are captured or stored, and the temporal evolution of the signal encodes the arbitrary distribution in space of a heterogeneous array of photodiodes, as well as the particular motion it detects, they are expected to be resilient to eavesdropping attacks and capable of protecting user privacy by keeping sensitive data and computation at the hardware level. We demonstrate computational photodetectors using thin and flexible low-noise OPDs that operate even under dimly-lit environments (≪1 lux and down to tens of nanowatt cm<sup>−2</sup> in the visible spectral range). In addition to thin and flexible form factors, the potential of achieving low-noise silicon photodiode-like performance<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, and the possibility to further tailor their properties through synthetic chemistry, make OPD attractive to implement light-sensing surfaces based on computational photodetectors.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Principle and design.</title><p><bold>a</bold> Computational photodetector with a pattern design produces a motion-incited output photocurrent, which corresponds to the convolution between the signed heterogeneous responsivity distribution and the incident optical power along the time axis. <bold>b</bold> The equivalent circuit of a typical computational photodetector without or with one photodiode blocked, and the corresponding current output (mean and standard deviation) versus the number of photodiodes connected in parallel with a single photodiode blocked at any given time. <bold>c</bold> Design space of a computational photodetector, which connects the target information, to relevant design parameters, and finally the pattern design. In this work, we demonstrate and evaluate the linear, crossing, and grid patterns as representatives.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig1_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><sec id="Sec3"><title>Principles of operation</title><p id="Par6">A computational photodetector is a parallelly connected photodiode array having a two-pin differential readout. Photodiodes are distributed on the surfaces of objects in space at locations <bold>r</bold><sub><italic>i</italic></sub> with a signed spectral responsivity <italic>R</italic><sub><italic>λ</italic>,<italic>i</italic></sub> that accounts for the polarities in which devices are connected with respect to others. Photodiodes connected with opposite polarities, as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, produce photocurrents flowing in different directions and thus of opposite signs. The spectral responsivity <bold>R</bold><sub><italic>λ</italic></sub> of the computational photodetector is characterized by a 1 × <italic>M</italic> vector of the form:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.33em"/><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mspace width="0.33em"/><mml:mo>…</mml:mo><mml:mspace width="0.33em"/><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\bf{R}}}}}_{\lambda }=\left[{R}_{\lambda ,1}\ {R}_{\lambda ,2}\ \ldots\ {R}_{\lambda ,M}\right]$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_Equ1.gif"/></alternatives></disp-formula>Similarly, a vector <bold>Φ</bold><sub><italic>λ</italic></sub>(<italic>t</italic>) of the time-dependent spectral optical power Φ<sub><italic>λ</italic>,<italic>i</italic></sub>(<italic>t</italic>) received by an individual photodiode of area <italic>A</italic><sub><italic>i</italic></sub> at location <bold>r</bold><sub><italic>i</italic></sub> can be defined as:<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.33em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.33em"/><mml:mo>…</mml:mo><mml:mspace width="0.33em"/><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\bf{\Phi}}_{\lambda }(t)=\left[{{{\Phi }}}_{\lambda ,1}(t)\ {{{\Phi }}}_{\lambda ,2}(t)\ \ldots \ {{{\Phi }}}_{\lambda ,M}(t)\right]$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_Equ2.gif"/></alternatives></disp-formula>The output current <italic>I</italic><sub>out</sub> of the computational photodetector follows Kirchhoff’s law, and can be described as a summation of pairwise products of <bold>R</bold><sub><italic>λ</italic></sub> and <bold>Φ</bold><sub><italic>λ</italic></sub>(<italic>t</italic>) (Hadamard products) integrated over the relevant spectral range:<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">out</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.33em"/><mml:mi>d</mml:mi><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${I}_{{{{\rm{out}}}}}(t)=\int \mathop{\sum }\limits_{i=1}^{M}{R}_{\lambda ,i}{{{\Phi }}}_{\lambda ,i}(t)\ d\lambda =\mathop{\sum }\limits_{i=1}^{M}{I}_{i}(t)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par7">Therefore, a computational photodetector achieves a weighted linear combination task within the analog domain. Knowledge of <bold>R</bold><sub><italic>λ</italic></sub> and the spatial location of each photodiode is needed to decode the information from <italic>I</italic><sub>out</sub>. Hence, a computational photodetector does not produce an ‘image’ of the optical power distribution in space. Since data acquisition and computation happen at the same time, they do not scale with the complexity of the pattern design, resulting in overall complexity of <italic>O</italic>(1). In comparison, conventional computing hierarchies that acquire and process the output from each individual array element involve: (1) digitizing analog signals, which takes <italic>M</italic> operations; (2) pairwise integer multiplication, which takes <italic>M</italic> operations; and (3) summation of <italic>M</italic> integers, which takes <italic>M</italic> − 1 operations, resulting in overall complexity of <italic>O</italic>(<italic>M</italic>).</p></sec><sec id="Sec4"><title>Design of computational photodetectors</title><p id="Par8">Flexible photodiodes, which allow the shape, size, distribution, and interconnection of individual photodiodes to be tuned in a scalable way, are ideal for the implementation of computational photodetectors. Their designs depend upon the type(s) of sensing of interest. The design space is very broad as shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>c. Here we present a brief discussion of the following parameters:</p><sec id="Sec5"><title>Interconnection and sensor output</title><p id="Par9">We use photodiodes connected in parallel with the same absolute spectral responsivity, i.e., ∣<italic>R</italic><sub><italic>λ</italic>,<italic>i</italic></sub>∣ = ∣<italic>R</italic><sub><italic>λ</italic>,<italic>j</italic></sub>∣. The example in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b consists an even number of identical devices (<italic>M</italic> = 2<italic>n</italic> with <italic>n</italic> an integer) with pairwise opposite polarities, i.e., <italic>R</italic><sub><italic>λ</italic>,<italic>i</italic></sub> = −<italic>R</italic><sub><italic>λ</italic>,<italic>j</italic></sub>. In such case, <italic>I</italic><sub>out</sub>(<italic>t</italic><sub>0</sub>) = 0 if the whole array is in the dark or under uniform illumination, and non-zero, e.g., <italic>I</italic><sub>out</sub> =<italic>I</italic><sub><italic>i</italic></sub>(<italic>t</italic>), if a photodiode at <bold>r</bold><sub><italic>i</italic></sub> is blocked, and <italic>I</italic><sub>out</sub> = − <italic>I</italic><sub><italic>i</italic>+1</sub>(<italic>t</italic>) if another photodiode at <bold>r</bold><sub><italic>i</italic>+1</sub> is blocked. Hence, the sign of the output signal in these types of arrays can be used as binary codes (e.g., <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(1\ {-}1\ 1\ {-}1\right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq1.gif"/></alternatives></inline-formula>). The measurements show that the combined photocurrent output is unaffected by the number of connected devices. We can also use an unbalanced design with a direct-circuit (DC) bias, e.g., <italic>I</italic><sub>out</sub>(<italic>t</italic><sub>0</sub>) = <italic>I</italic><sub>0</sub>, to capture the uniform ambient light illumination.</p></sec><sec id="Sec6"><title>Device shape</title><p id="Par10">There are two main design strategies of device shapes—continuous and discrete. A continuous design uses a large, irregular, and asymmetric shape to generate a distinct ‘analog’ signal profile from object motion. A discrete design uses small area photodiodes that generate a ‘digitized’ signal as described in the previous section. The shape of the photodiodes is not important since their size is assumed to be significantly smaller than the object size—but large enough to produce a photocurrent above the sensor noise. Although the continuous approach can be more aesthetically appealing, the discrete approach is more practical and robust from fabrication and signal processing perspectives.</p></sec><sec id="Sec7"><title>Device size</title><p id="Par11">Instead of having a uniform device area, photodiodes can also modulate their photocurrent output by controlling their sizes <italic>A</italic><sub>i</sub>, where <italic>A</italic><sub><italic>i</italic></sub> = <italic>η</italic><sub><italic>i</italic></sub><italic>A</italic><sub><italic>j</italic></sub> such that ∣<italic>I</italic><sub><italic>i</italic></sub>(<italic>t</italic>)∣ = <italic>η</italic><sub>i</sub>∣<italic>I</italic><sub><italic>j</italic></sub>(<italic>t</italic>)∣, <italic>η</italic><sub>i</sub> &gt; 0. This allows the combination of sign and magnitude to extend beyond binary encoding.</p></sec><sec id="Sec8"><title>Spatial layout</title><p id="Par12">Patterns in 1D (e.g., a straight line, a curve, or a circle), 2D (e.g., a matrix, or a checkerboard), or 3D (e.g., faces of a 3D object or an origami structure) can be used to extract spatial information of motion, position, and orientation.</p></sec><sec id="Sec9"><title>Interspace</title><p id="Par13">The sensing application determines the interspace: <italic>d</italic><sub><italic>i</italic></sub> = ∣<bold>r</bold><sub><italic>i</italic>+1</sub> − <bold>r</bold><sub><italic>i</italic></sub>∣. To sense the position or speed of an object, <italic>d</italic><sub><italic>i</italic></sub> should be greater than the size of the shadow projected by the object onto the photodetector plane, <italic>h</italic><sub>obj</sub>, such that the object position is registered to a single photodiode. If <italic>d</italic><sub><italic>i</italic></sub> ≤ <italic>h</italic><sub>obj</sub>, the ‘entry’ or ‘exit’ can still be detected as the object begins to cover or uncover multiple devices.</p><p id="Par14">In the remainder of this section, we present linear, crossing, and grid patterns implemented with flexible OPDs as representatives of the design space that covers a full spectrum of motion, position, and orientation sensing. We discuss the design strategy for each pattern based on simulations, and evaluate their effectiveness against factors such as sensing distance, ambient light, object size, etc., in realistic settings.</p></sec></sec><sec id="Sec10"><title>Linear pattern</title><p id="Par15">Figure <xref rid="Fig2" ref-type="fig">2</xref>a shows the architecture of the flexible OPDs, which have a spectral responsivity in the red to the near-infrared spectral region (Fig. <xref rid="Fig2" ref-type="fig">2</xref>b) and low dark current values under reverse bias (Fig. <xref rid="Fig2" ref-type="fig">2</xref>c), allowing them to operate under dim illumination conditions. A linear pattern of two OPDs connected in parallel and opposite polarity with a user blocking each OPD is shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>d. Figure <xref rid="Fig2" ref-type="fig">2</xref>f shows the sensor output in response to sequentially occluding each OPD in a different order or direction. Figure <xref rid="Fig2" ref-type="fig">2</xref>f also shows that these gestures could be resolved with a high signal-to-noise at irradiance values down to 2.8 nW cm<sup>−2</sup>, where only a thermal camera could be used to capture the interaction, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>e. The noise in the dark and under homogeneous illumination was found to have a median value of 150 fA, as shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>g. Hence these OPDs operate under illumination conditions that the human eye perceives as near-darkness, opening the possibility of achieving computational photodetectors that operate under a wide dynamic range of illumination conditions that are relevant for human activity.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Device and characteristics.</title><p><bold>a</bold> Schematics of a single organic photodiode (OPD) used in a computational photodetector. <bold>b</bold> The spectral responsivity of the OPD. <bold>c</bold> The current-voltage (<italic>I–V</italic>) characteristics of the OPD in the dark and at 270 lux under fluorescent room lights. <bold>d</bold> Photographs of a computational photodetector—a pair of OPDs connected in parallel and opposite polarity, under ~100 nW cm<sup>−2</sup> optical power of red LED with 635 nm peak emission and close proximity interactions with each OPD. <bold>e</bold> A thermal image of the interaction under ~3 nW cm<sup>−2</sup> optical power of the same 635 nm LED; conditions that are too dark for a regular camera sensor to capture. <bold>f</bold> Distinctive signals from finger swiping in down-up and up-down directions are well above the noise floor. <bold>g</bold> Distribution of root-mean-squared noise current values measured during the experiments in <bold>f</bold>.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig2_HTML.png"/></fig></p><p id="Par16">Next, we extended the linear pattern to four OPDs in a 1D sequence to study in detail the linear motion of an object (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). Mathematically, this pattern can be compared to a series of line detecting kernels, which ‘convolves’ with the image of a passing object. For instance, if the object can be represented as <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mfenced close=")" open="("><mml:mrow><mml:mn>0</mml:mn><mml:mspace width="0.33em"/><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mn>0</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(0\ 1\ 0\right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq2.gif"/></alternatives></inline-formula> and the normalized signed output of the linear OPD pattern as <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mfenced close=")" open="("><mml:mrow><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mn>1</mml:mn><mml:mspace width="0.33em"/><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(1\ {-}1\ 1\ {-}1\right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq3.gif"/></alternatives></inline-formula>, we can acquire the sequence of spikes {1, −1, 1, 1} as the 1D convolution result and detect the following motion information:<list list-type="bullet"><list-item><p id="Par17">Direction can be detected by the sequence of spikes, e.g., {−1, 1, −1, 1} and {1, −1, 1, −1} corresponding to motion in opposite directions.</p></list-item><list-item><p id="Par18">Speed <italic>v</italic>, and acceleration Δ<italic>v</italic>, can be detected by dividing the photodiode interspace <italic>d</italic> and the duration between neighbouring spikes <italic>t</italic> as <italic>v</italic> = <italic>d</italic><italic>t</italic><sup>−1</sup>, and obtaining first derivatives Δ<italic>v</italic> = <italic>v</italic><sub><italic>i</italic>+1</sub> − <italic>v</italic><sub><italic>i</italic></sub>.</p></list-item><list-item><p id="Par19">Identification of the sensors can be achieved by having distinctive patterns of 1 and −1. An equal number of OPDs in opposite polarity is connected in asymmetric sequences to distinguish one direction from another.</p></list-item></list><fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Linear pattern.</title><p><bold>a</bold> Design and prototype of a computational photodetector with a linear pattern. <bold>b</bold> Simulation of responses from a linear pattern with alternating-polarity photodiodes and an interspace of 4 cm when an object hovers over with varying object sizes or sensing distances. <bold>c</bold> Experiment setup of technical evaluation using a linear actuator and laser-cut wooden bars with variable widths for light blocking. <bold>d</bold> Sensor output in various conditions of ambient light, sensing distance, object size, and object speed. <bold>e</bold> Speed detection result for the above conditions.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig3_HTML.png"/></fig></p><p id="Par20">We simulate the signal output versus device interspace and sensing distance to facilitate the computational photodetector design as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>b.</p><p id="Par21">Based on the simulations, a linear-pattern computational photodetector using OPDs with a device area of 4 × 4 mm and interspace of 4 cm were fabricated (Fig. <xref rid="Fig3" ref-type="fig">3</xref>a). To evaluate its capability in detecting the linear motion of an object, we develop a test rig based on a linear actuator (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c) and precisely control the test conditions, such as motion speed, sensing distance, object size, and ambient light illumination.</p><p id="Par22">We apply noise filtering to the sensor output to remove the power-line noise and the direct-current (DC) component. Figure <xref rid="Fig3" ref-type="fig">3</xref>d shows the filtered waveform of sensor signal upon two motion events in opposite directions. The distinctive spikes of {−1, 1, −1, 1} and {1, −1, 1, −1} can be seen at 75 lux. The effect of sensing distance and object size aligns well with our simulations. We segment the signal based on its magnitude and spike–spike interval, apply peak detection to the segment, and derive the speed of the object motion. As shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>e, our sensor can accurately detect the motion speed under various conditions with the highest error of −0.053 m s<sup>−1</sup> and a standard deviation of 0.09 m s<sup>−1</sup> at 20 mm sensing distance.</p></sec><sec id="Sec11"><title>Crossing pattern</title><p id="Par23">A crossing pattern can be viewed as a 2D matrix of OPDs designed to invoke distinctive signal sequences in response to a larger object approaching or departing in different 2D directions, blocking or unblocking multiple OPDs (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a). We can draw an analogy to an edge detection kernel. For example, an 8 × 8 asymmetric matrix<disp-formula id="Equa"><alternatives><mml:math id="Equa_Math"><mml:mfenced close=")" open="("><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><tex-math id="Equa_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(\begin{array}{llllllll}0&amp;0&amp;-1&amp;0&amp;0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;1&amp;0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;1\\ 0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;-1&amp;0\\ 0&amp;-1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\ 1&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0&amp;-1&amp;0&amp;0&amp;0\\ 0&amp;0&amp;0&amp;0&amp;0&amp;1&amp;0&amp;0\end{array}\right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_Equa.gif"/></alternatives></disp-formula>can generate a sequence of {1, −1, −1, −1} when a large object enters from the left and sequentially blocks the OPDs of each column, {−1, 1, 1, 1} when it exits to the right; so forth for all 8 unique sequences corresponding to 4 motion directions (top-bottom, bottom-top, left-right, right-left) × 2 moments (entry and exit). Therefore, we can acquire the following motion information:<list list-type="bullet"><list-item><p id="Par24">2D direction can be determined by the sequence of spikes, which is grouped by a set of 4 unique sequences: {original, reverse-order, inverse-polarity, reverse-order, and inverse-polarity}, corresponding to 4 motion events along one of the two axes of the sensor. The minimum length of a sequence <italic>n</italic> is 4. To translate the sequence to the pattern, we use 2<italic>n</italic> number of OPDs distributed in a 2<italic>n</italic> × 2<italic>n</italic> matrix with one device per each row or column. OPDs of opposite polarity are used for every two columns or rows (i.e., a ‘−1’ followed by an immediate ‘−1’ and vice versa) to ‘reset’ the output for the next spike.</p></list-item><list-item><p id="Par25">2D speed and identification can be detected at the moment of entry or exit for both axes, based on the eight distinctive sequences of length <italic>n</italic>.</p></list-item></list><fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Crossing pattern.</title><p><bold>a</bold> Design of a computational photodetector with a crossing pattern. <bold>b</bold> A prototype with an interspace of 1 cm and overall size of 8 × 8 cm. <bold>c</bold> Simulation of the computational photodetector responding to an object of 10 and 20 cm width moving in the horizontal and vertical direction at a height of 0.25 and 0.5 mm. <bold>d</bold> Sensor output from all the motion events with varying object speed, under 250 lux illumination. Under 500 and 750 lux illumination, the signal has a similar envelope with slightly larger magnitudes. The object size is fixed at 16 cm and the sensing range is 0.25 cm for all conditions. <bold>e</bold> Speed detection result for the various conditions of ambient light and object speed.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig4_HTML.png"/></fig></p><p id="Par26">We simulate the output of the 8 × 8 asymmetric matrix in the example in response to the 2D motion of a large object (Fig. <xref rid="Fig4" ref-type="fig">4</xref>c). Based on these simulations, we develop a crossing-pattern computational photodetector using OPDs with a column or row interspace of 1 cm (Fig. <xref rid="Fig4" ref-type="fig">4</xref>b).</p><p id="Par27">Figure <xref rid="Fig4" ref-type="fig">4</xref>d, e shows that using a characterization method similar to the one used for the linear arrays, a computational photodetector with a crossing pattern can accurately detect motion and speed in 4 directions and under various conditions, with the highest error of 0.06 m s<sup>−1</sup> and standard deviation of 0.10 m s<sup>−1</sup> at the exit right event for an object moving at 0.6 m s<sup>−1</sup> under 750 lux ambient light illumination.</p></sec><sec id="Sec12"><title>Grid pattern</title><p id="Par28">A grid pattern is a distribution of OPDs with distinct photocurrent values that can be mapped to their position or orientation (Fig. <xref rid="Fig5" ref-type="fig">5</xref>a). For instance, a grid pattern designed as<disp-formula id="Equb"><alternatives><mml:math id="Equb_Math"><mml:mfenced close=")" open="("><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>4</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="left"><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math><tex-math id="Equb_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left(\begin{array}{lllll}0&amp;2&amp;0&amp;4&amp;0\\ -1&amp;0&amp;-2&amp;0&amp;-4\end{array}\right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_Equb.gif"/></alternatives></disp-formula>can detect any of its 5 OPDs being occluded from ambient light based on the steady-state signal output. The grid pattern allows detection of:<list list-type="bullet"><list-item><p id="Par29">2D position from the one-to-one mapping of the sensor output and the corresponding OPD. To account for different ambient light conditions, an unbalanced design that naturally exhibits a DC bias in the unblocked state is used to calibrate the output signal and remove the factor of ambient light illumination. The given example has a DC bias of 1.</p></list-item><list-item><p id="Par30">3D orientation when OPDs are mounted on an object facing different directions. A simple example would be two OPDs of opposite polarity (i.e., (−1, 1)) on the top and bottom sides of an object: large magnitudes indicate either the top or bottom side is placed facing down, while small magnitudes (close to zero) indicate the object is placed sideways. The same setup can also be used for shake or impact detection from high-frequency components of the signal due to rapid orientation changes.</p></list-item></list><fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Grid pattern.</title><p><bold>a</bold> Design of a computational photodetector with a grid pattern. <bold>b</bold> A prototype with an increasing device area and alternating polarity from left to right. <bold>c</bold> Sensor output and object position detection result based on various conditions of ambient light. <bold>d</bold> Sensor output and finger position detection result based on various conditions of ambient light. The same thresholds from the object position detection are used without modification.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig5_HTML.png"/></fig></p><p id="Par31">To implement a grid design, the photocurrent was controlled by using OPDs with different areas. Assuming that objects can only block a single OPD at a time, we develop a grid-pattern computational photodetector using OPDs with areas of 12, 24, and 48 mm<sup>2</sup> (Fig. <xref rid="Fig5" ref-type="fig">5</xref>b).</p><p id="Par32">To extract the object position, we use the unblocked-state signal under ambient light to calibrate the signal and apply a series of thresholds on the calibrated signal. Figure <xref rid="Fig5" ref-type="fig">5</xref>c shows the raw and calibrated signals measured for different object positions. An empirically chosen threshold of <italic>τ</italic><sub>0</sub> = 7 and a set of threshold values { ±<italic>τ</italic><sub>0</sub>, ±2<italic>τ</italic><sub>0</sub>, ±4<italic>τ</italic><sub>0</sub>} clearly separate the calibrated signal apart and achieve 100% object position detection accuracy.</p><p id="Par33">We also collected data from two researchers (1 male, 1 female) using their index fingers to block each position under the same set of conditions (Fig. <xref rid="Fig5" ref-type="fig">5</xref>d). While the magnitude of the signals generated by a finger blocking each OPD is on average 12.3% weaker than those from completely opaque objects, the same set of threshold values used before achieve 100% detection accuracy.</p></sec><sec id="Sec13"><title>System architecture and applications</title><p id="Par34">Computational photodetectors enable a myriad of ubiquitous sensing applications on everyday surfaces, by capturing relative disposition or motion information between surfaces, objects, and humans. To demonstrate these applications, we develop a self-powered wireless system (Fig. <xref rid="Fig6" ref-type="fig">6</xref>a, b) to acquire and communicate the sensor signal. The computational photodetector operates in photovoltaic mode (i.e., at <italic>V</italic> = 0 V) and is entirely self-powered, with the rest of the system powered by a photovoltaic cell. Unlike other self-powered optical sensing systems (e.g., refs. <sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>), a key advantage enabled by a computational photodetector is that it operates with constant latency and power consumption independent of the number of OPDs or pattern design. To capture the motion of everyday objects and support real-time interactions, we fix the measurement frequency (100 Hz) and communication interval (160 ms) for all applications. Our sensing platform relies only on harvested energy from ambient light. As shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c, the system operates at 96.6 <italic>μ</italic><italic>W</italic>, which is 44% less power than that supplied by a 14.5 cm<sup>2</sup> C-Si photovoltaic cell at 250 lux.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>System and applications.</title><p><bold>a</bold> Schematics of the self-powered wireless sensing system compatible with various computational photodetector designs. <bold>b</bold> The wireless sensing circuit and a rechargeable battery can hide behind a solar cell. <bold>c</bold> Power consumption and harvesting characteristics: 100 Hertz sensor measurement frequency and 160 millisecond wireless communication interval for all applications can be powered solely by the solar cell under 250 lux. <bold>d</bold> Music player: a linear pattern can sense the finger swipe to switch track or adjust the volume. <bold>e</bold> Board game and buttons: a grid pattern can sense the location of a chess piece or a finger touch. <bold>f</bold> Braille: a combination of linear and grid patterns can be used to encode braille alphabets from finger motion. <bold>g</bold> Package monitoring: a grid pattern deployed on a package can sense its orientation and monitor its handling. <bold>h</bold> Inventory management: a crossing pattern deployed on a shelf rack can sense the motion direction of an item.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41528_2022_137_Fig6_HTML.png"/></fig></p><p id="Par35">Computational photodetectors support interactions on everyday surfaces by creating an expressive palette for finger touch, hover inputs, and object detection. A computational photodetector with a linear pattern can be used as a music player interface (Fig. <xref rid="Fig6" ref-type="fig">6</xref>d). With an L-shaped pattern—horizontally (1, −1, 1, −1) and vertically (1, −1, −1), the sensor can detect four finger-swiping gestures for volume control and track selection. A computational photodetector with a grid pattern (e.g., the unbalanced 5-device example) can be used for interactive board games or touch-sensing buttons (Fig. <xref rid="Fig6" ref-type="fig">6</xref>e). The sensor can track object location on the table, play interactive board games such as monopoly, or enable button input such as ordering food on a restaurant table. Finally, combining the linear and the grid pattern, a computational photodetector can be designed in a braille-like pattern for textual input (Fig. <xref rid="Fig6" ref-type="fig">6</xref>f). Identical to the arrangement of the six-dot modern braille alphabets, the computational photodetector uses up to six OPDs to formulate a letter, i.e., (4, 2, 1) from top to bottom on the left, and (−4, −2, −1) on the right. As the user swipes a finger from left to right, each letter will generate a positive signal followed by a negative one, with unique magnitudes. For instance, the letter ‘T’ has an arrangement of (2, 1 ∣ −4, −2), and generates a sequence of {3, −6}. In this way, we can uniquely represent each letter compatible with the tactile representations for visually impaired users.</p><p id="Par36">Computational photodetectors also enable implicit detection of human activities in various (e.g., industrial) settings. A computational photodetector with a grid pattern can provide a cost-effective and self-sustainable solution for package handling monitoring in the delivery process (Fig. <xref rid="Fig6" ref-type="fig">6</xref>g). A 2-device (1, −1) pattern placed on the top and bottom sides of a parcel can sense the static (e.g., top-side-down) or dynamic (e.g., shaken or dropped) orientation of the package. A computational photodetector with a crossing pattern (e.g., the 8 × 8 example) can keep track of the items on a shelf (Fig. <xref rid="Fig6" ref-type="fig">6</xref>h). Placed near the edge of a rack, the sensor picks up motions of an item being slid in, out, left, and right for inventory management, without explicit scanning or logging of human workers.</p></sec></sec><sec id="Sec14" sec-type="discussion"><title>Discussion</title><p id="Par37">We presented an approach to computational photodetectors that achieves in-sensor computation in functional patterns and demonstrated a variety of pattern designs for human activity sensing. A computational photodetector operating under the photovoltaic mode is a self-powered sensor and only draws constant power from the wireless sensing system fueled by a single fixed-size solar panel. As it compresses the higher-dimensional signal space to a single-value output in the analog domain, a computational photodetector by design extracts specific information related to the motion, position, and orientation without capturing or storing digital images, which reduces the probability of capturing unwanted information and is more privacy-preserving than sensing approaches based on von Neumann architectures. When implemented with flexible OPDs computational photodetectors can function under a very wide dynamic range of illumination conditions that are relevant for human activity, and show compelling form factors being thin, flexible, or bendable, and aesthetically pleasing. While these proof-of-principle demonstrations validate the general properties of computational photodetectors and suggest wide applicability for gestural input and activity recognition, it should be clear that to fully exploit the potential of OPDs for implementing computational photodetectors, further optimization of their properties will be needed, from introducing amplification for the detection of faint light, tailoring their field-of-view, spectral bandwidth, and responsivity, to investigating approaches to make some of these properties programmable. Also, further studies will be needed to develop cost models for large-scale manufacturing and to assess the overall reliability of OPDs tailored for computational photodetectors and within the context of their potential applications. Additional usability evaluations are needed for the confounding factors in real-world deployment, such as complex object shapes, unintended interactions, and user variability. However, we believe that the general approach described in this paper provides a solid platform to tackle these real-world challenges.</p></sec><sec id="Sec15" sec-type="methods"><title>Methods</title><sec id="Sec16"><title>Fabrication of computational photodetectors</title><p id="Par38">We fabricate the OPD devices and interconnections on flexible polyethylene terephthalate (PET) substrates. Three different shadow masks are designed in Adobe Illustrator and cut out for patterning.</p><p id="Par39">Indium doped tin oxide (ITO)-coated PET substrates are etched with dilute hydrochloric acid using the first mask for the bottom electrode to recreate the required pattern. In the final OPD devices, ITO acts as the hole-collecting electrode. The photoactive and charge transport layers are then subsequently grown on top of the etched ITO regions via physical vapor deposition at 10<sup>−7</sup> mbar, using the second mask. The photoactive layers are comprised of the molecular semiconductors, Indium (III) phthalocyanine chloride (InClPc) and fullerene (C<sub>60</sub>), both of which are commercially available. This active layer combination has been previously used to fabricate ITO-free semitransparent solar cells on flexible PET substrates<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Lastly, the top electrode and array interconnects are created by physical vapor deposition of silver using the third mask at 10<sup>−7</sup> mbar.</p><p id="Par40">To inhibit device degradation and prevent abrasive damage to the devices, the completed array is protected with commercially available UV-curable resin as an encapsulant. The curing time is optimized to make sure that the cured encapsulant did not impact the flexibility of the substrate.</p></sec><sec id="Sec17"><title>Organic photodiode characterization</title><p id="Par41">We measure the spectral responsivity and irradiance-dependent steady-state current density vs. voltage (<italic>I</italic>–<italic>V</italic>) characteristic of the organic photodiodes using an electrometer (Keithley 6517A) averaged over a set period of time, as shown in <xref rid="Fig1" ref-type="fig">1</xref>b. The OPD displays high responsivity at red to near-infrared region, a region human eyes are less sensitive to. The I–V curve is measured under the illumination of a 635 nm red light-emitting diode (LED), which is typically used in a UL 924 compliant exit light in the US. We reduce the optical power of the light source to barely visible to human eyes (&lt;10 nW cm<sup>−2</sup>) and measure the photocurrent of the OPD device. As a reference, the optical power of the exit light at 0.5 lux and 0.1 lux is 180 and 43 nW cm<sup>−2</sup>, respectively. We record the sensor output while manually occluding each of the devices from the illumination (Fig. <xref rid="Fig2" ref-type="fig">2</xref>e).</p></sec><sec id="Sec18"><title>Sensor identification</title><p id="Par42">For linear and crossing patterns, sensor identification can be achieved by distinctive arrangement of the sensor patterns. A linear pattern with 2<italic>n</italic> OPDs (<italic>n</italic> ≥ 1) can generate <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=\frac{1}{2}\left({2n}\atop{n}\right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq4.gif"/></alternatives></inline-formula> combinations if <italic>n</italic> is odd, or <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mfenced close=")" open="("><mml:mrow><mml:mfrac linethickness="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$N=\frac{1}{2}\left({2n}\atop{n}\right)-\frac{1}{2}\left({n}\atop{n/2}\right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq5.gif"/></alternatives></inline-formula> combinations if <italic>n</italic> is even. For instance, <italic>N</italic> = 126 for <italic>n</italic> = 10, and <italic>N</italic> = 92252 for <italic>n</italic> = 20.</p><p id="Par43">A crossing pattern with 2<italic>n</italic> OPDs can generate <italic>N</italic> = 2<sup><italic>n</italic>−2</sup> − 2<sup>(<italic>n</italic>−2)/2</sup> such sequences if <italic>n</italic> is even, or <italic>N</italic> = 2<sup><italic>n</italic>−2</sup> − 2<sup>(<italic>n</italic>−3)/2</sup> sequences if <italic>n</italic> is odd. For instance, <italic>N</italic> = 240 for <italic>n</italic> = 10, and <italic>N</italic> = 261632 for <italic>n</italic> = 20. Then, by picking one sequence for one axis and one for the other, the crossing pattern can have <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mfrac><mml:mrow><mml:mi>N</mml:mi><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{N!}{{2}^{N/2}(N/2)!}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41528_2022_137_Article_IEq6.gif"/></alternatives></inline-formula> different combinations for <italic>N</italic>/2 unique IDs.</p></sec><sec id="Sec19"><title>Sensor evaluation and signal processing</title><p id="Par44">To evaluate the motion sensing capability of a computational photodetector with a linear or crossing pattern, we use laser-cut wooden bars with variable widths fixed onto the moving rack of the test rig (Fig. <xref rid="Fig3" ref-type="fig">3</xref>c) to represent an object in motion. A digital oscilloscope (Analog Discovery 2<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>) is used to acquire the signal from the sensor. For the linear pattern, we vary one of the test conditions including motion speed (0.25–1 m s<sup>−1</sup>), sensing distance (0.5–4 cm), object size (1–8 cm), and ambient light illumination (75–750 lux). For the crossing pattern, we fix the object size to be 16 cm and the sensing range to 0.25 cm. We vary one of the test conditions including motion speed (0.3–0.6 m s<sup>−1</sup>), and ambient light illumination (250–750 lux).</p><p id="Par45">To evaluate the object position sensing capability of a computational photodetector with a grid pattern, we measure the output in the unblocked state and the blocked state for each device under a 2 × 2 cm laser-cut wooden square. We vary the ambient light condition (250–750 lux).</p><p id="Par46">To process the signal, we use a notch filter centered at 60 Hz with a quality factor <italic>Q</italic> = 30 to remove the alternating current (AC) power-line noise, followed by a DC blocker<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> with <italic>R</italic> = 0.995 to remove the DC component. For object motion detection, we segment the signal of a motion event based on a minimum spike magnitude (10 mV) and a maximum spike–spike interval of 200 ms. To capture the speed of the object motion, we apply peak (local maximum) detection to the magnitude of the signal with a minimum amplitude of 15 mV, a minimum peak-to-peak interval of 0.02 s, and a minimum prominence of 10 mV. The speed of the object traveling over two OPDs is then calculated by dividing the interspace by peak–peak traveling time.</p></sec><sec id="Sec20"><title>System design</title><p id="Par47">As Fig. <xref rid="Fig6" ref-type="fig">6</xref>a shows, we use a low-power microcontroller (Nordic nRF52832 with an ARM Cortex M4 CPU) that supports 12-bit differential ADC and Bluetooth Low Energy (BLE) communication to measure and wirelessly communicate the output of a computational photodetector to a laptop (MacBook Pro, 2.3 GHz Intel Core i5 processor, 8 GB memory) for processing. We measure the power consumption of the system at different signal acquisition and communication frequencies in Fig. <xref rid="Fig6" ref-type="fig">6</xref>c. To reduce the maintenance efforts and battery waste, we use an mc-Si PV cell (IXYS SM141K09L) to harvest power from ambient light. A low-power boost charger (TI bq25505) is used for power management, and the surplus energy is stored in a thin 3.7 V 105 mAh LiPo battery as a backup power source. We measure the amount of harvested energy at the output of the low-power boost charger within indoor ambient light conditions at 250–750 lux.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>We acknowledge Bernard Kippelen for providing lab space, prototyping, and characterization tools; Victor Rodriguez Toro, Yi-Chien Chang, Felipe A. Larrain for technical support in device fabrication; Charles Ramey and Bashima Islam for fruitful discussion. This research was partially supported by the Georgia Tech CRNCH (Center for Research into Novel Computing Hierarchies) Ph.D. Fellowship.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>D.Z. and C.F. conceived the idea and planned the research. R.V. fabricated the devices. D.Z. and C.F. performed device characterization experiments. D.Z., Y.Z., J.P., Y.H.Z., N.A., A.M., and T.C. developed the hardware. D.Z., Y.Z., Y.L., Y.W., Y.H.Z., and Y.D. developed the software. D.Z., Y.Z., Y.L., and J.P. designed and executed the sensing experiments. D.Z. and C.F. performed signal analysis. D.Z., C.F., R.V., Y.Z., J.P., Y.W., S.S., T.S., T.A., and G.D.A. contributed to the applications. T.A. and G.D.A. supervised the overall research. All authors contributed to the manuscript writing.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are available from the corresponding author upon reasonable request.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The code used in this study is available from the corresponding author upon reasonable request.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par48">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Harper, R. <italic>Inside the Smart Home.</italic> (Springer Science &amp; Business Media, 2006).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giannetsos</surname><given-names>T</given-names></name><name><surname>Dimitriou</surname><given-names>T</given-names></name><name><surname>Prasad</surname><given-names>NR</given-names></name></person-group><article-title xml:lang="en">People-centric sensing in assistive healthcare: privacy challenges and directions</article-title><source>Secur. Commun. Netw.</source><year>2011</year><volume>4</volume><fpage>1295</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1002/sec.313</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Wingfield, N. <italic>Inside Amazon Go, a Store of the Future</italic>. <ext-link xlink:href="https://www.nytimes.com/2018/01/21/technology/inside-amazon-go-a-store-of-the-future.html" ext-link-type="uri">https://www.nytimes.com/2018/01/21/technology/inside-amazon-go-a-store-of-the-future.html</ext-link> (2018).</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poppe</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">A survey on vision-based human action recognition</article-title><source>Image Vis. Comput.</source><year>2010</year><volume>28</volume><fpage>976</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1016/j.imavis.2009.11.014</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beddiar</surname><given-names>DR</given-names></name><name><surname>Nini</surname><given-names>B</given-names></name><name><surname>Sabokrou</surname><given-names>M</given-names></name><name><surname>Hadid</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Vision-based human activity recognition: a survey</article-title><source>Multimed. Tools. Appl.</source><year>2020</year><volume>79</volume><fpage>30509</fpage><lpage>30555</lpage><pub-id pub-id-type="doi">10.1007/s11042-020-09004-3</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rautaray</surname><given-names>SS</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Vision based hand gesture recognition for human computer interaction: a survey</article-title><source>Artif. Intell. Rev.</source><year>2015</year><volume>43</volume><fpage>1</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1007/s10462-012-9356-9</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Wu, Y. &amp; Huang, T. S. In <italic>International Gesture Workshop</italic>, 103–115 (Springer, 1999).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiser</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">The computer for the 21st century</article-title><source>Sci. Am.</source><year>1991</year><volume>265</volume><fpage>94</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/scientificamerican0991-94</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Ma, D. et al. SolarGest: Ubiquitous and Battery-free Gesture Recognition using Solar Cells. In <italic>The 25th Annual International Conference on Mobile Computing and Networking</italic>, MobiCom ’19, 1–15 (Association for Computing Machinery, 2019).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">OptoSense: Towards ubiquitous self-powered ambient light sensing surfaces</article-title><source>Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.</source><year>2020</year><volume>4</volume><fpage>103:1–103:27</fpage></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Meena, Y. K. et al. PV-Tiles: Towards Closely-Coupled Photovoltaic and Digital Materials for Useful, Beautiful and Sustainable Interactive Surfaces. In <italic>Proc. 2020 CHI Conference on Human Factors in Computing Systems</italic>, 1–12 (Association for Computing Machinery, 2020).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Raju, D. K. et al. PV-Pix: Slum Community Co-design of Self-Powered Deformable Smart Messaging Materials. In <italic>Proc. 2021 CHI Conference on Human Factors in Computing Systems</italic>, 1–14 (Association for Computing Machinery, 2021).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>García de Arquer</surname><given-names>FP</given-names></name><name><surname>Armin</surname><given-names>A</given-names></name><name><surname>Meredith</surname><given-names>P</given-names></name><name><surname>Sargent</surname><given-names>EH</given-names></name></person-group><article-title xml:lang="en">Solution-processed semiconductors for next-generation photodetectors</article-title><source>Nat. Rev. Mater.</source><year>2017</year><volume>2</volume><fpage>1</fpage><lpage>17</lpage></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuentes-Hernandez</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Large-area low-noise flexible organic photodiodes for detecting faint visible light</article-title><source>Science</source><year>2020</year><volume>370</volume><fpage>698</fpage><lpage>701</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXit1Kru7nK</pub-id><pub-id pub-id-type="doi">10.1126/science.aba2624</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunetti</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Printed solar cells and energy storage devices on paper substrates</article-title><source>Adv. Funct. Mater.</source><year>2019</year><volume>29</volume><fpage>1806798</fpage><pub-id pub-id-type="doi">10.1002/adfm.201806798</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vak</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">3D printer based slot-die coater as a lab-to-fab translation tool for solution-processed solar cells</article-title><source>Adv. Energy Mater.</source><year>2015</year><volume>5</volume><fpage>1401539</fpage><pub-id pub-id-type="doi">10.1002/aenm.201401539</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eggenhuisen</surname><given-names>TM</given-names></name><etal/></person-group><article-title xml:lang="en">High efficiency, fully inkjet printed organic solar cells with freedom of design</article-title><source>J. Mater. Chem. A</source><year>2015</year><volume>3</volume><fpage>7255</fpage><lpage>7262</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXisVygurg%3D</pub-id><pub-id pub-id-type="doi">10.1039/C5TA00540J</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Someya</surname><given-names>T</given-names></name><name><surname>Bauer</surname><given-names>S</given-names></name><name><surname>Kaltenbrunner</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Imperceptible organic electronics</article-title><source>MRS Bull.</source><year>2017</year><volume>42</volume><fpage>124</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1557/mrs.2017.1</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>C-Y</given-names></name><name><surname>Fuentes-Hernandez</surname><given-names>C</given-names></name><name><surname>Chou</surname><given-names>W-F</given-names></name><name><surname>Kippelen</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Top-gate organic field-effect transistors fabricated on paper with high operational stability</article-title><source>Org. Electron.</source><year>2017</year><volume>41</volume><fpage>340</fpage><lpage>344</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhvFCntrnK</pub-id><pub-id pub-id-type="doi">10.1016/j.orgel.2016.11.026</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonat</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">4% efficient polymer solar cells on paper substrates</article-title><source>J. Phys. Chem. C</source><year>2014</year><volume>118</volume><fpage>16813</fpage><lpage>16817</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXmtlSlsrw%3D</pub-id><pub-id pub-id-type="doi">10.1021/jp5020912</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tobjörk</surname><given-names>D</given-names></name><name><surname>Österbacka</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Paper electronics</article-title><source>Adv. Mater.</source><year>2011</year><volume>23</volume><fpage>1935</fpage><lpage>1961</lpage><pub-id pub-id-type="doi">10.1002/adma.201004692</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Fuentes-Hernandez</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>C-Y</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Kippelen</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Stable organic thin-film transistors</article-title><source>Sci. Adv.</source><year>2018</year><volume>4</volume><fpage>eaao1705</fpage><pub-id pub-id-type="doi">10.1126/sciadv.aao1705</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">All-plastic solar cells with a high photovoltaic dynamic range</article-title><source>J. Mater. Chem. A</source><year>2014</year><volume>2</volume><fpage>3492</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXisVCitLo%3D</pub-id><pub-id pub-id-type="doi">10.1039/c3ta15073a</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Highly stretchable polymer semiconductor films through the nanoconfinement effect</article-title><source>Science</source><year>2017</year><volume>355</volume><fpage>59</fpage><lpage>64</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs1Wjsg%3D%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.aah4496</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oh</surname><given-names>JY</given-names></name><etal/></person-group><article-title xml:lang="en">Intrinsically stretchable and healable semiconducting polymer for organic transistors</article-title><source>Nature</source><year>2016</year><volume>539</volume><fpage>411</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhvV2it7bE</pub-id><pub-id pub-id-type="doi">10.1038/nature20102</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>F</given-names></name><name><surname>Chai</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Near-sensor and in-sensor computing</article-title><source>Nat. Electron.</source><year>2020</year><volume>3</volume><fpage>664</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1038/s41928-020-00501-9</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kagawa</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Pulse-domain digital image processing for vision chips employing low-voltage operation in deep-submicrometer technologies</article-title><source>IEEE J. Sel. Top. Quantum Electron.</source><year>2004</year><volume>10</volume><fpage>816</fpage><lpage>828</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2cXovVOlsrg%3D</pub-id><pub-id pub-id-type="doi">10.1109/JSTQE.2004.833888</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Hsu, T.-H. et al. A 0.5V Real-Time Computational CMOS Image Sensor with Programmable Kernel for Always-On Feature Extraction. In <italic>2019 IEEE Asian Solid-State Circuits Conference (A-SSCC)</italic>, 33–34 (IEEE, 2019).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Li, Y., Li, T., Patel, R. A., Yang, X.-D. &amp; Zhou, X. Self-Powered Gesture Recognition with Ambient Light. In <italic>The 31st Annual ACM Symposium on User Interface Software and Technology - UIST ’18</italic>, 595–608 (ACM Press, 2018).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peng</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Cheng</surname><given-names>N</given-names></name><name><surname>Andrew</surname><given-names>TL</given-names></name></person-group><article-title xml:lang="en">ITO-free transparent organic solar cell with distributed Bragg reflector for solar harvesting windows</article-title><source>Energies</source><year>2017</year><volume>10</volume><fpage>707</fpage><pub-id pub-id-type="doi">10.3390/en10050707</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Digilent. <italic>Analog Discovery 2 Specifications</italic>. <ext-link xlink:href="https://reference.digilentinc.com/reference/instrumentation/analog-discovery-2/specifications" ext-link-type="uri">https://reference.digilentinc.com/reference/instrumentation/analog-discovery-2/specifications</ext-link>. (2017).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Smith, J. O. <italic>Introduction to digital filters: with audio applications</italic>, vol. 2 (Julius Smith, 2007).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec21"><title>Supplementary information</title><p id="Par49"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41528_2022_137_MOESM1_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Application video (updated, hi-res)</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41528-022-00137-z" ext-link-type="doi">https://doi.org/10.1038/s41528-022-00137-z</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Electronics and Microelectronics, Instrumentation</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Optical and Electronic Materials</facet-value><facet-value count="1">Polymer Sciences</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Flexible Electronics</facet-value></facet><facet name="year"><facet-value count="1">2022</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
