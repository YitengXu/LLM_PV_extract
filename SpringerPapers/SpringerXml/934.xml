<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-022-00947-9</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-022-00947-9</article-id><article-id pub-id-type="manuscript">947</article-id><article-id pub-id-type="doi">10.1038/s41524-022-00947-9</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/638/298</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/638/563</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">A multi-fidelity machine learning approach to high throughput materials screening</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5862-3014</contrib-id><name><surname>Fare</surname><given-names>Clyde</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Fenner</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8611-3596</contrib-id><name><surname>Benatan</surname><given-names>Matthew</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Varsi</surname><given-names>Alessandro</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au5"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8232-8282</contrib-id><name><surname>Pyzer-Knapp</surname><given-names>Edward O.</given-names></name><address><email>epyzerk3@uk.ibm.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs41524022009479_cor5">e</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution content-type="org-name">IBM Research - Europe - Daresbury</institution></institution-wrap><addr-line content-type="city">Daresbury</addr-line><country country="GB">UK</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.10025.36</institution-id><institution-id institution-id-type="ISNI">0000 0004 1936 8470</institution-id><institution content-type="org-name">University of Liverpool</institution></institution-wrap><addr-line content-type="city">Liverpool</addr-line><country country="GB">UK</country></aff></contrib-group><author-notes><corresp id="IDs41524022009479_cor5"><label>e</label><email>epyzerk3@uk.ibm.com</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>19</day><month>12</month><year>2022</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2022</year></pub-date><volume>8</volume><issue seq="257">1</issue><elocation-id>257</elocation-id><history><date date-type="registration"><day>8</day><month>12</month><year>2022</year></date><date date-type="received"><day>24</day><month>8</month><year>2022</year></date><date date-type="accepted"><day>6</day><month>12</month><year>2022</year></date><date date-type="online"><day>19</day><month>12</month><year>2022</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2022</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">The ever-increasing capability of computational methods has resulted in their general acceptance as a key part of the materials design process. Traditionally this has been achieved using a so-called computational funnel, where increasingly accurate - and expensive – methodologies are used to winnow down a large initial library to a size which can be tackled by experiment. In this paper we present an alternative approach, using a multi-output Gaussian process to fuse the information gained from both experimental and computational methods into a single, dynamically evolving design. Common challenges with computational funnels, such as mis-ordering methods, and the inclusion of non-informative steps are avoided by learning the relationships between methods on the fly. We show this approach reduces overall optimisation cost on average by around a factor of three compared to other commonly used approaches, through evaluation on three challenging materials design problems.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>Hartree National Centre for Digital Innovation</institution></institution-wrap></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>257</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>12</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>8</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2022_Article_947.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Engineers and material scientists frequently seek to discover new materials that exhibit specific sets of properties. Most properties of active interest whether they be opto-electronic, structural, catalytic or physio-chemical have a complex relationship with the variables that are under experimental control. This fact, in combination with the vast number of synthesisable materials and the relatively high cost of experimental synthesis and characterisation is the central challenge of materials discovery<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par3">One tool in the material designers toolbox is the use of simulation as a proxy for experiment<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Due to the reduced cost of simulation over synthesis and characterisation of a material this offers the potential for orders of magnitude increases in the number of materials that can be evaluated during the materials discovery process. The mitigating factor for most cases of molecular and materials design is that given practical limits on computational resources, simulation is not sufficiently accurate to match experiment<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. This means that materials discovery cannot proceed using simulation alone and consequently some means of combining simulation and experimental synthesis and characterisation must be used<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. This remains the case despite the significant improvements in underlying electronic structure methods and the increases in computational resources that have occurred over the past decades.</p><p id="Par4">Inspired by drug discovery, the traditional way workflows make use of cheaper approximate measures is through the use of a screening approach known as a computational funnel<sup><xref ref-type="bibr" rid="CR7">7</xref></sup><bold>–</bold>Fig. <xref rid="Fig1" ref-type="fig">1</xref>. Here, starting with cheap less accurate methods (e.g. QSAR models), increasingly more expensive and accurate methods including more complex simulation methodologies (e.g. molecular dynamics and/or ab-initio quantum calculations) and readily accessible experimental measures (e.g. single property measurements or spectroscopic characterisation in the materials discovery setting or in-vitro experiments in the drug discovery setting) are applied to screen out a smaller and smaller fraction of a potential material candidate pool eventually yielding a small set of highly promising candidates which can be evaluated using the most accurate experimental measurements (e.g. full experimental characterisation in the materials discovery setting or animal and human trials in the drug discovery setting).<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Abstract Representation of a funnel based screening approach to molecular discovery.</title><p>An entire pool of molecules is evaluated with the cheapest and assumed least accurate method, a fixed fraction of the highest performing species are then evaluated with the next cheapest and assumed second least accurate method passing on a fixed fraction of the highest performing species to the next method. In this way the pool of candidate molecules progressively reduced until a final set of candidates remain.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig1_HTML.png"/></fig></p><p id="Par5">Recently, emerging technologies such as machine-learning have driven ever-more efficient materials screening campaigns<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. One particularly impactful approach has been to replace the expensive to evaluate simulations with data driven models, either through replacement of the potential energy calls<sup><xref ref-type="bibr" rid="CR10">10</xref>–<xref ref-type="bibr" rid="CR12">12</xref></sup> or through direct modelling of the property of interest<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR15">15</xref></sup>.</p><p id="Par6">Whilst computational funnels have proven successful, several disadvantages can be identified:<list list-type="bullet"><list-item><p id="Par7">To construct the hierarchy of methods detailed upfront knowledge about the relative accuracies of each method along with its cost is required.</p></list-item><list-item><p id="Par8">The total quantity of resources to be used in the entire design process need to be known and specified a priori.</p></list-item><list-item><p id="Par9">The relative spread of resources amongst the different levels must also be known and specified a priori.</p></list-item></list></p><p id="Par10">The first of these challenges is particularly relevant when integrating machine-learning models as layers within the funnel since it is often impossible to know the true accuracy of a data-driven model ahead of time, for an arbitrary data-point, since generalised performance is often intrinsically linked to the data and methods used to train the model, rather than the model itself.</p><p id="Par11">In this paper, we present an alternative to the computational funnel for materials discovery which instead relies on an extension of Bayesian optimization that can make use of cheaper approximate measurements- Fig. <xref rid="Fig2" ref-type="fig">2</xref>. In our approach, a Bayesian model is constructed which dynamically learns to relate the different approximate methods and the ground truth experimental value (referred to here as the different methodological fidelities) to each other. This model is used to dynamically traverse the full set of candidate materials in a budget aware, accuracy aware manner. It is progressive rather than hierarchical, allows termination to be decided by the user rather than fixed ahead of time, is implicitly dynamic in its allocation of resources to the different methods and does not require knowledge of the accuracies of the different fidelities ahead of time.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Abstract representation of an iteration of multifidelity optimisation.</title><p>For iteration i, the model is trained on data for all molecules and measured fidelities, then based on this new model (and a suitable acquisition function) a new target molecule and fidelity is chosen trading off exploration of new molecular species, exploitation of the areas of molecular space predicted to be good, the relative costs of evaluating the different fidelities and how informative those evaluations are likely to be.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig2_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results and discussion</title><p id="Par12">We demonstrate the effectiveness of our proposed approach through application to three hybrid simulation-experiment discovery challenges, comparing its performance to the commonly used computational funnel, and Bayesian optimization – an emerging approach to sample-efficient experimental design applied only to the target fidelity. We also investigate how fidelity cost and cross-correlation influence the behaviour of our approach relative to these reference methods through the use of a set of artificial functions where these factors can be directly controlled.</p><sec id="Sec3"><title>Multi-Fidelity machine learning</title><p id="Par13">Whilst machine learning has shown strong potential as an emerging paradigm for rapidly generating predictions of materials’ properties of interest, as a data-driven technology its utility can be limited by the availability of high-quality data. An emerging approach to deal with this challenge is to build machine learning models built from multiple different fidelities of data, which can then act as predictors for cases where sufficient amounts of data are not available to build traditional QSAR or machine-learning models<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. These approaches typically rely on building a model which is able to relate the different fidelities of information to each other, typically by building a single model with multiple output values – one per fidelity. It should be noted that this is distinct from the D-machine learning approach<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>, in which a single-output model learns a correction to apply to a low fidelity to better approximate a high-fidelity output. Applying multi-fidelity machine learning approaches to the materials domain has seen some notable early successes. For example, Chen et al. apply a multi-fidelity setting of a graph network to the prediction of band-gaps<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. They found that the inclusion of information from a lower fidelity calculation, in their case using the Perdew-Burke-Ernzehof methodology<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>, led to an improvement in the mean absolute error they were able to achieve of between 22 and 45%. Similar information fusion approaches have been used in the polymer space, for example Patra et al. use a co-kriging scheme to fuse information from a variety of sources to build a predictive model for polymer bandgaps<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. In their study, they observed both an increased performance over a single-fidelity Gaussian process approach, but also greater generalisation for their model.</p></sec><sec id="Sec4"><title>Bayesian optimization</title><p id="Par14">Bayesian optimisation is a family of sample efficient optimisers which balance the twin pressures of exploration (knowledge generation) and exploitation (knowledge utilisation) through the iterative construction of a scoring function based on Bayesian machine-learning models<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. It has shown promise in diverse fields from hyper-parameter optimization<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> to drug discovery<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> to engineering<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> to materials discovery<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>.</p><p id="Par15">This scoring function - sometimes known as the acquisition function - relates the parameters being optimised to their expected utility given the current state of the model. Perhaps the most commonly used acquisition function is Expected Improvement (EI)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, which balances exploration and exploitation through considering both the likelihood of an improvement and its potential magnitude. Iterative training of the model, calculating posterior predictions followed by maximisation of the acquisition function then drives the candidate selection within the Bayesian optimization paradigm. Gaussian processes are the most commonly utilised Bayesian machine learning model for this task, though others such as Bayesian neural networks (BNNs) have also been effectively deployed<sup><xref ref-type="bibr" rid="CR29">29</xref>–<xref ref-type="bibr" rid="CR31">31</xref></sup>. We note that the proliferation of new Bayesian models such as BNNs which can scale to large data sizes together with advances that mitigate the cubic scaling<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> previously associated with GPs mean that practical limitations that prevented Bayesian optimisation being applied large scale design problems are no longer the barrier they once were.</p></sec><sec id="Sec5"><title>Multi-fidelity bayesian optimization</title><p id="Par16">Inspired by the successes of the inclusion of lower fidelity information through multi-fidelity model building, and the emerging area of Bayesian optimization for materials screening, this paper extends standard approaches to Bayesian optimization of materials from a sample-efficient method for optimising a target property, to a multi-fidelity technique, capable of taking advantage of all available fidelities. Since this approach is agnostic to the source of the data, this naturally allows for combination of experimental and theoretical data in a way not achieved by current machine-learning driven screening approaches. To achieve this goal, it is necessary to build a model which is able to link data from each fidelity and draw inferences from the composite information. For this purpose, we utilise a multi-output Gaussian process<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>.</p><p id="Par17">Multi-fidelity Bayesian optimization makes use of the same approach of iteratively training a probabilistic model and using it to rank possible materials for measurement. It extends the search space from a set of materials or molecules to a combination of these candidates and a choice of a particular measurement modality or fidelity. Therefore, given an effective acquisition function, it is possible to efficiently trade off information collection at cheap but noisy fidelities with targeted acquisition of data at the highest fidelity when required. Typically, the reason for employing such an approach is to reduce the total budget spent on the optimization, since the highest fidelity may be very technically or financially challenging to acquire. Throughout this paper we use the terms low fidelity and high fidelity to mean low expense and high expense for continuity of language with previous works in the field. We note, however, that our approach does not require that the accuracy and the cost of the fidelities be both monotonic and ranked, rather the only requirement is that a target fidelity be specified at the start of the optimization.</p><p id="Par18">Different approaches to multi-fidelity optimization can be decomposed into choices modelling the relationships between the different fidelities and choices regarding how to construct the acquisition function. For example, Song et al. utilise a phased approach, with initial exploration performed using a low fidelity until some stopping criterion is hit, at which point high-fidelity data acquisition is considered<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>. Palizhati et al. consider both epsilon-greedy and lower-confidence-bound (LCB) settings to build multi-fidelity screening approaches. They found that the best results were when the entire low-fidelity data set was given as a priori knowledge to a multi-fidelity model, which resulted in acceleration of at least 20% on materials discovery tasks<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>. Our approach, which we name Targeted Variance Reduction (TVR), naturally extends arbitrary single fidelity Bayesian optimization acquisition functions to a multi-fidelity domain. The TVR algorithm is described in detail in the Methods section with pseudo-code presented in the ESI, but is summarised as follows: after computing a standard acquisition function on the target fidelity samples, (in this paper, we use the aforementioned EI acquisition function) the combination of the choice of input sample and fidelity is made by picking the pair that will minimise the variance of the model prediction at the point with the greatest acquisition function score per unit cost. This process is repeated iteratively until the budget is exhausted.</p></sec><sec id="Sec6"><title>Synthetic data set</title><p id="Par19">Approaches to screening challenges, such as those encountered in materials discovery, are affected by two main effects – the relative cost of making evaluations at the different fidelities, and the correlation between each of the fidelities. In an ideal system, cheaper fidelities are highly correlated both to each other, and to the target fidelity, enabling an efficient winnowing of the candidate pool without significant computational expense. In a worst-case scenario, fidelities are completely uncorrelated, essentially reducing each stage of a computational funnel to a lottery.</p><p id="Par20">To demonstrate and systematically probe the effects of cost and accuracy of the lower fidelity proxies on optimisation based on computational funnels and the TVR-EI algorithm, we make use of a synthetic function as the target of our optimisation, and generate lower fidelity proxies in a manner that allows us to control the degree to which the lower fidelity is correlated to the ground truth target. We utilise Liu’s 1D function -Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>) - as our target function, as it is complex enough to differentiate different optimization strategies, but not so complex as to obfuscate the effects of algorithmic component choices.<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn><mml:mfenced close=")" open="("><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mn>2.5</mml:mn></mml:mrow></mml:mfenced><mml:msqrt><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:msup><mml:mrow><mml:mfenced close=")" open="("><mml:mrow><mml:mn>6</mml:mn><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∧</mml:mo></mml:mrow></mml:msup><mml:mn>2</mml:mn><mml:mi>sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>12</mml:mn><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mn>4</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:msqrt></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f_{\left( x \right)} = 1.5\left( {x + 2.5} \right)\sqrt {\left( {\left( {6x - 2} \right)^ \wedge 2\sin (12x - 4) + 10} \right)}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_Equ1.gif"/></alternatives></disp-formula></p><p id="Par21">We generate a set of functions with differing degrees of correlation to our target function using our previously described method<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> to act as the lower fidelity proxy to (1). Plotted examples of generated functions can be found in the ESI. To examine the effect of the relative cost we consider range of discount factors that express the degree to which the lower fidelity proxy is cheaper to evaluate than the target function.</p><p id="Par22">The results of the experiments making use of the synthetic functions are seen in Fig. <xref rid="Fig3" ref-type="fig">3</xref> which shows a heatmap indicating relative performance of TVR-EI, and a computational funnel. Performance is scored by the difference between the total computational cost the optimal computational funnel required to discover a solution scored at the 99th percentile best values and the total computational cost required by TVR-EI to discovery a 99th percentile value, averaged over computational replicates where a unit of cost is defined by the price of a single evaluation of the ground truth target. The two axes show the effect of the discount (in cost) of taking measurements using the lower fidelity proxy relative to the ground truth function and the Pearson correlation of the lower fidelity proxy to the ground truth function. The lower left corner of the grid is associated with expensive accurate proxies whilst the upper right corner is associated with cheap inaccurate proxies.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Effects of cost and correlation on method performance.</title><p>The x-axis shows the effect of varying the magnitude of the discount of the low fidelity measurement relative to a measurement of the actual target function, while the y-axis shows the effect of varying the correlation of the lower fidelity to the target function. Each cell in the heatmap is shaded to reflect the difference between the number of epochs taken by the computational funnel and TVR-EI. Positive (blue shading) indicates that the TVR-EI algorithm was more efficient, whilst negative (red shading) indicates that the computational funnel was more efficient.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig3_HTML.png"/></fig></p><p id="Par23">The figure shows that for the synthetic case the TVR-EI algorithm outperforms when either the expense of the proxy is relatively high, or when the accuracy of the proxy is relatively high, whilst the computational funnel shows higher performance when the proxy is both lower cost and lower accuracy. The greater performance for both the expensive, yet inaccurate proxy functions and the cheap accurate proxies can be rationalised by TVR-EI’s capacity to dynamically adjust how it allocates budget based upon information gained during the optimization. Unlike a computational funnel, where budget is fixed and pre-allocated, TVR-EI can dynamically allocate more budget to proxies if it is determined that the proxy is informative, or less if the correlation is deemed to be low. We postulate that the utility of the funnel for cheaper lower accuracy proxies may be because TVR-EI is more sensitive to mismatches between the proxy and the target and thus can exhibit an overly conservative behaviour, avoiding proxies that can still be somewhat useful.</p><p id="Par24">It can also be observed that the magnitude of the difference between the two methods is marked, with no score lower than −20 (i.e. the computational funnel effectively required 20 fewer target samples), but with the highest score over 100 (i.e. TVR-EI effectively required 100 fewer target samples).</p></sec><sec id="Sec7"><title>Materials discovery challenges</title><p id="Par25">Building from our understanding of the algorithm taken from its use on synthetic functions, we now test our approach on three materials discovery examples. A detailed description of these data sets can be found in the Data Sets section, but in summary each contain a mixture of computationally calculated and experimentally measured fidelities for impactful materials properties - polarizability (Alexandria), power conversion efficiency (HOPV-15) and band-gap (Chen).</p><p id="Par26">As we have previously stated, computational funnels require the user to provision the computational budget in advance. It is worth noting that throughout this study we effectively assume that the funnel is capable of being provisioned perfectly, which is not a situation which reflects reality. Our primary point of comparison for each task is to a composite of funnels, where for each budget we run separate funnel which are provisioned with the specified budget and report the final performance of said funnel. This is contrasted with the other methods which are being run once and their performance tracked as they expend increasingly greater resources. We note that this represents an upper limit on the performance of a computational funnel – perfectly budgeted, ideally provisioned. A comparison of performance between TVR-EI, single fidelity EI, an ideally provisioned composite funnel and random (Monte Carlo) are shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. In this study, we use ‘regret’ as a measure of performance, where a score of zero regret indicates that the best possible solution has been discovered. Here, regret was calculated with respect to an exhaustive search at the highest fidelity.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Comparison of methods for materials science challenges.</title><p>Normalised Regret vs. cost expenditure for the single fidelity EI, multi-fidelityTVR-EI, a composite of computational funnels and random search algorithms applied to the materials discovery challenges (<bold>a</bold>) Alexandria, <bold>b</bold> HOPV-15 and (<bold>c</bold>) Chen. The composite funnel displays results associated with separately provisioned funnels one for each potential budget value and the associated final regret of these funnels, it thus represents the best case scenario for the method. Median regret values are plotted from 15 optimisations with different random seeds. Shading shows the interquartile range of the runs.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig4_HTML.png"/></fig></p><p id="Par27">We can observe that for each of these challenges, the multi-fidelity Bayesian optimisation approach using TVR-EI equals or betters the performance of both the computational funnel and the single fidelity EI method. However, as we would expect given the insight given by the varying of correlation and cost with the synthetic function, the behaviour of the different optimisation algorithms varies considerably among the different datasets.</p><p id="Par28">Table <xref rid="Tab1" ref-type="table">1</xref> shows a numerical summary of performance of TVR-EI in comparison to an ideally provisioned composite funnel and expected improvement Bayesian optimization run on the target fidelity. We can measure performance in two ways:<list list-type="bullet"><list-item><p id="Par29">Relative efficiency of the methodologies compared to TVR-EI (Expense multiplier in the table): Here a score of 1 means that the same budget is consumed, and greater than one means that TVR-EI was more efficient</p></list-item><list-item><p id="Par30">Relative regret compared to TVR-EI: here we calculate how much worse a solution has been discovered by the comparison methods when TVR-EI has found the optimal solution. A score of zero means that the method has also discovered the optimal solution.</p></list-item></list><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Relative increase in expense needed for the computational funnel to reach median zero regret vs. TVR-EI and median regret at the expense TVR required to reach median zero regret for composite funnel and Bayesian optimization (EI) methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2"><p>Expense multiplier vs TVR-EI at zero regret</p></th><th colspan="2"><p>Normalised Regret at TVR-EI zero regret</p></th></tr><tr><th><p>Dataset</p></th><th><p>Composite funnel</p></th><th><p>Bayesian optimization</p></th><th><p>Composite funnel</p></th><th><p>Bayesian optimization</p></th></tr></thead><tbody><tr><td><p>Alexandria</p></td><td><p>2.5</p></td><td><p>2.7</p></td><td><p>0.3</p></td><td><p>0.3</p></td></tr><tr><td><p>HOPV</p></td><td><p>4.4</p></td><td><p>1.4</p></td><td><p>0.3</p></td><td><p>0.04</p></td></tr><tr><td><p>Chen</p></td><td><p>1.2</p></td><td><p>4.8</p></td><td><p>0.1</p></td><td><p>0.3</p></td></tr><tr><td><p>Average</p></td><td><p>2.7</p></td><td><p>3.0</p></td><td><p>0.2</p></td><td><p>0.2</p></td></tr></tbody></table><table-wrap-foot><p>Values are averaged over 15 repeated optimisations each with different random seeds.</p></table-wrap-foot></table-wrap></p><p id="Par31">We observe that on average TVR-EI has an average efficiency gain of 2.8x compared to competitive methods analysed in this study, and an average normalised relative regret gain of 20%, indicating the potential for this approach to deliver significant improvements in materials screening challenges.</p><p id="Par32">The Chen dataset results highlight the advantages of using well-correlated lower fidelity proxies with both the computational funnel and TVR-EI able to reach 99th percentile insulators with an order of magnitude reduction in cost relative to the random search baseline and a factor of 5 decrease in cost relative to the single fidelity EI optimisation. The good performance of the funnel can be attributed to the large difference in cost between the experimental target and computational surrogates. The relatively poor performance of a single fidelity Bayesian optimization approach suggests that the relationship between the molecular representations and the bandgap poses challenges to building a powerful internal model in the low data regime demanded by the high data acquisition cost. We posit that this could be due to a rough functional relationship, where small changes in structure can lead to large differences in bandgap, requiring a greater volume of data to resolve satisfactorily. Thus, this optimization challenge is characterised by informative proxies in combination with a relatively challenging optimisation target function. We see that TVR-EI offers comparable performance to the funnel. A breakdown of how the algorithm allocates its budget (Table <xref rid="Tab2" ref-type="table">2</xref>) indicate that it achieves this by making significant use of the lower fidelities to focus on sampling only the most valuable of the more expensive experimental samples.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Breakdown of percentage of samples, and corresponding budget allocation, spent on target (highest) fidelity samples for each task, and for each method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="3"><p>% High fidelity samples</p></th><th colspan="3"><p>% High fidelity budget</p></th></tr><tr><th/><th><p>Alexandria</p></th><th><p>HOPV-15</p></th><th><p>Chen</p></th><th><p>Alexandria</p></th><th><p>HOPV-15</p></th><th><p>Chen</p></th></tr></thead><tbody><tr><td><p>Computational funnel</p></td><td><p>10.0</p></td><td><p>1.3</p></td><td><p>4.8</p></td><td><p>33.0</p></td><td><p>20.0</p></td><td><p>50.0</p></td></tr><tr><td><p>Single fidelity BO</p></td><td><p>100</p></td><td><p>100</p></td><td><p>100</p></td><td><p>100</p></td><td><p>100</p></td><td><p>100</p></td></tr><tr><td><p>TVR-EI</p></td><td><p>6.6</p></td><td><p>55.0</p></td><td><p>26.2</p></td><td><p>24.9</p></td><td><p>95.3</p></td><td><p>87.7</p></td></tr></tbody></table><table-wrap-foot><p>Values are averaged over 15 repeated optimisations each with different random seeds.</p></table-wrap-foot></table-wrap></p><p id="Par33">In contrast to the Chen dataset, for the HOPV dataset both the single the EI and the TVR algorithms were able to rapidly identify optimal candidates, significantly outperforming the random search baseline which itself significantly outperforms the computational funnel. We note that this problem represents the worst-case scenario for the computational funnel approach, and is characterised by reasonably constructed, yet poorly correlated fidelities, as demonstrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>. We also note that the success of the single fidelity Bayesian optimisation algorithm (EI) indicates that the functional relationship between the reduced dimensional representation and the experimental power conversion efficiency does not suffer pathologies.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>A cross-correlation plot of the five fidelities present in the HOPV data set.</title><p>Scatter plots in off-diagonal elements describe the correlation between the X and Y data sets, whilst the diagonal plots show the distribution of data within each data set.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig5_HTML.png"/></fig></p><p id="Par34">Since this dataset is characterised by relatively expensive, yet mostly low-quality surrogate fidelities in combination with a relatively easy optimisation target function, it is easy to perceive why TVR-EI will outperform a computational funnel approach. This is born out through inspection of the budget allocation, which can be seen in Table <xref rid="Tab3" ref-type="table">3</xref>, with a significant number of the samples being drawn from the target fidelity, despite the large cost of doing so. This indicates that the method has learned that for this target, the surrogate fidelities do not carry much information and once this is determined, allocates almost no budget to these fidelities. Further inspection of the breakdown of budget allocation can yield additional insights. For example, in this task, the MO6-2x fidelity is both relatively expensive and yet also uncorrelated to the target (see Fig. <xref rid="Fig5" ref-type="fig">5</xref>), and indeed TVR-EI consistently allocates almost no budget to investigating this fidelity. Additionally, Fig. <xref rid="Fig5" ref-type="fig">5</xref> also shows that the PBE0 and B3LYP fidelities are strongly correlated, leading to the TVR-EI algorithm to consistently invest more budget into the cheaper PBE0 fidelity, given that the information content is similar. Correlation plots of the other two data sets can be found in the <xref ref-type="supplementary-material" rid="MOESM1">supplementary information</xref>.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Breakdown of budget allocation for HOPV task by TVR-EI.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Fidelity</p></th><th><p>Cost</p></th><th><p>Average number of samples</p></th><th><p>Average percentage of budget spent</p></th></tr></thead><tbody><tr><td><p>Experiment</p></td><td><p>20.0</p></td><td><p>55.0</p></td><td><p>95.3</p></td></tr><tr><td><p>MO6-2X</p></td><td><p>2.0</p></td><td><p>1.2</p></td><td><p>0.2</p></td></tr><tr><td><p>B3LYP</p></td><td><p>1.75</p></td><td><p>7.9</p></td><td><p>1.2</p></td></tr><tr><td><p>PBE0</p></td><td><p>1.5</p></td><td><p>19.4</p></td><td><p>2.5</p></td></tr><tr><td><p>BP86</p></td><td><p>0.5</p></td><td><p>16.3</p></td><td><p>0.7</p></td></tr></tbody></table><table-wrap-foot><p>Averages are calculated over 15 runs.</p></table-wrap-foot></table-wrap></p><p id="Par35">In contrast to Chen and HOPV-15, which have been chosen to demonstrate situations that favour either a computational funnel or Bayesian optimization approach, the Alexandria dataset shows an intermediate case. Both the computational funnel and the single-fidelity EI method have comparable performance - despite achieving this performance through fundamentally different mechanisms - with both outperforming random searches. This indicates that the target function is effectively optimisable and that the balance of cost vs. correlation for the computational proxies means they are highly informative. For this challenge, we observe that our approach significantly outperforms both computational funnels and single-fidelity Bayesian optimization approaches, enhancing the signal exploited by the single fidelity Bayesian optimization approach with additional information taken from the lower fidelity approaches.</p><p id="Par36">It is also informative to consider the effects of non-ideal provisioning, which is more akin to a real-world situation. The results of such a comparison are shown in Fig. <xref rid="Fig6" ref-type="fig">6</xref>. When comparing to a non-ideal (either over, or under, provisioned) funnel, we observe that the effects we describe throughout are enhanced. For the purposes of this study, we define an ideally provisioned funnel as the minimum budget required to reach zero regret in the median case, an over-provisioned funnel as having twice this budget, and an under provisioned funnel as having half this budget. An equivalent to Fig. <xref rid="Fig4" ref-type="fig">4</xref> where the worst case is tracked in place of the median – thus providing a lower bound to the composite funnel performance – can be found in the ESI.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>The effects of provisioning budgets on computational funnels’ performance.</title><p>Normalised Regret vs. cost expenditure for ideally provisioned, under provisioned and over provisioned computational funnels alongside TVR-EI applied to the materials discovery challenges (<bold>a</bold>) Alexandria, (<bold>b</bold>) HOPV-15 and (<bold>c</bold>) Chen. The ideally provisioned funnel is defined as the funnel with the lowest possible budget that is able to achieve 0 median regret while the under-provisioned funnel is assigned half this budget and the over provisioned funnel is assigned twice this budget. In contrast to Fig. 4 these funnel results show the change in regret as the funnel uses its budget. Median regret values are plotted from 15 optimisations with different random seeds. Shading shows the interquartile range of the runs.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig6_HTML.png"/></fig></p><p id="Par37">These three challenges span a range of thermochemical and optoelectronic properties including both experimental and computational value, and represent a wide spectrum of characteristics, designed to test our approach against a variety of situations in which it may reasonably be applied. In the datasets we have examined, we observe clear benefits can be seen to application of TVR-EI which matches or betters best case performance of the commonly used computational funnel and emerging Bayesian optimization methodology, while not requiring front loading of resources nor definitive knowledge of the relative accuracy of possible proxies. Our method is demonstrated to be robust to uninformative proxies and able to leverage internal correlations to remove the requirement to expend budget on proxies which share high correlation to lower cost alternatives. We believe that this demonstrates that our TVR-EI algorithm has promise as a tool for molecular and materials design where cheaper proxy measures are available as an alternative to the well-established computational funnel or single-fidelity Bayesian optimization methods, and establishes its utility for mixed simulation-experiment experimental designs.</p></sec></sec><sec id="Sec8" sec-type="methods"><title>Methods</title><sec id="Sec9"><title>Targeted variance reduction</title><p id="Par38">Multi-Fidelity Targeted Variance Reduction (MF-TVR) is a conceptually simple algorithm. After computing a standard acquisition function on the target fidelity, in this case EI, the combination of the choice of input sample and fidelity is made by picking the pair that minimise the variance of the model prediction at the point with the greatest Expected Improvement, scaled by the cost of making the evaluation. We do not separate out the low fidelity search, and high fidelity exploit, into distinct stages, but instead use the lower fidelities to improve the quality of the acquisition function itself, thus directly impacting the sampling efficiency. These steps are illustrated graphically in Fig. <xref rid="Fig7" ref-type="fig">7</xref> and pseudo-code for the TVR-EI algorithm can be found in the ESI.<fig id="Fig7"><label>Fig. 7</label><caption xml:lang="en"><title>Illustration of the steps within a single iteration of the TVR-EI algorithm applied to a three-fidelity problem.</title><p><bold>a</bold>–<bold>f</bold> Show the 6 stages of the algorithm. Within each panel the top (blue) plots refer to the ground truth fidelity whilst the middle (green) and lower (orange) plots refer to the two approximate fidelities. <bold>a</bold>, <bold>b</bold> Show a set of initial data points (<bold>a</bold>) used to train a Bayesian model and the posterior predictions of that Bayesian model (<bold>b</bold>) juxtaposed against the reference fidelity functions (dashed lines). <bold>c</bold> Shows the Expected Improvement acquisition function applied to the high fidelity posterior used to discover the optimal high fidelity point which is highlighted with a dashed blue line. <bold>d</bold> Shows the posterior squared correlation between the identified optimal high fidelity point (location shown again with the dashed blue line) and points within the domain for the three different fidelities. <bold>e</bold> Shows the final scoring function which is a scaled version of <bold>d</bold> that takes into account the cost of evaluating the different fidelities and shows the highest scoring point chosen (shown with a dashed green line) which in this case is within the second fidelity. <bold>f</bold> Shows the new set of data points after evaluating the highest scoring point (indicated with the green circle).</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_947_Fig7_HTML.png"/></fig></p></sec><sec id="Sec10"><title>Multi-output Gaussian process</title><p id="Par39">In the case of a single fidelity GP, training data takes the form of a matrix of material representations <italic>X</italic> and corresponding property values <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\vec y$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_IEq1.gif"/></alternatives></inline-formula>, and we have another matrix of representations <italic>X</italic><sub>*</sub> for which we would like to make predictions. We suppose we have a kernel function defined by a set of hyperparameters, which is typically a universal smoothing kernel such as the radial basis function (RBF) or a Matern kernel. This kernel function can be used to compute prior covariances between vector representations of materials, and by extension can be used to compute a prior covariance matrix among a set of materials. The posterior predicted means for the materials to be evaluated are then given by:<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">μ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {{{{\mathrm{\mu }}}}_ \ast } = K_ \ast ^TK^{ - 1}\vec y$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_Equ2.gif"/></alternatives></disp-formula>where <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {\mu _ \ast }$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_IEq2.gif"/></alternatives></inline-formula> is the vector of predicted mean values, <italic>K</italic><sub>*</sub> is the prior covariance matrix between <italic>X</italic> and <italic>X</italic><sub>*</sub> as determined by the kernel function, and <italic>K</italic><sup>−1</sup> is the inverse prior covariance matrix between <italic>X</italic> and <italic>X</italic> again as determined using the kernel function. Similarly, the posterior covariances for the materials to be evaluated are given by:<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">σ</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo><mml:mo>*</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathrm{\sigma }}}}_ \ast = K_{ \ast \ast } - K_ \ast ^TK^{ - 1}K_ \ast$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_Equ3.gif"/></alternatives></disp-formula>where <italic>σ</italic><sub>*</sub> is the posterior covariance matrix between <italic>X</italic><sub>*</sub> and <italic>X</italic><sub>*</sub> and <italic>K</italic><sub>**</sub> is the equivalent prior covariance matrix between <italic>X</italic><sub>*</sub> and <italic>X</italic><sub>*</sub>.</p><p id="Par40">Hyper parameters of the kernel function can either be sampled or learned by maximising the log marginal likelihood of the training data<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p><p id="Par41">This setup can be extended to the multifidelity case by creating a representation for the fidelities and concatenating it with the representation of the materials. This allows us to make use of the same kernel functions to generate a prior sample-fidelity covariance matrix.</p><p id="Par42">We choose to represent the fidelities via a one-hot encoding, where we drop the ground truth high fidelity dimension – this results in representing a fidelity by a vector with the number of dimensions equal to the number of approximate fidelities, with the high fidelity reference mapping to the zero vector, and the other fidelities mapping to the unit vectors in each axis. This choice to drop the dimension that would normally represent the high fidelity biases the model to care more about the relationship between the various fidelities and the high fidelity than between the lower fidelities themselves. This is of direct benefit to our use cases, since here we care explicitly about the former rather than the latter.</p><p id="Par43">Thus the only addition to the single fidelity case defined above is that training/evaluation data for materials and property values have been replaced by training/evaluation data for material/fidelity combinations and property values. i.e. previously where the <italic>i</italic><sub><italic>th</italic></sub> row of the matrix of material representations <italic>X</italic> corresponded simply to the vector <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {x_i}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_IEq3.gif"/></alternatives></inline-formula> representing material <italic>i</italic>, we now have rows defined by<disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mfenced close="]" open="["><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover><mml:mo>,</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {x_i^k} = \left[ {\overrightarrow {x_i} ,\overrightarrow {f_k} } \right]$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_Equ4.gif"/></alternatives></disp-formula>where <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mover accent="true"><mml:mrow><mml:msubsup><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {x_i^k}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_IEq4.gif"/></alternatives></inline-formula> is the representation of the <italic>i</italic><sub><italic>th</italic></sub> material at the <italic>k</italic><sub><italic>th</italic></sub> fidelity, and <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>→</mml:mo></mml:mover></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overrightarrow {f_k}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_947_Article_IEq5.gif"/></alternatives></inline-formula> is the one-hot representation of the <italic>k</italic><sub><italic>th</italic></sub> fidelity.</p><p id="Par44">The prior covariance matrix for multifidelity training data can be thought of as a block matrix by partitioning it according to the fidelities. The on-diagonal blocks of this matrix characterise correlation between materials measured within the same fidelity (and are therefore equivalent to the covariance matrices for corresponding single fidelity GPs), while the off-diagonal blocks characterise correlation between materials measured at different fidelities. By optimising or sampling kernel hyperparameters the degree to which lower fidelities are correlated with the ground truth measurements is learned; if within the training data a given approximate fidelity is not correlated with the ground truth high fidelity measurements then the length-scale associated with that fidelity’s one-hot encoded dimension will shrink and correspondingly the prior covariance between the measurements at that fidelity and ground truth measurements will tend towards zero, while the opposite will occur for fidelities that are highly correlated with the ground truth high fidelity function.</p><p id="Par45">For our model, the full prior covariance matrix is constructed using a Matern 5/2 kernel in combination with automatic relevance determination. Hyperparameters are optimised via the log marginal likelihood.</p></sec><sec id="Sec11"><title>Data sets</title><p id="Par46">As previously stated, the three datasets used in this study were chosen to span a range of thermochemical and optoelectronic properties including both experimental and computational values. The three datasets selected were the Harvard organic photovoltaic dataset (HOPV)<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>, the Alexandria quantum chemical library (Alexandria)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and the Chen Alchemical library<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>.</p></sec><sec id="Sec12"><title>Harvard Organic Photovoltaic Dataset (HOPV)</title><p id="Par47">350 molecular structures were extracted alongside their experimental power conversion efficiencies and computational analogs computed using the Scharber model<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> built from energy levels calculated using four different density functionals - BP86<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>, PBE0<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR44">44</xref></sup>, B3LYP<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup> and M062X<sup><xref ref-type="bibr" rid="CR46">46</xref>,<xref ref-type="bibr" rid="CR47">47</xref></sup> in combination with the double-ζ def2-SVP basis set<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. For this dataset the optimization target was to discover the material with the highest power conversion efficiency, with the computational analogs available as lower fidelity proxies. Costs for each fidelity were assigned as 1.0, 1.25, 1.75, 2.0 and 20.0 to evaluating at the BP86, PBE0, B3LYP, M062X level of theory and experiment respectively. Molecular structures were described using the SOAP descriptors<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> which were reduced to a 20D representation using principle component analysis.</p></sec><sec id="Sec13"><title>Alexandria dataset</title><p id="Par48">946 structures were extracted from the Alexandria dataset, containing structure which had both experimental polarizabilities and computational analogs calculated at both the Hartree-Fock level of theory in combination with the 6-31 G** basis set<sup><xref ref-type="bibr" rid="CR50">50</xref>–<xref ref-type="bibr" rid="CR52">52</xref></sup> and using the B3LYP functional<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup> in combination with the aug-cc-pVTZ basis set<sup><xref ref-type="bibr" rid="CR53">53</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>. For this dataset the optimization target was to locate the material with the highest experimental polarisability with the HF and B3LYP calculations available as lower fidelity proxies for the experimental target. Costs for each fidelity were assigned at of 1.0, 2.0 and 6.0 to evaluating at the HF and B3LYP levels of theory and via experiment respectively. Molecular structures were described using MAACS keys<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, which were reduced to 20D using a principle component analysis.</p></sec><sec id="Sec14"><title>Chen dataset</title><p id="Par49">1766 structures were extracted containing examples containing measurements of experimental bandgaps and a computational analog using the PBE functional using the projector augmented wave method and a 520 eV cut off. For this dataset the optimization target was to discover the most insulating i.e. highest bandgap material as determined by the experimental measurement with the PBE calculations available as lower fidelity proxies for the experimental target. Costs were assigned as 0.5 for evaluating the PBE calculated bandgap and 10 for evaluating the experimental values. Molecular structures were described using the SOAP descriptors<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> which were reduced to a 20D representation using principle component analysis.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>This work was supported by the Hartree National Centre for Digital Innovation, a collaboration between Science and Technology Facilities Council and IBM.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>E.P.K. and C.F. conceived the project, E.P.K. supervised the project, C.F., P.F., and A.V. performed the computational experiments and all authors analysed the output of the experiments and contributed to writing the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>All materials datasets used are publicly available at the references cited within this manuscript.</p><p><sup><xref ref-type="bibr" rid="CR39">39</xref></sup> – Alexandria Data Set</p><p><sup><xref ref-type="bibr" rid="CR38">38</xref></sup> – Harvard Organic Photovoltaic Data Set (HOPV)</p><p><sup><xref ref-type="bibr" rid="CR40">40</xref></sup> – Chen Data Set</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>Experiments were performed using IBM’s Bayesian Optimization Accelerator, a commercial program. In order to aid reproducibility, the authors have included implementation details, including pseudocode, for functions key to this paper as a dedicated section in the supplementary information.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par50">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Combinatorial materials sciences: Experimental strategies for accelerated knowledge discovery</article-title><source>Ann. Rev. Mater. Res</source><year>2008</year><volume>38</volume><fpage>299</fpage><lpage>322</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXpvVKrsLo%3D</pub-id><pub-id pub-id-type="doi">10.1146/annurev.matsci.38.060407.130217</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potyrailo</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Combinatorial and high-throughput screening of materials libraries: Review of state of the art</article-title><source>ACS combinatorial Sci.</source><year>2011</year><volume>13</volume><fpage>579</fpage><lpage>633</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXhtFWjs7nJ</pub-id><pub-id pub-id-type="doi">10.1021/co200007w</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mennen</surname><given-names>SM</given-names></name><etal/></person-group><article-title xml:lang="en">The evolution of high-throughput experimentation in pharmaceutical development and perspectives on the future</article-title><source>Org. Process Res Dev.</source><year>2019</year><volume>23</volume><fpage>1213</fpage><lpage>1242</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXptFyhsrc%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.oprd.9b00140</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name><name><surname>Suh</surname><given-names>C</given-names></name><name><surname>Gómez-Bombarelli</surname><given-names>R</given-names></name><name><surname>Aguilera-Iparraguirre</surname><given-names>J</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">What is high-throughput virtual screening? A perspective from organic materials discovery</article-title><source>Ann. Rev. Mater. Res</source><year>2015</year><volume>45</volume><fpage>195</fpage><lpage>216</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhtFWqur3L</pub-id><pub-id pub-id-type="doi">10.1146/annurev-matsci-070214-020823</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name><name><surname>Simm</surname><given-names>GN</given-names></name><name><surname>Guzik</surname><given-names>AA</given-names></name></person-group><article-title xml:lang="en">A Bayesian approach to calibrating high-throughput virtual screening results and application to organic photovoltaic materials</article-title><source>Mater. Horiz.</source><year>2016</year><volume>3</volume><fpage>226</fpage><lpage>233</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28Xislejtbk%3D</pub-id><pub-id pub-id-type="doi">10.1039/C5MH00282F</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bajorath</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Integration of virtual and high-throughput screening</article-title><source>Nat. Rev. Drug Disco.</source><year>2002</year><volume>1</volume><fpage>882</fpage><lpage>894</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD38XotleqtLY%3D</pub-id><pub-id pub-id-type="doi">10.1038/nrd941</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hautier</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Finding the needle in the haystack: Materials discovery and design through computational ab initio high-throughput screening</article-title><source>Comput. Mater. Sci.</source><year>2019</year><volume>163</volume><fpage>108</fpage><lpage>116</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXlvFegtbw%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2019.02.040</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suh</surname><given-names>C</given-names></name><name><surname>Fare</surname><given-names>C</given-names></name><name><surname>Warren</surname><given-names>JA</given-names></name><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name></person-group><article-title xml:lang="en">Evolving the materials genome: How machine learning is fueling the next generation of materials discovery</article-title><source>Ann. Rev. Mater. Res</source><year>2020</year><volume>50</volume><fpage>1</fpage><lpage>25</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXnslWjsr8%3D</pub-id><pub-id pub-id-type="doi">10.1146/annurev-matsci-082019-105100</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name><etal/></person-group><article-title xml:lang="en">Accelerating materials discovery using artificial intelligence, high performance computing and robotics</article-title><source>NPJ Comput. Mater.</source><year>2022</year><volume>8</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41524-022-00765-z</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JS</given-names></name><name><surname>Isayev</surname><given-names>O</given-names></name><name><surname>Roitberg</surname><given-names>AE</given-names></name></person-group><article-title xml:lang="en">ANI-1: An extensible neural network potential with DFT accuracy at force field computational cost</article-title><source>Chem. Sci.</source><year>2017</year><volume>8</volume><fpage>3192</fpage><lpage>3203</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXitlGnsrs%3D</pub-id><pub-id pub-id-type="doi">10.1039/C6SC05720A</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behler</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Representing potential energy surfaces by high-dimensional neural network potentials</article-title><source>J. Phys.: Condens. Matter</source><year>2014</year><volume>26</volume><fpage>183001</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXovFGgtbw%3D</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behler</surname><given-names>J</given-names></name><name><surname>Martoňák</surname><given-names>R</given-names></name><name><surname>Donadio</surname><given-names>D</given-names></name><name><surname>Parrinello</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Metadynamics simulations of the high-pressure phases of silicon employing a high-dimensional neural network potential</article-title><source>Phys. Rev. Lett.</source><year>2008</year><volume>100</volume><fpage>185501</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.100.185501</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name><name><surname>Li</surname><given-names>K</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Learning from the harvard clean energy project: The use of neural networks to accelerate materials discovery</article-title><source>Adv. Funct. Mater.</source><year>2015</year><volume>25</volume><fpage>6495</fpage><lpage>6502</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhsFWrtL3I</pub-id><pub-id pub-id-type="doi">10.1002/adfm.201501919</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balachandran</surname><given-names>PV</given-names></name></person-group><article-title xml:lang="en">Machine learning guided design of functional materials with targeted properties</article-title><source>Comput. Mater. Sci.</source><year>2019</year><volume>164</volume><fpage>82</fpage><lpage>90</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXntVGjs7c%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2019.03.057</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chibani</surname><given-names>S</given-names></name><name><surname>Coudert</surname><given-names>F-X</given-names></name></person-group><article-title xml:lang="en">Machine learning approaches for the prediction of materials properties</article-title><source>APL Mater.</source><year>2020</year><volume>8</volume><fpage>080701</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFGisb3J</pub-id><pub-id pub-id-type="doi">10.1063/5.0018384</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meng</surname><given-names>X</given-names></name><name><surname>Karniadakis</surname><given-names>GE</given-names></name></person-group><article-title xml:lang="en">A composite neural network that learns from multi-fidelity data: Application to function approximation and inverse PDE problems</article-title><source>J. Comput. Phys.</source><year>2020</year><volume>401</volume><fpage>109020</fpage><pub-id pub-id-type="doi">10.1016/j.jcp.2019.109020</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C-H</given-names></name><etal/></person-group><article-title xml:lang="en">Multi-fidelity machine learning models for structure–property mapping of organic electronics</article-title><source>Comput. Mater. Sci.</source><year>2022</year><volume>213</volume><fpage>111599</fpage><pub-id pub-id-type="doi">10.1016/j.commatsci.2022.111599</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramakrishnan</surname><given-names>R</given-names></name><name><surname>Dral</surname><given-names>PO</given-names></name><name><surname>Rupp</surname><given-names>M</given-names></name><name><surname>von Lilienfeld</surname><given-names>OA</given-names></name></person-group><article-title xml:lang="en">Big data meets quantum chemistry approximations: The Δ-machine learning approach</article-title><source>J. Chem. Theory Comput.</source><year>2015</year><volume>11</volume><fpage>2087</fpage><lpage>2096</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXmtlams7Y%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jctc.5b00099</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Learning properties of ordered and disordered materials from multi-fidelity data</article-title><source>Nat. Comput Sci.</source><year>2021</year><volume>1</volume><fpage>46</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1038/s43588-020-00002-x</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdew</surname><given-names>JP</given-names></name><name><surname>Burke</surname><given-names>K</given-names></name><name><surname>Ernzerhof</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Generalized gradient approximation made simple</article-title><source>Phys. Rev. Lett.</source><year>1996</year><volume>77</volume><fpage>3865</fpage><lpage>3868</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK28XmsVCgsbs%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.77.3865</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patra</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">A multi-fidelity information-fusion approach to machine learn and predict polymer bandgap</article-title><source>Comput. Mater. Sci.</source><year>2020</year><volume>172</volume><fpage>109286</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvFWitbbI</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2019.109286</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Brochu, E., Cora, V. M. &amp; de Freitas, N. A. Tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and Hierarchical reinforcement learning. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1012.2599" ext-link-type="uri">https://arxiv.org/abs/1012.2599</ext-link></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kandasamy</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Tuning hyperparameters without grad students: Scalable and Robust Bayesian optimisation with dragonfly</article-title><source>J. Mach. Learn. Res.</source><year>2020</year><volume>21</volume><fpage>1</fpage><lpage>27</lpage></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Bayesian optimization for accelerated drug discovery</article-title><source>IBM J. Res Dev.</source><year>2018</year><volume>62</volume><fpage>2</fpage><lpage>1</lpage><pub-id pub-id-type="doi">10.1147/JRD.2018.2881731</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Lam, R., Poloczek, M., Frazier, P. &amp; Willcox, K. E. Advances in Bayesian optimization with applications in aerospace engineering. In <italic>2018 AIAA Non-Deterministic Approaches Conference</italic> 1656 (2018).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pyzer-Knapp</surname><given-names>EO</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Day</surname><given-names>GM</given-names></name><name><surname>Cooper</surname><given-names>AI</given-names></name></person-group><article-title xml:lang="en">Accelerating computational discovery of porous solids through improved navigation of energy-structure-function maps</article-title><source>Sci. Adv.</source><year>2021</year><volume>7</volume><fpage>eabi4763</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXitVKku7%2FM</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.abi4763</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Apley</surname><given-names>DW</given-names></name><name><surname>Chen</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Bayesian optimization for materials design with mixed quantitative and qualitative variables</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>13</lpage></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Mockus, J. The Bayesian approach to global optimization. in <italic>System Modeling and Optimization</italic> 473–481 (Springer, Berlin, Heidelberg, 1982).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Springenberg</surname><given-names>JT</given-names></name><name><surname>Klein</surname><given-names>A</given-names></name><name><surname>Falkner</surname><given-names>S</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Bayesian optimization with robust Bayesian neural networks. Adv. Neural Inform</article-title><source>Process. Sys.</source><year>2016</year><volume>29</volume><fpage>2171</fpage><lpage>2180</lpage></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Snoek, J. et al. Scalable Bayesian Optimization Using Deep Neural Networks. <italic>arXiv Preprint at</italic><ext-link xlink:href="https://arxiv.org/abs/1502.05700" ext-link-type="uri">https://arxiv.org/abs/1502.05700</ext-link> (2015).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Hernández-Lobato, J. M., Requeima, J., Pyzer-Knapp, E. O. &amp; Aspuru-Guzik, A. Parallel and distributed Thompson sampling for large-scale accelerated exploration of chemical space. in <italic>International Conference On Machine Learning</italic> 1470–1479 (PMLR, 2017).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Wang, K. A. et al. Exact Gaussian Processes on a Million Data Points. Preprint at <ext-link xlink:href="https://arxiv.org/abs/1903.08114" ext-link-type="uri">https://arxiv.org/abs/1903.08114</ext-link> (2019).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Ong</surname><given-names>Y-S</given-names></name></person-group><article-title xml:lang="en">Remarks on multi-output Gaussian process regression</article-title><source>Knowl. Based Syst.</source><year>2018</year><volume>144</volume><fpage>102</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.knosys.2017.12.034</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Song, J., Yuxin, C. &amp; Yue, Y. A General Framework for Multi-fidelity Bayesian Optimization with Gaussian Processes. <italic>The 22nd International Conference on Artificial Intelligence and Statistics</italic>. PMLR, 2019.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Palizhati, A., Aykol, M., Suram, S., Hummelshøj, J. S. &amp; Montoya, J. H. Multi-fidelity Sequential Learning for Accelerated Materials Discovery. Preprint at <ext-link xlink:href="10.26434/chemrxiv.14312612.v1" ext-link-type="doi">https://doi.org/10.26434/chemrxiv.14312612.v1</ext-link> (2021)</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Fare, C., Fenner, P. &amp; Pyzer-Knapp, E. O. A Principled Method for the Creation of Synthetic Multi-fidelity Data Sets. Preprint at <ext-link xlink:href="https://arxiv.org/abs/2208.05667" ext-link-type="uri">https://arxiv.org/abs/2208.05667</ext-link> (2022).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Rasmussen, C. &amp; Williams, C. <italic>Gaussian Processes for Machine Learning</italic>. (MIT Press, 2006).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>SA</given-names></name><etal/></person-group><article-title xml:lang="en">The Harvard organic photovoltaic dataset</article-title><source>Sci. Data</source><year>2016</year><volume>3</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/sdata.2016.86</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghahremanpour</surname><given-names>MM</given-names></name><name><surname>Van Maaren</surname><given-names>PJ</given-names></name><name><surname>Van Der Spoel</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">The Alexandria library, a quantum-chemical database of molecular properties for force field development</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.62</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Chen, G. et al. Alchemy: A quantum chemistry dataset for benchmarking ai models. <italic>Preprint at</italic><ext-link xlink:href="https://arxiv.org/abs/1906.09427" ext-link-type="uri">https://arxiv.org/abs/1906.09427</ext-link> (2019).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scharber</surname><given-names>MC</given-names></name><etal/></person-group><article-title xml:lang="en">Design rules for donors in bulk-heterojunction solar cellstextemdashtowards 10% energy-conversion efficiency</article-title><source>Adv. Mater.</source><year>2006</year><volume>18</volume><fpage>789</fpage><lpage>794</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD28XjsV2isrw%3D</pub-id><pub-id pub-id-type="doi">10.1002/adma.200501717</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becke</surname><given-names>AD</given-names></name></person-group><article-title xml:lang="en">Density-functional exchange-energy approximation with correct asymptotic behavior</article-title><source>Phys. Rev. A</source><year>1988</year><volume>38</volume><fpage>3098</fpage><lpage>3100</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1cXmtlOhsLo%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevA.38.3098</pub-id></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdew</surname><given-names>JP</given-names></name></person-group><article-title xml:lang="en">Density-functional approximation for the correlation energy of the inhomogeneous electron gas</article-title><source>Phys. Rev. B</source><year>1986</year><volume>33</volume><fpage>8822</fpage><lpage>8824</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DC%2BC2sfgsFSktA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevB.33.8822</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perdew</surname><given-names>JP</given-names></name><name><surname>Ernzerhof</surname><given-names>M</given-names></name><name><surname>Burke</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Rationale for mixing exact exchange with density functional approximations</article-title><source>J. Chem. Phys.</source><year>1996</year><volume>105</volume><fpage>9982</fpage><lpage>9985</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK28XnsFahtbg%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.472933</pub-id></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becke</surname><given-names>AD</given-names></name></person-group><article-title xml:lang="en">Densityfunctional thermochemistry. III. The role of exact exchange</article-title><source>J. Chem. Phys.</source><year>1993</year><volume>98</volume><fpage>5648</fpage><lpage>5652</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK3sXisVWgtrw%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.464913</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Truhlar</surname><given-names>DG</given-names></name></person-group><article-title xml:lang="en">The M06 suite of density functionals for main group thermochemistry, thermochemical kinetics, noncovalent interactions, excited states, and transition elements: two new functionals and systematic testing of four M06-class functionals and 12 other functionals</article-title><source>Theor. Chem. Acc.</source><year>2008</year><volume>120</volume><fpage>215</fpage><lpage>241</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXltFyltbY%3D</pub-id><pub-id pub-id-type="doi">10.1007/s00214-007-0310-x</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Truhlar</surname><given-names>DG</given-names></name></person-group><article-title xml:lang="en">Density functionals with broad applicability in chemistry</article-title><source>Acc. Chem. Res.</source><year>2008</year><volume>41</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXksV2iug%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/ar700111a</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weigend</surname><given-names>F</given-names></name><name><surname>Ahlrichs</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Balanced basis sets of split valence, triple zeta valence and quadruple zeta valence quality for H to Rn: Design and assessment of accuracy</article-title><source>Phys. Chem. Chem. Phys.</source><year>2005</year><volume>7</volume><fpage>3297</fpage><lpage>3305</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2MXpsFWgu7o%3D</pub-id><pub-id pub-id-type="doi">10.1039/b508541a</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartók</surname><given-names>AP</given-names></name><name><surname>Kondor</surname><given-names>R</given-names></name><name><surname>Csányi</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">On representing chemical environments</article-title><source>Phys. Rev. B</source><year>2013</year><volume>87</volume><fpage>184115</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.87.184115</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hehre</surname><given-names>WJ</given-names></name><name><surname>Stewart</surname><given-names>RF</given-names></name><name><surname>Pople</surname><given-names>JA</given-names></name></person-group><article-title xml:lang="en">SelfConsistent molecularorbital methods. I. Use of gaussian expansions of slatertype atomic orbitals</article-title><source>J. Chem. Phys.</source><year>1969</year><volume>51</volume><fpage>2657</fpage><lpage>2664</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaF1MXltVGnsrY%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.1672392</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hehre</surname><given-names>WJ</given-names></name><name><surname>Ditchfield</surname><given-names>R</given-names></name><name><surname>Stewart</surname><given-names>RF</given-names></name><name><surname>Pople</surname><given-names>JA</given-names></name></person-group><article-title xml:lang="en">SelfConsistent molecular orbital methods. IV. Use of gaussian expansions of slatertype orbitals. extension to secondrow molecules</article-title><source>J. Chem. Phys.</source><year>1970</year><volume>52</volume><fpage>2769</fpage><lpage>2773</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaE3cXptlCjtQ%3D%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.1673374</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hehre</surname><given-names>WJ</given-names></name><name><surname>Ditchfield</surname><given-names>R</given-names></name><name><surname>Pople</surname><given-names>JA</given-names></name></person-group><article-title xml:lang="en">Self—consistent molecular orbital methods. XII. Further extensions of Gaussian—type basis sets for use in molecular orbital studies of organic molecules</article-title><source>J. Chem. Phys.</source><year>1972</year><volume>56</volume><fpage>2257</fpage><lpage>2261</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaE38XptVemsw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.1677527</pub-id></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kendall</surname><given-names>RA</given-names></name><name><surname>Dunning</surname><given-names>TH</given-names><suffix>Jr.</suffix></name><name><surname>Harrison</surname><given-names>RJ</given-names></name></person-group><article-title xml:lang="en">Electron affinities of the first-row atoms revisited. Systematic basis sets and wave functions</article-title><source>J. Chem. Phys.</source><year>1992</year><volume>96</volume><fpage>6796</fpage><lpage>6806</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK38XktFClurw%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.462569</pub-id></mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woon</surname><given-names>DE</given-names></name><name><surname>Dunning</surname><given-names>TH</given-names><suffix>Jr.</suffix></name></person-group><article-title xml:lang="en">Benchmark calculations with correlated molecular wave functions. I. Multireference configuration interaction calculations for the second row diatomic hydrides</article-title><source>J. Chem. Phys.</source><year>1993</year><volume>99</volume><fpage>1914</fpage><lpage>1929</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK3sXmtFWjs78%3D</pub-id><pub-id pub-id-type="doi">10.1063/1.465306</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Durant</surname><given-names>JL</given-names></name><name><surname>Leland</surname><given-names>BA</given-names></name><name><surname>Henry</surname><given-names>DR</given-names></name><name><surname>Nourse</surname><given-names>JG</given-names></name></person-group><article-title xml:lang="en">Reoptimization of MDL keys for use in drug discovery</article-title><source>J. Chem. Inf. Comput. Sci.</source><year>2002</year><volume>42</volume><fpage>1273</fpage><lpage>1280</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD38XmvFKktLY%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci010132r</pub-id></mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec15"><title>Supplementary information</title><p id="Par51"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2022_947_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>A Multi-Fidelity Machine Learning Approach to High Throughput Materials Screening – Supplemental Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-022-00947-9" ext-link-type="doi">https://doi.org/10.1038/s41524-022-00947-9</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2022</facet-value></facet><facet name="country"><facet-value count="1">United Kingdom</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
