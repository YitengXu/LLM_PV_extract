<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-022-00921-5</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-022-00921-5</article-id><article-id pub-id-type="manuscript">921</article-id><article-id pub-id-type="doi">10.1038/s41524-022-00921-5</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/638/298</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Crystal twins: self-supervised learning for crystalline material property prediction</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6216-0518</contrib-id><name><surname>Magar</surname><given-names>Rishikesh</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0723-6246</contrib-id><name><surname>Wang</surname><given-names>Yuyang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2952-8576</contrib-id><name><surname>Barati Farimani</surname><given-names>Amir</given-names></name><address><email>barati@cmu.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="corresp" rid="IDs41524022009215_cor3">c</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Materials Science and Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Chemical Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs41524022009215_cor3"><label>c</label><email>barati@cmu.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>10</day><month>11</month><year>2022</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2022</year></pub-date><volume>8</volume><issue seq="230">1</issue><elocation-id>231</elocation-id><history><date date-type="registration"><day>25</day><month>10</month><year>2022</year></date><date date-type="received"><day>24</day><month>5</month><year>2022</year></date><date date-type="accepted"><day>23</day><month>10</month><year>2022</year></date><date date-type="online"><day>10</day><month>11</month><year>2022</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2022</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Machine learning (ML) models have been widely successful in the prediction of material properties. However, large labeled datasets required for training accurate ML models are elusive and computationally expensive to generate. Recent advances in Self-Supervised Learning (SSL) frameworks capable of training ML models on unlabeled data mitigate this problem and demonstrate superior performance in computer vision and natural language processing. Drawing inspiration from the developments in SSL, we introduce Crystal Twins (CT): a generic SSL method for crystalline materials property prediction that can leverage large unlabeled datasets. CT adapts a twin Graph Neural Network (GNN) and learns representations by forcing graph latent embeddings of augmented instances obtained from the same crystalline system to be similar. We implement Barlow Twins and SimSiam frameworks in CT. By sharing the pre-trained weights when fine-tuning the GNN for downstream tasks, we significantly improve the performance of GNN on 14 challenging material property prediction benchmarks.</p></abstract><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>231</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>10</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>25</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2022_Article_921.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Machine Learning (ML) based predictive models have made rapid strides in computational chemistry due to their efficiency and performance. Characterized by their computational efficiency and accuracy, these methods are capable of faster high-throughput screening compared to classical physics models<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. This capability has roots in both novel learning algorithms and improved hardware. Even though ML models can offer faster predictions, the accuracy of these models is highly correlated with the availability of clean labeled data<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. In general, it is difficult to develop accurate and robust ML models without sufficiently large labeled data<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Moreover, the acquisition of labeled data is expensive as it involves performing Density Functional Theory (DFT) simulations or experiments to characterize materials<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref></sup>. On the other hand, gigantic databases containing structures and compositions of materials without labels (properties) are available. These databases cannot be used in supervised learning tasks due to the lack of labels. Given the availability of large unlabeled datasets, two interesting questions are raised: (1) can we develop more efficient ML models that are capable of learning the underlying structural chemistry from unlabeled data, and (2) can these models be used to make the supervised learning tasks more accurate?</p><p id="Par3">In this work, we aim to address these questions by leveraging Self-Supervised Learning (SSL) for material property prediction. Unlike supervised learning which uses labels for supervision, SSL makes use of the large unlabeled data for supervision to learn robust and generalizable representations that can be used for various tasks. Recently, SSL frameworks such as SimCLR<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, Barlow Twins<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, BYOL<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>, SwAV<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, MoCo<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, SimSiam<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, Albert<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>, and self-supervised dialog learning<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> have been successfully applied to computer vision and natural language processing tasks. The success of these SSL methods has inspired many works in molecular ML, leading to the development of highly accurate frameworks such as MolCLR<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>, dual view molecule pre-training<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, 3D Infomax<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>, and numerous other popular works<sup><xref ref-type="bibr" rid="CR18">18</xref>–<xref ref-type="bibr" rid="CR25">25</xref></sup>. It should be noted that SSL-based methods have been developed for molecules, which have finite structures. However, the periodic crystalline materials are different from the molecules, since crystalline materials are composed of infinitely repeating unit cells of atoms, ions, or molecules. Besides, crystalline materials can have non-covalent bonds that are different from covalent bonds in molecules. Based on the differences, specialized deep learning architectures explicitly modeling crystals are required.</p><p id="Par4">Most of the promising works developed for material property prediction tasks are using graph neural networks (GNN). GNNs consider non-Euclidean topology to construct a graph representation that can be learned and modified according to the task<sup><xref ref-type="bibr" rid="CR26">26</xref>–<xref ref-type="bibr" rid="CR28">28</xref></sup>. In general, the GNNs developed for material property prediction take input the 3D coordinates of the crystal and construct the graph by modeling atoms as the nodes and the interactions between the atoms as edges. GNNs developed for material property prediction include CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, SchNet<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>, MegNet<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, and other models<sup><xref ref-type="bibr" rid="CR33">33</xref>–<xref ref-type="bibr" rid="CR44">44</xref></sup>. Developments have also been made in tasks such as material structure generation and prediction<sup><xref ref-type="bibr" rid="CR45">45</xref>–<xref ref-type="bibr" rid="CR50">50</xref></sup> as well as identifying new materials with specific properties<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. Despite progress being made in developing self-supervised ML architectures in the molecular ML, there is a noticeable lack of research works implementing such techniques for the periodic crystalline systems property prediction.</p><p id="Par5">In this work, we introduce Crystal Twins (CT): an SSL framework for crystalline material property prediction with GNNs (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). In pre-training, the models in the CT framework does not make use of any labeled data to learn crystalline representations, instead, it trains ML models in a self-supervised manner. In the CT framework, we use the CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> as the encoder to learn expressive representations of crystalline system. We adapt two different SSL pretraining methods based on Barlow Twins<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> and SimSiamese<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> loss functions. In CT<sub>Barlow</sub> which uses Barlow twins loss for pre-training, the GNN encoder generates representations of two augmented instances from the same crystal and the objective of pre-training is to make the cross-correlation matrix of the two embeddings as close as possible to the identity matrix (Fig. <xref rid="Fig1" ref-type="fig">1A</xref>). In the other model CT<sub>SimSiam</sub> that uses SimSiamese<sup><xref ref-type="bibr" rid="CR7">7</xref></sup> loss function for pre-training, the objective is to maximize the cosine similarity between the embeddings generated from the graph encoder CGCNN for two augmented instances. Additionally, in CT<sub>SimSiam</sub>, one branch has the stop gradient operation and the other has predictor head after the graph encoder (Fig. <xref rid="Fig1" ref-type="fig">1B</xref>). To create augmented instances, we introduce the combination of three different augmentation techniques: random perturbations, atom masking, and edge masking (Fig. <xref rid="Fig1" ref-type="fig">1C</xref>). The representations learned by the encoder are later used for downstream material property prediction tasks in the fine-tuning stage (Fig. <xref rid="Fig1" ref-type="fig">1D</xref>). In the pre-training stage, graph encoder learns representations from unlabeled data. Using the pre-trained weights to initialize the graph encoder for fine-tuning, both CT<sub>Barlow</sub> and CT<sub>SimSiam</sub> demonstrate superior prediction performances on 14 challenging datasets. We also compare the performance of the CT models with other competitive supervised learning baselines. We have successfully demonstrated the use of self-supervised learning for crystalline material property prediction.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Overview of the crystal twins (CT) framework.</title><p>We propose two methodologies for SSL pre-training based on the Barlow Twins loss and SimSiamese loss function. The CT framework takes the structural file (CIF) as the input and then augments the structure to create two different augmented instances. (<bold>A</bold>) In CT<sub>Barlow</sub>, each instance is passed to the CGCNN graph encoder followed by a projector to generate embedding. The pre-training objective aims to maximize the cross-correlation between the two embeddings. (<bold>B</bold>) The CT<sub>SimSiam</sub>, each instance is passed through same CGCNN encoder branch to generate embeddings. One branch has an projector MLP head after the encoder and the other branch has stop-gradient operation. The pre-training objective is to maximize similarity between the embeddings. (<bold>C</bold>) To create augmented instances, three augmentation techniques are used in this work: random perturbations, atom masking, and edge masking. (<bold>D</bold>) In the pre-training stage we trained using SSL. In the fine-tuning stage, the pre-trained weights are shared with the encoder (CGCNN) which is trained to predict the material property.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_921_Fig1_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><p id="Par6">To comprehensively evaluate the performance of models using the CT framework, we test its performance on 13 challenging regression benchmark datasets and 1 classification dataset. The capabilities of the models in the CT framework are tested on a wide variety of properties including exfoliation energy, frequency of the highest frequency optical phonon mode peak, band gap, formation energy, refractive index, bulk modulus, shear modulus, Fermi energy, and metallicity. An overview of the datasets used for benchmarking the performance of the models in the CT framework is shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Among the total 14 datasets, we benchmark the performance of the models on 9 datasets (Table <xref rid="Tab2" ref-type="table">2</xref>) from the MatBench suite and the remaining 5 datasets (Table <xref rid="Tab3" ref-type="table">3</xref>) follow the datasets used in previously published works of CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. More detailed descriptions of these datasets are available in the <xref ref-type="supplementary-material" rid="MOESM1">Supplementary Information</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Overview of the datasets used for benchmarking the performance of the CT framework.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th><p># Crystals</p></th><th><p>Property</p></th><th><p>Unit</p></th></tr></thead><tbody><tr><td><p>JDFT2D(JDFT)<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></p></td><td><p>636</p></td><td><p>Exfoliation Energy</p></td><td><p>meV per atom</p></td></tr><tr><td><p>Phonons<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></p></td><td><p>1,265</p></td><td><p>Last Phdos Peak</p></td><td><p>1 per cm</p></td></tr><tr><td><p>HOIP<sup><xref ref-type="bibr" rid="CR61">61</xref></sup></p></td><td><p>1,345</p></td><td><p>Band Gap</p></td><td><p>eV</p></td></tr><tr><td><p>Lanthanides<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></p></td><td><p>4,166</p></td><td><p>Formation Energy</p></td><td><p>eV per atom</p></td></tr><tr><td><p>Dielectric<sup><xref ref-type="bibr" rid="CR65">65</xref></sup></p></td><td><p>4,764</p></td><td><p>Refractive Index</p></td><td><p>Unitless</p></td></tr><tr><td><p>GVRH<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup></p></td><td><p>10,987</p></td><td><p>Shear Modulus</p></td><td><p><inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\log }_{10}VRH$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq1.gif"/></alternatives></inline-formula></p></td></tr><tr><td><p>KVRH<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></p></td><td><p>10,987</p></td><td><p>Bulk Modulus</p></td><td><p><inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mi>V</mml:mi><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:mrow></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\log }_{10}VRH$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq2.gif"/></alternatives></inline-formula></p></td></tr><tr><td><p>Perovskites<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></p></td><td><p>18,928</p></td><td><p>Formation Energy</p></td><td><p>eV per atom</p></td></tr><tr><td><p>Fermi Energy<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>26,447</p></td><td><p>Fermi energy</p></td><td><p>eV</p></td></tr><tr><td><p>Formation Energy (FE)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>26,078</p></td><td><p>Formation energy</p></td><td><p>eV per atom</p></td></tr><tr><td><p>Band Gap (BG)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>26,709</p></td><td><p>Band Gap</p></td><td><p>eV</p></td></tr><tr><td><p>MP-Is Metal (Is Metal)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>106,113</p></td><td><p>Metallicity</p></td><td><p>NA</p></td></tr><tr><td><p>MP-Gap (MP-BG)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>106,113</p></td><td><p>Band Gap</p></td><td><p>eV</p></td></tr><tr><td><p>MP-E-Form (MP-FE)<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td><td><p>132,752</p></td><td><p>Formation Energy</p></td><td><p>eV per atom</p></td></tr></tbody></table><table-wrap-foot><p>Five of these datasets are in accordance to the previously published works of CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> and OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. Additionally, we have also benchmarked on 9 datasets aggregated from the MatBench suite<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>.</p></table-wrap-foot></table-wrap><table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Mean and standard deviation of test MAE of Crystal Twins (CT) in comparison to the supervised baselines on MatBench<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> regression benchmarks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th><p>JDFT<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></p></th><th><p>Phonons<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></p></th><th><p>Dielectric<sup><xref ref-type="bibr" rid="CR65">65</xref></sup></p></th><th><p>GVRH<sup><xref ref-type="bibr" rid="CR59">59</xref>,<xref ref-type="bibr" rid="CR66">66</xref></sup></p></th><th><p>KVRH<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></p></th><th><p>Perovskites<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></p></th><th><p>MP-BG<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th><th><p>MP-FE<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th><th><p>Is Metal<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th></tr><tr><th><p># Crystals</p></th><th><p>636</p></th><th><p>1265</p></th><th><p>4764</p></th><th><p>10,987</p></th><th><p>10,987</p></th><th><p>18,928</p></th><th><p>106,113</p></th><th><p>132,752</p></th><th><p>106,113</p></th></tr></thead><tbody><tr><td><p>CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></p></td><td><p>49.24 ± 11.58</p></td><td><p>57.36 ± 12.31</p></td><td><p>0.599 ± 0.083</p></td><td><p>0.089 ± 0.001</p></td><td><p>0.071 ± 0.002</p></td><td><p>0.045 ± 0.001</p></td><td><p>0.297 ± 0.003</p></td><td><p><underline>0.033</underline> <underline>±</underline> <underline>0.001</underline></p></td><td><p><bold>0.952</bold> <bold>±</bold> <bold>0.007</bold></p></td></tr><tr><td><p>AMMExpress<sup><xref ref-type="bibr" rid="CR42">42</xref></sup></p></td><td><p><bold>39.84</bold> <bold>±</bold> <bold>09.88</bold></p></td><td><p>56.17 ± 06.80</p></td><td><p><bold>0.315</bold> <bold>±</bold> <bold>0.067</bold></p></td><td><p><underline>0.087</underline> <underline>±</underline> <underline>0.002</underline></p></td><td><p><underline>0.065</underline> <underline>±</underline> <underline>0.002</underline></p></td><td><p>0.200 ± 0.009</p></td><td><p>0.282 ± 0.006</p></td><td><p>0.172 ± 0.208</p></td><td><p>0.909 ± 0.001</p></td></tr><tr><td><p>ALIGNN<sup><xref ref-type="bibr" rid="CR43">43</xref></sup></p></td><td><p><underline>43.42</underline> <underline>±</underline> <underline>08.95</underline></p></td><td><p><bold>29.53</bold> <bold>±</bold> <bold>02.11</bold></p></td><td><p><underline>0.345</underline> <underline>±</underline> <underline>0.087</underline></p></td><td><p><bold>0.071</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p><bold>0.057</bold> <bold>±</bold> <bold>0.003</bold></p></td><td><p><bold>0.029</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p><bold>0.186</bold> <bold>±</bold> <bold>0.003</bold></p></td><td><p><bold>0.022</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p>0.913 ± 0.001</p></td></tr><tr><td><p>CT<sub>Barlow</sub></p></td><td><p>46.79 ± 19.92</p></td><td><p>50.33 ± 08.88</p></td><td><p>0.434 ± 0.100</p></td><td><p><underline>0.086</underline> <underline>±</underline> <underline>0.004</underline></p></td><td><p><underline>0.067</underline> <underline>±</underline> <underline>0.003</underline></p></td><td><p><underline>0.042</underline> <underline>±</underline> <underline>0.001</underline></p></td><td><p><underline>0.264</underline> <underline>±</underline> <underline>0.011</underline></p></td><td><p>0.037 ± 0.001</p></td><td><p>0.945 ± 0.004</p></td></tr><tr><td><p>CT<sub>SimSiam</sub></p></td><td><p>48.38 ± 18.68</p></td><td><p><underline>48.86</underline> <underline>±</underline> <underline>07.69</underline></p></td><td><p>0.417 ± 0.079</p></td><td><p><underline>0.087</underline> <underline>±</underline> <underline>0.003</underline></p></td><td><p><underline>0.067</underline> <underline>±</underline> <underline>0.003</underline></p></td><td><p><underline>0.042</underline> <underline>±</underline> <underline>0.001</underline></p></td><td><p>0.281 ± 0.025</p></td><td><p>0.037 ± 0.000</p></td><td><p><underline>0.947</underline> <underline>±</underline> <underline>0.003</underline></p></td></tr></tbody></table><table-wrap-foot><p>The results in the table follow the protocols from MatBench. The best performing result has been shown in boldface and next best performing result has been underlined. The mean and the standard deviation are calculated over 5 folds following the MatBench protocol.</p></table-wrap-foot></table-wrap><table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Mean and standard deviation of test MAE of Crystal Twins (CT) in comparison to the supervised baselines on 5 regression benchmarks.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Dataset</p></th><th><p>HOIP<sup><xref ref-type="bibr" rid="CR61">61</xref></sup></p></th><th><p>Lanthanides<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></p></th><th><p>Fermi Energy<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th><th><p>FE<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th><th><p>BG<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></th></tr><tr><th><p># Crystals</p></th><th><p>1333</p></th><th><p>4166</p></th><th><p>26,447</p></th><th><p>26,078</p></th><th><p>26,709</p></th></tr></thead><tbody><tr><td><p>GIN<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></p></td><td><p>0.666 ± 0.123</p></td><td><p>0.197 ± 0.038</p></td><td><p>0.605 ± 0.015</p></td><td><p>0.109 ± 0.007</p></td><td><p>0.601 ± 0.038</p></td></tr><tr><td><p>CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></p></td><td><p>0.170 ± 0.013</p></td><td><p>0.080 ± 0.003</p></td><td><p><underline>0.400</underline> <underline>±</underline> <underline>0.003</underline></p></td><td><p>0.040 ± 0.001</p></td><td><p>0.369 ± 0.003</p></td></tr><tr><td><p>OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></p></td><td><p>0.164 ± 0.013</p></td><td><p>0.072 ± 0.002</p></td><td><p>0.446 ± 0.018</p></td><td><p><underline>0.035</underline> <underline>±</underline> <underline>0.001</underline></p></td><td><p>0.353 ± 0.008</p></td></tr><tr><td><p>GIN<sub>Barlow</sub></p></td><td><p>0.395 ± 0.007</p></td><td><p>0.094 ± 0.000</p></td><td><p>0.478 ± 0.125</p></td><td><p>0.085 ± 0.003</p></td><td><p>0.337 ± 0.004</p></td></tr><tr><td><p>CT<sub>Barlow</sub></p></td><td><p><underline>0.153</underline> <underline>±</underline> <underline>0.003</underline></p></td><td><p><underline>0.058</underline> <underline>±</underline> <underline>0.001</underline></p></td><td><p><underline>0.399</underline> <underline>±</underline> <underline>0.004</underline></p></td><td><p><bold>0.025</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p><underline>0.328</underline> <underline>±</underline> <underline>0.002</underline></p></td></tr><tr><td><p>CT<sub>SimSiam</sub></p></td><td><p><bold>0.140</bold> <bold>±</bold> <bold>0.004</bold></p></td><td><p><bold>0.054</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p><bold>0.384</bold> <bold>±</bold> <bold>0.004</bold></p></td><td><p><bold>0.024</bold> <bold>±</bold> <bold>0.001</bold></p></td><td><p><bold>0.302</bold> <bold>±</bold> <bold>0.001</bold></p></td></tr></tbody></table><table-wrap-foot><p>The benchmark datasets have been taken from previously published OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>. The best performing result has been shown in boldface and the second best performing result has been underlined. The mean and the standard deviation have been calculated over 3 different runs.</p></table-wrap-foot></table-wrap></p><sec id="Sec3"><title>Benchmarking the models on the MatBench Suite</title><p id="Par7">The MatBench<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> suite consists of multiple material property prediction datasets. In this work, we consider 9 datasets that have crystal structures as input for benchmarking our self-supervised learning models CT<sub>Barlow</sub> and CT<sub>SimSiam</sub>. We compare the results of our framework with the previously published supervised learning baselines available on MatBench. The protocols for benchmarking the performance of CT<sub>Barlow</sub> and CT<sub>SimSiam</sub> are exactly same as introduced in MatBench. We make use of nested 5 fold cross validation to generate the results in Table <xref rid="Tab2" ref-type="table">2</xref>. The detailed hyperparameters used for finetuning models are listed in the <xref ref-type="supplementary-material" rid="MOESM1">Supplementary Information</xref> (Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). We observe that the models trained using SSL based approach consistently outperform the supervised learning CGCNN baseline. Improved results for models in CT framework over the CGCNN baseline are observed for 7 out of the 9 datasets. For the Is Metal dataset, the performance of the models in CT framework are within the standard deviation of the supervised model. We also compare the performance of our SSL model with AMMExpress<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> model in the MatBench suite. We observe that models in the CT framework outperform AMMExpress on 6 out of the 9 datasets. Additionally, we also benchmark our model against the state-of-the-art model for material property prediction ALIGNN<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. It was observed that our model performs better than ALIGNN only for the classification task. It must be noted that the ALIGNN achieves this high performance by modeling three-body interactions whereas CGCNN models two-body interactions. The enhancement of explicitly modeling three-body interactions gives ALIGNN more expressive power than CGCNN making it a more accurate baseline. Since we are using CGCNN as the graph encoder model in the CT framework, the CT<sub>Barlow</sub> and CT<sub>SimSiam</sub> are essentially modeling two-body interactions and are unable to compete with ALIGNN. The improvements demonstrated in our results over supervised learning baselines CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, AMMExpress<sup><xref ref-type="bibr" rid="CR42">42</xref></sup> show the promise of using SSL for learning representation of crystalline materials.</p></sec><sec id="Sec4"><title>Benchmarking the models on additional datasets</title><p id="Par8">Apart from benchmarking the performance on datasets from the MatBench suite. We also benchmarked the performance of our models on additional datasets similar to previously published works OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The datasets include properties like formation energy, band gap and Fermi energy. As we pre-trained the model with CGCNN encoder, the comparison with the CGCNN model is the most direct and fair, and it offers insights into how self-supervised learning methods can help in predicting the crystalline material properties with a high degree of accuracy. We also compare the performance of the models in the CT framework with other popular supervised GNN models, i.e., GIN<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> and OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> for the datasets in Table <xref rid="Tab3" ref-type="table">3</xref>. We would like to note that all the models used for comparison in Table <xref rid="Tab3" ref-type="table">3</xref> are trained with the same hyperparameters as suggested in their publicly available codes. The train/validation/test split for all the datasets is the same and set to 0.6/0.2/0.2 following previous standard benchmarking protocols. The data splitting is performed randomly following the protocols in the previously published works. The test Mean Absolute Errors (MAEs) for the supervised training baselines and the models in CT framework are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The detailed hyperparameters used for supervised models are listed in the supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">4</xref>.</p><p id="Par9">It is observed that the CT models outperform all supervised learning baselines on all the 5 regression tasks. We would like to note that the performance improvements (Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">1</xref>) achieved by the CT models over the baseline CGCNN model are non-trivial. We observed an average improvement of 17.09% for CT<sub>Barlow</sub> and 21.83% for CT<sub>SimSiam</sub> when compared to CGCNN. The results in Table <xref rid="Tab3" ref-type="table">3</xref> clearly demonstrate the merit of using self-supervised learning frameworks for periodic crystal property prediction. In order to test the generic nature of our SSL framework, we also implement GIN<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> pre-trained via the Barlow Twins loss. We observed impressive gains in performance for the GIN<sub>Barlow</sub> over the supervised GIN model. The average improvement of the GIN<sub>Barlow</sub> model when compared to the supervised GIN model is 36.97%. The improvement in case of GIN<sub>Barlow</sub> indicates that CT framework can be applied with other graph encoder architectures and performance gains may be expected for those GNN models when compared to their supervised counterpart.</p></sec><sec id="Sec5"><title>Ablation study</title><p id="Par10">To compare the effectiveness of the different augmentations techniques, we pre-train three CT<sub>Barlow</sub> models, (1) using only random perturbation augmentations (RP), (2) using only atom masking and edge masking augmentations (AM+EM), (3) using all three random perturbation, atom masking, and edge masking augmentations (RP+AM+EM). We report the MAE of the model on different fine-tuning datasets to determine the effectiveness of the augmentation techniques (Fig. <xref rid="Fig2" ref-type="fig">2</xref>).<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Ablation study of three augmentation techniques, random perturbation (RP), atom masking (AM), and edge masking (EM), for CT<sub>Barlow</sub> model.</title><p>(<bold>A</bold>) Evaluating the effect of different augmentation techniques in Band Gap and HOIP dataset where the label is band gap. (<bold>B</bold>) Evaluating the effect of augmentation techniques on the FE, Lanthanides, and Perovskites datasets for which the label is formation energy. (<bold>C</bold>) Evaluating the effect of different augmentation strategies on the Fermi energy and <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:msub><mml:mrow><mml:mi>log</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\log }_{10}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq3.gif"/></alternatives></inline-formula> VRH - shear modulus of the structures prediction. The error bars indicate variation in MAE over 3 different runs.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_921_Fig2_HTML.png"/></fig></p><p id="Par11">The performance of AM+EM augmentation is better than RP for perovskites, BG and GVRH datasets, whereas RP augmentation has better performance than AM+EM for Fermi energy, lanthanides, and HOIP datasets. For FE dataset the performance of both RP and AM+EM augmentation techniques is the same. It must be noted that the performance of models trained with different augmentation techniques is almost identical, making it difficult to conclusively ascertain which augmentation technique is better. Moreover, we also observe that the effectiveness of the augmentation techniques is dataset dependent. We would also like to note that the standard deviation of MAE is always lower when using the pre-trained model with all augmentation techniques. Therefore, using a combination of all three augmentation techniques is most effective.</p></sec><sec id="Sec6"><title>Understanding the CT representations</title><p id="Par12">To understand the CT representations, we visualize the representations from the pre-trained and fine-tuned CT<sub>Barlow</sub> framework in comparison to the CGCNN model in 2D using t-SNE<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. The t-SNE representation maps the embedding based on the similarity in the 2D space. The comparison between the representations of the CGCNN model and the CT<sub>Barlow</sub> model for the perovskites dataset is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Each point is colored by the formation energy of perovskites which is the label that the model is trained on in the fine-tuning stage.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Visualizing the embeddings space for the perovskites dataset using t-SNE.</title><p>Every point on the t-SNE plot is colored corresponding to the formation energy of the crystalline system. (<bold>A</bold>) The t-SNE plot for the embedding was generated from the CGCNN model. (<bold>B</bold>) The t-SNE plot for the embedding was generated from the graph encoder of CT model after fine-tuning.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_921_Fig3_HTML.png"/></fig></p><p id="Par13">We observe that the t-SNE projection from the CT<sub>Barlow</sub> model has a better clustering, namely, the crystalline materials with higher formation energy are clustered at the top left of the t-SNE projection plot (Fig. <xref rid="Fig3" ref-type="fig">3B</xref>) when compared to the CGCNN (Fig. <xref rid="Fig3" ref-type="fig">3A</xref>). Similarly the materials with lower formation energy are clustered at the bottom of the t-SNE plot (Fig. <xref rid="Fig3" ref-type="fig">3B</xref>) for the CT<sub>Barlow</sub> model. For example, perovskites <italic>I</italic><italic>n</italic><italic>O</italic><italic>s</italic><italic>O</italic><sub>3</sub> and <italic>L</italic><italic>a</italic><italic>R</italic><italic>e</italic><italic>O</italic><sub>3</sub> with relatively lower formation energies of −0.58 and −0.64 eV/atom, respectively, are clustered closely together in t-SNE projection from CT<sub>Barlow</sub> compared to CGCNN. This demonstrates the generalizability of the representations learned by the CT<sub>Barlow</sub> model when compared to supervised learning. Such representation learnt from the CT framework can also be used to characterize and understand the large chemical space of materials.</p></sec></sec><sec id="Sec7" sec-type="discussion"><title>Discussion</title><p id="Par14">In this work, we develop Crystal Twins (CT), a generic SSL framework for crystalline material property prediction. In this framework, we propose two SSL strategies using the twin graph neural networks to learn representations by leveraging the Barlow Twins loss and SimSiamese loss during pre-training. The models in CT framework (CT<sub>Barlow</sub> and CT<sub>SimSiam</sub>) achieve superior performance compared to other competitive supervised learning baselines. The models in CT framework demonstrate high generalizability and robustness by learning representations that can be used to predict a variety of properties like formation energy, band gap, Fermi energy, shear modulus, bulk modulus, and refractive index of different crystalline materials. The pre-training of models in the CT framework has been performed on significantly less amount of data compared to SSL models in other domains like molecular machine learning, computer vision, and natural language processing. In general, SSL models are known to demonstrate better performance with larger unlabeled data as it allows them to learn more generalizable representations. We expect the models in the CT framework to demonstrate a superior performance with larger training data when compared to our current results. The representations learned by the models in the CT framework are of great promise and can open up avenues for exciting research in understanding the chemical space and designing materials with desired properties.</p></sec><sec id="Sec8" sec-type="methods"><title>Methods</title><p id="Par15">In this section, we describe the components of the CT framework (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). In general, SSL frameworks employ correlations in the input itself to learn robust and generalizable representations from unlabeled data.<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>. As a part of CT framework we propose two different SSL pretraining models namely CT<sub>Barlow</sub> and CT<sub>SimSiam</sub>. For the CT<sub>Barlow</sub>, the goal during pre-training is to force the empirical cross-correlation matrix created from the encoder embeddings of two different augmentations generated by the same crystal towards the identity matrix. All the elements in the cross-correlation matrix lie between −1 and 1, with 1 representing maximum correlation. Intuitively, since the embeddings are generated from augmentations of the same crystalline system, the cross-correlation matrix must be close to the identity matrix. For the CT<sub>SimSiam</sub>, the pre-training objective is to maximize the cosine similarities between encoder embeddings of augmented instances generated from the same crystalline system. To avoid model collapse, CT<sub>SimSiam</sub> implements an extra projection head on one side of the twin networks and applies the stop-gradient technique on the other side in training. Using such objectives during pre-training allows the graph encoder to learn robust representations. To create the augmented instances, we use augmentation techniques, including atom masking, edge masking, and random perturbation (refer <xref ref-type="supplementary-material" rid="MOESM1">Supporting Information</xref>). The embeddings for the augmented instances of the crystalline system are generated via the CGCNN graph encoder. We pre-train the CGCNN model with two SSL strategies using Barlow Twins and SimSiamese loss function. The weights of the pre-trained self-supervised model are used to initialize the graph encoder model during the fine-tuning stage for material property prediction.</p><sec id="Sec9"><title>Graph neural network encoder</title><p id="Par16">Most recent successful deep learning approaches for crystalline material property prediction are based on GNNs because of their ability to capture structural geometry and chemistry. In a crystal graph (<italic>G</italic>), we consider the atoms as the nodes (<italic>V</italic>), and interactions between them are modeled via edges (<italic>E</italic>). In general, GNNs aggregate information from the neighborhood of the node to construct embeddings that are updated iteratively. The update for the GNN can be described as in Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>).<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mtable><mml:mtr><mml:mtd columnalign="right"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mtd><mml:mtd columnalign="left"><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">COMBINE</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">AGGREGATE</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mfenced close=")" open="("><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo><mml:mi>u</mml:mi><mml:mo>∈</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{r}\begin{array}{ll}{{{{\boldsymbol{h}}}}}_{v}^{(k)}&amp;={{{{\rm{COMBINE}}}}}^{(k)}\left({{{{\boldsymbol{h}}}}}_{v}^{(k-1)},{{{{\rm{AGGREGATE}}}}}^{(k)}\left(\{{{{{\boldsymbol{h}}}}}_{u}^{(k-1)}| u\in {{{\mathcal{N}}}}(v)\}\right)\right),\end{array}\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ1.gif"/></alternatives></disp-formula>where <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\boldsymbol{h}}}}}_{v}^{(k)}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq4.gif"/></alternatives></inline-formula> is the feature of the node <italic>v</italic> at the <italic>k</italic>-th layer and <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\boldsymbol{h}}}}}_{v}^{(0)}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq5.gif"/></alternatives></inline-formula> is initialized by node feature <bold><italic>x</italic></bold><sub><italic>v</italic></sub>. <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathcal{N}}}}(v)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq6.gif"/></alternatives></inline-formula> denotes the set of all the neighbors of node <italic>v</italic>. <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\boldsymbol{a}}}}}_{v}^{(k)}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq7.gif"/></alternatives></inline-formula> is the output from the aggregation operation at the <italic>k</italic><sup>th</sup> layer. The aggregation operation collects the features of neighboring nodes and the combination operation combines the original node feature with the aggregated features. To extract the feature of the entire crystal system, <bold><italic>h</italic></bold><sub><italic>G</italic></sub>, readout operation integrates all the node features among the graph <italic>G</italic> as given in Eq. (<xref rid="Equ2" ref-type="disp-formula">2</xref>).<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">READOUT</mml:mi><mml:mfenced close=")" open="("><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">h</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>∣</mml:mo><mml:mi>v</mml:mi><mml:mo>∈</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>.</mml:mo></mml:mrow></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\boldsymbol{h}}}}}_{G}={{{\rm{READOUT}}}}\left(\{{{{{\boldsymbol{h}}}}}_{v}^{(k)}| v\in G\}\right).$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ2.gif"/></alternatives></disp-formula>The readout operations such as summation, averaging, and max pooling are most commonly used<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>.</p><p id="Par17">In this work, we implement the CGCNN<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> architecture as the GNN encoder. We choose CGCNN because of its competitive performance and computational efficiency when compared to other GNN baselines. Moreover, CGCNN is one of the most widely benchmarked baseline models for material property prediction allowing us to compare the performance of our SSL framework with CGCNN and other baselines. To encode crystal features and obtain an embedding, we use mean pooling to generate a latent representation with the dimension of 64. After the GNN encoder, the projection head with 2 MLP layers is attached to generate the final embedding on which the SSL loss functions are applied for pre-training. Additionally, we also implement a general purpose graph neural network GIN<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> to test the fidelity of SSL methods on another architecture apart from CGCNN.</p><p id="Par18">To generate self-supervised learning representations, we need to construct different augmentations of the crystalline system. Inspired by AugLiChem,<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> we devise three different augmentation techniques (Fig. <xref rid="Fig1" ref-type="fig">1C</xref>), namely, random perturbation, atom masking and edge masking. The random perturbation augmentation perturbs each atom by a distance drawn from the uniform distribution between 0 Å and 0.05 Å. For atom masking, we randomly mask 10% of the atoms in the crystal, similarly for edge masking we randomly mask 10% of the edge features between two neighboring atoms. More details on atom masking and edge masking are provided in the <xref ref-type="supplementary-material" rid="MOESM1">Supplementary Information</xref> (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>). These augmentations are applied to the crystalline systems and two augmented instances are generated randomly on the fly at each epoch during pre-training. These augmented instances are fed into the GNN encoder to generate embeddings on which SSL loss functions are applied.</p></sec><sec id="Sec10"><title>Barlow Twins loss</title><p id="Par19">In the pre-training stage for CT<sub>Barlow</sub>, we use the Barlow Twins loss function to learn graph representations from crystals. This loss is based on the redundancy reduction principle proposed by neuroscientist H. Barlow<sup><xref ref-type="bibr" rid="CR56">56</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup> and was introduced to SSL by Zbontar et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. We use the Barlow Twins loss function in CT because of its high performance and ease of implementation. Moreover, the Barlow Twins loss, unlike other contrastive loss functions, does not explicitly require positive and negative pairs for pre-training. The Barlow Twins loss function is applied to the cross-correlation matrix created from encoder-generated embeddings of the two different augmentations generated from the same crystalline system. The Barlow Twins loss function is represented by Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>),<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">BT</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>≜</mml:mo><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{{{\rm{BT}}}}}\,{\triangleq}\mathop{\sum}\limits_{i}{(1-{C}_{ii})}^{2}+\lambda \mathop{\sum}\limits_{i}\mathop{\sum}\limits_{j\ne i}{C}_{ij}^{2},$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ3.gif"/></alternatives></disp-formula>where <bold><italic>C</italic></bold> is the cross-correlation matrix of embeddings from two augmented instances, the cross correlation matrix is given by Eq. (<xref rid="Equ4" ref-type="disp-formula">4</xref>). The <italic>λ</italic> used in this work is 0.0051 same as the original paper.<disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>≜</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${C}_{ij}\,{\triangleq} \frac{{\sum }_{b}{{{{\boldsymbol{Z}}}}}_{b,i}^{A}{{{{\boldsymbol{Z}}}}}_{b,j}^{B}}{\sqrt{{({{{{\boldsymbol{Z}}}}}_{b,i}^{A})}^{2}}\sqrt{{({{{{\boldsymbol{Z}}}}}_{b,j}^{B})}^{2}}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ4.gif"/></alternatives></disp-formula>where <italic>b</italic> is the index of the data in batch and <italic>i</italic>, <italic>j</italic> index the vector dimensions of the projector output (<italic>Z</italic><sup><italic>A</italic></sup> and <italic>Z</italic><sup><italic>B</italic></sup>), for both the augmented instances <italic>A</italic> and <italic>B</italic> from the same crystalline material.</p></sec><sec id="Sec11"><title>SimSiam loss</title><p id="Par20">We developed another variant of CT that uses SimSiam<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> denoted as CT<sub>SimSiam</sub>. In this case, an extra MLP head <italic>f</italic>( ⋅ ) is added to the GNN backbone to map the latent vector <italic>Z</italic> to <italic>P</italic>, namely <bold><italic>P</italic></bold> = <italic>f</italic>(<bold><italic>Z</italic></bold>). The distance between two vectors is defined as Eq. (<xref rid="Equ5" ref-type="disp-formula">5</xref>).<disp-formula id="Equ5"><label>5</label><alternatives><mml:math id="Equ5_Math"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mo>∥</mml:mo><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\mathcal{D}}}}({{{{\boldsymbol{P}}}}}_{b}^{A},{{{{\boldsymbol{Z}}}}}_{b}^{B})=-\frac{{{{{\boldsymbol{P}}}}}_{b}^{A}}{\parallel {{{{\boldsymbol{P}}}}}_{b}^{A}{\parallel }_{2}}\cdot \frac{{{{{\boldsymbol{Z}}}}}_{b}^{B}}{\parallel {{{{\boldsymbol{Z}}}}}_{b}^{B}{\parallel }_{2}},$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ5.gif"/></alternatives></disp-formula>where <italic>b</italic> denotes the index of the data in a batch and <italic>A</italic>, <italic>B</italic> denote two augmented instances from the same crystalline material. The objective to minimize given a batch is further shown in Eq. (<xref rid="Equ6" ref-type="disp-formula">6</xref>).<disp-formula id="Equ6"><label>6</label><alternatives><mml:math id="Equ6_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">SimSiam</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.25em"/><mml:mo>≜</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mrow><mml:mo>∑</mml:mo></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munder><mml:mfenced close=")" open="("><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">stopgrad</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">P</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:mi mathvariant="monospace">stopgrad</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfenced><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${L}_{{{{\rm{SimSiam}}}}}\,{\triangleq}\,\frac{1}{2}\mathop{\sum}\limits_{b}\left({{{\mathcal{D}}}}({{{{\boldsymbol{P}}}}}_{b}^{A},{\mathtt{stopgrad}}({{{{\boldsymbol{Z}}}}}_{b}^{B}))+{{{\mathcal{D}}}}({{{{\boldsymbol{P}}}}}_{b}^{B},{\mathtt{stopgrad}}({{{{\boldsymbol{Z}}}}}_{b}^{A}))\right),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_Equ6.gif"/></alternatives></disp-formula>where <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mrow><mml:mi mathvariant="monospace">stopgrad</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>⋅</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathtt{stopgrad}}(\cdot )$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_921_Article_IEq8.gif"/></alternatives></inline-formula> means the gradient is not back propagated on this branch of the CT<sub>SimSiam</sub> model. Such an asymmetric architecture and the stop-gradient operation avoid the collapse of learned representations.</p></sec><sec id="Sec12"><title>Training details</title><p id="Par21">In the pre-training stage, the embedding dimension of the CGCNN encoder is set to 128 for the CT<sub>Barlow</sub> and 256 for the CT<sub>SimSiam</sub> model. We use the Adam optimizer<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> with a learning rate of 0.00001 and a batch size of 64 and pre-train the model for 15 epochs. The other hyperparameters for the CGCNN (graph encoder) model are kept the same as in the original paper. The train/validation ratio for pre-training data is 95%/5%. For pre-training, we combine the datasets from the Matminer database<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> and the hypothetical Metal-Organic Framework dataset<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, aggregating a total of 428,275 samples. The labels in the datasets are not used during CT pre-training. Additional details about the hyperparameters during the pretraining stage are available in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">2</xref>. In the fine-tuning stage, we add a randomly initialized MLP head with two fully connected layers to generate the final property prediction. The CT<sub>Barlow</sub> and CT<sub>SimSiam</sub> are tested on a variety of datasets including some datasets from previously published work OGCNN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> and matbench suite<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Additional details about the hyperparameters used during the finetuning stage are mentioned in Supplementary Table <xref ref-type="supplementary-material" rid="MOESM1">3</xref>.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>The work has been supported by the start-up fund provided by Department of Mechanical Engineering at Carnegie Mellon University. The authors would like to thank Prakarsh Yadav, Alison Bartsch, and Cooper Lorsung for their comments on the manuscript.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>R.M. and A.B.F. ideated the concept. R.M. wrote the code and benchmarked the performance of the model. Y.W. assisted in the benchmarking and writing the manuscript. A.B.F. supervised the work.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>All data used in this work are publicly available. The authors used datasets from the matbench suite and Materials project, which are public data repositories. Matbench benchmark datasets are available at the website (<ext-link xlink:href="https://matbench.materialsproject.org/" ext-link-type="uri">https://matbench.materialsproject.org/</ext-link>). The datasets can also be found on the Materials Project website (<ext-link xlink:href="https://materialsproject.org/" ext-link-type="uri">https://materialsproject.org/</ext-link>). The codes and documentation for matbench can be found at the website (<ext-link xlink:href="https://github.com/materialsproject/matbench" ext-link-type="uri">https://github.com/materialsproject/matbench</ext-link>). For the HOIP<sup><xref ref-type="bibr" rid="CR61">61</xref></sup> (Dryad Digital Repository: <ext-link xlink:href="10.5061/dryad.gq3rg" ext-link-type="doi">https://doi.org/10.5061/dryad.gq3rg</ext-link>) and Lanthanides datasets<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>, relevant publications have been cited from which data were obtained. These datasets are also made available at <ext-link xlink:href="https://github.com/RishikeshMagar/Crystal-Twins" ext-link-type="uri">https://github.com/RishikeshMagar/Crystal-Twins</ext-link>.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>The code developed for this work is available at <ext-link xlink:href="https://github.com/RishikeshMagar/Crystal-Twins" ext-link-type="uri">https://github.com/RishikeshMagar/Crystal-Twins</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par22">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>J</given-names></name><name><surname>Marques</surname><given-names>MR</given-names></name><name><surname>Botti</surname><given-names>S</given-names></name><name><surname>Marques</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Recent advances and applications of machine learning in solid-state materials science</article-title><source>npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1038/s41524-019-0221-0</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keith</surname><given-names>JA</given-names></name><etal/></person-group><article-title xml:lang="en">Combining machine learning and computational chemistry for predictive insights into chemical systems</article-title><source>Chem. Rev.</source><year>2021</year><volume>121</volume><fpage>9816</fpage><lpage>9872</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhsVOrtr3J</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemrev.1c00107</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Najafabadi</surname><given-names>MM</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning applications and challenges in big data analytics</article-title><source>J. Big Data</source><year>2015</year><volume>2</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1186/s40537-014-0007-7</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Lecun</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Deep learning for ai</article-title><source>Commun. ACM</source><year>2021</year><volume>64</volume><fpage>58</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1145/3448250</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schleder</surname><given-names>GR</given-names></name><name><surname>Padilha</surname><given-names>AC</given-names></name><name><surname>Acosta</surname><given-names>CM</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Fazzio</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">From dft to machine learning: recent approaches to materials science–a review</article-title><source>J. Phys. Mater.</source><year>2019</year><volume>2</volume><fpage>032001</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFGhtrjJ</pub-id><pub-id pub-id-type="doi">10.1088/2515-7639/ab084b</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">Machine learning: accelerating materials development for energy storage and conversion</article-title><source>InfoMat</source><year>2020</year><volume>2</volume><fpage>553</fpage><lpage>576</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsVKntr%2FL</pub-id><pub-id pub-id-type="doi">10.1002/inf2.12094</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Chen, T., Kornblith, S., Norouzi, M. &amp; Hinton, G. A simple framework for contrastive learning of visual representations. In <italic>International Conference on Machine Learning</italic> 1597–1607 (PMLR, 2020).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Zbontar, J., Jing, L., Misra, I., LeCun, Y. &amp; Deny, S. Barlow twins: self-supervised learning via redundancy reduction. In <italic>International Conference on Machine Learning</italic> 12310–12320 (PMLR, 2021).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill</surname><given-names>J-B</given-names></name><etal/></person-group><article-title xml:lang="en">Bootstrap your own latent-a new approach to self-supervised learning</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>21271</fpage><lpage>21284</lpage></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Unsupervised learning of visual features by contrasting cluster assignments</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>9912</fpage><lpage>9924</lpage></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">He, K., Fan, H., Wu, Y., Xie, S. &amp; Girshick, R. Momentum contrast for unsupervised visual representation learning. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 9729–9738 (2020).</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Chen, X. &amp; He, K. Exploring simple siamese representation learning. In <italic>Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 15750–15758 (2021).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Lan, Z. et al. Albert: A lite BERT for self-supervised learning of language representations. In <italic>International Conference on Learning Representations</italic> (ICLR, 2019).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Wu, J., Wang, X. &amp; Wang, W. Y. Self-supervised dialogue learning. In <italic>Proc. 57th Annual Meeting of the Association for Computational Linguistics</italic> 3857–3867 (ACL, 2019).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Wang, Y., Wang, J., Cao, Z. &amp; Barati Farimani, A. Molecular contrastive learning of representations via graph neural networks. <italic>Nat. Mach. Intell.</italic><bold>4</bold>, 279–287 (2022).</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Zhu, J. et al. Dual-view molecule pre-training. Preprint at <italic>arXiv</italic><ext-link xlink:href="https://arxiv.org/abs/2106.10234" ext-link-type="uri">https://arxiv.org/abs/2106.10234</ext-link> (2021).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Stärk, H. et al. 3D infomax improves gnns for molecular property prediction. In <italic>International Conference on Machine Learning</italic> 20479–20502 (PMLR, 2022).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Liu, S., Demirel, M. F. &amp; Liang, Y. N-gram graph: simple unsupervised representation for graphs, with applications to molecules. <italic>Adv. Neural. Inf. Process. Syst.</italic><bold>32</bold>, (2019).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rong</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Self-supervised graph transformer on large-scale molecular data</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>12559</fpage><lpage>12571</lpage></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Hu, W. et al. Strategies for pre-training graph neural networks. In <italic>International Conference on Learning Representations (ICLR)</italic> (ICLR, 2020).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">An effective self-supervised framework for learning expressive molecular global representations to drug discovery</article-title><source>Brief. Bioinform.</source><year>2021</year><volume>22</volume><fpage>bbab109</fpage><pub-id pub-id-type="doi">10.1093/bib/bbab109</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Chithrananda, S., Grand, G. &amp; Ramsundar, B. ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. Preprint at <italic>arXiv</italic><ext-link xlink:href="https://arxiv.org/abs/2010.09885" ext-link-type="uri">https://arxiv.org/abs/2010.09885</ext-link> (2020).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Rong, Y. et al. Grover: self-supervised message passing transformer on large-scale molecular data. In <italic>Proc. 34th International Conference on Neural Information Processing Systems</italic> 12559–12571 (NIPS, 2020).</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Zhang, Z., Liu, Q., Wang, H., Lu, C. &amp; Lee, C.-K. Motif-based graph self-supervised learning for molecular property prediction. Preprint at <italic>arXiv</italic><ext-link xlink:href="https://arxiv.org/abs/2110.00987" ext-link-type="uri">https://arxiv.org/abs/2110.00987</ext-link> (2021).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Magar</surname><given-names>R</given-names></name><name><surname>Liang</surname><given-names>C</given-names></name><name><surname>Barati Farimani</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Improving molecular contrastive learning via faulty negative mitigation and decomposed fragment contrast</article-title><source>J. Chem. Inf. Modeling</source><year>2022</year><volume>62</volume><fpage>2714</fpage><lpage>2725</lpage><pub-id pub-id-type="doi">10.1021/acs.jcim.2c00495</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXht1WlurzP</pub-id><pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">A comprehensive survey on graph neural networks</article-title><source>IEEE Trans. Neural Netw. Learn. Syst.</source><year>2020</year><volume>32</volume><fpage>4</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1109/TNNLS.2020.2978386</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Welling, M. &amp; Kipf, T. N. Semi-supervised classification with graph convolutional networks. In <italic>International Conference on Learning Representations (ICLR 2017)</italic> (ICLR, 2016).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>145301</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSnu7c%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.145301</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Karamad</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Orbital graph convolutional neural network for material property prediction</article-title><source>Phys. Rev. Mater.</source><year>2020</year><volume>4</volume><fpage>093801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitVOktr3M</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.4.093801</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schütt</surname><given-names>KT</given-names></name><name><surname>Sauceda</surname><given-names>HE</given-names></name><name><surname>Kindermans</surname><given-names>P-J</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name></person-group><article-title xml:lang="en">Schnet–a deep learning architecture for molecules and materials</article-title><source>J. Chem. Phys.</source><year>2018</year><volume>148</volume><fpage>241722</fpage><pub-id pub-id-type="doi">10.1063/1.5019779</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Graph networks as a universal machine learning framework for molecules and crystals</article-title><source>Chem. Mater.</source><year>2019</year><volume>31</volume><fpage>3564</fpage><lpage>3572</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXntFaqt7g%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.9b01294</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louis</surname><given-names>S-Y</given-names></name><etal/></person-group><article-title xml:lang="en">Graph convolutional neural networks with global attention for improved materials property prediction</article-title><source>Phys. Chem. Chem. Phys.</source><year>2020</year><volume>22</volume><fpage>18141</fpage><lpage>18148</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFSmu7bL</pub-id><pub-id pub-id-type="doi">10.1039/D0CP01474E</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Gasteiger, J., Groß, J. &amp; Günnemann, S. Directional message passing for molecular graphs. In <italic>International Conference on Learning Representations</italic> (ICLR, 2019).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Gasteiger, J., Giri, S., Margraf, J. T. &amp; Günnemann, S. Fast and uncertainty-aware directional message passing for non-equilibrium molecules. Preprint at <italic>arXiv</italic><ext-link xlink:href="https://arxiv.org/abs/2011.14115" ext-link-type="uri">https://arxiv.org/abs/2011.14115</ext-link> (2020).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palizhati</surname><given-names>A</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Tran</surname><given-names>K</given-names></name><name><surname>Back</surname><given-names>S</given-names></name><name><surname>Ulissi</surname><given-names>ZW</given-names></name></person-group><article-title xml:lang="en">Toward predicting intermetallics surface properties with high-throughput DFT and convolutional neural networks</article-title><source>J. Chem. Inf. Modeling</source><year>2019</year><volume>59</volume><fpage>4742</fpage><lpage>4749</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitVajtrnI</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00550</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Back</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Convolutional neural network of atomic surface structures to predict binding energies for high-throughput screening of catalysts</article-title><source>J. Phys. Chem. Lett.</source><year>2019</year><volume>10</volume><fpage>4401</fpage><lpage>4408</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtlyju7jM</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpclett.9b01428</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. &amp; Dahl, G. E. Neural message passing for quantum chemistry. In <italic>International Conference on Machine Learning</italic> 1263–1272 (PMLR, 2017).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Unke</surname><given-names>OT</given-names></name><name><surname>Meuwly</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Physnet: a neural network for predicting energies, forces, dipole moments, and partial charges</article-title><source>J. Chem. Theory Comput.</source><year>2019</year><volume>15</volume><fpage>3678</fpage><lpage>3693</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXosF2ms7g%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jctc.9b00181</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>GH</given-names></name><etal/></person-group><article-title xml:lang="en">Practical deep-learning representation for fast heterogeneous catalyst screening</article-title><source>J. Phys. Chem. Lett.</source><year>2020</year><volume>11</volume><fpage>3185</fpage><lpage>3191</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXlt1SjtbY%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpclett.0c00634</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Elemnet: deep learning the chemistry of materials from only elemental composition</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-35934-y</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ganose</surname><given-names>A</given-names></name><name><surname>Dopp</surname><given-names>D</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>10</lpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>DeCost</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Atomistic line graph neural network for improved materials property predictions</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00650-1</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Ihalage, A. &amp; Hao, Y. Formula graph self-attention network for representation-domain independent materials discovery. <italic>Adv. Sci.</italic><bold>9</bold>, 2200164 (2022).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moosavi</surname><given-names>SM</given-names></name><name><surname>Jablonka</surname><given-names>KM</given-names></name><name><surname>Smit</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">The role of machine learning in the understanding and design of materials</article-title><source>J. Am. Chem. Soc.</source><year>2020</year><volume>142</volume><fpage>20273</fpage><lpage>20287</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXit1yntr3O</pub-id><pub-id pub-id-type="doi">10.1021/jacs.0c09105</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>K</given-names></name><name><surname>Lengyel</surname><given-names>J</given-names></name><name><surname>Shatruk</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Crystal structure prediction via deep learning</article-title><source>J. Am. Chem. Soc.</source><year>2018</year><volume>140</volume><fpage>10158</fpage><lpage>10168</lpage><pub-id pub-id-type="doi">10.1021/jacs.8b03913</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>H</given-names></name><name><surname>Stanev</surname><given-names>V</given-names></name><name><surname>Kusne</surname><given-names>AG</given-names></name><name><surname>Takeuchi</surname><given-names>I</given-names></name></person-group><article-title xml:lang="en">Cryspnet: crystal structure predictions via neural networks</article-title><source>Phys. Rev. Mater.</source><year>2020</year><volume>4</volume><fpage>123802</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXjtVOnurc%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.4.123802</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Constrained crystals deep convolutional generative adversarial network for the inverse design of crystal structures</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00526-4</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Noh</surname><given-names>J</given-names></name><name><surname>Gu</surname><given-names>GH</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name><name><surname>Jung</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Generative adversarial networks for crystal structure prediction</article-title><source>ACS Cent. Sci.</source><year>2020</year><volume>6</volume><fpage>1412</fpage><lpage>1420</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhtlCku77P</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.0c00426</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Xie, T., Fu, X., Ganea, O.-E., Barzilay, R. &amp; Jaakkola, T. S. Crystal diffusion variational autoencoder for periodic material generation. In <italic>International Conference on Learning Representations</italic> (2021).</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Inverse design of nanoporous crystalline reticular materials with deep generative models</article-title><source>Nat. Mach. Intell.</source><year>2021</year><volume>3</volume><fpage>76</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1038/s42256-020-00271-1</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Visualizing data using t-sne</article-title><source>J. Mach. Learn. Res.</source><year>2008</year><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Hadsell, R., Chopra, S. &amp; LeCun, Y. Dimensionality reduction by learning an invariant mapping. In <italic>2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06)</italic> 1735–1742 (IEEE, 2006).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Xu, K., Hu, W., Leskovec, J. &amp; Jegelka, S. How powerful are graph neural networks? In <italic>International Conference on Learning Representations</italic> (ICLR, 2018).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Magar, R. et al. AugLiChem: data augmentation library of chemical structures for machine learning. <italic>Mach. learn.: sci. technol</italic>. (IOP Publishing) (2022).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Redundancy reduction revisited</article-title><source>Network</source><year>2001</year><volume>12</volume><fpage>241</fpage><lpage>253</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DC%2BD3Mrhs1CgsQ%3D%3D</pub-id><pub-id pub-id-type="doi">10.1080/net.12.3.241.253</pub-id></mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Barlow, H. B. &amp; Rosenblith, W. A. <italic>Possible Principles Underlying the Transformations of Sensory Messages</italic> 217–234 (MIT Press, 1961).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Kingma, D. P. &amp; Ba, J. Adam: a method for stochastic optimization. In <italic>International Conference on Learning Representations</italic> (ICLR, 2015).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Matminer: An open source toolkit for materials data mining</article-title><source>Comput. Mater. Sci.</source><year>2018</year><volume>152</volume><fpage>60</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.commatsci.2018.05.018</pub-id></mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilmer</surname><given-names>CE</given-names></name><etal/></person-group><article-title xml:lang="en">Large-scale screening of hypothetical metal–organic frameworks</article-title><source>Nat. Chem.</source><year>2012</year><volume>4</volume><fpage>83</fpage><lpage>89</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXhsVagtL3K</pub-id><pub-id pub-id-type="doi">10.1038/nchem.1192</pub-id></mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Huan</surname><given-names>TD</given-names></name><name><surname>Krishnan</surname><given-names>S</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">A hybrid organic-inorganic perovskite dataset</article-title><source>Sci. Data</source><year>2017</year><volume>4</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXmvVSqsbY%3D</pub-id><pub-id pub-id-type="doi">10.1038/sdata.2017.57</pub-id></mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pham</surname><given-names>TL</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning reveals orbital interaction in materials</article-title><source>Sci. Technol. Adv. Mater.</source><year>2017</year><volume>18</volume><fpage>756</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1080/14686996.2017.1378060</pub-id></mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>Kalish</surname><given-names>I</given-names></name><name><surname>Beams</surname><given-names>R</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">High-throughput identification and characterization of two-dimensional materials using density functional theory</article-title><source>Sci. Rep.</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41598-017-05402-0</pub-id></mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petretto</surname><given-names>G</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput density-functional perturbation theory phonons for inorganic materials</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.65</pub-id></mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petousis</surname><given-names>I</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput screening of inorganic compounds for the discovery of novel dielectric and optical materials</article-title><source>Sci. Data</source><year>2017</year><volume>4</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2016.134</pub-id></mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Jong</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Charting the complete elastic properties of inorganic crystalline compounds</article-title><source>Sci. Data</source><year>2015</year><volume>2</volume><pub-id pub-id-type="doi">10.1038/sdata.2015.9</pub-id></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castelli</surname><given-names>IE</given-names></name><etal/></person-group><article-title xml:lang="en">Computational screening of perovskite metal oxides for optimal solar light capture</article-title><source>Energy Environ. Sci.</source><year>2012</year><volume>5</volume><fpage>5814</fpage><lpage>5819</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38Xht12iurc%3D</pub-id><pub-id pub-id-type="doi">10.1039/C1EE02717D</pub-id></mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Commentary: The materials project: a materials genome approach to accelerating materials innovation</article-title><source>APL Mater.</source><year>2013</year><volume>1</volume><fpage>011002</fpage><pub-id pub-id-type="doi">10.1063/1.4812323</pub-id></mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec13"><title>Supplementary information</title><p id="Par23"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2022_921_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information Crystal Twins: Self-supervised Learning for Crystalline Material Property Prediction</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-022-00921-5" ext-link-type="doi">https://doi.org/10.1038/s41524-022-00921-5</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2022</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
