<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41467-020-18375-y</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41467</journal-id><journal-id journal-id-type="doi">10.1038/41467.2041-1723</journal-id><journal-title-group><journal-title>Nature Communications</journal-title><abbrev-journal-title abbrev-type="publisher">Nat Commun</abbrev-journal-title></journal-title-group><issn pub-type="epub">2041-1723</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41467-020-18375-y</article-id><article-id pub-id-type="manuscript">18375</article-id><article-id pub-id-type="doi">10.1038/s41467-020-18375-y</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/166/987</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1005/1009</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/925/927/356</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/14/63</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/120</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/128</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/147</subject></subj-group><subj-group subj-group-type="TechniquePath"><subject>/142/126</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">An artificial sensory neuron with visual-haptic fusion</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3210-6673</contrib-id><name><surname>Wan</surname><given-names>Changjin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2665-5932</contrib-id><name><surname>Cai</surname><given-names>Pingqiang</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Guo</surname><given-names>Xintong</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au4"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0976-9871</contrib-id><name><surname>Wang</surname><given-names>Ming</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au5"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5978-2778</contrib-id><name><surname>Matsuhisa</surname><given-names>Naoji</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au6"><name><surname>Yang</surname><given-names>Le</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au7"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-0777-110X</contrib-id><name><surname>Lv</surname><given-names>Zhisheng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au8"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-4454-6318</contrib-id><name><surname>Luo</surname><given-names>Yifei</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" id="Au9"><name><surname>Loh</surname><given-names>Xian Jun</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au10"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3312-1664</contrib-id><name><surname>Chen</surname><given-names>Xiaodong</given-names></name><address><email>chenxd@ntu.edu.sg</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs4146702018375y_cor10">k</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.59025.3b</institution-id><institution-id institution-id-type="ISNI">0000 0001 2224 0361</institution-id><institution content-type="org-division">Innovative Center for Flexible Devices (iFLEX), Max Planck – NTU Joint Lab for Artificial Senses, School of Materials Science and Engineering</institution><institution content-type="org-name">Nanyang Technological University</institution></institution-wrap><addr-line content-type="postcode">639798</addr-line><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.418788.a</institution-id><institution-id institution-id-type="ISNI">0000 0004 0470 809X</institution-id><institution content-type="org-name">Institute of Materials Research and Engineering, Agency for Science, Technology and Research (A*STAR)</institution></institution-wrap><addr-line content-type="postcode">138634</addr-line><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff></contrib-group><author-notes><corresp id="IDs4146702018375y_cor10"><label>k</label><email>chenxd@ntu.edu.sg</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>14</day><month>9</month><year>2020</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2020</year></pub-date><volume>11</volume><issue seq="4601">1</issue><elocation-id>4602</elocation-id><history><date date-type="registration"><day>21</day><month>8</month><year>2020</year></date><date date-type="received"><day>6</day><month>2</month><year>2020</year></date><date date-type="accepted"><day>18</day><month>8</month><year>2020</year></date><date date-type="online"><day>14</day><month>9</month><year>2020</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2020</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Human behaviors are extremely sophisticated, relying on the adaptive, plastic and event-driven network of sensory neurons. Such neuronal system analyzes multiple sensory cues efficiently to establish accurate depiction of the environment. Here, we develop a bimodal artificial sensory neuron to implement the sensory fusion processes. Such a bimodal artificial sensory neuron collects optic and pressure information from the photodetector and pressure sensors respectively, transmits the bimodal information through an ionic cable, and integrates them into post-synaptic currents by a synaptic transistor. The sensory neuron can be excited in multiple levels by synchronizing the two sensory cues, which enables the manipulating of skeletal myotubes and a robotic hand. Furthermore, enhanced recognition capability achieved on fused visual/haptic cues is confirmed by simulation of a multi-transparency pattern recognition task. Our biomimetic design has the potential to advance technologies in cyborg and neuromorphic systems by endowing them with supramodal perceptual capabilities.</p></abstract><abstract xml:lang="en" id="Abs2" abstract-type="ShortSummary"><p id="Par2">Designing bioinspired perceptual system remains a challenge. Here, the authors report a bimodal artificial sensory neuron, integrating a resistive pressure sensor, a perovskite-based photodetector, a hydrogel-based ionic cable, and a synaptic transistor, to implement the visual-haptic fusion for motion control and patterns recognition.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>Agency for Science, Technology and Research (A*STAR)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100001348</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">A18A1b0045</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Xiaodong</given-names></name></principal-award-recipient></award-group><award-group><funding-source><institution-wrap><institution>National Research Foundation Singapore (National Research Foundation-Prime Minister’s office, Republic of Singapore)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/501100001381</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">NRF2016NRF-NRFI001-21</award-id><principal-award-recipient><name><surname>Chen</surname><given-names>Xiaodong</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Research</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>4602</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>8</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>21</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41467_2020_Article_18375.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Science, Humanities and Social Sciences, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Science, Humanities and Social Sciences, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Science, multidisciplinary</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Science (multidisciplinary)</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par3">When interacting with the real, dynamic world, biological systems always outshine their electronic counterparts due to their sophisticated sensorimotor skills<sup><xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref></sup>. Emulating the functionality and/or structuralism of the natural system would intrinsically address unmet needs in current digital systems<sup><xref ref-type="bibr" rid="CR4">4</xref>–<xref ref-type="bibr" rid="CR6">6</xref></sup>. Although bioinspired systems using silicon-based circuits and software have realized some complicated and dexterous sensorimotor functions<sup><xref ref-type="bibr" rid="CR7">7</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>, their efficiency still suffers when data size increases due to centralized and sequential operation. In contrast, the biological systems are essentially run on distributed computing paradigm, whose superior fault tolerance and power efficiency are inherent in the adaptive, plastic, and event-driven network of sensory neurons<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. Therefore, emulating the biological processes from the level of sensory neuron would fundamentally achieve biological perceptual capabilities.</p><p id="Par4">Artificial sensory neuron development can benefit from an improved understanding of sensory processing in biology. One notable advantage of the biological sensory systems is that they analyze multiple cues, making reactions more reliable than with a unimodal cue. For example, visual and haptic cues are intensively received and perceived in our interactions with the surroundings, and the two cues are closely associated and interpreted in the inferior parietal cortex to provide supramodal spatial ability<sup><xref ref-type="bibr" rid="CR12">12</xref>–<xref ref-type="bibr" rid="CR14">14</xref></sup> (Fig. <xref rid="Fig1" ref-type="fig">1a</xref>). The neurons in this cortex area subsume both the macrospace of vision and the microspace encompassed by the hand, to avoid misjudgment from environmental complexity such as variances in the object’s pose<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The behavioral and psychological experiments also indicate a higher precision of appreciating an object when combining the two cues<sup><xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>A bimodal artificial sensory neuron with visual-haptic fusion.</title><p><bold>a</bold> The visual-haptic fusion by biological neural network. <bold>b</bold> The BASE patch for visual-haptic fusion. Sub-figures i to iv: photodetector, pressure sensor, hydrogel (dyed by 0.04% methylene blue), and synaptic transistor, respectively. Inset: a photograph of the BASE patch. The scale bar is 5 mm.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41467_2020_18375_Fig1_HTML.png"/></fig></p><p id="Par5">Artificial sensory neurons/synapses with either haptic or visual modalities<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>, have been achieved for applications such as pattern recognition<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup> and muscular contraction control<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. However, supramodal perceptual capabilities that can increase the reliability and accuracy of these machineries are still absent, possibly due to a lack of platform for mediating multimodal sensory data. Synaptic transistors allow parallel gating of active channels via ions in electrolytes, hence representing a core element for implementing sensory fusion at a neuronal level<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>.</p><p id="Par6">Here, we develop a bimodal artificial sensory neuron (BASE) based on ionic/electronic hybrid neuromorphic electronics to implement the visual-haptic fusion. This BASE unit consists of four core components: resistive pressure sensor, perovskite-based photodetector, hydrogel-based ionic cable, and a synaptic transistor (Fig. <xref rid="Fig1" ref-type="fig">1b</xref>). The photodetector and pressure sensor function as the receptors in the retina and skin, respectively, converting external haptic and visual stimuli into electrical signals. The electrical signals from the two sensors are then transmitted through the ionic cable to the synaptic transistors for integration and conversion into a transient channel current, analogous to the biological excitatory postsynaptic currents (EPSC). Bimodal stimuli in closer succession can induce stronger changes in EPSC, which can be used to determine the extent of synchronization between the two. This, in turn, is used to provide multi-dimensional spatial information thereby controlling a biohybrid neuromuscular junction or a robotic hand, mimicking the process of ‘perception for action’<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. We also design and simulate a matrix of BASE as the feature extraction layer of a perceptron for recognition of multi-transparency alphabetic patterns. The results further confirm that multimodal sensory fusion by BASE can increase the recognition rate (ratio of successful recognition of both the letter and its transparency to total trials) even with a reduced data size. Using BASE for constructing a bioinspired perceptual system would have profound implications for neurorobotics, cyborg systems, and autonomous artificial intelligence.</p></sec><sec id="Sec2" sec-type="results"><title>Result</title><sec id="Sec3"><title>Fabrication and characterization of the BASE</title><p id="Par7">The hybrid neural circuits essentially encompass two artificial sensory channels: the visual and the haptic channel. The visual channel comprises a perovskite-based photodetector, mimicking the photoreceptors in the retina, with a configuration of Zn<sub>2</sub>SnO<sub>4</sub>/PEA<sub>2</sub>MA<sub>2</sub>Pb<sub>3</sub>I<sub>10</sub>/Poly[bis(4-phenyl) (2,4,6-trimethylphenyl) amine] (PTAA)/Au on PET/indium tin oxide (ITO)-coated substrate. 2D perovskite — PEA<sub>2</sub>MA<sub>2</sub>Pb<sub>3</sub>I<sub>10</sub> is stable in ambient environment for reliable and repeatable photo-detection<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup> (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1</xref>). The haptic channel comprises a pressure sensor that incorporating microstructures in the top carbon nanotube (CNT)-coated poly (dimethylsiloxane) (PDMS) layer<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup> (Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">2</xref> and <xref ref-type="supplementary-material" rid="MOESM1">3</xref>). When a pressure is loaded on this layer, it forms a resistive pathway with the electrodes on the bottom layer. An increase in pressure increases the contact area and therefore decreases the resistance between the top CNT film and the bottom electrodes. The polyvinyl alcohol (PVA) hydrogel-based ionic cables represent the ionic transmission pathway as the axon in an afferent nerve, which carry information from the two artificial sensory channels to the electrolyte gated synaptic transistors for further integration and processing<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The electrolyte gated synaptic transistors are fabricated as shown in Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4</xref>. The exponential relaxation of ions in the electrolyte after an applied voltage stimulus on the gate (as the presynaptic terminal) explains the slow decay of channel currents (as the EPSC) based on electrostatic coupling<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. Such decay properties have been investigated for mimicking some essential synaptic plasticity — the basic neurological principle underling learning and memory<sup><xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR31">31</xref></sup>.</p><p id="Par8">These components are wired by the CNT electrodes fabricated through printing-filtration-transferring processes (Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">5</xref> and <xref ref-type="supplementary-material" rid="MOESM1">6</xref>) on PDMS films, and such ionic/electronic hybrid circuits are illustrated in Fig. <xref rid="Fig2" ref-type="fig">2a</xref>. Three input terminals are connected to the voltage supply for photodetector (<italic>V</italic><sub>V</sub>), pressure sensor (<italic>V</italic><sub>H</sub>), and global inhibitory input (<italic>V</italic><sub>IH</sub>), respectively. The energy consumption of the BASE could decrease down to &lt;0.05 μW by introducing the global inhibitory input (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">7</xref>). Two output terminals (<italic>V</italic><sub>D</sub> and <italic>V</italic><sub>S</sub>) are connected to the drain and source electrodes of the synaptic transistor, and a voltage bias (<italic>V</italic><sub>DS</sub> = 0.5 V) is applied between them for measuring the EPSC. The lowest sheet resistance of 7.0 Ω sq<sup>−1</sup> of such CNT electrodes has been obtained with a density of 33 μg cm<sup>−2</sup> (Fig. <xref rid="Fig2" ref-type="fig">2b</xref>), indicating decent conductivity for applying/collecting electronic signals from each terminal. Moreover, such CNT electrodes exhibit much lower interfacial impedance (<italic>f</italic> &lt; 1 kHz) with hydrogel than that of gold electrodes (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">8</xref>), enabling effective electronic–ionic current transduction at the electrode-hydrogel interfaces. Figure <xref rid="Fig2" ref-type="fig">2c</xref> shows the impedance characterizations of PVA hydrogel rods bonded to gold and CNT electrodes, respectively, and the latter is almost three orders of magnitude lower than the former. The ionic cable is then fixed on the CNT electrodes by using the instant tough bonding method<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>, maintaining low interfacial impedance (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">9</xref>).<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Characteristics of visual and haptic sensory channel illustrating the bimodal sensing capability.</title><p><bold>a</bold> The schematic diagram of the BASE for visual-haptic fusion (the sensors are encapsulated by PDMS). <bold>b</bold> The sheet resistance of the CNT electrodes with different density. Inset: the digital image of the test sample with density of 25 μg cm<sup>−2</sup>. The error bars are the standard deviations. The scale bar is 1 cm. <bold>c</bold> The impedance of the PVA hydrogel rod contacted with different electrodes. Inset: the LED connected to different electrodes that are connected with the PVA hydrogel. The drive voltage is 3.0 V with a frequency of 1 kHz. Visual sensory channel characterizations: <bold>d</bold> IV characterizations; <bold>e</bold> the EPSC responses to four different intensities (duration: 1 s); <bold>f</bold> the EPSC responses to four different durations (intensity: 4.8 mW cm<sup>−2</sup>). Haptic sensory channel characterizations: <bold>g</bold> IV characterizations; <bold>h</bold> the EPSC responses to four different intensities (duration: 1 s); <bold>i</bold> the EPSC responses to four different durations (intensity: 2 kPa).</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41467_2020_18375_Fig2_HTML.png"/></fig></p><p id="Par9">The information flow in such circuits starts from triggering specific sensors in response to an external stimulus. For artificial visual receptor, the resistance of the photodetector would decrease with an increase in incident light intensity as shown by the current–voltage (IV) measurement (Fig. <xref rid="Fig2" ref-type="fig">2d</xref>). The voltage supply (<italic>V</italic><sub>V</sub>) thus could induce an ion flux through the ionic cable due to the reduced resistance of the sensor. As the other side of the hydrogel is connected to the gate of the synaptic transistor, the accumulation of the ions would electrostatically couple an EPSC through the semiconducting channel of the transistor. The peak EPSCs are influenced by not only the magnitude of the pressure stimulus (Fig. <xref rid="Fig2" ref-type="fig">2e</xref>) but also the duration of the light stimulus (Fig. <xref rid="Fig2" ref-type="fig">2f</xref>). A similar trend can be found also in the artificial haptic receptor (Fig. <xref rid="Fig2" ref-type="fig">2g–i</xref>). Although slight differences between the output characteristics of the two artificial receptors are observed — the pressure sensors show a better linearity and faster responsivity than the photodetectors, the output range of the two kinds of receptors are very similar.</p></sec><sec id="Sec4"><title>Motion control based on visual-haptic fusion</title><p id="Par10">Next, we have designed and fabricated a biohybrid neuromuscular junction (BNJ) to transmit signals from the BASE and to innervate the skeletal myotubes, mimicking body motion control based on visual-haptic fusion (Fig. <xref rid="Fig3" ref-type="fig">3a</xref>). The BNJ consists of interdigital electrodes and cultured skeletal myotubes (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). To reduce the impedance between the gold electrode and the myotubes, a layer of Polypyrrole (PPy) has been coated onto the gold electrode by electroplating. As the conducting polymers can transfer charge both by electronic and ionic mechanisms<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR34">34</xref></sup>, thus the impedance magnitude has been greatly reduced from ~1 MΩ to ~3 kΩ at <italic>f</italic> = 1 Hz (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">10</xref>). C2C12 myoblast have been seeded after fibronectin functionalization of the BNJ surface. Myoblasts adhere and spread evenly upon overnight incubation in the seeding medium (Fig. <xref rid="Fig3" ref-type="fig">3b</xref>). To induce formation of myotubes, the cells were incubated in the differentiation medium for another 5 days. The voltage stimulation can then be applied on the BNJ as shown in Fig. <xref rid="Fig3" ref-type="fig">3c</xref>. Particle image velocimetry (PIV) analysis confirms the adhesion between electrodes and cells, and several areas of myotubes can be effectively activated by applying 1.0 V stimulation within 1 min (Fig. <xref rid="Fig3" ref-type="fig">3d</xref>). We intentionally choose three regions (region I to III) which are apparently active under electric field. Generally, the myotubes would shrink and show a motion pattern directing from the positive terminal to the ground (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">11</xref>). Region III is used to illustrate the stimulation intensity dependent contraction property (Fig. <xref rid="Fig3" ref-type="fig">3e</xref>), and a clearly positive correlation between voltage intensity and activeness (indicated by mapping of the magnitude of velocity) can be observed. The histogram of <italic>x</italic> component of velocity has also quantitatively verified such conclusion (Fig. <xref rid="Fig3" ref-type="fig">3f</xref>).<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>The visual-haptic fusion based on BASE patch for motion control.</title><p><bold>a</bold> The schematic of visual-haptic fusion for muscle actuation. <bold>b</bold> Confocal fluorescent imaging of biohybrid neuromuscular junction. The scale bar is 200 μm (left) and 50 μm (right), respectively. <bold>c</bold> Phase contrast image of a target-area (7.3 × 7.3 mm<sup>2</sup>) in the biohybrid neuromuscular junction. The scale bar is 1 mm. <bold>d</bold> Representative speed magnitude mapping of the target-area under 1.0 V stimulation. Region I to III indicate three robust regions of interest. The arrows indicate the dominant myotubes motion direction. The scale bar is 1 mm. <bold>e</bold> Representative mapping of the velocity vector (upper row) and speed magnitude (|<italic>v</italic>|, lower row) in Region III, under stimuli of 0.2 V, 0.6 V, and 1.0 V bias. The white arrows indicate the mean velocity vector. The scale bar is 200 μm. <bold>f</bold> Histogram plot of the motion velocity under 0.2 V, 0.6 V, and 1.0 V. Solid curves: Gaussian fitting of the histogram. <bold>g</bold> Plot of the relative change of ∆EPSC% as a function of the time interval (Δ<italic>T</italic>). The voltage applied on the photodetector and pressure sensor is positive (<italic>V</italic><sub>V</sub> = 1.0 V) and negative (<italic>V</italic><sub>H</sub> = −1.0 V), respectively. In all, 20% is defined as the criterion of synchronization. <bold>h</bold> The mean speed of region I to III in response to different EPSC changes. The texts in blue indicate the applied voltage converted from ΔEPSC%. <bold>i</bold> The “YES” and “NO” positions inferred by visual (top, pink) or haptic (bottom, blue) feedback. If the ball could be held by the robotic hand based on one sensory feedback, then the position is annotated as “YES” otherwise “NO”. <bold>j</bold> The modified BASE patch on the robotic hand and the magnified image of the BASE patch. The scale bar is 5 mm. <bold>k</bold> The <italic>ΔEPSC</italic> of the BASE with the ball at different positions (<italic>V</italic> = YES, <italic>H</italic> = NO; <italic>V</italic> = NO, <italic>H</italic> = YES; <italic>V</italic> = YES, <italic>H</italic> = NO) through the exploration process. The scale bars are 1 μA and 1 s for <italic>y</italic>-axis and <italic>x</italic>-axis, respectively.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41467_2020_18375_Fig3_HTML.png"/></fig></p><p id="Par11">To localize a nearby object, we might glance by eye (<italic>t</italic> = <italic>T</italic><sub>V</sub>) and explore by finger (<italic>t</italic> = <italic>T</italic><sub>H</sub>). The eye-hand coordination enables us to combine the two cues for spatial inference. Then we grasp the object via the contraction on muscle groups of the hand allows us to grasp the object, once its location has been confirmed. Moreover, human beings usually tolerate certain amount of temporal discrepancy between two signals while still perceiving them as being synchronous<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. To mimic such synchronous action, the visual stimulus (~4.8 mW cm<sup>−2</sup>, ~250 ms) and haptic stimulus (~2 kPa, ~350 ms) with different time intervals (∆<italic>T</italic> = <italic>T</italic><sub>H</sub>–<italic>T</italic><sub>V</sub>) are applied on the visual and haptic sensory channels of the BASE, respectively. The voltages applied on the two channels (<italic>V</italic><sub>V</sub> and <italic>V</italic><sub>H</sub>) are 1.0 and −1.0 V, respectively. Since both the EPSCs triggered by each of the two stimuli exhibit a gradual decay as demonstrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, they may affect the response following an immediate subsequent stimulus<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. Consequently, the EPSC amplitudes (∆EPSC) measured at <italic>t</italic> = <italic>T</italic><sub>V</sub> show no obvious change compared with ∆EPSC<sub>V</sub> (the EPSC amplitude triggered by visual stimulation only) when |∆<italic>T</italic>| &gt; 1 s, while with shorter time intervals, the amplitudes decrease gradually with decreasing |Δ<italic>T</italic>| (Fig. <xref rid="Fig3" ref-type="fig">3g</xref>). Here, we set the relative change of ∆EPSC against ∆EPSC<sub>V</sub> ((∆EPSC-∆EPSC<sub>V</sub>)/∆EPSC<sub>V</sub>, annotated as ∆EPSC%) of 20% as a threshold for determining synchronous (|ΔEPSC%| &gt; 20%) or asynchronous (|ΔEPSC%| ≤ 20%) signals. The ΔEPSC are detected and linearly converted to voltage outputs within the range of 0–1.0 V, which are then sent to the biohybrid neuromuscular junction for myotubes control (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">12</xref>). Our results show the voltage applied on the BNJ can trigger a perceptible migration when the voltage is &gt;0.4 V (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">13</xref>). The output voltage is no more than 0.4 V for the asynchronous signals, which cannot trigger the perceptible action of the myotubes (Fig. <xref rid="Fig3" ref-type="fig">3h</xref> and Supplementary Movies <xref ref-type="supplementary-material" rid="MOESM3">1–3</xref>). Conversely, the synchronous signals can induce the obvious migration of the myotubes.</p><p id="Par12">The strategy of fusing visual and haptic feedbacks can also enable robotic motion control and is superior to a unimodal one. A tennis ball located at the proximity of a robotic hand can be noted as “YES” (or “NO”) if it could (or not) deliver either the visual or haptic feedback to trigger the hand closing for catching the ball (Fig. <xref rid="Fig4" ref-type="fig">4i</xref>, visual feedback for <italic>z</italic>-axis or haptic feedback for <italic>y</italic>-axis). The ball can only be caught when it is at the “YES” position for both directions. The unimodal feedback could only be used to differentiate “YES” or “NO” at one-dimension, due to the binary states of each sensor. By contrast, a modified BASE patch is able to obtain the spatial information from the two dimensions (Fig. <xref rid="Fig4" ref-type="fig">4j</xref>) for guiding a robotic hand. The spatial information is obtained through an exploration process similar as before: (1) triggering a LED on the ball, providing the visual stimulus; (2) half-closing the hand to touch the ball, providing the haptic stimulus. As shown in Fig. <xref rid="Fig4" ref-type="fig">4k</xref>, the BASE can generate three types of responses by the exploration process (<italic>ΔT</italic> = 100 ms), when the ball is at three typical positions (<italic>V</italic> = YES, <italic>H</italic> = NO; <italic>V</italic> = NO, <italic>H</italic> = YES; <italic>V</italic> = YES, <italic>H</italic> = YES). In this way, the robotic hand can take more appropriate action (open or close) based on the multi-dimension information. Otherwise, the utilization of one-dimension spatial information might lead to wrong decision for the robotic hand (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">14</xref>, Tables <xref ref-type="supplementary-material" rid="MOESM1">1</xref> and <xref ref-type="supplementary-material" rid="MOESM1">2</xref>, Scheme <xref ref-type="supplementary-material" rid="MOESM1">1</xref>, and Movies <xref ref-type="supplementary-material" rid="MOESM6">4</xref>–<xref ref-type="supplementary-material" rid="MOESM10">8</xref>).<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>The BASE matrix based visual-haptic fusion for multi-transparency pattern recognition.</title><p><bold>a</bold> The multi-transparency alphabetic patterns (top left) and the visual-haptic fusion matrix (bottom left). The sensing data of each pattern obtained from the VH fusion matrix are fed into a perceptron for recognition. Each pixel could be opaque (top right, shown in black), translucent (middle right, shown in gray), or transparent (bottom right, shown in light blue). The patterns are labeled based on the shape of the main body and their transparency. For example, the ‘T1’, ‘T2’, and ‘T3’ denote the ‘T’ shape patterns which are opaque, translucent, and transparent, respectively. <bold>b</bold> The fusion results of two equidistance inputs. Two voltage inputs with different amplitudes are applied to a synaptic transistor through ionic cable with two isometric branches. The expected sum is the arithmetic summation of the EPSCs by individually triggering the two inputs, and the measured sum is the EPSCs by simultaneously triggering the two inputs. <bold>c</bold> The output measured from different inputs that are connected to the synaptic transistor through one common ionic cable. The weight for each input thereby annotated as <italic>w</italic>(1) to <italic>w</italic>(<italic>m</italic>). <bold>d</bold> The normalized mapping results based on unimodal information (visual or haptic) and VH fusion information. n-VH fusion data is merged from every 1, 3, and 5-VH units to 1 synaptic transistor in each line of the VH fusion matrix. The scale bar (in blue) is 5 mm. <bold>e</bold> The recognition rates for the mapping of unimodal and bimodal modes with kernel sizes of 1–5, respectively. The error bars are the standard deviations.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41467_2020_18375_Fig4_HTML.png"/></fig></p></sec><sec id="Sec5"><title>Multi-transparency pattern recognition</title><p id="Par13">Subsequently, we design BASE-based sensory fusion matrices to use the fused visual and haptic cues for recognition tasks (Fig. <xref rid="Fig4" ref-type="fig">4a</xref>). Multi-transparency alphabetic patterns (a mixture of PDMS and carbon black, Supplementary Figs. <xref ref-type="supplementary-material" rid="MOESM1">15</xref> and <xref ref-type="supplementary-material" rid="MOESM1">16</xref>) have been used in this work. The alphabetic patterns are labeled based on the shape and transparency of their main body. To verify the robustness of the recognition, 720 alphabetic patterns with different combinations of random noise pixels have been used for recognition (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">17</xref>). The sensing data from each matrix are then fed into a perceptron with one hidden layer built by MATLAB for the pattern recognition tasks (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">18</xref> and Supplementary Training Samples). In this case, this matrix serves as the feature extraction layer of this artificial neural network.</p><p id="Par14">The bimodal fusion matrix and unimodal matrices (optics or pressure) are designed with 10 × 10 pixels. Each pixel of the bimodal fusion matrix contains one photodetector and one pressure sensor (noted as VH unit). In each line, the ten VH units are connected to synaptic transistors through the ionic cable. As shown in Fig. <xref rid="Fig4" ref-type="fig">4b</xref>, the transistor can integrate stimuli from the VH unit inducing a joint EPSC current which is dependent on the intensity of the two stimuli and the weight of the input (<italic>w</italic>). Our results show that the summation output (measured sum) from one VH unit is slightly lower than the arithmetic summation (expected sum) of the outputs (Fig. <xref rid="Fig4" ref-type="fig">4b</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">19</xref>). The weight of the input (<italic>w</italic>(<italic>m</italic>)) is dependent on the distance between the transistor and the VH unit — a gradual decay with an increasing distance (Fig. <xref rid="Fig4" ref-type="fig">4c</xref> and Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">20</xref>). Therefore, the integration effect of the synaptic transistor to multiple inputs could be deemed as the integral of the product of the input intensities and their distance-dependent weights. Here, we define the kernel size (<italic>n</italic>) of the feature extraction layer as the numbers of VH units in one row connected to one transistor in series through one common ionic cable (i.e., VH unit <italic>i</italic> to <italic>i</italic> + <italic>n</italic>−1 (<italic>i</italic> = 1, 2, … 10−<italic>n</italic> + 1). Such configuration thereby could implement the convolution-like operation to the input from each VH unit (annotated by <italic>n</italic>-VH, Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">21</xref>). The convolutional layer is the core building block of a convolutional neural network (CNN), which serves as the filter and efficiently extracts the essential features of input. As such, the mapping output (EPSC amplitude from each transistor) of the sensory fusion matrices can be calculated by computer using the extracted device parameters from these results (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">22</xref> and Note <xref ref-type="supplementary-material" rid="MOESM1">1</xref>).</p><p id="Par15">To obtain the visual and haptic cues of the multi-transparency patterns, we place the patterns on the arrays for ~1.5 s with a light source (~4.8 mW cm<sup>−2</sup>) turned on at the same time, and thereafter remove the pattern with the light turned off. The normalized sensing data by unimodal matrices and bimodal matrix are shown in Fig. <xref rid="Fig4" ref-type="fig">4d</xref>. When mapping the multi-transparency patterns, simply exploiting the optic matrix would lose some essential features, while simply exploiting the pressure matrix could only provide the shape information without the degree of transparency. Only the visual-haptic fusion matrix could extract both the shape and the transparency adequately. Consequently, the recognition rate based on the visual-haptic fusion matrix shows the best outcome as shown in Fig. <xref rid="Fig4" ref-type="fig">4e</xref>. Interestingly, although the features of the alphabetic patterns extracted by increasing the kernel size of the matrix would be lost (60% of original data size with <italic>n</italic> = 5) due to the spatial integration, the recognition rate (~66%) by the fusion mode with <italic>n</italic> = 5 is still slightly better than that of the unimodal mode (~65% for the optic matrix with <italic>n</italic> = 1). Such results indicate we can achieve robust recognition by using the VH fusion matrix as the feature extraction layer of an artificial neural network.</p></sec></sec><sec id="Sec6" sec-type="discussion"><title>Discussion</title><p id="Par16">In summary, inspired by the supramodal sensory fusion in the sensory nervous system, we developed an ionic/electronic hybrid neuromorphic device that combines optic and pressure stimuli to generate a summed EPSC through the synaptic transistor. Such combined current carries the bimodal information in time-dependent and nonlinear manners, which closely resemble the neuronal behaviors. The fused signal was then used to innervate the skeletal myotubes and provide multi-dimensional spatial information for the robotic hand, successfully mimicking the motion control by using bimodal sensory cues at the cellular level. More interestingly, the bimodal sensory data could be implemented for recognition of multi-transparency alphabetic patterns and exhibits superior performance to unimodal sensory data. The multi-transparency patterns could represent and abstract the core factors of some real scenarios where the transparency of the object could be inferred by visual feedback and the shape or weight could be inferred by tactile feedback.</p><p id="Par17">We have summarized recent achievements in artificial sensory neurons as shown in Table <xref rid="Tab1" ref-type="table">1</xref>. Compared with other works, our BASE can fuse the cues of two sensory modalities and use them for manipulation, recognition, and synaptic emulations. In addition, with the successful incorporation of myotubes as the actuator, all components in the current system are performing functions at the cellular or equivalent level, which is of great potential for assembling the bionic sensorimotor system from the bottom up. As distinct from previous biohybrid robots using direct bias or optogenetic stimuli not representative of actual real-world cues<sup><xref ref-type="bibr" rid="CR38">38</xref>–<xref ref-type="bibr" rid="CR40">40</xref></sup>, our biohybrid neuromuscular junction can trigger the contraction of myoblasts by visual and/or tactile -like stimulation, hence moving towards biologically relevant sensorimotor systems. Conventional approaches for sensory processing rely on centralized and sequential operations of data, in which the efficiency dramatically decreases with the increase in data size. In contrast, the neuronal-level implementation, incorporating sensing, refining, and processing would eventually achieve the biological advantages of fault tolerance and power efficiency. Although multi-sensory capabilities have been realized in some e-skin systems<sup><xref ref-type="bibr" rid="CR41">41</xref>–<xref ref-type="bibr" rid="CR43">43</xref></sup>, their sensory processing has always been completed using conventional digital units. As such, mimicking sensory fusion from the neuronal-level could help to build a highly integrated perceptual system to access massive sensory data for improving current cyborg technologies<sup><xref ref-type="bibr" rid="CR44">44</xref>,<xref ref-type="bibr" rid="CR45">45</xref></sup> and artificial intelligence<sup><xref ref-type="bibr" rid="CR46">46</xref>–<xref ref-type="bibr" rid="CR48">48</xref></sup>.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Summary of state-of-the-art in artificial sensory neurons.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Name</p></th><th><p>Sensory modality</p></th><th><p>Actuator &amp; size</p></th><th><p>Feature action dimension</p></th><th><p>Recognition pattern</p></th><th><p>Synaptic emulations</p></th></tr></thead><tbody><tr><td><p>Artificial afferent nerve<sup><xref ref-type="bibr" rid="CR19">19</xref></sup></p></td><td><p>Pressure</p></td><td><p>Insect leg ~3 cm</p></td><td><p>~1 mm</p></td><td><p>Braille pattern</p></td><td><p>Short-term potentiation</p></td></tr><tr><td><p>NeuTap<sup><xref ref-type="bibr" rid="CR18">18</xref></sup></p></td><td><p>Pressure</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>Braille pattern</p></td><td><p>Short-term potentiation; dendritic integration</p></td></tr><tr><td><p>Artificial somatic reflex Arc<sup><xref ref-type="bibr" rid="CR49">49</xref></sup></p></td><td><p>Pressure</p></td><td><p>Electrochemical actuator ~2.5 cm</p></td><td><p>~1 mm</p></td><td><p>NA</p></td><td><p>Neural all-or-none law</p></td></tr><tr><td><p>Artificial optic-neural synapse<sup><xref ref-type="bibr" rid="CR50">50</xref></sup></p></td><td><p>Light</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>Colour-mixed pattern</p></td><td><p>Long-term potentiation &amp; depression</p></td></tr><tr><td><p>Optoelectronic sensorimotor artificial synapse<sup><xref ref-type="bibr" rid="CR20">20</xref></sup></p></td><td><p>Light</p></td><td><p>Polymer actuator ~2 cm</p></td><td><p>~1 mm</p></td><td><p>NA</p></td><td><p>Short-term potentiation</p></td></tr><tr><td><p>Optoelectronic neuromorphic device<sup><xref ref-type="bibr" rid="CR51">51</xref></sup></p></td><td><p>Light</p></td><td><p>NA</p></td><td><p>NA</p></td><td><p>Illuminance gradation pattern</p></td><td><p>Visual synaptic functions; environment-adaptable perception</p></td></tr><tr><td><p>This work (BASE)</p></td><td><p>Pressure &amp; light</p></td><td><p>Myotubes ~1.5 mm</p></td><td><p>~5 µm</p></td><td><p>Multi-transparency pattern</p></td><td><p>Dendritic integration</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec7" sec-type="methods"><title>Methods</title><sec id="Sec8"><title>Fabrication of the photodetectors</title><p id="Par18">The fabrication processes of the photodetectors are briefed as follows. ITO/PET was chemically etched by reaction of zinc powder with HCl (2 M) for the desired pattern before being cut into 2.5 × 2.5 cm<sup>2</sup> pieces. The ITO/PET was washed and rinsed with detergent, DI water, and ethanol sequentially. After drying with nitrogen gas, the ITO/PET was secured onto glass substrates and treated in UV-ozone plasma for 15 min. Zn<sub>2</sub>SnO<sub>4</sub> nanoparticle solution was deposited on prepared ITO substrate by spin coating at 3000 rpm for 30 s. The coated substrates were dried at 100 °C for 10 min, followed by annealing at 150 °C for 2 h. Perovskite precursor (1.2 M) and substrates were preheated to 100 °C, before the hot precursor solution was spin coated onto substrates at 4000 rpm for 30 s, followed by annealing at 100 °C for 15 min. PTAA solution was spin coated on cool perovskite films at 3000 rpm for 60 s. Au electrodes (~100 nm) were thermal evaporated on samples with the size of 0.2 cm × 0.2 cm.</p></sec><sec id="Sec9"><title>Fabrication of pyramidal structured PDMS film</title><p id="Par19">Silicon masters with recessed pyramidal microstructure arrays were fabricated using our previous method by photolithography and wet etching process<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. The mixture of PDMS elastomer and crosslinker in 10:1 (w/w) ratio (Sylgard 184, Dow Corning) were spin coated on PDMS at 800 rpm. The elastomer mixture was degassed in vacuum and cured at 90 °C for 1 h. The films were then sectioned by a scalpel and peeled off from the silicon master.</p></sec><sec id="Sec10"><title>CNT deposition through ultrasonic vibrating method</title><p id="Par20">The CNT were sprayed on the microstructured PDMS film through ultrasonic vibrating method (Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">2a</xref>). In brief, 5 μL CNT solution (1 mg/mL) was diluted in 2 mL deionized water to obtain a homogeneous CNT solution of 2.5 μg/mL through stirring. A commercial ultrasonic humidifier was used to generate microdroplets of the CNT solution through a filter screen with hole size of 60 μm. A time controller was used to trigger/stop the generation process. For fabrication, the PDMS films were placed on a hotplate at 120 °C. One fabrication cycle takes 1 min. The CNT microdroplet were generated through the ultrasonic vibrator and sprayed onto the PDMS film for 5 s. It was then dried out to form a CNT film on the electrodes for another 55 s.</p></sec><sec id="Sec11"><title>Fabrication of the synaptic transistors</title><p id="Par21">The fabrication of the PVA gated synaptic transistors can be obtained in previous reports<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. In brief, the patterned ITO electrodes (drain, source, and gate electrodes) with a thickness of ~60 nm were deposited by radio-frequency magnetron sputtering through a stainless steel shadow mask. Then a ~10 nm ITO channel was deposited using the same method. PVA powder was dissolved in 5% CaCl<sub>2</sub> solution at a concentration of 10 wt%, then heated gently using a hot oil bath for complete dissolution without thermal decomposition of the polymer. Then the PVA gate dielectric was cast onto the channel and gate electrode, and dried at 60 °C for 2 h.</p></sec><sec id="Sec12"><title>CNT patterning process</title><p id="Par22">The patterns were designed using PowerPoint software. The Nylon filter membrane (1 μm) was adhered to A4 paper for printing. The designed patterns thus were printed onto the filter membrane. Then the CNT solution was vacuum-filtered on the filter membrane. CNT could only be concentrated on the white areas of the filter membrane, because the micropores of the membrane at the black areas were penetrated with the printing ink which impedes the passing through of solvent. The membrane was then thoroughly rinsed with DI water to remove the residual sodium dodecyl benzene sulfonate (SDBS) and CNT on the black area. Next, the PDMS (10:1) which was heated in the oven for 35 min (60 °C), was used for transferring the CNT pattern by the hot-press approach. The membrane with the CNT pattern face was put onto the half-cured PDMS, and a glass and a 200-g weight were covered onto the membrane for achieving close contact between the membrane and PDMS. The PDMS and the membrane were then heated in the oven for 4 h (60 °C) before separating of them.</p></sec><sec id="Sec13"><title>PVA based hydrogel fabrication process</title><p id="Par23">The PVA (Aldrich, MW = 124,000) powder was dissolved in deionized water at a concentration of 10 wt%, then heated gently using a hot oil bath (90 °C) for complete dissolution without thermal decomposition of the polymer. The hot solution was stirred until the polymer was completely dissolved and a clear viscous solution is formed. Then the PVA hydrogel is obtained through three freeze-thaw cycles. To obtain a high ionic conductivity, the PVA hydrogel was immersed in the 1 mol L<sup>−1</sup> LiCl for 4 h.</p></sec><sec id="Sec14"><title>Fabrication of the biohybrid neuromuscular junction</title><p id="Par24">The interdigital Au electrodes with thickness of 70 nm were deposited through thermal evaporation using a shadow mask. Then the Polypyrrole (PPy) was coated onto the Au electrode by electroplating. The PPy was synthesized on the interdigital Au electrodes by anodic oxidation of pyrrole in an electrochemical cell containing 0.42 g pyrrole monomer, p-toluenesulfonic acid (PTSA, 25 mg) dissolved in 30 mL 5% phosphate buffer saline (PBS) at temperature of ~0 °C. Pyrrole was first distilled and kept refrigerated until use. Dissolved oxygen was removed by bubbling the solution with nitrogen for several minutes prior to polymerization. The galvanostatic polymerization was carried out by using Keithley 4200 with the constant current density of 0.8 mA/cm<sup>2</sup>. An Au plate serves as the counter electrode. The solution was kept under stirring by a magnetic stirrer during the polymerization process. After 3 min of electrochemical polymerization, a dense layer of PPy was deposited on the surface of Au electrode. The obtained electrodes were dipped in DI water for three times at 20 min each. C2C12 myoblasts were seeded onto the electrode substrate in the seeding medium (DMEM with 10% fetal bovine serum) overnight. Subsequently, the cells were induced to form myotubes in the differentiation medium (DMEM supplemented with 2% fetal bovine serum and 1 µg/mL insulin) for 5 days, forming the biohybrid neuromuscular junction.</p></sec><sec id="Sec15"><title>The recognition simulation</title><p id="Par25">The six basic alphabetic patterns were extended with three transparencies: transparent, translucent, and opaque. Each pattern was added with five noise pixels (40 cases in total) as one multi-transparency pattern for recognition task. The distribution and the transparency of noise pixel were generated by MATLAB. As such, there are 720 multi-transparency patterns in total. The data obtained from the sensory fusion matrix was fed into the two layers of perceptron. There are 10 nodes in the hidden layer and 18 nodes in the output layer corresponding to the 18 labels of patterns. The network was then trained by MATLAB.</p></sec><sec id="Sec16"><title>Reporting summary</title><p id="Par26">Further information on research design is available in the <xref ref-type="supplementary-material" rid="MOESM11">Nature Research Reporting Summary</xref> linked to this article.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>The authors thank the financial support from the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (project no. A18A1b0045) Cyber-Physiochemical Interfaces (CPI) Programme, the National Research Foundation, Prime Minister’s office, Singapore, under its NRF Investigatorship (NRF-NRFI2017-07), and Singapore Ministry of Education (MOE2017-T2-2-107).</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>X.C. and C.W. conceived the project and constructed the research frame. These authors contributed equally: C.W. and P.C.; C.W. and P.C. built the biohybrid neuromuscular junction and performed the cell-related measurements. C.W. and X.G. prepared the devices and performed the hardware measurements. C.W. and M.W. built the software package. X.C. and C.W. analyzed the experimental data and simulation results. P.C., Y.L., and Z.L. prepared the hydrogels and CNT electrodes. C.W., L.Y., N.M., Y.L., X.L., and X.C. edited the manuscript. All authors discussed the results and implications and commented on the manuscript at all stages.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The data that support the plots within this paper and other finding of this study are available from the corresponding author on reasonable request.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>Code from this study is available from the corresponding author upon request.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par27">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Yang J., Liu H., Sun F. &amp; Gao M. in <italic>2015 IEEE Int. Conf. Robot. Biomimetics (ROBIO)</italic> (IEEE, 2015).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Güler P., Bekiroglu Y., Gratal X., Pauwels K. &amp; Kragic D. What’s in the container? Classifying object contents from vision and touch. in <italic>Proc.</italic><italic>2014 IEEE/RSJ Int. Conf. Intell. Robots Syst.</italic> (IEEE, 2014).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zidan</surname><given-names>MA</given-names></name><name><surname>Strachan</surname><given-names>JP</given-names></name><name><surname>Lu</surname><given-names>WD</given-names></name></person-group><article-title xml:lang="en">The future of electronics based on memristive systems</article-title><source>Nat. Electron.</source><year>2018</year><volume>1</volume><fpage>22</fpage><lpage>29</lpage></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>S</given-names></name><name><surname>Okun</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Learning to play Go from scratch</article-title><source>Nature</source><year>2017</year><volume>550</volume><fpage>336</fpage><lpage>337</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs12ltLnN</pub-id><pub-id pub-id-type="pmid">29052631</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2017Natur.550..336S</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shen</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning with coherent nanophotonic circuits</article-title><source>Nat. Photon.</source><year>2017</year><volume>11</volume><fpage>441</fpage><lpage>446</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhtVSjt7bJ</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2017NaPho..11..441S</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H-L</given-names></name><etal/></person-group><article-title xml:lang="en">Flexible neuromorphic electronics for computing, soft robotics, and neuroprosthetics</article-title><source>Adv. Mater.</source><year>2019</year><volume>32</volume><fpage>1903558</fpage></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>S</given-names></name><name><surname>Rind</surname><given-names>FC</given-names></name><name><surname>Keil</surname><given-names>MS</given-names></name><name><surname>Cuadri</surname><given-names>J</given-names></name><name><surname>Stafford</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">A bio-inspired visual collision detection mechanism for cars: optimisation of a model of a locust neuron to a novel environment</article-title><source>Neurocomputing</source><year>2006</year><volume>69</volume><fpage>1591</fpage><lpage>1598</lpage></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yue</surname><given-names>S</given-names></name><name><surname>Rind</surname><given-names>FC</given-names></name></person-group><article-title xml:lang="en">Collision detection in complex dynamic scenes using an LGMD-based visual neural network with feature enhancement</article-title><source>IEEE Trans. Neural Netw.</source><year>2006</year><volume>17</volume><fpage>705</fpage><lpage>716</lpage><pub-id pub-id-type="pmid">16722174</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Toprak</surname><given-names>S</given-names></name><name><surname>Navarro-Guerrero</surname><given-names>N</given-names></name><name><surname>Wermter</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Evaluating integration strategies for visuo-haptic object recognition</article-title><source>Cognit. Comput.</source><year>2018</year><volume>10</volume><fpage>408</fpage><lpage>425</lpage><pub-id pub-id-type="pmid">29881470</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraira</surname><given-names>VE</given-names></name><name><surname>Ginty</surname><given-names>DD</given-names></name></person-group><article-title xml:lang="en">The sensory neurons of touch</article-title><source>Neuron</source><year>2013</year><volume>79</volume><fpage>618</fpage><lpage>639</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3sXhtlahu77N</pub-id><pub-id pub-id-type="pmid">23972592</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lumpkin</surname><given-names>EA</given-names></name><name><surname>Caterina</surname><given-names>MJ</given-names></name></person-group><article-title xml:lang="en">Mechanisms of sensory transduction in the skin</article-title><source>Nature</source><year>2007</year><volume>445</volume><fpage>858</fpage><lpage>865</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2sXhvFGiu7Y%3D</pub-id><pub-id pub-id-type="pmid">17314972</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2007Natur.445..858L</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishkin</surname><given-names>M</given-names></name><name><surname>Ungerleider</surname><given-names>LG</given-names></name><name><surname>Macko</surname><given-names>KA</given-names></name></person-group><article-title xml:lang="en">Object vision and spatial vision: two cortical pathways</article-title><source>Trends Neurosci.</source><year>1983</year><volume>6</volume><fpage>414</fpage><lpage>417</lpage></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Polanen</surname><given-names>V</given-names></name><name><surname>Davare</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Interactions between dorsal and ventral streams for controlling skilled grasp</article-title><source>Neuropsychologia</source><year>2015</year><volume>79</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="pmid">26169317</pub-id><pub-id pub-id-type="pmcid">4678292</pub-id></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freud</surname><given-names>E</given-names></name><name><surname>Plaut</surname><given-names>DC</given-names></name><name><surname>Behrmann</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">‘What’ is happening in the dorsal visual pathway</article-title><source>Trends Cogn. Sci.</source><year>2016</year><volume>20</volume><fpage>773</fpage><lpage>784</lpage><pub-id pub-id-type="pmid">27615805</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><article-title xml:lang="en">Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><year>2002</year><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD38XhtVCntrY%3D</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2002Natur.415..429E</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Artificial sensory memory</article-title><source>Adv. Mater.</source><year>2019</year><volume>32</volume><fpage>1902434</fpage></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y</given-names></name><name><surname>Ahn</surname><given-names>J-H</given-names></name></person-group><article-title xml:lang="en">Biomimetic tactile sensors based on nanomaterials</article-title><source>ACS Nano</source><year>2020</year><volume>14</volume><fpage>1220</fpage><lpage>1226</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvFKrsLw%3D</pub-id><pub-id pub-id-type="pmid">32011120</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">An artificial sensory neuron with tactile perceptual learning</article-title><source>Adv. Mater.</source><year>2018</year><volume>30</volume><fpage>1801291</fpage></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">A bioinspired flexible organic artificial afferent nerve</article-title><source>Science</source><year>2018</year><volume>360</volume><fpage>998</fpage><lpage>1003</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtVagsrfK</pub-id><pub-id pub-id-type="pmid">29853682</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018Sci...360..998K</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Stretchable organic optoelectronic sensorimotor synapse</article-title><source>Sci. Adv.</source><year>2018</year><volume>4</volume><fpage>eaat7387</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsF2jur3O</pub-id><pub-id pub-id-type="pmid">30480091</pub-id><pub-id pub-id-type="pmcid">6251720</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018SciA....4.7387L</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>H-L</given-names></name><etal/></person-group><article-title xml:lang="en">Flexible neuromorphic electronics for computing, soft robotics, and neuroprosthetics</article-title><source>Adv. Mater.</source><year>2020</year><volume>32</volume><fpage>1903558</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvVGnsbrP</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Ravinder S. &amp; Dahiya M. V. in <italic>Robotic Tactile Sensing</italic> 14–15 (Springer, 2013).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>X</given-names></name><etal/></person-group><article-title xml:lang="en">Two-dimensional additive diethylammonium iodide promoting crystal growth for efficient and stable perovskite solar cells</article-title><source>RSC Adv.</source><year>2019</year><volume>9</volume><fpage>7984</fpage><lpage>7991</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXksFelur4%3D</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>HG</given-names></name><name><surname>Zeng</surname><given-names>HC</given-names></name></person-group><article-title xml:lang="en">Preparation of hollow anatase TiO2 nanospheres via ostwald ripening</article-title><source>J. Phys. Chem. B</source><year>2004</year><volume>108</volume><fpage>3492</fpage><lpage>3495</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2cXhsVKrsrs%3D</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Skin-inspired haptic memory arrays with an electrically reconfigurable architecture</article-title><source>Adv. Mater.</source><year>2016</year><volume>28</volume><fpage>1559</fpage><lpage>1566</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXitVCltrbL</pub-id><pub-id pub-id-type="pmid">26676965</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>C</given-names></name><name><surname>Suo</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">Hydrogel ionotronics</article-title><source>Nat. Rev. Mater.</source><year>2018</year><volume>3</volume><fpage>125</fpage><lpage>142</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXht1Cjs7rN</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018NatRM...3..125Y</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>C</given-names></name><name><surname>Zhu</surname><given-names>L</given-names></name><name><surname>Wan</surname><given-names>X</given-names></name><name><surname>Shi</surname><given-names>Y</given-names></name><name><surname>Wan</surname><given-names>Q</given-names></name></person-group><article-title xml:lang="en">Organic/inorganic hybrid synaptic transistors gated by proton conducting methylcellulose films</article-title><source>Appl. Phys. Lett.</source><year>2016</year><volume>108</volume><fpage>043508</fpage><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2016ApPhL.108d3508W</pub-id></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wan</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Flexible metal oxide/graphene oxide hybrid neuromorphic transistors on flexible conducting graphene substrates</article-title><source>Adv. Mater.</source><year>2016</year><volume>28</volume><fpage>5878</fpage><lpage>5885</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XnsFWlsbw%3D</pub-id><pub-id pub-id-type="pmid">27159546</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>John</surname><given-names>RA</given-names></name><etal/></person-group><article-title xml:lang="en">Synergistic gating of electro-iono-photoactive 2D chalcogenide neuristors: coexistence of hebbian and homeostatic synaptic metaplasticity</article-title><source>Adv. Mater.</source><year>2018</year><volume>30</volume><fpage>1800220</fpage></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Ion gated synaptic transistors based on 2D van der waals crystals with tunable diffusive dynamics</article-title><source>Adv. Mater.</source><year>2018</year><volume>30</volume><fpage>1800195</fpage></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moon</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Temporal data classification and forecasting using a memristor-based reservoir computing system</article-title><source>Nat. Electron.</source><year>2019</year><volume>2</volume><fpage>480</fpage><lpage>487</lpage></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wirthl</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Instant tough bonding of hydrogels for soft machines and electronics</article-title><source>Sci. Adv.</source><year>2017</year><volume>3</volume><fpage>e1700053</fpage><pub-id pub-id-type="pmid">28691092</pub-id><pub-id pub-id-type="pmcid">5479648</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2017SciA....3E0053W</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qi</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Highly stretchable, compliant, polymeric microelectrode arrays for in vivo electrophysiological interfacing</article-title><source>Adv. Mater.</source><year>2017</year><volume>29</volume><fpage>1702800</fpage></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Soft conductive micropillar electrode arrays for biologically relevant electrophysiological recording</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2018</year><volume>115</volume><fpage>11718</fpage><lpage>11723</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXit1SgtL3F</pub-id><pub-id pub-id-type="pmid">30377271</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018PNAS..11511718L</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>IM</given-names></name></person-group><article-title xml:lang="en">Detection of temporal delays in visual-haptic interfaces</article-title><source>Hum. Factors</source><year>2004</year><volume>46</volume><fpage>118</fpage><lpage>134</lpage><pub-id pub-id-type="pmid">15151159</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewkowicz</surname><given-names>DJ</given-names></name></person-group><article-title xml:lang="en">Perception of auditory–visual temporal synchrony in human infants</article-title><source>J. Exp. Psychol. Hum. Percept. Perform.</source><year>1996</year><volume>22</volume><fpage>1094</fpage><lpage>1106</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DyaK2s%2FhvFSjsw%3D%3D</pub-id><pub-id pub-id-type="pmid">8865617</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>C-L</given-names></name><name><surname>Truong</surname><given-names>Q</given-names></name><name><surname>Shen</surname><given-names>AM</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">A carbon nanotube synapse with dynamic logic and learning</article-title><source>Adv. Mater.</source><year>2013</year><volume>25</volume><fpage>1693</fpage><lpage>1698</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhvFWhu77E</pub-id><pub-id pub-id-type="pmid">23281020</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nawroth</surname><given-names>JC</given-names></name><etal/></person-group><article-title xml:lang="en">A tissue-engineered jellyfish with biomimetic propulsion</article-title><source>Nat. Biotechnol.</source><year>2012</year><volume>30</volume><fpage>792</fpage><lpage>797</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhtVKnurzM</pub-id><pub-id pub-id-type="pmid">22820316</pub-id><pub-id pub-id-type="pmcid">4026938</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S-J</given-names></name><etal/></person-group><article-title xml:lang="en">Phototactic guidance of a tissue-engineered soft-robotic ray</article-title><source>Science</source><year>2016</year><volume>353</volume><fpage>158</fpage><lpage>162</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtFSisrjO</pub-id><pub-id pub-id-type="pmid">27387948</pub-id><pub-id pub-id-type="pmcid">5526330</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2016Sci...353..158P</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>SR</given-names></name><etal/></person-group><article-title xml:lang="en">Electrically driven microengineered bioinspired soft robots</article-title><source>Adv. Mater.</source><year>2018</year><volume>30</volume><fpage>1704189</fpage></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hua</surname><given-names>Q</given-names></name><etal/></person-group><article-title xml:lang="en">Skin-inspired highly stretchable and conformable matrix networks for multifunctional sensing</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><pub-id pub-id-type="pmid">29339793</pub-id><pub-id pub-id-type="pmcid">5770430</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018NatCo...9..244H</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>WW</given-names></name><etal/></person-group><article-title xml:lang="en">A neuro-inspired artificial peripheral nervous system for scalable electronic skins.</article-title><source>Sci. Robot.</source><year>2019</year><volume>4</volume><fpage>eaax2198</fpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>MK</given-names></name><etal/></person-group><article-title xml:lang="en">Soft-packaged sensory glove system for human-like natural interaction and control of prosthetic hands</article-title><source>NPG Asia Mater.</source><year>2019</year><volume>11</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXit1yit7%2FI</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2019npjAM..11...43K</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>G-H</given-names></name><etal/></person-group><article-title xml:lang="en">Multifunctional materials for implantable and wearable photonic healthcare devices</article-title><source>Nat. Rev. Mater.</source><year>2020</year><volume>5</volume><fpage>149</fpage><lpage>165</lpage><pub-id pub-id-type="pmid">32728478</pub-id><pub-id pub-id-type="pmcid">7388681</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2020NatRM...5..149L</pub-id></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arab Hassani</surname><given-names>F</given-names></name><name><surname>Jin</surname><given-names>H</given-names></name><name><surname>Yokota</surname><given-names>T</given-names></name><name><surname>Someya</surname><given-names>T</given-names></name><name><surname>Thakor</surname><given-names>NV</given-names></name></person-group><article-title xml:lang="en">Soft sensors for a sensing-actuation system with high bladder voiding efficiency</article-title><source>Sci. Adv.</source><year>2020</year><volume>6</volume><fpage>eaba0412</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DC%2BB38rlsFegsQ%3D%3D</pub-id><pub-id pub-id-type="pmid">32494686</pub-id><pub-id pub-id-type="pmcid">7195140</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2020SciA....6A.412A</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>H</given-names></name><name><surname>Hwang</surname><given-names>S</given-names></name><name><surname>Park</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>B-G</given-names></name></person-group><article-title xml:lang="en">Silicon synaptic transistor for hardware-based spiking neural network and neuromorphic system</article-title><source>Nanotechnology</source><year>2017</year><volume>28</volume><fpage>405202</fpage><pub-id pub-id-type="pmid">28820141</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">A fully integrated reprogrammable memristor–CMOS system for efficient multiply–accumulate operations</article-title><source>Nat. Electron.</source><year>2019</year><volume>2</volume><fpage>290</fpage><lpage>299</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtlOlsLjP</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">Locally coupled electromechanical interfaces based on cytoadhesion-inspired hybrids to identify muscular excitation-contraction signatures</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXovVChsbk%3D</pub-id><pub-id pub-id-type="pmid">32366821</pub-id><pub-id pub-id-type="pmcid">7198512</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2020NatCo..11.2183C</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">An artificial somatic reflex arc</article-title><source>Adv. Mater.</source><year>2020</year><volume>32</volume><fpage>1905399</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitlejt77O</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seo</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Artificial optic-neural synapse for colored and color-mixed pattern recognition</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><pub-id pub-id-type="pmid">30504804</pub-id><pub-id pub-id-type="pmcid">30504804</pub-id><pub-id pub-id-type="other" assigning-authority="NASA Astrophysics Data System">2018NatCo...9.5106S</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kwon</surname><given-names>SM</given-names></name><etal/></person-group><article-title xml:lang="en">Environment-adaptable artificial visual perception behaviors using a light-adjustable optoelectronic neuromorphic device array</article-title><source>Adv. Mater.</source><year>2019</year><volume>31</volume><fpage>1906433</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitFKrsLzO</pub-id></mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec17"><title>Supplementary information</title><p id="Par28"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41467_2020_18375_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>SUPPLEMENTARY INFO</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="msword" xlink:href="MediaObjects/41467_2020_18375_MOESM2_ESM.docx" position="anchor"><caption xml:lang="en"><p>Description of Additional Supplementary Files</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM3" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM3_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 1</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM4" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM4_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 2</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM5" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM5_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 3</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM6" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM6_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 4</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM7" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM7_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 5</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM8" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM8_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 6</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM9" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM9_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 7</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM10" xlink:title="Supplementary information"><media mimetype="video" mime-subtype="mp4" xlink:href="MediaObjects/41467_2020_18375_MOESM10_ESM.mp4" position="anchor"><caption xml:lang="en"><p>Movie 8</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM11" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41467_2020_18375_MOESM11_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Reporting Summary</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p><bold>Supplementary information</bold> is available for this paper at <ext-link xlink:href="10.1038/s41467-020-18375-y" ext-link-type="doi">https://doi.org/10.1038/s41467-020-18375-y</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Peer review information</bold><italic>Nature Communications</italic> thanks the anonymous reviewer(s) for their contribution to the peer review of this work.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Science, Humanities and Social Sciences, multidisciplinary</facet-value><facet-value count="1">Science, multidisciplinary</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">Nature Communications</facet-value></facet><facet name="year"><facet-value count="1">2020</facet-value></facet><facet name="country"><facet-value count="1">Singapore</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
