<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-023-01048-x</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-023-01048-x</article-id><article-id pub-id-type="manuscript">1048</article-id><article-id pub-id-type="doi">10.1038/s41524-023-01048-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1023/303</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/166/987</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/299/2736</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Fast Bayesian optimization of Needle-in-a-Haystack problems using zooming memory-based initialization (ZoMBI)</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8841-7887</contrib-id><name><surname>Siemenn</surname><given-names>Alexander E.</given-names></name><address><email>asiemenn@mit.edu</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs4152402301048x_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-6987-1715</contrib-id><name><surname>Ren</surname><given-names>Zekun</given-names></name><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>Li</surname><given-names>Qianxiao</given-names></name><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Buonassisi</surname><given-names>Tonio</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.116068.8</institution-id><institution-id institution-id-type="ISNI">0000 0001 2341 2786</institution-id><institution content-type="org-division">Department of Mechanical Engineering</institution><institution content-type="org-name">Massachusetts Institute of Technology</institution></institution-wrap><addr-line content-type="city">Cambridge</addr-line><addr-line content-type="state">MA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.429485.6</institution-id><institution-id institution-id-type="ISNI">0000 0004 0442 4521</institution-id><institution content-type="org-division">Department of Electrical and Computer Engineering</institution><institution content-type="org-name">Singapore-MIT Alliance for Research and Technology</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution content-type="org-name">Xinterra</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution content-type="org-division">Department of Mathematics</institution><institution content-type="org-name">National University of Singapore</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.4280.e</institution-id><institution-id institution-id-type="ISNI">0000 0001 2180 6431</institution-id><institution content-type="org-division">Institute for Functional Intelligent Materials</institution><institution content-type="org-name">National University of Singapore</institution></institution-wrap><addr-line content-type="city">Singapore</addr-line><country country="SG">Singapore</country></aff></contrib-group><author-notes><corresp id="IDs4152402301048x_cor1"><label>a</label><email>asiemenn@mit.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>26</day><month>5</month><year>2023</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2023</year></pub-date><volume>9</volume><issue seq="79">1</issue><elocation-id>79</elocation-id><history><date date-type="registration"><day>15</day><month>5</month><year>2023</year></date><date date-type="received"><day>26</day><month>8</month><year>2022</year></date><date date-type="accepted"><day>12</day><month>5</month><year>2023</year></date><date date-type="online"><day>26</day><month>5</month><year>2023</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2023</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Needle-in-a-Haystack problems exist across a wide range of applications including rare disease prediction, ecological resource management, fraud detection, and material property optimization. A Needle-in-a-Haystack problem arises when there is an extreme imbalance of optimum conditions relative to the size of the dataset. However, current state-of-the-art optimization algorithms are not designed with the capabilities to find solutions to these challenging multidimensional Needle-in-a-Haystack problems, resulting in slow convergence or pigeonholing into a local minimum. In this paper, we present a Zooming Memory-Based Initialization algorithm, entitled ZoMBI, that builds on conventional Bayesian optimization principles to quickly and efficiently optimize Needle-in-a-Haystack problems in both less time and fewer experiments. The ZoMBI algorithm demonstrates compute time speed-ups of 400× compared to traditional Bayesian optimization as well as efficiently discovering optima in under 100 experiments that are up to 3× more highly optimized than those discovered by similar methods.</p></abstract><funding-group><award-group><funding-source><institution-wrap><institution>U.S. Department of Energy (DOE)</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">https://doi.org/10.13039/100000015</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">DE-EE0009366</award-id><principal-award-recipient><name><surname>Siemenn</surname><given-names>Alexander E.</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>79</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Springer Nature Limited</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>Yes</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>5</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>15</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2023_Article_1048.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Current optimization algorithms achieve good results on low-dimensional problems that are smooth and have wide basins of attraction. Examples of smooth manifolds with wide basins of attraction within material science include process- and recipe-optimization problems such as tuning perovskite manufacturing variables to achieve higher efficiency<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, optimizing microfluidics flow parameters to achieve ideal droplet formation<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>, optimizing silver nanoparticle recipes for optical properties<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>, and tuning perovskite compositions with physics-based constraints to maximize stability<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Optimization techniques like Bayesian optimization (BO) are well-suited to model these simple manifolds using a Gaussian Process (GP) surrogate<sup><xref ref-type="bibr" rid="CR5">5</xref>–<xref ref-type="bibr" rid="CR9">9</xref></sup>. However, the performance of this BO with a GP breaks down as the manifold complexity increases. Material property optimization problems that have high technological significance, such as discovering materials with rare properties or materials with a specific combination of properties, have search space manifolds that more closely resemble a <italic>Needle-in-a-Haystack</italic><sup><xref ref-type="bibr" rid="CR10">10</xref></sup>, shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, rather than a smooth or convex space.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Archetypal Manifolds in Materials Science Optimization.</title><p><bold>a</bold> In process optimization, there often exists a real and continuous path between each condition. This 3D projected manifold is from a perovskite process optimization process, where <italic>X</italic><sub>1</sub> is spray flow rate, <italic>X</italic><sub>2</sub> is plasma voltage, and <italic>f</italic>(<italic>X</italic>) is cell efficiency<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. <bold>b</bold> However, in materials optimization, there are often only discrete combinations of properties that define real materials, resulting in a rough topology with extreme outliers. For example, Li<sub>2</sub>NbF<sub>6</sub> and Li<sub>2</sub>ZrF<sub>6</sub> lay close to each other in space because they have similar density, formation energy, and structure, however, they have vastly different target properties: Li<sub>2</sub>NbF<sub>6</sub> has a Poisson’s ratio of − 1.7 while Li<sub>2</sub>ZrF<sub>6</sub> has a Poisson’s ratio of 0.3<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. Extreme outliers, such as Li<sub>2</sub>NbF<sub>6</sub>, consist of only a small fraction of the manifold hypervolume, resulting in a Needle-in-a-Haystack regime arising. This 3D projected manifold is obtained from the 6D Poisson’s ratio optimization problem presented in this paper, where <italic>X</italic><sub>1</sub> is density, <italic>X</italic><sub>2</sub> is formation energy, and <italic>f</italic>(<italic>X</italic>) is negative Poisson’s ratio<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig1_HTML.png"/></fig></p><p id="Par3">This Needle-in-a-Haystack (NiaH) problem arises when only few optimum conditions exist within the entire dataset, resulting in an extreme imbalance. Interpolating the parameter space of an imbalanced dataset with an estimation function, such as a GP, results in smoothing over the optimum or over-predicting the properties of the materials found near the optimum<sup><xref ref-type="bibr" rid="CR11">11</xref>–<xref ref-type="bibr" rid="CR13">13</xref></sup>. Examples of NiaH materials optimization problems include discovering auxetic materials (i.e., materials that have a highly negative Poisson’s ratio, <italic>ν</italic>) for energy absorptive medical devices or protective armor<sup><xref ref-type="bibr" rid="CR14">14</xref>–<xref ref-type="bibr" rid="CR16">16</xref></sup> and discovering materials that have a combination of high electrical conductivity and low thermal conductivity (i.e., a highly positive thermoelectric figure of merit, ZT) used for improving sensor technology to enable ubiquitous solid-state cooling<sup><xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>. Optimization of these rare material properties illustrates examples where an extreme data balance exists in the dataset because only a fraction of the total number of materials exhibit these rare properties<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR20">20</xref>–<xref ref-type="bibr" rid="CR23">23</xref></sup>. This NiaH optimization challenge of extremely imbalanced datasets is largely applicable to many fields, not just materials science, including the fields of ecological resource management<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, fraud detection<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>, and rare diseases<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par4">Several challenges exist for the current landscape of computational tools that inhibit effective optimization of these complex NiaH problems. Firstly, the “needle" makes up only a small percentage of the total manifold search space, resulting in a weak correlation between the measured input parameters and the target property of interest, inhibiting discovery of the region containing the needle<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. This challenge requires the development of an algorithm that can more quickly determine the plausible region of the manifold where the needle exists. The second challenge for algorithms, such as BO, to optimize NiaH manifolds is in the nature of the acquisition function to pigeonhole sampling into local minima because of the narrowness of the needle’s basin of attraction<sup><xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. Standard BO acquisition functions, including expected improvement (EI)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> and lower confidence bound (LCB)<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>, are static sampling techniques that only adjust sampling based on the output of the surrogate model, which enacts smoothing of the needle<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. To overcome this challenge, active learning-based tuning of the acquisition function hyperparameters can be implemented to improve the sampling quality and avoid pigeonholing. Lastly, there exists a computing challenge for NiaH problems where, typically, several thousands of samples must be observed to find an optimum when using an algorithm that is poorly suited to tackle NiaH manifolds<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. The compute time of BO using a GP surrogate scales with the complexity <italic>O</italic>(<italic>n</italic><sup>3</sup>), where <italic>n</italic> is the number of experiments sampled, hence, the compute time of traditional BO blows up as more data is required to find the optimum<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup>. To solve this computing challenge, an algorithm must be designed that both efficiently optimizes the space in as few experiments as possible and reduces the effect of compounding compute times over the length of the optimization procedure.</p><p id="Par5">In recent literature, algorithms have been developed to address some of these challenges individually, but not all of them together. The first class of solutions bound the search space using a trust region approach to sample regions with higher probability of containing the optimum. Eriksson et al. develop TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> that compiles a set of independent model runs, using separate GP surrogate models to compute a new, smaller search region, narrowed in on the target optimum. Regis develops TRIKE<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> that utilizes maximization of the EI acquisition function to bound a trust region containing the global optimum. Diouane et al. develop TREGO<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>, which interleaves sampling between global and local search regions, where the local search regions are defined by the single best historical experiment sampled. Although these methods offer solutions to one of the three challenges presented, each method has its downfalls when optimizing NiaH problems. For example, TuRBO requires the computation of several GP model runs, which increases compute time and also does not guarantee that the needle will be resolved due to interpolation effects; TRIKE is inflexible to the use of other acquisition functions as it locks the user in to only using EI, which may pigeonhole into local minima; TREGO uses only the best sampled experiment to define its search regions, which will yield inconsistent or sub-optimal results when the needle consists of a fractional region of the manifold and single point is unlikely to land in its basin of attraction. The second class of solutions to the challenges presented in this paper are designed to decrease the computing time required to run an optimization procedure. A common method for reducing the compute time of BO with a GP surrogate is to introduce a sparse GP<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR42">42</xref></sup>. A sparse GP uses a small subset of pseudo data, often denoted as <italic>m</italic>, to reduce the GP time complexity from <italic>O</italic>(<italic>n</italic><sup>3</sup>) to <italic>O</italic>(<italic>n</italic><italic>m</italic><sup>2</sup>)<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. However, the process of selecting a useful subset requires minimizing the Kullback-Leibler divergence between the sparse GP and true posterior GP, which is often a computationally intensive procedure of using variational inference<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. In addition to sparse GPs, algorithms have been developed in literature to improve the compute time of optimization in various ways. Van Stein et al. develop MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, which parallelizes the function evaluations of efficient global optimization (EGO) to discover optima faster and in fewer experiments using derivative-free computation<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. Joy et al.<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> use directional derivatives to accelerate hyperparameter tuning by 100× and achieve higher accuracy than the FABOLAS baseline by Klein et al.<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. Zhang et al. develop FLASH<sup><xref ref-type="bibr" rid="CR49">49</xref></sup> to achieve optimization speed-ups of 50% by using a linear parametric model to guide algorithm search within high-dimensional spaces. Snoek et al.<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> design a neural network-based parametric model that reduces the overall time complexity of BO to <italic>O</italic>(<italic>n</italic>) compared to the complexity of <italic>O</italic>(<italic>n</italic><sup>3</sup>) of standard BO with a GP surrogate model. These existing methods from literature within the class of solutions for accelerating compute time are generally introducing external models necessary to perform optimization, such as neural networks, variational inference, or parametric models. While these external models do speed up compute time, they often lack the predictive capabilities to capture the weak correlation between measured input parameters and the target property of interest in NiaH problems. We illustrate this mechanism later in the paper when comparing the optimization results on two materials science NiaH problems of a fast algorithm MiP-EGO with that of TuRBO, an algorithm better suited for discovering optima within narrow basins of attraction.</p><p id="Par6">Although these methods from existing literature address some of the challenges in optimizing NiaH problems, none of them have been designed specifically to quickly and efficiently discover a needle-like optimum within a haystack of sub-optimal points, resulting in all of them falling short of a full solution. Therefore, in this paper, we design an algorithm that addresses all three of the challenges faced when optimizing NiaH problems by (1) zooming in the manifold search bounds iteratively and independently for each dimension based on <italic>m</italic> number of best memory points to quickly converge to the plausible region containing the global optimum needle, (2) relieving compute utilization by pruning the low-performing and redundant memory points not being used to zoom in the search bounds, (3) anti-pigeonholing into local minima by using actively learned acquisition function hyperparameters to tune the exploitation-to-exploration ratio. The proposed algorithm, entitled [Zo]oming [M]emory-[B]ased [I]nitialization (ZoMBI), combines these three contributions into a method that efficiently optimizes NiaH problems quickly. Figure <xref rid="Fig2" ref-type="fig">2</xref> demonstrates the accelerated convergence ability of the proposed (ZoMBI) algorithm compared to standard BO. In essence, this process of scanning broadly and then focusing in on points of interest based on memory was inspired by the way we humans solve similar problems, but stands in contrast to the way standard BO methods with static acquisition functions solve problems. We demonstrate the performance of this algorithm on three vastly different NiaH problems in materials science and ecological resource management: (1) discovery of materials with negative Poisson’s ratio, (2) discovery of materials with both high electrical conductivity and low thermal conductivity, and (3) detection of environmental conditions conducive of sustaining wildfires. The performance of the proposed ZoMBI algorithm is compared against standard BO with static acquisition functions as well as against three more algorithms: (1) HEBO, the winning submission of the NeurIPS 2020 Black-Box optimization challenge<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> and one algorithm from each of the two classes of partial NiaH solutions (2) TuRBO (bounded search space)<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and (3) MiP-EGO (faster compute)<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Finally, we stress-test the proposed ZoMBI algorithm across 174 additional datasets varying the optimum needle width, optimum distance to edges, dimensionality, and initialization conditions.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Accelerated Convergence to True Target using ZoMBI.</title><p>Using a standard Bayesian optimization procedure, the discovery of a Needle-in-a-Haystack condition does not progress significantly after 10 additional experiments from the initial GP guess. However, using ZoMBI to zoom the bounds inward and prune redundant memory points, the needle-like optimum region is resolved to be accurately aligned with the true target. <bold>a</bold> The true target to optimize, which is a slice from the 6D Poisson’s Ratio dataset. <bold>b</bold> The initial guess of the target function using a GP surrogate with 20 randomly sampled experiments. <bold>c</bold> (top) The estimated target resolved by standard BO after 10 additional experiments sampled using a greedy LCB acquisition function (<italic>β</italic> = 0.1); (bottom) the estimated target resolved by ZoMBI after 10 additional experiments sampled using the same greedy LCB acquisition function. The red memory points do not assist in resolving this target after zooming in the bounds, hence, they are pruned from memory by ZoMBI.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig2_HTML.png"/></fig></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><p id="Par7">Zooming in the search bounds on the manifold addresses challenge number one of optimizing NiaH problems, which is the challenge of finding the general hypervolume region that contains the needle-like optimum. Figure <xref rid="Fig3" ref-type="fig">3</xref> illustrates how the ZoMBI algorithm iteratively zooms in the search bounds based on the number of activations, <italic>α</italic>. An Ackley function is used as a simulated example due to its non-convexity and needle-like global optimum<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>. For each activation, <italic>m</italic> prior points that achieved the lowest target values, <italic>y</italic>, are retained in memory and used to zoom the search bounds in. This zooming occurs independently across each dimension and is based on the minimum and maximum values of the <italic>m</italic> memory points along each dimension, as shown in Equation (<xref rid="Equ2" ref-type="disp-formula">2</xref>). The red and orange rectangles illustrate the evolution of the bounds over space and time. Initially, sampling occurs across the entire manifold for <italic>ϕ</italic> forward experiments per activation, shown by the black markers. However, by using the best-performing memory points to zoom in the search bounds, pigeonholing into local minima can also be avoided as the search bounds are pulled away from these trap minima and move closer towards the global minimum basin of attraction. The iterative zooming of ZoMBI does not guarantee convergence on the global optimum, but if a sufficient initialization set is obtained, convergence often gets close to the global optimum as shown across several examples in Fig. <xref rid="Fig5" ref-type="fig">5</xref> and Figs. <xref rid="Fig8" ref-type="fig">8</xref>, <xref rid="Fig9" ref-type="fig">9</xref>, <xref rid="Fig10" ref-type="fig">10</xref>. Furthermore, we comprehensively demonstrate the performance limitations of ZoMBI where initializations miss extreme needle-like optima in Fig. <xref rid="Fig6" ref-type="fig">6</xref> and where optima are near the edges of a manifold in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">4</xref>.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Zooming Search Bounds.</title><p>For every activation of ZoMBI, the search bounds are zoomed inward based on the prior best-performing memory points. A 4D Ackley function manifold is projected in 2D. The bounding regions of each 2D slice are illustrated by the red and orange boxes. The <italic>ϕ</italic> number forward experiments sampled for each activation, <italic>α</italic>, are illustrated as black markers. The global optimum is indicated by the red region of the heatmap.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig3_HTML.png"/></fig></p><p id="Par8">As more experiments are amassed and committed to memory to run traditional BO by computing the GP surrogate, the compute time increases polynomially, following the <italic>O</italic>(<italic>n</italic><sup>3</sup>) time complexity of GP matrix inversion<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR34">34</xref>,<xref ref-type="bibr" rid="CR37">37</xref>,<xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR53">53</xref></sup>. This complexity is unfavorable as it leads to compounding compute times as more experiments are run. Therefore, we implement a memory pruning feature into the ZoMBI algorithm that iteratively selects which prior data points to keep and which to prune from the memory during each activation, <italic>α</italic>. Memory pruning is demonstrated to remove redundant features during the optimization procedure. Figure <xref rid="Fig2" ref-type="fig">2</xref> illustrates how ZoMBI accelerates the convergence of a GP prediction to the precise location of the true. However, only data within the newly computed bounds of ZoMBI are used prediction of the true target, hence, all data outside this boundary becomes redundant and is pruned to decrease compute time.</p><p id="Par9">Through memory pruning, the number of experiments used to train the GP surrogate varies between [<italic>i</italic>, <italic>i</italic> + <italic>ϕ</italic>] for every <italic>α</italic>, rather than being proportional to <italic>n</italic>, where the number of initialization samples is fixed at <italic>i</italic> = 5. In this paper, we use <italic>ϕ</italic> ∈ [0, 10], i.e., once <italic>ϕ</italic> = 10, the activation is complete and resets to <italic>ϕ</italic> = 0. This is computationally favorable because {<italic>X</italic><sub><italic>i</italic></sub>} ∪ {<italic>X</italic><sub><italic>ϕ</italic></sub>} ⊆ {<italic>X</italic><sub><italic>n</italic></sub>}. Thus, for a single <italic>α</italic>, the time complexity is <italic>O</italic>((<italic>i</italic>+<italic>ϕ</italic>)<sup>3</sup>) ≈ <italic>O</italic>(<italic>ϕ</italic><sup>3</sup>), since <italic>i</italic> is fixed. Furthermore, since the range of <italic>ϕ</italic> is capped, a non-increasing sawtooth pattern in compute time is exhibited, illustrated in Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Therefore, the compute complexity of ZoMBI trends towards <italic>O</italic>(1) for <italic>α</italic> &gt; 1 as a result of the efficient memory pruning process. After collecting 1000 experiments, the compute time of traditional BO trends towards &gt; 400 seconds per experiment, whereas for ZoMBI the compute time maintains a constant trend of approximately 1 second per experiment. Therefore, the memory pruning feature of ZoMBI accelerates the optimization compute time by over 400× at <italic>n</italic> = 1000 and achieves further relative acceleration as <italic>n</italic> increases.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Wall-clock Compute Time.</title><p>The compute time per experiment is illustrated for traditional BO with a GP surrogate (orange) and for ZoMBI with a GP surrogate (blue) with the <italic>y</italic>-axis in log-scale. Four independent trials of each method were run to optimize a 6D Ackley function with a narrow basin of attraction using an NVIDIA Tesla Volta V100 GPU<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>. Each trial of standard BO and ZoMBI is run using one of the four acquisition functions: LCB, LCB Adaptive, EI, and EI Abrupt. The averages of the trials are shown as solid orange and blue lines while the shaded regions indicate the maximum and minimum compute times bounds. The red dashed line indicates the trend of the ZoMBI compute times. The measured compute time includes the time to compute the GP surrogate model and the time to acquire an experiment from the surrogate.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig4_HTML.png"/></fig></p><p id="Par10">Pigeonholing into the local minima of a function occurs when an optimization algorithm has insufficient learned knowledge of the manifold topology to continue exploring potentially profitable regions or when the algorithm’s hyperparameters are improperly tuned, leading to overly exploitative tendencies<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. The ZoMBI algorithm’s anti-pigeonholing capabilities are two-fold: (1) the zooming search bounds help the acquisition function to quickly stop sampling local minima once a better performing data point is found and (2) actively learned acquisition function hyperparameters use knowledge about the domain to help exit a local minimum. Figure <xref rid="Fig5" ref-type="fig">5</xref> demonstrates the anti-pigeonholing capabilities of ZoMBI on optimizing a 6D Ackley function with both static and dynamic acquisition functions, compared to that of traditional BO.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>Acquisition Function Sampling Density.</title><p>The colored heatmaps indicate the regions of a 2D slice from a 6D Ackley function where sampling density is high for each respective acquisition function: <bold>a</bold> LCB, <bold>b</bold> LCB Adaptive, <bold>c</bold> EI, and <bold>d</bold> EI Abrupt. The contour lines indicate the manifold topology with local minima as the circular and pointed regions of the contours. The red “x" indicates the global minimum. For each acquisition function, the left panel shows the sampling density after <italic>n</italic> = {20, 40, 80} evaluated experiments without the use of ZoMBI while the right panel shows the sampling density after <italic>n</italic> = {20, 40, 80} evaluated experiments with the use of ZoMBI.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig5_HTML.png"/></fig></p><p id="Par11">The needle-like global minimum is indicated by the red “x" and the local minima are indicated by the circular and pointed regions of the contour lines. The sampling density of each acquisition function is illustrated by the heatmap, where the darker colors indicate higher sampling density regions. The goal is to get high sampling density near the red “x". It is shown that without ZoMBI being activated, the LCB, LCB Adaptive, and EI acquisition functions all end up pigeonholing into local minima. However, EI Abrupt initially pigeonholes into a local minima but then switches from an exploitative to an explorative mode to jump out of the local minimum and converge closer to the global. Conversely, when running the optimization procedure with ZoMBI active, all of the acquisition functions except the most exploitative, EI, converge onto the global minimum fast. LCB Adaptive and EI are shown to initially start sampling towards a local minima, but as ZoMBI is iteratively activated, the search bounds zoom in closer to the global minimum. Thus, with the combination of dynamic acquisition functions and zooming search bounds, pigeonholing into sub-optimal local minima can be more readily avoided while optimizing NiaH problems, although avoidance is not guaranteed, as shown by the sampling density of EI. The combination of the three foundational features of ZoMBI, (1) zooming bounds, (2) memory pruning, and (3) anti-pigeonholing drives fast optimization of NiaH problems and in most cases, does not sacrifice the ability to converge on the global optimum.</p><p id="Par12">Before assessing the performance of ZoMBI on the three real-world datasets, we use 144 permutations of the Ackley function to stress-test the capability of ZoMBI to discover the global optimum basin of attraction, given two varying dataset hyperparameters: (1) basin of attraction width and (2) dimensionality. The basin of attraction hypervolume is determined by both the width of the basin and the dimensionality of the manifold, hence, as the basin becomes narrower in width and as the dimensionality increases, the percentage of hypervolume space taken up by the basin decreases, <italic>i.e</italic>. the optimum becomes more needle-like. The Ackley permutations have varying basin hypervolumes from 0.001% to 100% and varying manifold dimensionalities from 2D to 10D. For this experiment, we aim to determine types of manifold topologies that ZoMBI best optimizes while quantifying those limits with the Pareto front.</p><p id="Par13">Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the results of this large-scale optimization experiment of 48 independent trials of ZoMBI across each of the 144 unique permutations of the Ackley function dataset with varying optimum hypervolumes and dimensionality. All points below the grey-shaded region fall within the optimum basin of attraction. The red trace of the Pareto front indicates the narrowest optimum hypervolume and dimensionality conditions of a dataset that result in the best minimum function value being discovered. We show that with an initialization set of <italic>i</italic> = 5, ZoMBI can reliably discover the global minimum region for needles as narrow as 0.05% of total hypervolume space. Moreover, as the optimum becomes narrower than 0.05% of the total hypervolume, the initialization set is no longer sufficient and ZoMBI gets trapped in local minima, as indicated by the greyed-out region. Conversely, as the optimum becomes wider than 5% of the total hypervolume, the manifold becomes flatter, expressing the greedy nature of ZoMBI to falsely zoom inward to less ideal function values than it would for narrower optimum conditions. This experiment quantifies the range of ZoMBI’s Goldilocks zone to be between 0.05% and 5% optimum hypervolume. Therefore, for ideal performance, ZoMBI is best used on datasets with optimum conditions consisting of between 0.05% and 5% of the total number of conditions. This optimum hypervolume trade-off of ZoMBI is further assessed relative to other optimization methods in Supplementary Figure <xref ref-type="supplementary-material" rid="MOESM1">3</xref>.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><title>Varying Optimum Hypervolume.</title><p>(left) Depiction of decreasing optimum basin of attraction hypervolume in 1D. (right) The Pareto-optimal dataset hyperparameters for usage with the ZoMBI algorithm over 144 analytical datasets with 48 independent trials each: 12 trials for each of the four acquisition functions, LCB, LCB Adaptive, EI, and EI Abrupt, for a total of 6912 independent trials. Each analytical dataset is a permutation of the Ackley function with a different optimum basin of attraction width and manifold dimensionality. Hypervolume percent makeup is synthetically decreased both by decreasing the basin of attraction width and by increasing the manifold dimensionality. Each scatter point represents the median final minimum function evaluation after 1000 experiments across the 48 independent trials initialized with a fixed set of <italic>i</italic> = 5 samples. The colorbar of the scatter point represents the dimensionality of the manifold tested and the error bars represent the variance across the 48 trials. The possible function values for every dataset vary between [0, 25], hence, for the Ackley function as further detailed in the Supplementary Information, trials achieving minimum function values &lt; 10 are considered to have found the optimum basin of attraction while trials with function values ≥10 after 1000 experiments are considered to be trapped in local minima. Both the <italic>x</italic>- and <italic>y</italic>-axes are in log-scale.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig6_HTML.png"/></fig></p><p id="Par14">Three real-world datasets are optimized using ZoMBI—each of these datasets has an extreme data imbalance, illustrated in Figure <xref rid="Fig7" ref-type="fig">7</xref> within the specified ideal ranges of ZoMBI performance. The 6D Poisson’s Ratio dataset has an imbalance of 0.82% optimum conditions, the 6D Thermoelectric Figure of Merit dataset has an imbalance of roughly 1.32% optimum conditions, and the 11D wildfire detection dataset has an imbalance of 4.16% optimum conditions. This range of ideal performance of ZoMBI between 0.05% and 5% optimum hypervolume is facilitated by the initialization set. Hence, to improve performance for narrower optima, either the number of initialization samples must be increased, or initialization conditions should be adjusted. Additional initialization conditions experiments of ZoMBI are shown in Supplementary Information.<fig id="Fig7"><label>Fig. 7</label><caption xml:lang="en"><title>Data Distributions of Real-world Needle-in-a-Haystack Datasets.</title><p>(top) The histogram distributions of the full real-world datasets with callouts for optimum conditions: <bold>a</bold> Poisson’s Ratio with 146k materials in the dataset and <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>ν</mml:mi></mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1.7</mml:mn><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mn>1.2</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\nu }_{\min }=\{-1.7,-1.2\}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq1.gif"/></alternatives></inline-formula>, <bold>b</bold> Thermoelectric Figure of Merit with 1k materials in the dataset computed by BoltzTraP<sup><xref ref-type="bibr" rid="CR57">57</xref></sup> and ZT<inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mrow><mml:msub><mml:mrow/><mml:mrow><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>,</mml:mo><mml:mn>1.9</mml:mn></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${}_{\max }=\{1.4,1.9\}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq2.gif"/></alternatives></inline-formula>, <bold>c</bold> Wildfire Detection with 128k meteorological conditions collected over 33 months from January 2018 to September 2020 from CIMIS<sup><xref ref-type="bibr" rid="CR62">62</xref></sup> and <italic>ψ</italic> &lt; 0 conditions indicating those with a high likelihood of wildfire outbreaks. (bottom) The noisy, non-convex manifold topologies of each dataset generated by a random forest regression with 500 trees. Each manifold is a projected 3D slice of higher dimensional space with the <italic>z</italic>-axis and colorbar indicating the target property, where <bold>a</bold><italic>X</italic><sub>1</sub> is density and <italic>X</italic><sub>2</sub> is formation energy, <bold>b</bold><italic>X</italic><sub>1</sub> is formation energy and <italic>X</italic><sub>2</sub> is band gap, <bold>c</bold><italic>X</italic><sub>1</sub> is evapotranspiration and <italic>X</italic><sub>2</sub> is precipitation.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig7_HTML.png"/></fig></p><p id="Par15">The first experimental dataset is 6-dimensional and consists of 146k materials from the publicly available Materials Project database with different mechanical properties, described by Poisson’s Ratio, <italic>ν</italic><sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Only 0.82% of the total 146k materials have a negative Poisson’s Ratio, <italic>ν</italic> &lt; 0<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Hence, for this experiment, we aim to minimize <italic>ν</italic>. A positive <italic>ν</italic> &gt; 0, describes a material that expands when a compressive load is applied to the orthogonal direction<sup><xref ref-type="bibr" rid="CR54">54</xref>,<xref ref-type="bibr" rid="CR55">55</xref></sup>. Conversely, a negative <italic>ν</italic> &lt; 0 describes a material that contracts rather than expands when compressed in the orthogonal direction, denoted as an auxetic material—a rare phenomenon<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. Auxetic materials with highly negative Poisson’s ratios have energy absorptive properties that are ideal materials for wearable medical devices and protective armor that must absorb the energy of large impacts to keep bones from shifting or to inhibit the penetration of the protective layer<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref></sup>.</p><p id="Par16">Figure <xref rid="Fig8" ref-type="fig">8</xref> demonstrates the optimization performance of ZoMBI on the Poisson’s Ratio dataset compared to MiP-EGO, TuRBO, and HEBO. The ZoMBI algorithm is run with each of the four acquisition functions: LCB, LCB Adaptive, EI, and EI Abrupt. In under 100 evaluated experiments, LCB and LCB Adaptive discover the global minimum NiaH material, Li<sub>2</sub>NbF<sub>6</sub> (<italic>ν</italic> ≈ − 1.7). The variance of <italic>ν</italic> values for the final experiment across all ensemble runs is illustrated as a KDE plot for each method to highlight the sampling density and general rate of success. HEBO discovers the global minimum after ZoMBI with LCB and LCB Adaptive, however, the spread of runs for ZoMBI is narrower than that of HEBO, which indicates that for this problem, ZoMBI can more consistently discover the minimum, that is 3× lower than those discovered by MiP-EGO and TuRBO. Furthermore, the rate of convergence on Needle 1 is faster for ZoMBI than HEBO.<fig id="Fig8"><label>Fig. 8</label><caption xml:lang="en"><title>Discovery of Rare Negative Poisson’s Ratio Materials.</title><p>The optimization objective is to find the material with the minimum Poisson’s ratio in 100 experiments from the dataset presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>a. The green, blue, red, and orange lines indicate the median best running evaluated sample of ZoMBI using the LCB, LCB Adaptive, EI, and EI Abrupt acquisition functions, respectively. The pink, black, and teal lines indicate the median best running evaluated sample of the methods MiP-EGO, TuRBO, and HEBO respectively. Random sampling is illustrated as a dashed grey line for benchmarking. The median for each method is taken over the best 12 independent model runs. The shaded regions indicate the variance between model runs. The cross-hatched region indicates the space discovered by standard BO methods, without the use of ZoMBI, which use the same hyperparameters. The distribution across all 12 model runs of the final sampled experiment for each method is shown as a kernel density estimation (KDE) along the <italic>y</italic>-axis. The y-values for the needle-like optima are indicated by dashed black lines.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig8_HTML.png"/></fig></p><p id="Par17">Figure <xref rid="Fig7" ref-type="fig">7</xref>a illustrates the distribution of <italic>ν</italic> values within the full dataset. The ground truth “needle" materials with the lowest <italic>ν</italic> values are Li<sub>2</sub>NbF<sub>6</sub> with <italic>ν</italic> ≈ −1.7 and Na<sub>2</sub>CO<sub>3</sub> with <italic>ν</italic> ≈ −1.2. ZoMBI with the LCB and LCB Adaptive acquisition functions and HEBO discover Li<sub>2</sub>NbF<sub>6</sub>, while ZoMBI with the EI Abrupt acquisition function discovers Na<sub>2</sub>CO<sub>3</sub>.</p><p id="Par18">The second experimental dataset is 6-dimensional and consists of 1k materials with different thermal and electrical properties, described by the Thermoelectric Figure of Merit, ZT. Since ZT values are always positive, there is no clear cutoff for what “optimum" conditions are, but with a threshold of ZT &gt; 0.8, 1.32% of the total 1k materials are considered optimum. A higher ZT indicates that the material is better able to convert a thermal gradient into an electrical current<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Hence for this experiment, we aim to maximize ZT. Unlike Poisson’s Ratio, Thermoelectric Merit is determined by a combination of several variables, rather than a single variable<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math id="Equ1_Math"><mml:mrow><mml:mi mathvariant="normal">ZT</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>κ</mml:mi></mml:mrow></mml:mfrac><mml:mi>T</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{\rm{ZT}}}}=\frac{{S}^{2}\sigma }{\kappa }T,$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_Equ1.gif"/></alternatives></disp-formula>where <italic>S</italic> is the Seebeck coefficient, <italic>σ</italic> is electrical conductivity, <italic>T</italic> is the average temperature, and <italic>κ</italic> is thermal conductivity. The ZT is computed for each material with valid thermal and electrical properties in the Materials Project database using BoltzTraP<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. ZT is a common figure of merit used to describe the thermal-to-electrical or electrical-to-thermal conversion efficiency of thermoelectric materials<sup><xref ref-type="bibr" rid="CR58">58</xref>–<xref ref-type="bibr" rid="CR61">61</xref></sup>. Materials with high ZT values have a range of applications from usage as solid-state cooling devices to being used as sensors that when heated up, will produce an electrical signal<sup><xref ref-type="bibr" rid="CR17">17</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p><p id="Par19">Figure <xref rid="Fig9" ref-type="fig">9</xref> demonstrates the optimization performance of ZoMBI on the Thermoelectric Figure of Merit dataset compared to MiP-EGO, TuRBO, and HEBO. In this experiment, although none of the tested methods are able to discover the maximum needle, LCB Adaptive discovers the second highest needle-in-a-haystack material, Na<sub>4</sub>Al<sub>3</sub>Ge<sub>3</sub>IO<sub>12</sub> (ZT ≈ 1.4) in under 100 experiments. Neither HEBO, TuRBO, nor MiP-EGO are capable of discovering any needle-like ZT optima and MiP-EGO performs worse than random sampling in this experiment. The wide variance across runs for ZoMBI and HEBO, shown in the KDE plots, indicate that both methods operate relatively explorative to discover maxima in this topology. Ultimately, this experiment demonstrates that ZoMBI can optimize material objective functions that have a complex combination of variables (Equation (<xref rid="Equ1" ref-type="disp-formula">1</xref>)) with roughly 2× better performance than HEBO.<fig id="Fig9"><label>Fig. 9</label><caption xml:lang="en"><title>Discovery of Rare Positive Thermoelectric Figure of Merit Materials.</title><p>The optimization objective is to find the material with the maximum Thermoelectric Figure of Merit in 100 experiments from the dataset presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>b. The green, blue, red, and orange lines indicate the median best running evaluated sample of ZoMBI using the LCB, LCB Adaptive, EI, and EI Abrupt acquisition functions, respectively. The pink, black, and teal lines indicate the median best running evaluated sample of the methods MiP-EGO, TuRBO, and HEBO respectively. Random sampling is illustrated as a dashed grey line for benchmarking. The median for each method is taken over the best 12 independent model runs. The shaded regions indicate the variance between model runs. The cross-hatched region indicates the space discovered by standard BO methods, without the use of ZoMBI, which use the same hyperparameters. The distribution across all 12 model runs of the final sampled experiment for each method is shown as a kernel density estimation (KDE) along the <italic>y</italic>-axis. The y-values for the needle-like optima are indicated by dashed black lines.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig9_HTML.png"/></fig></p><p id="Par20">Figure <xref rid="Fig7" ref-type="fig">7</xref>b illustrates the distribution of ZT values within the full dataset. The ground truth “needle" materials with the highest ZT values are Sr<sub>4</sub>Al<sub>6</sub>SO<sub>12</sub> with ZT ≈ 1.9 and Na<sub>4</sub>Al<sub>3</sub>Ge<sub>3</sub>IO<sub>12</sub> with ZT ≈ 1.4. ZoMBI with the LCB Adaptive acquisition function is the only method that discovers one of these needles, Na<sub>4</sub>Al<sub>3</sub>Ge<sub>3</sub>IO<sub>12</sub>.</p><p id="Par21">The third experimental dataset is 11-dimensional and consists of 128k meteorological conditions and an index, <italic>ψ</italic>, that determines whether the set of conditions has a high likelihood of generating or sustaining a wildfire in the state of California—publicly available from the California Irrigation Management Information System (CIMIS) weather stations<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>. Only 4.16% of the total 128k meteorological conditions have a negative wildfire detection index, <italic>ψ</italic> &lt; 0. A highly negative <italic>ψ</italic> indicates a high risk of wildfires. Hence, for this experiment, we aim to minimize <italic>ψ</italic>, to best detect meteorological conditions at high risk of wildfires. The dataset spans over two years of data collected from 2018 to 2020, during which over 2500 wildfires have occurred, burning over 24 million acres of land<sup><xref ref-type="bibr" rid="CR63">63</xref></sup>. In California, temperature and precipitation alone are poor indicators for wildfire outbreaks (see Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">1)</xref>, resulting in researchers using computer-vision methods or convolutions of many meteorological variables to reliably detect wildfire conditions instead<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>. Thus, there is a high need for algorithmic support to aid humans in early wildfire detection.</p><p id="Par22">Figure <xref rid="Fig10" ref-type="fig">10</xref> demonstrates the optimization performance of ZoMBI on the Wildfire Detection dataset compared to MiP-EGO, TuRBO, and HEBO. In this experiment, LCB Adaptive, EI, and HEBO discover the lowest index value, <italic>ψ</italic> ≈ − 3.5, for detecting wildfires based on a high-dimensional convolution of ten meteorological variables. TuRBO and MiP-EGO also discover a low index value, <italic>ψ</italic> ≈ − 2.5, however, these methods have widely distributed variances, as shown by the KDE plots, indicating inconsistent optimization results given only 100 sampled experiments. Similarly, HEBO has high variance across model runs while the LCB Adaptive and EI ZoMBI methods have a tight distribution, indicating more reliable optimization results with a higher rate of success. Furthermore, ZoMBI methods achieve a faster rate of convergence than HEBO onto the Needle 1 optimum, similar to the optimization results on the Poisson’s Ratio dataset.<fig id="Fig10"><label>Fig. 10</label><caption xml:lang="en"><title>Detection of Environmental Conditions with Wildfire Risk.</title><p>The optimization objective is to find the meteorological conditions with the minimum wildfire detection index, <italic>ψ</italic>, in 100 experiments from the dataset presented in Fig. <xref rid="Fig7" ref-type="fig">7</xref>c. Conditions with <italic>ψ</italic> &lt; 0 have the highest risk of sustaining wildfire. The green, blue, red, and orange lines indicate the median best running evaluated sample of ZoMBI using the LCB, LCB Adaptive, EI, and EI Abrupt acquisition functions, respectively. The pink, black, and teal lines indicate the median best running evaluated sample of the methods MiP-EGO, TuRBO, and HEBO respectively. Random sampling is illustrated as a dashed grey line for benchmarking. The median for each method is taken over the best 12 independent model runs. The shaded regions indicate the variance between model runs. The cross-hatched region indicates the space discovered by standard BO methods, without the use of ZoMBI, which use the same hyperparameters. The distribution across all 12 model runs of the final sampled experiment for each method is shown as a kernel density estimation (KDE) along the <italic>y</italic>-axis. The y-values for the needle-like optima are indicated by dashed black lines.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Fig10_HTML.png"/></fig></p><p id="Par23">Figure <xref rid="Fig7" ref-type="fig">7</xref>c illustrates the distribution of <italic>ψ</italic> values within the full dataset. The ground truth “needle" conditions for detecting wildfires are those with the most negative detection index values, <italic>ψ</italic>. Although ZoMBI with the LCB Adaptive and EI acquisition functions as well as HEBO discover the lowest needle-like <italic>ψ</italic> conditions after 100 sampled experiments, none of the tested methods are able to find the global <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow><mml:mrow><mml:mi>min</mml:mi></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\psi }_{\min }\approx -12$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq3.gif"/></alternatives></inline-formula>. These results imply that, even for ZoMBI, with a narrow enough needle-like optimum, an LHS initialization of <italic>i</italic> = 5 experiments, may not be sufficient. Supplementary Fig. <xref ref-type="supplementary-material" rid="MOESM1">4</xref> demonstrates that extending the bounds of LHS initialization is shown to improve the performance of ZoMBI on certain manifold topologies.</p></sec><sec id="Sec3" sec-type="discussion"><title>Discussion</title><p id="Par24">In this paper, we proposed the [Zo]oming [M]emory-[B]ased [I]nitialization (ZoMBI) algorithm that builds on the principles of Bayesian optimization to accelerate the optimization of Needle-in-a-Haystack problems by two-fold, firstly by requiring fewer experiments to achieve better optimum faster than existing MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, and HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> on a variety of real-world applications, and secondly by pruning the memory of low-performing historical experiments to speed-up compute time. The ZoMBI algorithm convergences onto narrow and sharp optima quickly in Needle-in-a-Haystack datasets by (1) using the values of the <italic>m</italic> best performing previously sampled memory points to iteratively zoom in the search bounds of the manifold uniquely on each dimension and (2) implementing two custom acquisition functions, LCB Adaptive and EI Abrupt, that adapt their hyperparameters to tune sampling of new experimental conditions based on learned information from the surrogate model. The main contributions of this algorithm solve three fundamental challenges of optimizing non-convex Needle-in-a-Haystack problems: (1) the challenge of locating the hypervolume region of the manifold containing the narrow global optimum basin of attraction<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup> is alleviated by introducing iterative search bounds based on learned knowledge of the manifold; (2) the challenge of polynomially increasing compute times of BO using a GP surrogate<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR34">34</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup> is addressed by actively pruning the retained memory of the algorithm after each activation, <italic>α</italic>, in turn, reducing the time complexity from <italic>O</italic>(<italic>n</italic><sup>3</sup>) to <italic>O</italic>(<italic>ϕ</italic><sup>3</sup>) for <italic>ϕ</italic> forward experiments per activation, <italic>α</italic>, which trends to a constant <italic>O</italic>(1) when <italic>α</italic> &gt; 1; (3) unwanted pigeonholing into local minima<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup> is avoided by both the zooming mechanics of ZoMBI as well as using the two acquisition functions developed in this paper, LCB Adaptive and EI Abrupt, that tune their hyperparameters through adaptive learning. By developing the ZoMBI algorithm to solve these challenges, it becomes possible to quickly and efficiently find optimal solutions to complex Needle-in-a-Haystack problems in fewer experiments.</p><p id="Par25">Solving a Needle-in-a-Haystack problem that arises from extremely imbalanced data is a significant challenge that has important implications in science and engineering, especially within the field of materials science<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. In this paper, we use ZoMBI to discover the optimum materials in two real-world materials science Needle-in-a-Haystack datasets where only a small fraction of the entire search space consists of the target optimum conditions. For breadth, we also extend our analysis to a third real-world dataset but for ecological resource management with the objective of discovering the environmental conditions that have a high likelihood of sustaining wildfires for early detection of wildfires. In the first materials dataset, we discover a material with a highly negative Poisson’s ratio, <italic>ν</italic>,<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>; in the second materials dataset, we discover a material with a highly positive thermoelectric figure of merit, ZT<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR57">57</xref></sup>, both rare material properties; and in the third dataset for ecological resource management, we discover a set of environmental conditions with a highly negative wildfire detection index, <italic>ψ</italic><sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR62">62</xref>,<xref ref-type="bibr" rid="CR63">63</xref></sup>. For the first dataset, both the ZoMBI algorithm with the LCB and LCB Adaptive custom acquisition functions and HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> discover the material with the minimum <italic>ν</italic> ≈ −1.7, however, the ZoMBI methods converge on this minimum in only 70 experiments while HEBO takes 90 experiments. TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> only discover materials with <italic>ν</italic> ≈ − 0.55 and <italic>ν</italic> ≈ − 0.20, respectively. For the second dataset, the ZoMBI algorithm with the LCB Adaptive custom acquisition function discovers the material with the maximum ZT ≈ 1.4, while HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>, TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, and MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> only discover ZT ≈ 0.78, ZT ≈ 0.65, and ZT ≈ 0.45, respectively. For the third dataset the ZoMBI algorithm with all acquisition functions and HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> discover a minimum <italic>ψ</italic> ≈ − 3, while TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> both only discover <italic>ψ</italic> ≈ − 2. However, the ZoMBI methods converge on the minimum faster and with less variance. In general, we note HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> outperforms the other benchmark methods, TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> and MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Thus, for future investigation, we believe the performance of ZoMBI may be further improved by running optimization within the latent space of a variational autoencoder, similar to HEBO<sup><xref ref-type="bibr" rid="CR64">64</xref>,<xref ref-type="bibr" rid="CR65">65</xref></sup>. Overall, these results demonstrate that the ZoMBI algorithm is more well-suited to tackle various real-world Needle-in-a-Haystack optimization problems than current methods, however, ZoMBI has performance limitations for extremely narrow optima when instantiated with an insufficient initialization set. Therefore to assess these limitations, we stress tested ZoMBI on an additional 174 analytical datasets with varying optimum needle widths, optimum distance to edges, dimensionality, and initialization conditions. These results concluded that with a fixed initialization set of 5 samples, ZoMBI has ideal performance on datasets with needle-like optima consisting of between 0.05% and 5% of total hypervolume space. Furthermore, by extending the range of the initialization set, ZoMBI is capable of discovering global minima that lay on the absolute edge of a manifold’s limits. Thus, in these certain cases, convergence to a global optimum using ZoMBI is not guaranteed, but with slight modifications based on some a priori domain knowledge of the optimization landscape, ZoMBI produces high-performance and low-variance results.</p><p id="Par26">Ultimately, the significance of developing the ZoMBI algorithm is to quickly and efficiently tackle difficult Needle-in-a-Haystack optimization problems in extremely imbalanced datasets. In this paper, we showcased the ability of the developed algorithm to discover rare materials and conditions with highly-optimized properties in a short period of time using few experiments. Discovering rare materials quickly and efficiently enables widespread access to a new range of materials applications from engineering high-performance medical devices to ubiquitous solid-state cooling systems<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR19">19</xref></sup>. However, the application space for ZoMBI to accelerate the efficient discovery of highly-optimized solutions extends past materials science and is generally applicable for many Needle-in-a-Haystack problems, including those found in ecological resource management<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>, fraud detection<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref></sup>, and rare disease prediction<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR28">28</xref></sup>. We aim for this contribution to support the elimination of the time and resource barriers previously inhibiting the throughput of optimizing complex and challenging Needle-in-a-Haystack problems across a broad range of application spaces.</p></sec><sec id="Sec4" sec-type="methods"><title>Methods</title><sec><p id="Par27">In this paper, we develop two major contributions: (1) the ZoMBI algorithm and (2) adaptive learning acquisition functions. Through the combination of these two contributions, the optimum region of a NiaH manifold can be quickly discovered in fewer experiments without pigeonholing into local minima. Thus, the three challenges of optimizing NiaH problems are addressed: (1) the challenge of finding a hypervolume within the manifold that contains the needle-like optimum<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, (2) the challenge of the polynomially increasing compute times of BO using a GP surrogate<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR35">35</xref>–<xref ref-type="bibr" rid="CR38">38</xref></sup>, (3) the challenge of avoiding pigeonholing into local minima<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR9">9</xref>,<xref ref-type="bibr" rid="CR31">31</xref>,<xref ref-type="bibr" rid="CR32">32</xref></sup>. We demonstrate the implementation of ZoMBI on a 6D analytical Ackley function, a 6D dataset of materials with Poisson’s ratios, a 6D dataset of thermoelectric materials, and an 11D dataset for wildfire detection, all of which exhibit an extreme data imbalance and a NiaH regime, and compare the performance to that of MiP-EGO<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>, TuRBO<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>, and HEBO<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. For each of the three problems, the objective is to find the target value, <italic>y</italic>, with either the lowest or highest value depending on if the problem is minimization or maximization. This optimum <italic>y</italic>-value resembles a needle for each problem because it is located within a narrow and steep basin of attraction. Precisely, the needle optimum for each problem has a value of <italic>y</italic> = 0 for the Ackley function (minimization), <italic>y</italic> = −1.7 for Poisson’s ratio dataset (minimization), <italic>y</italic> = 1.9 for the thermoelectric merit dataset (maximization), and <italic>y</italic> = −12 for the wildfire detection dataset (minimization). To extend the applicability of ZoMBI optimization performance results to a wider array of applications, additional stress tests are conducted on 174 analytical datasets. First, a set of 144 analytical datasets are optimized to assess the failure and success conditions of ZoMBI on problems with extremely narrow optima and few initialization data points. Then, in the Supplemental Information, a set of 30 analytical datasets are optimized to assess the failure and success conditions of ZoMBI on problems with insufficient initialization data and cases where the global optimum is near the edge of the manifold.</p></sec><sec><p id="Par28">The ZoMBI algorithm has two key features: (1) iterative inward bounding of proceeding search spaces using the <italic>m</italic> number of best-performing memory points within the prior search space and (2) iterative pruning of low-performing historical search space memory. The newly computed search space bounds are unique for each dimension, such that the optimum basin of attraction of complex, non-convex NiaH manifolds can be discovered. This algorithm leverages these two key features to guide the acquisition of new data towards more optimal regions while only fitting the surrogate within the suggested optimum region to resolve more detail of the space of interest, as shown in Figs. <xref rid="Fig3" ref-type="fig">3</xref> and <xref rid="Fig2" ref-type="fig">2</xref>. This process subsequently reduces the compute time significantly compared to the compute of a GP in a standard BO procedure, as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>.</p></sec><sec id="FPar1"><title>Algorithm 1</title><p id="Par29">Zooming Memory-Based Initialization (ZoMBI)<fig id="Figa" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2023_1048_Figa_HTML.png" position="anchor"/></fig></p></sec><sec><p id="Par30">We define <italic>m</italic> as the number of retained memory points during an activation of ZoMBI. The <italic>m</italic> memory points are saved to memory while all other data are erased from memory. These are the historical data points that achieve the <italic>m</italic> lowest (for minimization) target values, <italic>y</italic>, and they are used to zoom in the search bounds. Using these memory points, the multi-dimensional upper and lower bounds of the zoomed search space are computed for each dimension, <italic>d</italic>. Let <bold>X</bold> ≔ {<italic>X</italic><sub>1</sub>, <italic>X</italic><sub>2</sub>, …, <italic>X</italic><sub><italic>n</italic></sub>} be a set of data points, where <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X}_{j}\in {{\mathbb{R}}}^{d}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq4.gif"/></alternatives></inline-formula>. Let <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mrow><mml:mi>f</mml:mi><mml:mo>:</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>→</mml:mo><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$f:{{\mathbb{R}}}^{d}\to {\mathbb{R}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq5.gif"/></alternatives></inline-formula> be the objective function. We first assume that the points in <bold>X</bold> are in general position so that <italic>f</italic>(<bold>X</bold>) contains unique elements. Then, for each <italic>m</italic>≤<italic>n</italic> define <bold>X</bold><sup>(<italic>m</italic>)</sup> = {<italic>X</italic><sub><italic>π</italic>(1)</sub>, …, <italic>X</italic><sub><italic>π</italic>(<italic>m</italic>)</sub>} where <italic>π</italic> is a permutation on {1, …, <italic>n</italic>} so that {<italic>f</italic>(<italic>X</italic><sub><italic>π</italic>(<italic>j</italic>)</sub>)} is in ascending order. If <italic>f</italic>(<bold>X</bold>) contains repeated elements, we may first remove the points with repeated <italic>f</italic> values and apply the definition above. Then, for each <italic>d</italic>, the bounds are defined as:<disp-formula id="Equ2"><label>2</label><alternatives><mml:math id="Equ2_Math"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:munder><mml:mrow><mml:mi>min</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:munder><mml:mrow><mml:mi>max</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">X</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{{{{\mathcal{B}}}}}_{d}^{l}\,=\,\mathop{\min }\limits_{X\in {{{{\bf{X}}}}}^{(m)}}\{{X}_{d}\}\\ {{{{\mathcal{B}}}}}_{d}^{u}\,=\,\mathop{\max }\limits_{X\in {{{{\bf{X}}}}}^{(m)}}\{{X}_{d}\},\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_Equ2.gif"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{B}}}}}_{d}^{l}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq6.gif"/></alternatives></inline-formula> and <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{{{\mathcal{B}}}}}_{d}^{u}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq7.gif"/></alternatives></inline-formula> computed lower and lower bounds for each dimension, <italic>d</italic>, respectively. The bounds <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">B</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>u</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$[{{{{\mathcal{B}}}}}_{d}^{l},{{{{\mathcal{B}}}}}_{d}^{u}]$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_IEq8.gif"/></alternatives></inline-formula> constrain the proceeding acquisition of new data as well as the computation of a GP, such that sampling cannot occur outsides of the bounded region. This constraining process operates independently for each dimension, such that each dimension has a unique lower and upper bound. To initialize the algorithm with data from the constrained space, <italic>i</italic> data points are sampled from the bounded region using Latin Hypercube Sampling (LHS). LHS splits a <italic>d</italic>-dimensional space into <italic>i</italic>*<italic>d</italic> equally spaced strata, where <italic>i</italic> is the number of points to sample uniformly over <italic>d</italic> dimensions with low variability, unlike random sampling that has high sampling variability<sup><xref ref-type="bibr" rid="CR66">66</xref></sup>. A GP surrogate model is retrained on these <italic>i</italic> LHS points sampled from the constrained space and then for every proceeding experiment sampled from the space, denoted as a forward experiment, the surrogate model is retrained. Thus, the GP is only being trained on information within the constrained region and as the constrained region iteratively zooms inward and decreases in hypervolume, so does the region computed by the GP. This process allows for more information to be resolved within regions plausibly containing the global optimum basin of attraction. Up to <italic>ϕ</italic> forward experiments are sampled in serial, where {<italic>X</italic><sub><italic>i</italic></sub>} ∪ {<italic>X</italic><sub><italic>ϕ</italic></sub>} ⊆ {<italic>X</italic><sub><italic>n</italic></sub>}. These forward experiments are sampled by maximizing an acquisition value, <italic>a</italic> ∈ [0, 1], computed by a user-selected acquisition function from one of the four functions EI, EI Abrupt, LCB, and LCB Adaptive, described in the Methods. Once <italic>i</italic> + <italic>ϕ</italic> number of experiments are sampled, the bounds are re-constrained using the <italic>m</italic> best performing experiments, <italic>i</italic> new experiments are sampled from the zoomed-in space using LHS, and then the memory is pruned. The process of collecting <italic>ϕ</italic> forward experiments is repeated. A complete constraining-resetting iteration is denoted as an activation, <italic>α</italic>. This iterative zooming and pruning process over several <italic>α</italic> significantly speeds up compute time. Implementation of ZoMBI is shown in Algorithm <xref rid="FPar1" ref-type="">1</xref>.</p></sec><sec><p id="Par31">Traditional BO acquisition functions, such as EI<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> and LCB<sup><xref ref-type="bibr" rid="CR68">68</xref></sup>, use the computed means and variances from a surrogate model to compute an acquisition value; maximizing this acquisition value guides sampling of the manifold<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR33">33</xref></sup>. However, these traditional acquisition functions are static, such that they do not actively use any information about the performance of previously sampled experiments to guide sampling. Hence, we implement an adaptive learning approach into the acquisition functions to develop two functions, EI Abrupt and LCB Adaptive, that dynamically adapt their sampling based on the quantity and quality of previously sampled experiments. In contrast to a static acquisition function, these adaptive acquisition functions are initialized with an initial set of hyperparameter values to guide their search but then tune these values as sampling progresses. The developed EI Abrupt and LCB Adaptive functions are used within the ZoMBI framework to further accelerate optimization and avoid pigeonholing, see line 9 of Algorithm 1.</p></sec><sec><p id="Par32"><bold>LCB Adaptive</bold> builds off of previous work that also tune sampling based on the number of experiments collected, <italic>n</italic><sup><xref ref-type="bibr" rid="CR69">69</xref>–<xref ref-type="bibr" rid="CR71">71</xref></sup>. In this paper, we design LCB Adaptive to tune its hyperparameter to become less explorative as more samples are collected. For example, as the <italic>n</italic> increases, LCB Adaptive decays its <italic>β</italic> hyperparameter value to become less explorative and more exploitative. Specifically, this information feedback received by the function determines the contribution of both <italic>μ</italic>(<italic>X</italic>) and <italic>σ</italic>(<italic>X</italic>) to the acquisition value, <italic>a</italic>. Similar to EI Abrupt, LCB Adaptive computes an acquisition value, <italic>a</italic> ∈ [0, 1], for a given <italic>X</italic>, wherein the <italic>X</italic> with the highest <italic>a</italic> is selected by the acquisition function as the next suggested experiment to measure. LCB Adaptive is implemented for a minimization problem as:<disp-formula id="Equ3"><label>3</label><alternatives><mml:math id="Equ3_Math"><mml:mrow><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">LCB</mml:mi><mml:mi mathvariant="normal">Adaptive</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>;</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mi>β</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${a}_{{{{\rm{LCB}}}}{{{\rm{Adaptive}}}}}(X,n;\beta ,\epsilon )=\mu (X)-{\epsilon }^{n}\beta \sigma (X),$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_Equ3.gif"/></alternatives></disp-formula>where <italic>n</italic> is the number of experiments sampled, and <italic>β</italic> = 3 and <italic>ϵ</italic> = 0.9 are hand-tuned initialization hyperparameters selected based on a priori domain knowledge of the function’s performance on a variety of different problems. Having a large <italic>β</italic> and an <italic>ϵ</italic> close to 1 supports a gradual decay from very explorative to very exploitative, rather than a rapid decay. The dynamic EI Abrupt and LCB Adaptive are shown to both discover optima faster and avoid pigeonholing into local minima better than their static counterparts by actively balancing the ratio of exploitation to exploration using learned information about the quality and quantity of previously sampled experiments.</p></sec><sec><p id="Par33"><bold>EI Abrupt</bold> is an acquisition function that flips between the exploitative EI<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> and explorative LCB<sup><xref ref-type="bibr" rid="CR68">68</xref></sup> acquisition functions based on the computed finite differences of recently evaluated experiments. For example, if the evaluated experiment <italic>y</italic>-values plateaus for three or more experiments in a row, EI Abrupt will abruptly switch from a greedy sampling policy to a more explorative sampling policy. Specifically, this information feedback received by the function determines if the current round of sampling should exploit the surrogate mean values, <italic>μ</italic>(<italic>X</italic>), or explore the surrogate variances, <italic>σ</italic>(<italic>X</italic>). EI Abrupt computes an acquisition value, <italic>a</italic> ∈ [0, 1], for a given <italic>X</italic>, wherein the <italic>X</italic> with the highest <italic>a</italic> is selected by the acquisition function as the next suggested experiment to measure. EI Abrupt is implemented for a minimization problem as:<disp-formula id="Equ4"><label>4</label><alternatives><mml:math id="Equ4_Math"><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">EI</mml:mi><mml:mi mathvariant="normal">Abrupt</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>;</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfenced open="{"><mml:mrow><mml:mtable><mml:mtr><mml:mtd columnalign="left"><mml:mfenced close=")" open="("><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mfenced><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Φ</mml:mi><mml:mspace width="0.25em"/><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>ψ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Z</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">if</mml:mi><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mo>∣</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mo>≤</mml:mo><mml:mi>η</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd><mml:mtd columnalign="left"><mml:mi mathvariant="normal">otherwise</mml:mi><mml:mspace width="0.25em"/></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mspace width="2.0em"/><mml:mspace width="2.0em"/><mml:mspace width="2.0em"/><mml:mspace width="2.0em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mspace width="0.25em"/><mml:mi>Z</mml:mi><mml:mspace width="0.25em"/><mml:mo>=</mml:mo><mml:mspace width="0.25em"/><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="0.25em"/><mml:mo>−</mml:mo><mml:mspace width="0.25em"/><mml:msup><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:msup><mml:mspace width="0.25em"/><mml:mo>−</mml:mo><mml:mspace width="0.25em"/><mml:mi>ξ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{a}_{{{{\rm{EI}}}}{{{\rm{Abrupt}}}}}(X,y;\beta ,\xi ,\eta )\,=\,\left\{\begin{array}{ll}\left(\mu (X)-{y}^{* }-\xi \right)\,\Phi \,(Z)+\sigma (X)\psi (Z),&amp; {{\rm{if}}}\,\,| \Delta \{{y}_{n-3...n}\}| \le \eta \\ \mu (X)-\beta \sigma (X),&amp; {{\rm{otherwise}}}\,\end{array}\right.\\ \qquad \qquad \qquad \qquad\,\,\, Z\,=\,\dfrac{\mu (X)\,-\,{y}^{* }\,-\,\xi }{\sigma (X)},\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2023_1048_Article_Equ4.gif"/></alternatives></disp-formula>where <italic>y</italic><sup>*</sup> is the lowest measured target value thus far (i.e., the running minimum), Φ( ⋅ ) is the cumulative density function of the normal distribution, <italic>ψ</italic>( ⋅ ) is the probability density function of the normal distribution, and ∣Δ{<italic>y</italic><sub><italic>n</italic>−3...<italic>n</italic></sub>}∣ is the absolute value of the finite differences of the set of target values of the last three sampled experiments. Moreover, <italic>β</italic> = 0.1, <italic>ξ</italic> = 0.1, and <italic>η</italic> = 0 are hand-tuned initialization hyperparameters used for the rest of the paper for EI Abrupt. Moreover, for standard LCB and EI, <italic>β</italic> = 1 and <italic>ξ</italic> = 0.1 hyperparameters are used, respectively. These hyperparameters were selected based on a priori domain knowledge of EI Abrupt performance on a variety of different problems. The most important hyperparameter for efficient sampling is <italic>β</italic>, whose ideal value is non-obvious, but it is found that <italic>β</italic> = 0.1 allows EI Abrupt to switch into an explorative sampling policy while still having a strong weight on the surrogate means, implying that exploration does not veer far.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>Basita Das is thanked for help in naming the algorithm. Xiaonan Wang is thanked for initial discussions for this study. John Dagdelen, Hongbin Zhang, and Shyam Dwaraknath are thanked for discussion of and reference to different Needle-in-a-Haystack problems within materials science. We acknowledge the MIT SuperCloud and Lincoln Laboratory Supercomputing Center for providing HPC resources that have contributed to the research results reported within this paper. A.E.S acknowledges support from the U.S. Department of Energy’s Office of Energy Efficiency and Renewable Energy (EERE) under the Solar Energy Technology Office (SETO) award number DE-EE0009366. Q.L. acknowledges support from the National Research Foundation, Singapore (project No. NRF-NRFF13-2021-0005) and the Ministry of Education, Singapore, under its Research Centre of Excellence award to I-FIM (project No. EDUNC-33-18-279-V12).</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>A.E.S., Z.R., and T.B. conceived of and designed the study. Q.L. and T.B. provided guidance on machine learning methods, benchmark functions, and datasets. A.E.S. and Z.R. wrote the code. A.E.S. performed the machine learning modeling and analysis. A.E.S. wrote the paper, while all co-authors reviewed the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>Implementation of the ZoMBI algorithm, the experimental dataset analyzed during the current study, the simulated data and labeled data supporting the findings of this study, and the data comprising the figures in this paper are all available in the following GitHub repository: <ext-link xlink:href="https://github.com/PV-Lab/ZoMBI" ext-link-type="uri">https://github.com/PV-Lab/ZoMBI</ext-link>.</p></sec><sec sec-type="ethics-statement"><sec id="FPar2" sec-type="COI-statement"><title>Competing interests</title><p id="Par34">Although our laboratory has IP filed covering photovoltaic technologies and materials informatics broadly, we do not envision a direct COI with this study, the content of which is open-sourced. Two of the authors (Z.R. and T.B.) own equity in Xinterra Pte Ltd, which applies machine learning to accelerate novel materials development.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning with knowledge constraints for process optimization of open-air perovskite solar cell manufacturing</article-title><source>Joule</source><year>2022</year><volume>6</volume><fpage>834</fpage><lpage>849</lpage><pub-id pub-id-type="doi">10.1016/j.joule.2022.03.003</pub-id></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siemenn</surname><given-names>AE</given-names></name><etal/></person-group><article-title xml:lang="en">A machine learning and computer vision approach to rapidly optimize multiscale droplet generation</article-title><source>ACS Appl. Mater. Interfaces</source><year>2022</year><volume>14</volume><fpage>4668</fpage><lpage>4679</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB38XhtVSksL8%3D</pub-id><pub-id pub-id-type="doi">10.1021/acsami.1c19276</pub-id></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mekki-Berrada</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Two-step machine learning enables optimized nanoparticle synthesis</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00520-w</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">A data fusion approach to optimize compositional stability of halide perovskites</article-title><source>Matter</source><year>2021</year><volume>4</volume><fpage>1305</fpage><lpage>1322</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhvVemsLvJ</pub-id><pub-id pub-id-type="doi">10.1016/j.matt.2021.01.008</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Snelson, E. &amp; Ghahramani, Z. <italic>Sparse Gaussian Processes using Pseudo-inputs</italic>, vol. 18 (MIT Press, 2005).</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Rasmussen, C. E. &amp; Williams, C. K. I. <italic>Gaussian Processes for Machine Learning</italic> (The MIT Press, 2005).</mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Brochu, E., Cora, V. M. &amp; de Freitas, N. A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning (2010).</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">Snoek, J., Larochelle, H. &amp; Adams, R. P. Practical Bayesian optimization of machine learning algorithms 1–12 (2001). arXiv:1206.2944v2.</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liang</surname><given-names>Q</given-names></name><etal/></person-group><article-title xml:lang="en">Benchmarking the performance of Bayesian optimization across multiple experimental materials science domains</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00656-9</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Antono</surname><given-names>E</given-names></name><name><surname>Meredig</surname><given-names>B</given-names></name><name><surname>Ling</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Machine-learned metrics for predicting the likelihood of success in materials discovery</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXmt12jsrw%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41524-020-00401-8</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andricioaei</surname><given-names>I</given-names></name><name><surname>Straub</surname><given-names>JE</given-names></name></person-group><article-title xml:lang="en">Finding the needle in the haystack: algorithms for conformational optimization</article-title><source>Comput. Phys.</source><year>1996</year><volume>10</volume><fpage>449</fpage><pub-id pub-id-type="doi">10.1063/1.168582</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seeger</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Gaussian processes for machine learning</article-title><source>Int. J. Neural Syst.</source><year>2004</year><volume>14</volume><fpage>69</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1142/S0129065704001899</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Snoek</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Scalable Bayesian optimization using deep neural networks</article-title><source>32nd Int. Conf. Mach. Learn. ICML 2015</source><year>2015</year><volume>3</volume><fpage>2161</fpage><lpage>2170</lpage></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dagdelen</surname><given-names>J</given-names></name><name><surname>Montoya</surname><given-names>J</given-names></name><name><surname>De Jong</surname><given-names>M</given-names></name><name><surname>Persson</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Computational prediction of new auxetic materials</article-title><source>Nat. Commun.</source><year>2017</year><volume>8</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXos1Smu74%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41467-017-00399-6</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>KK</given-names></name><name><surname>Das</surname><given-names>R</given-names></name><name><surname>Calius</surname><given-names>EP</given-names></name></person-group><article-title xml:lang="en">Three decades of auxetics research materials with negative Poisson’s ratio: a review</article-title><source>Adv. Eng. Mater.</source><year>2016</year><volume>18</volume><fpage>1847</fpage><lpage>1870</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhvVCqu77N</pub-id><pub-id pub-id-type="doi">10.1002/adem.201600053</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Liu, Q. Literature review: materials with negative Poisson’s ratios and potential applications to aerospace and defense. Tech. Rep., Australian Government Department of Defense (2006).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salah</surname><given-names>WA</given-names></name><name><surname>Abuhelwa</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Review of thermoelectric cooling devices recent applications</article-title><source>J. Eng. Sci. Technol.</source><year>2020</year><volume>15</volume><fpage>455</fpage><lpage>476</lpage></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>R</given-names></name><name><surname>Schierning</surname><given-names>G</given-names></name><name><surname>Nielsch</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Thermoelectric devices: a review of devices, architectures, and contact optimization</article-title><source>Adv. Mater. Technol.</source><year>2018</year><volume>3</volume><fpage>1700256</fpage><pub-id pub-id-type="doi">10.1002/admt.201700256</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Ren</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">Thermoelectric cooling materials</article-title><source>Nat. Mater.</source><year>2020</year><volume>20</volume><fpage>454</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1038/s41563-020-00852-w</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Commentary: the materials project: a materials genome approach to accelerating materials innovation</article-title><source>APL Mater.</source><year>2013</year><volume>1</volume><fpage>011002</fpage><pub-id pub-id-type="doi">10.1063/1.4812323</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Jong</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Charting the complete elastic properties of inorganic crystalline compounds</article-title><source>Sci. Data</source><year>2015</year><volume>2</volume><fpage>1</fpage><lpage>13</lpage></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeganeh-Haeri</surname><given-names>A</given-names></name><name><surname>Weidner</surname><given-names>DJ</given-names></name><name><surname>Parise</surname><given-names>JB</given-names></name></person-group><article-title xml:lang="en">Elasticity of <italic>α</italic>-Cristobalite: a silicon dioxide with a negative Poisson’s ratio</article-title><source>Science</source><year>1992</year><volume>257</volume><fpage>650</fpage><lpage>652</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK38XmtVaitbY%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.257.5070.650</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakes</surname><given-names>R</given-names></name><name><surname>Wojciechowski</surname><given-names>KW</given-names></name></person-group><article-title xml:lang="en">Negative compressibility, negative Poisson’s ratio, and stability</article-title><source>Phys. Status Solidi Basic Res.</source><year>2008</year><volume>245</volume><fpage>545</fpage><lpage>551</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1cXktVehtbk%3D</pub-id><pub-id pub-id-type="doi">10.1002/pssb.200777708</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rew</surname><given-names>LJ</given-names></name><name><surname>Maxwell</surname><given-names>BD</given-names></name><name><surname>Dougher</surname><given-names>FL</given-names></name><name><surname>Aspinall</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Searching for a needle in a haystack: evaluating survey methods for non-indigenous plant species</article-title><source>Natl. Park Biol. Invasions</source><year>2006</year><volume>8</volume><fpage>523</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1007/s10530-005-6420-2</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bouguettaya</surname><given-names>A</given-names></name><name><surname>Zarzour</surname><given-names>H</given-names></name><name><surname>Taberkit</surname><given-names>AM</given-names></name><name><surname>Kechida</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">A review on early wildfire detection from unmanned aerial vehicles using deep learning-based computer vision algorithms</article-title><source>Signal Process.</source><year>2022</year><volume>190</volume><fpage>108309</fpage><pub-id pub-id-type="doi">10.1016/j.sigpro.2021.108309</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>W</given-names></name><etal/></person-group><article-title xml:lang="en">Effective detection of sophisticated online banking fraud on extremely imbalanced data</article-title><source>World Wide Web</source><year>2012</year><volume>16</volume><fpage>449</fpage><lpage>475</lpage><pub-id pub-id-type="doi">10.1007/s11280-012-0178-0</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Marchant, N. G. &amp; Rubinstein, B. I. P. Needle in a haystack: label-efficient evaluation under extreme class imbalance. <italic>KDD ’21, August 14–18, 2021, Virtual Event, Singapore</italic> 11 (2021). <ext-link xlink:href="10.1145/3447548.3467435" ext-link-type="doi">https://doi.org/10.1145/3447548.3467435</ext-link>.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khalilia</surname><given-names>M</given-names></name><name><surname>Chakraborty</surname><given-names>S</given-names></name><name><surname>Popescu</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Predicting disease risks from highly imbalanced data using random forest</article-title><source>BMC Med. Inform. Decis. Mak.</source><year>2011</year><volume>11</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1186/1472-6947-11-51</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Crammer, K. &amp; Chechik, G. A Needle in a haystack: local one-class optimization. Proc. 21st Int. Conf. Mach. Learn. Banff, Canada (2004).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>H</given-names></name><name><surname>Hu</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>B</given-names></name><name><surname>Matusik</surname><given-names>W</given-names></name><name><surname>Sifakis</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Narrow-band topology optimization on a sparsely populated grid</article-title><source>ACM Trans. Graph.</source><year>2018</year><volume>37</volume><fpage>1</fpage><lpage>14</lpage></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nusse</surname><given-names>HE</given-names></name><name><surname>Yorke</surname><given-names>JA</given-names></name></person-group><article-title xml:lang="en">Basins of attraction</article-title><source>Science</source><year>1996</year><volume>271</volume><fpage>1376</fpage><lpage>1380</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaK28XhsFegtLo%3D</pub-id><pub-id pub-id-type="doi">10.1126/science.271.5254.1376</pub-id></mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Datseris</surname><given-names>G</given-names></name><name><surname>Wagemakers</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Effortless estimation of basins of attraction</article-title><source>Chaos An Interdiscip. J. Nonlinear Sci.</source><year>2022</year><volume>32</volume><fpage>023104</fpage><pub-id pub-id-type="doi">10.1063/5.0076568</pub-id></mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennig</surname><given-names>P</given-names></name><name><surname>Schuler</surname><given-names>CJ</given-names></name></person-group><article-title xml:lang="en">Entropy search for information-efficient global optimization</article-title><source>J. Mach. Learn. Res.</source><year>2012</year><volume>13</volume><fpage>1809</fpage><lpage>1837</lpage></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Mikhail, B., Evgeny, B. &amp; Yermek, K. Exact Inference for Gaussian Process Regression in case of Big Data with the Cartesian Product Structure (2014).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Li, C. et al. High dimensional Bayesian optimization using dropout. <italic>Proc. 26th Int. Jt. Conf. Artif. Intell. IJCAI</italic> (2017).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Wang, Z., Li, C., Jegelka, S. &amp; Kohli, P. Batched high-dimensional Bayesian optimization via structural kernel learning. <italic>Proc. 34th Int. Conf. Mach. Learn. Sydney, Aust. PMLR</italic><bold>70</bold> (2017).</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bui</surname><given-names>TD</given-names></name><name><surname>Yan</surname><given-names>J</given-names></name><name><surname>Turner</surname><given-names>RE</given-names></name></person-group><article-title xml:lang="en">A unifying framework for Gaussian process pseudo-point approximations using power expectation propagation</article-title><source>J. Mach. Learn. Res.</source><year>2017</year><volume>18</volume><fpage>1</fpage><lpage>72</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhslSiu7c%3D</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Lan, G., Tomczak, J. M., Roijers, D. M. &amp; Eiben, A. E. Time Efficiency in Optimization with a Bayesian-Evolutionary Algorithm (2020).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Eriksson, D., Pearce, M., Gardner, J. R., Turner, R. &amp; Poloczek, M. Scalable Global Optimization via Local Bayesian Optimization (2020).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Regis</surname><given-names>RG</given-names></name></person-group><article-title xml:lang="en">Trust regions in Kriging-based optimization with expected improvement</article-title><source>Eng. Optim.</source><year>2015</year><volume>48</volume><fpage>1037</fpage><lpage>1059</lpage><pub-id pub-id-type="doi">10.1080/0305215X.2015.1082350</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Diouane, Y., Picheny, V., Le Riche, R., Scotto, A. &amp; Perrotolo, D. TREGO: a Trust-Region Framework for Efficient Global Optimization (2021).</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Titsias</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Variational learning of inducing variables in sparse gaussian processes</article-title><source>Proc. Mach. Learn. Res.</source><year>2009</year><volume>5</volume><fpage>567</fpage><lpage>574</lpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Leibfried, F., Dutordoir, V., John, S. T. &amp; Durrande, N. A Tutorial on Sparse Gaussian Processes and Variational Inference (2021).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Turner, R. E. &amp; Sahani, M. Two problems with variational expectation maximisation for time-series models. In Barber, D., Cemgil, T. &amp; Chiappa, S. (eds.) <italic>Bayesian Time series models</italic>, chap. 5, 109–130 (Cambridge University Press, 2011).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">van Stein, B., Wang, H. &amp; Back, T. Automatic configuration of deep neural networks with parallel efficient global optimization. <italic>2019 Int. Jt. Conf. Neural Netw.</italic> 1–7 (2019).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>DR</given-names></name><name><surname>Schonlau</surname><given-names>M</given-names></name><name><surname>Welch</surname><given-names>WJ</given-names></name></person-group><article-title xml:lang="en">Efficient global optimization of expensive black-box functions</article-title><source>J. Glob. Optim.</source><year>1998</year><volume>13</volume><fpage>455</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1023/A:1008306431147</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joy</surname><given-names>TT</given-names></name><name><surname>Rana</surname><given-names>S</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Venkatesh</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Fast hyperparameter tuning using Bayesian optimization with directional derivatives</article-title><source>Knowledge-Based Syst.</source><year>2020</year><volume>205</volume><fpage>106247</fpage><pub-id pub-id-type="doi">10.1016/j.knosys.2020.106247</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Klein, A., Falkner, S., Bartels, S., Hennig, P. &amp; Hutter, F. Fast Bayesian Optimization of Machine Learning Hyperparameters on Large Datasets (2017).</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Zhang, Y., Bahadori, M. T., Su, H. &amp; Sun, J. FLASH: Fast Bayesian Optimization for Data Analytic Pipelines. <italic>Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discov. Data Min</italic>. (2016).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cowen-Rivers</surname><given-names>AI</given-names></name><etal/></person-group><article-title xml:lang="en">Hebo: pushing the limits of sample-efficient hyperparameter optimisation honorary position</article-title><source>J. Artif. Intell. Res.</source><year>2021</year><volume>70</volume><fpage>1</fpage><lpage>15</lpage></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Ackley, D. H. <italic>A connectionist machine for genetic hillclimbing</italic> (Kluwer Academic Publishers, 1987).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Adorio, E. P. MVF - Multivariate Test Functions Library in C for Unconstrained Global Optimization (2005).</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Correa, E. S. &amp; Shapiro, J. L. <italic>Model Complexity vs. Performance in the Bayesian Optimization Algorithm</italic> (Springer, 2006).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belyadi</surname><given-names>H</given-names></name><name><surname>Fathi</surname><given-names>E</given-names></name><name><surname>Belyadi</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Rock mechanical properties and in situ stresses</article-title><source>Hydraul. Fract. Unconv. Reserv.</source><year>2019</year><volume>13</volume><fpage>215</fpage><lpage>231</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-817665-8.00013-8</pub-id></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poplavko</surname><given-names>YM</given-names></name></person-group><article-title xml:lang="en">Mechanical properties of solids</article-title><source>Electron. Mater.</source><year>2019</year><volume>2</volume><fpage>71</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-815780-0.00002-5</pub-id></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinterleitner</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Thermoelectric performance of a metastable thin-film Heusler alloy</article-title><source>Nature</source><year>2019</year><volume>576</volume><fpage>85</fpage><lpage>90</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitFGns7rF</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1751-9</pub-id></mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>GK</given-names></name><name><surname>Singh</surname><given-names>DJ</given-names></name></person-group><article-title xml:lang="en">BoltzTraP. A code for calculating band-structure dependent quantities</article-title><source>Comput. Phys. Commun.</source><year>2006</year><volume>175</volume><fpage>67</fpage><lpage>71</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD28Xltlegt7w%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.cpc.2006.03.007</pub-id></mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>HS</given-names></name><name><surname>Liu</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Chu</surname><given-names>CW</given-names></name><name><surname>Ren</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">Relationship between thermoelectric figure of merit and energy conversion efficiency</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2015</year><volume>112</volume><fpage>8205</fpage><lpage>8210</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhtVOhsbnI</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1510231112</pub-id></mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>WH</given-names></name><name><surname>Wu</surname><given-names>PH</given-names></name><name><surname>Wang</surname><given-names>XD</given-names></name><name><surname>Lin</surname><given-names>YL</given-names></name></person-group><article-title xml:lang="en">Power output and efficiency of a thermoelectric generator under temperature control</article-title><source>Energy Convers. Manag.</source><year>2016</year><volume>127</volume><fpage>404</fpage><lpage>415</lpage><pub-id pub-id-type="doi">10.1016/j.enconman.2016.09.039</pub-id></mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldsmid</surname><given-names>HJ</given-names></name></person-group><article-title xml:lang="en">Bismuth telluride and its alloys as materials for thermoelectric generation</article-title><source>Materials</source><year>2014</year><volume>7</volume><fpage>2577</fpage><lpage>2592</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXhs1Crtr7O</pub-id><pub-id pub-id-type="doi">10.3390/ma7042577</pub-id></mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodrigo</surname><given-names>PM</given-names></name><name><surname>Valera</surname><given-names>A</given-names></name><name><surname>Fernandez</surname><given-names>EF</given-names></name><name><surname>Almonacid</surname><given-names>FM</given-names></name></person-group><article-title xml:lang="en">Annual energy harvesting of passively cooled hybrid thermoelectric generator-concentrator photovoltaic modules</article-title><source>IEEE J. Photovoltaics</source><year>2019</year><volume>9</volume><fpage>1652</fpage><lpage>1660</lpage><pub-id pub-id-type="doi">10.1109/JPHOTOV.2019.2939878</pub-id></mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kohli</surname><given-names>G</given-names></name><etal/></person-group><article-title xml:lang="en">Ecostress and cimis: a comparison of potential and reference evapotranspiration in riverside county, california</article-title><source>Remote Sens.</source><year>2020</year><volume>12</volume><fpage>4126</fpage><pub-id pub-id-type="doi">10.3390/rs12244126</pub-id></mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohapatra</surname><given-names>A</given-names></name><name><surname>Trinh</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Early wildfire detection technologies in practice - a review</article-title><source>Sustainability</source><year>2022</year><volume>14</volume><fpage>12270</fpage><pub-id pub-id-type="doi">10.3390/su141912270</pub-id></mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Maus, N. et al. Local latent space bayesian optimization over structured inputs (2022). <ext-link xlink:href="https://arxiv.org/abs/2201.11872v1" ext-link-type="uri">https://arxiv.org/abs/2201.11872v1</ext-link>.</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="other">Grosnit, A. et al. High-dimensional bayesian optimisation with variational autoencoders and deep metric learning (2021). <ext-link xlink:href="https://arxiv.org/abs/2106.03609v3" ext-link-type="uri">https://arxiv.org/abs/2106.03609v3</ext-link>.</mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKay</surname><given-names>MD</given-names></name><name><surname>Beckman</surname><given-names>RJ</given-names></name><name><surname>Conover</surname><given-names>WJ</given-names></name></person-group><article-title xml:lang="en">A comparison of three methods for selecting values of input variables in the analysis of output from a computer code</article-title><source>Technometrics</source><year>2000</year><volume>42</volume><fpage>55</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1080/00401706.2000.10485979</pub-id></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saltenis</surname><given-names>VR</given-names></name></person-group><article-title xml:lang="en">One method of multiextremum optimization</article-title><source>Automatic Control and Comput. Sci.</source><year>1971</year><volume>5</volume><fpage>33</fpage><lpage>38</lpage></mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auer</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Using confidence bounds for exploitation-exploration trade-offs</article-title><source>J. Mach. Learn. Res.</source><year>2002</year><volume>3</volume><fpage>397</fpage><lpage>422</lpage></mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Srinivas, N., Krause, A., Kakade, S. &amp; Seeger, M. Gaussian process optimization in the bandit setting: no regret and experimental design. <italic>Proc. 27th Int. Conf. Mach. Learn. Haifa, Isr. 2010</italic> 1015–1022 (2010).</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Roch</surname><given-names>LM</given-names></name><name><surname>Kreisbeck</surname><given-names>C</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Phoenics: a bayesian optimizer for chemistry</article-title><source>ACS Cent. Sci.</source><year>2018</year><volume>4</volume><fpage>1134</fpage><lpage>1145</lpage><pub-id pub-id-type="doi">10.1021/acscentsci.8b00307</pub-id></mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Aldeghi</surname><given-names>M</given-names></name><name><surname>Hickman</surname><given-names>RJ</given-names></name><name><surname>Roch</surname><given-names>LM</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Gryffin: an algorithm for bayesian optimization of categorical variables informed by expert knowledge</article-title><source>Appl. Phys. Rev.</source><year>2021</year><volume>8</volume><fpage>031406</fpage><pub-id pub-id-type="doi">10.1063/5.0048164</pub-id></mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Reuther, A. et al. Interactive supercomputing on 40,000 cores for machine learning and data analysis. <italic>2018 IEEE,</italic> 2018 conference proceedings. <italic>High Perform. Extrem. Comput. Conf</italic>. 1–6 (2018).</mixed-citation></ref></ref-list></ref-list><app-group><app id="App1" specific-use="web-only"><sec id="Sec5"><title>Supplementary information</title><p id="Par35"><supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information"><media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/41524_2023_1048_MOESM1_ESM.pdf" position="anchor"><caption xml:lang="en"><p>Supplementary Information</p></caption></media></supplementary-material></p></sec></app></app-group><notes notes-type="ESMHint"><title>Supplementary information</title><p>The online version contains supplementary material available at <ext-link xlink:href="10.1038/s41524-023-01048-x" ext-link-type="doi">https://doi.org/10.1038/s41524-023-01048-x</ext-link>.</p></notes><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2023</facet-value></facet><facet name="country"><facet-value count="1">Singapore</facet-value><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
