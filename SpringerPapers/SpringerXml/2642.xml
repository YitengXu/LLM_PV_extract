<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1186/s13634-021-00763-1</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">13634</journal-id><journal-id journal-id-type="doi">10.1007/13634.1687-6180</journal-id><journal-title-group><journal-title>EURASIP Journal on Advances in Signal Processing</journal-title><abbrev-journal-title abbrev-type="publisher">EURASIP J. Adv. Signal Process.</abbrev-journal-title></journal-title-group><issn pub-type="epub">1687-6180</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s13634-021-00763-1</article-id><article-id pub-id-type="manuscript">763</article-id><article-id pub-id-type="doi">10.1186/s13634-021-00763-1</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Spatial and temporal learning representation for end-to-end recording device identification</article-title></title-group><contrib-group><contrib contrib-type="author" id="Au1"><name><surname>Zeng</surname><given-names>Chunyan</given-names></name><address><email>cyzeng@hbut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Zhu</surname><given-names>Dongliang</given-names></name><address><email>840771073@qq.com</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6960-509X</contrib-id><name><surname>Wang</surname><given-names>Zhifeng</given-names></name><address><email>zfwang@mail.ccnu.edu.cn</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs13634021007631_cor3">c</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Wu</surname><given-names>Minghu</given-names></name><address><email>wuxx1005@mail.hbut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au5"><name><surname>Xiong</surname><given-names>Wei</given-names></name><address><email>xw@mail.hbut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au6"><name><surname>Zhao</surname><given-names>Nan</given-names></name><address><email>nzhao@mail.hbut.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411410.1</institution-id><institution-id institution-id-type="ISNI">0000 0000 8822 034X</institution-id><institution content-type="org-name">Hubei Key Laboratory for High-efficiency Utilization of Solar Energy and Operation Control of Energy Storage System, Hubei University of Technology</institution></institution-wrap><addr-line content-type="street">Nanli Road 28</addr-line><addr-line content-type="postcode">430068</addr-line><addr-line content-type="city">Wuhan</addr-line><country country="CN">China</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.411407.7</institution-id><institution-id institution-id-type="ISNI">0000 0004 1760 2614</institution-id><institution content-type="org-name">Department of Digital Media Technology, Central China Normal University</institution></institution-wrap><addr-line content-type="street">Luoyu Road 152</addr-line><addr-line content-type="postcode">430079</addr-line><addr-line content-type="city">Wuhan</addr-line><country country="CN">China</country></aff></contrib-group><author-notes><corresp id="IDs13634021007631_cor3"><label>c</label><email>zfwang@mail.ccnu.edu.cn</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>17</day><month>7</month><year>2021</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2021</year></pub-date><volume>2021</volume><issue seq="41">1</issue><elocation-id>41</elocation-id><history><date date-type="registration"><day>6</day><month>7</month><year>2021</year></date><date date-type="received"><day>5</day><month>2</month><year>2021</year></date><date date-type="accepted"><day>5</day><month>7</month><year>2021</year></date><date date-type="online"><day>17</day><month>7</month><year>2021</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2021</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Deep learning techniques have achieved specific results in recording device source identification. The recording device source features include spatial information and certain temporal information. However, most recording device source identification methods based on deep learning only use spatial representation learning from recording device source features, which cannot make full use of recording device source information. Therefore, in this paper, to fully explore the spatial information and temporal information of recording device source, we propose a new method for recording device source identification based on the fusion of spatial feature information and temporal feature information by using an end-to-end framework. From a feature perspective, we designed two kinds of networks to extract recording device source spatial and temporal information. Afterward, we use the attention mechanism to adaptively assign the weight of spatial information and temporal information to obtain fusion features. From a model perspective, our model uses an end-to-end framework to learn the deep representation from spatial feature and temporal feature and train using deep and shallow loss to joint optimize our network. This method is compared with our previous work and baseline system. The results show that the proposed method is better than our previous work and baseline system under general conditions.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Spatial features</kwd><kwd>Temporal features</kwd><kwd>Convolution neural network (CNN)</kwd><kwd>Long Short-Term Memory (LSTM)</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">No.61901165</award-id></award-group><award-group><funding-source><institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/501100001809</institution-id></institution-wrap></funding-source><award-id award-type="FundRef grant">61501199</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>41</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>7</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>ContentOnly</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/13634_2021_Article_763.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Signal,Image and Speech Processing</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Quantum Information Technology, Spintronics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec2" sec-type="introduction"><title>Introduction</title><p>Nowadays, the popularity of portable device sources makes it more and more convenient to obtain digital audio data. At the same time, the emergence of various powerful multimedia editing software also makes audio editing and modification easier. In court and other important occasions, using tampered audio as electronic evidence will cause serious social problems [<xref ref-type="bibr" rid="CR1">1</xref>–<xref ref-type="bibr" rid="CR3">3</xref>]. Therefore, it is of great significance to classify the source of digital audio data through recording device source identification technology [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>], and it has broad application prospects in judicial authentication and news information authenticity recognition.</p><p>Because the electronic components and structures in the audio capture device have different tolerances in nominal value, each audio capture device has a unique conversion function (i.e., frequency response). Therefore, each audio capture device will leave a unique internal trace in the voice recording. The task of recording device source identification refers to identifying the class of device source from this inherent trace. This paper mainly focuses on portable recording devices (e.g., mobile phones, pad) source identification. In these previous works, most of the works used embedding methods to map devices into a feature space whose distance corresponds to the similarity of device sources. Although [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR6">6</xref>–<xref ref-type="bibr" rid="CR10">10</xref>] had promoted the development of device source identification algorithms in the past few years, the task of device source identification is still a feature engineering task before the deep learning method came out.</p><p>Deep learning techniques can automatically extract highly abstract and complex features from raw data due to their powerful representation learning ability. CNN (convolutional neural networks) [<xref ref-type="bibr" rid="CR11">11</xref>] and DNN (Deep Neural Networks) [<xref ref-type="bibr" rid="CR12">12</xref>] are effective methods for device source identification, extracting the spatial correlation of related feature domains. LSTM (Long Short-Term Memory) can reasonably handle the temporal correlation in sequential data [<xref ref-type="bibr" rid="CR13">13</xref>], which has certain advantages in the sequence model with long-term memory. In addition, the deep learning methods directly make predictions based on the input data, which are conducive to the design of an end-to-end framework for device source identification tasks [<xref ref-type="bibr" rid="CR14">14</xref>]. Deep learning has many advantages, but device source identification based on deep learning still has many challenges. For example, in terms of feature representation learning, most device source identification tasks based on deep learning rely on extracting shallow spatial information from general acoustic features for identification. For instance, MFCC is used as an input to train a shallow CNN network for device source identification tasks. However, as far as we know, shallow structure networks have many limitations. With more and more data, limited samples and computing units will limit the network’s generalization ability. In addition, these works only use DNN, CNN, and other networks to extract spatial information from device source features or directly use features that contain spatial information such as GSV. However, some device source features contain certain temporal information, such as MFCC. In terms of models, most traditional models or deep learning models contain many hand-designed parameters. The quality of feature selection depends on experience with human subjective factors. Moreover, some device source identification tasks lack transferability by combining deep learning methods with traditional machine learning methods.</p><p>Inspired by the success of deep learning for feature representation [<xref ref-type="bibr" rid="CR12">12</xref>] and RNN for music classification tasks [<xref ref-type="bibr" rid="CR13">13</xref>], we propose a device source identification method based on the end-to-end framework for the fusion of spatial and temporal features. In this proposed method, first, to fully explore the spatial information and temporal information of device source, spatial and temporal feature extraction networks are built to extract spatial and temporal information from GSV feature and MFCC feature. Then, to fuse the two features information of the device source and increase the depth of the network, the attention mechanism is used to fuse spatial information and temporal information into a deep representation feature to obtain inherent traces left in the device source. Finally, to solve the problems of too many hand-designed parameters and poor transferability in the device source identification task, we build an end-to-end framework with deep-and-shallow loss to simplify the network and improve performance.</p><p>This work is an extension of our previous work [<xref ref-type="bibr" rid="CR14">14</xref>]. In general, this work has the following contributions: 
<list list-type="bullet"><list-item><p>Parallel network representation learning is used to capture spatial and temporal information from device sources.</p></list-item><list-item><p>In terms of spatial and temporal information fusion, we add an attention mechanism that can assign the best weights to spatial and temporal features through autonomous network learning.</p></list-item><list-item><p>By establishing an end-to-end framework with strong transferability, a more compact model can be constructed, thereby reducing the complexity of the model.</p></list-item><list-item><p>In the optimization of the model, we design the deep-and-shallow loss to improve the model’s generalization performance by assigning fixed weights to the deep and shallow losses, respectively.</p></list-item></list></p><p>The rest of this paper is organized as follows. Section <xref rid="Sec3" ref-type="sec">2</xref> describes the related work. In Section <xref rid="Sec13" ref-type="sec">3</xref>, we introduced the main methods of this paper. In Section <xref rid="Sec16" ref-type="sec">4</xref>, we describe the parallel network used to extract the spatial information and temporal information of the device source in detail. The fifth part mainly introduces the attention mechanism and the back-end classification network. Section <xref rid="Sec27" ref-type="sec">6</xref> designs experiments results and analysis. Section <xref rid="Sec38" ref-type="sec">7</xref> introduces the conclusion and future work.</p></sec><sec id="Sec3"><title>Related work</title><p>In order to solve the problem of device source identification, many effective methods were proposed from different aspects. We divide the existing device source identification methods into two categories: feature-based methods and decision model-based methods. The feature-based method extracts the feature vector that characterizes the source information of the device from the original recording and then applies the existing vector data classification method. Decision model-based methods applied classifiers to solve the device source classification problems. The critical issue was to specify an appropriate learning function to measure the distance between two samples.</p><p>In this section, we discuss previous research work closely related to our method. In Section <xref rid="Sec4" ref-type="sec">2.1</xref>, we summarize the existing feature-based device source identification methods. In Section <xref rid="Sec8" ref-type="sec">2.2</xref>, we discuss several methods based on decision models.</p><sec id="Sec4"><title>The methods of device source identification based on features</title><p>In the past, the features used in the device source identification method mainly included the following: (1) devices source information characterization based on cepstrum features, (2) representation of devices source information based on Gaussian Super Vector, and (3) devices source information based on deep features.</p><sec id="Sec5"><title>Devices source information characterization based on cepstrum features</title><p>Cepstrum features were widely used in the field of device source identification. Christian Kraetzer et al. [<xref ref-type="bibr" rid="CR15">15</xref>] proposed using Mel cepstrum features as machine fingerprints to identify device sources, which opened up the research field of device source identification. On this basis, Cemal Hanili et al. [<xref ref-type="bibr" rid="CR7">7</xref>] proposed the use of the MFCC (Mel Frequency Cepstral Coefficient) as a feature for device source identification. Since then, Qin Tianyun et al. [<xref ref-type="bibr" rid="CR16">16</xref>], mer Eskidere et al. [<xref ref-type="bibr" rid="CR17">17</xref>], Daniel Garcia-Romero et al. [<xref ref-type="bibr" rid="CR4">4</xref>], Ling Zou et al. [<xref ref-type="bibr" rid="CR6">6</xref>], and Cemal Hanili et al. [<xref ref-type="bibr" rid="CR18">18</xref>] respectively verified the effectiveness of MFCC feature in device source identification tasks. Soon after, Aggarwal Rachit et al. [<xref ref-type="bibr" rid="CR19">19</xref>] first extracted the MFCC feature from the noise spectrum signal from the speech signal. This method is better than extracting the MFCC feature from speech segments based on the analysis of the experimental results. Meanwhile, other related cepstrum features are used in device source identification, for example, LFCC [<xref ref-type="bibr" rid="CR4">4</xref>], BFCC (Bark Frequency Cepstrum Coefficient) and LPCC (Linear Prediction Cepstrum Coefficient) [<xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR18">18</xref>], PNCC (Power Normalized Cepstrum Coefficient) [<xref ref-type="bibr" rid="CR6">6</xref>], and improved PNCC [<xref ref-type="bibr" rid="CR20">20</xref>] proposed in the device source task. MFCC was calculated based on equally spaced frequency bands on the Mel scale instead of linear spaced frequency bands used. Chao Jin et al. [<xref ref-type="bibr" rid="CR21">21</xref>] used the principle of audio coding to quantize the spectral components to extract device source features by encoding Huffman below the masking threshold and achieved significant results.</p></sec><sec id="Sec6"><title>Representation of devices source information based on Gaussian Super Vector</title><p>GSV (Gaussian Super Vector) [<xref ref-type="bibr" rid="CR22">22</xref>] was the feature data extracted from the mean vector of GMM (Gaussian Mixture Model). Traditional GSV included voice information and device source information. Yuechi Jiang et al. [<xref ref-type="bibr" rid="CR23">23</xref>] improved the quality of traditional GSV and mapped the traditional GSV to another dimension space. In this dimensional space, device source information and voice information can be divided into different dimensions, the back-end classifiers used SVM and SRC classifiers to conduct a comparative experiment. The experiment proved that the Kernel-Based GSV feature achieved better results compared to baseline experiments.</p></sec><sec id="Sec7"><title>Devices source information based on deep features</title><p>The essence of the deep neural network was to extract the inherent deep features of the data through the hidden layer of the network. There were two main ways to acquire deep features that supervised training or unsupervised training. In the feature extraction of supervised training, the device source data was used to train the appropriate network to extract the device source feature by giving the label. Unsupervised training extracted features by changing the data itself to extract feature data that could reflect the original. Inspired by this, Yanxiong Li et al. [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR24">24</xref>] proposed two types of deep features: the first used MFCC features to build a deep neural network, and then extracted the output of the middle layer of the DNN network as the feature; the second used MFCC feature train a deep auto-encoding network and then used the output of the middle layer as the output feature. Experiments showed that the deep feature used by the author is better than the available feature. Xiaodan Lin et al. [<xref ref-type="bibr" rid="CR25">25</xref>] used the attention mechanism to assign feature weights to the frequency spectrum of different frequency bands. Experiments showed that the feature proposed by the author is more effective compared to the baseline.</p></sec></sec><sec id="Sec8"><title>The methods of device source identification based on decision model</title><p>The decision model in device source identification mainly includes the following: (1) the device source identification decision model based on GMM (Gaussian Mixture Model), (2) the device source identification decision model based on SVM (Support Vector Machine), (3) the device source identification decision model based on the SRC (Sparse Representation Classifier), and (4) the device source decision model based on deep learning model.</p><sec id="Sec9"><title>Device source identification decision model based on Gaussian Mixture Model</title><p>The GMM-based classification model is to build a GMM for the data, compare the test data with the GMM model to calculate the probability, and finally take the highest probability, such as Cemal Hanili et al. [<xref ref-type="bibr" rid="CR18">18</xref>] proposed to use the maximum amount of mutual information to measure the GMM. Comparative experimental results showed that better decision-making ability to use the maximum mutual information to train the GMM was better than the traditional training method in the case of short data.</p></sec><sec id="Sec10"><title>Device source identification decision model based on Support Vector Machine</title><p>The SVM classifier had perfect theoretical derivation and perfect interpretability in mathematics, so the SVM classifier was the most widely used model in machine learning. Different kernel functions were used in the SVM classifier to map features into high-dimensional space. Commonly used kernel functions were RBF (Radial Basis Function Kernel) and GLDS (Generalized Linear Discriminant Sequence Kernel) [<xref ref-type="bibr" rid="CR26">26</xref>]. Gianmarco Baldini et al. [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>] used FFT frequency features and compared the SVM classifier as a baseline classifier with the CNN network. Da Luo et al. [<xref ref-type="bibr" rid="CR29">29</xref>] proposed the BED feature of Fourier transform after signal framing. The back-end used SVM classification model still achieved positive results on 141 device sources.</p></sec><sec id="Sec11"><title>Device source identification decision model based on Sparse Representation Classifier</title><p>SRC used the internal elements of the dictionary as a basis function to transform the original feature data into 0, 1 sparse feature data by constructing a complete function dictionary. Ling Zou et al. [<xref ref-type="bibr" rid="CR30">30</xref>, <xref ref-type="bibr" rid="CR31">31</xref>] used GSV to construct a database dictionary, and then used the K-SVD [<xref ref-type="bibr" rid="CR32">32</xref>] algorithm to calculate the score between the test device and the target device, at last, compared with a preset threshold to obtain the final recognition result. The K-SVD dictionary was obtained through unsupervised learning. Therefore, Ling Zou et al. [<xref ref-type="bibr" rid="CR30">30</xref>] proposed the use of D-KSVD (Discriminative K-SVD) [<xref ref-type="bibr" rid="CR33">33</xref>] algorithm to construct a supervised learning dictionary to improve the performance of devices source identification. When the training dataset was sufficient and complete, the SRC could improve the recognition efficiency by reducing the computational complexity. However, for classification problems with fewer samples, the improvement of SRC classification accuracy is a problem worthy of discussion.</p></sec><sec id="Sec12"><title>Devices source identification decision model based on deep learning model</title><p>The performance of deep learning models in various fields attracted attention. It could train not only large datasets but also has strong generalization and transferability. Gianmarco Baldini et al. [<xref ref-type="bibr" rid="CR9">9</xref>, <xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>] used CNN in the back-end to surpass traditional classification methods and achieve better results. However, the fitting of a shallow network does not fully reflect the effect of deep learning. Further, Chunyan Zeng [<xref ref-type="bibr" rid="CR14">14</xref>] et al. used a multi-feature parallel convolution network, combined with the attention mechanism for device source identification to achieve an improved effect. With further research on device source identification, the feature dimension of device source identification and the number of devices for device source identification tasks will further increase. The general statistical model required a large amount of calculation and is challenging to meet these demands. The application of the deep model could undoubtedly overcome these shortcomings.</p></sec></sec></sec><sec id="Sec13" sec-type="methods"><title>Methods</title><p>We integrate the steps of device source identification into a single network in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The input of this network includes “training” speech and “testing” speech. The output is a single node indicating the category. We propose the deep-and-shallow loss to optimize this network. In Section <xref rid="Sec14" ref-type="sec">3.1</xref>, we describe the overview architecture of our end-to-end device source identification framework. The details of its important components will be discussed in Section <xref rid="Sec16" ref-type="sec">4</xref> and Section <xref rid="Sec24" ref-type="sec">5</xref>. In Section <xref rid="Sec15" ref-type="sec">3.2</xref>, we describe the training process of the end-to-end framework.
<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>The architecture of our end-to-end framework</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13634_2021_763_Fig1_HTML.png" id="MO1"/></fig></p><sec id="Sec14"><title>End-to-end framework</title><p>The structure of the end-to-end framework for device source identification is depicted in Fig. <xref rid="Fig1" ref-type="fig">1</xref>. The framework consists of a parallel feature extraction network module, a feature fusion module, a back-end classification module, and a deep-and-shallow loss module. In the parallel feature extraction module, to extract device source spatial information and temporal information, GSV and MFCC features are put into the spatial and temporal information extraction network, respectively. In the feature fusion module, we use the attention mechanism to fuse the device source spatial information and temporal information to obtain more representative features. Based on our previous work [<xref ref-type="bibr" rid="CR14">14</xref>], we used full connection with a decreasing number of nodes in the back-end classification module in this paper. In the deep-and-shallow loss module, we designed a deep-and-shallow loss for the end-to-end framework, and the specific information is described in Section <xref rid="Sec15" ref-type="sec">3.2</xref>.</p></sec><sec id="Sec15"><title>End-to-end training</title><p>As shown in Table <xref rid="Tab1" ref-type="table">1</xref>, our end-to-end training process includes initialization, data loading, batch processing, loss calculation, gradient calculation, and parameter update. Steps 1 to 3 represent the initialization of the network and the loading of data, which will be described in Section <xref rid="Sec27" ref-type="sec">6</xref>. Step 4 belongs to the forward propagation of network training, including calculating deep-and-shallow loss. Steps 5–9 belong to the back-propagation stage of the network, including calculation of iterative gradients and update of network parameters.
<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>End-to-End joint training</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Optimization of end-to-end parameter update algorithm based on deep-and-shallow loss</p></th></tr></thead><tbody><tr><td align="left"><p>dataset: 45 device sources, each device source is 514 sentences for training</p></td></tr><tr><td align="left"><p>1: initialized: <italic>W</italic><sub><italic>t</italic><italic>e</italic><italic>m</italic></sub>, <italic>W</italic><sub><italic>s</italic><italic>p</italic><italic>a</italic></sub>, <italic>W</italic><sub><italic>att</italic></sub>, <italic>W</italic><sub><italic>class</italic></sub></p></td></tr><tr><td align="left"><p>2: for k=1,...,K (K=epoch)</p></td></tr><tr><td align="left"><p>3: for t=1,...,T (T=514/batch size)</p></td></tr><tr><td align="left"><p>4: deep-and-shallow loss: <italic>L</italic>(<italic>t</italic>)=<italic>λ</italic><sub>1</sub><italic>L</italic><sub><italic>T</italic></sub>(<italic>t</italic>)+<italic>λ</italic><sub>2</sub><italic>L</italic><sub><italic>S</italic></sub>(<italic>t</italic>)+(1−<italic>λ</italic><sub>1</sub>−<italic>λ</italic><sub>2</sub>)<italic>L</italic><sub><italic>A</italic></sub>(<italic>t</italic>)</p></td></tr><tr><td align="left"><p>5: backpropagation error:<inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mfrac><mml:mrow><mml:mi>∂L</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$\frac {\partial L(t)}{\partial x_{i}(t)}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13634_2021_763_Article_IEq1.gif"/></alternatives></inline-formula></p></td></tr><tr><td align="left"><p>6: <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>tem</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>tem</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>tem</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>tem</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{\text {tem}}(t+1){\gets }W_{\text {tem}}(t)-\mu * \frac {\partial L_{T}(t)}{\partial W_{\text {tem}}(t)}-\mu * \frac {\partial L_{A}(t)}{\partial W_{\text {tem}}(t)}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13634_2021_763_Article_IEq2.gif"/></alternatives></inline-formula></p></td></tr><tr><td align="left"><p>7: <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>spa</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>spa</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>spa</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>spa</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{\text {spa}}(t+1){\gets }W_{\text {spa}}(t)-\mu * \frac {\partial L_{s}(t)}{\partial W_{\text {spa}}(t)}-\mu * \frac {\partial L_{A}(t)}{\partial W_{\text {spa}}(t)}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13634_2021_763_Article_IEq3.gif"/></alternatives></inline-formula></p></td></tr><tr><td align="left"><p>8: <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>att</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{\text {att}}(t+1){\gets }W_{\text {att}}(t)-\mu * \frac {\partial L_{A}(t)}{\partial W_{\text {att}}(t)}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13634_2021_763_Article_IEq4.gif"/></alternatives></inline-formula></p></td></tr><tr><td align="left"><p>9: <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>cla</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>cla</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>∂</mml:mi><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mtext>cla</mml:mtext></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$W_{\text {cla}}(t+1){\gets }W_{\text {cla}}(t)-\mu * \frac {\partial L_{A}(t)}{\partial W_{\text {cla}}(t)}$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13634_2021_763_Article_IEq5.gif"/></alternatives></inline-formula></p></td></tr><tr><td align="left"><p>10: end for</p></td></tr><tr><td align="left"><p>11: end for</p></td></tr><tr><td align="left"><p>12: return <italic>W</italic><sub><italic>t</italic><italic>e</italic><italic>m</italic></sub>, <italic>W</italic><sub><italic>s</italic><italic>p</italic><italic>a</italic></sub>, <italic>W</italic><sub><italic>att</italic></sub>, <italic>W</italic><sub><italic>class</italic></sub></p></td></tr></tbody></table></table-wrap></p><p>In the forward propagation process of network training, the input information is passed through the input layer and the hidden layer, processed layer by layer, and passed to the output layer. Our deep-and-shallow loss has three outputs, and the three cross-entropy losses of the three outputs and the label are calculated as the loss function. In step 4, <italic>L</italic><sub><italic>T</italic></sub>,<italic>L</italic><sub><italic>S</italic></sub>,<italic>L</italic><sub><italic>A</italic></sub> represent temporal feature extraction network module loss, spatial feature extraction network module loss, and back-end classification module loss, respectively. We call <italic>L</italic><sub><italic>T</italic></sub> and <italic>L</italic><sub><italic>S</italic></sub> shallow loss, and <italic>L</italic><sub><italic>A</italic></sub> stands for deep loss. <italic>L</italic>(<italic>t</italic>) represents a total loss. <italic>λ</italic><sub>1</sub>,<italic>λ</italic><sub>2</sub> and (1−<italic>λ</italic><sub>1</sub>−<italic>λ</italic><sub>2</sub>) respectively represent the weight assigned to each loss. The deep-and-shallow loss we designed has several advantages; first of all, for the fusion method of temporal and spatial features, deep-and-shallow loss assigns different weights to different network branches so that the network can focus more on difficult-to-train branches. Second, the spatial and temporal feature extraction network is optimized by the deep-and-shallow loss, which can speed up the convergence of the shallow network parameters to a certain extent.</p><p>Then, transfer to the back-propagation stage. In step 5, the three losses respectively calculate the partial derivative of the objective function to the weight of each neuron layer by layer, which constitutes the gradient of the objective function to the weight vector, which serves as the basis for modifying the weight. In steps 6–9, the training of the network is completed in the parameter update. <italic>W</italic><sub><italic>t</italic><italic>e</italic><italic>m</italic></sub> represents the weight of the temporal feature extraction network. <italic>W</italic><sub><italic>s</italic><italic>p</italic><italic>a</italic></sub> represents the weight of the spatial feature extraction network. <italic>W</italic><sub><italic>att</italic></sub> represents the weight of the attention module, and <italic>W</italic><sub><italic>cla</italic></sub> represents the weight of the back-end classification module. Our deep-and-shallow loss includes three losses. Therefore, it can be seen from steps 6–7 in Table <xref rid="Tab1" ref-type="table">1</xref> that the network needs to refer to the two output weights to modify the gradient when updating the network based on spatial information extraction and temporal information extraction.</p></sec></sec><sec id="Sec16"><title>Parallel spatial and temporal feature extraction network</title><p>As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the PSTNN (Parallel Spatial-Temporal Network) model is divided into two parts. One is a spatial information extractor described in Section <xref rid="Sec17" ref-type="sec">4.1</xref>, and the other is a temporal information extractor described in Section <xref rid="Sec21" ref-type="sec">4.2</xref>. Spatial information extractors include DNN, CNN, and ResNet designed by us. We designed LSTM and Bi-LSTM for temporal information extractors. The rest of this section will describe the network we designed in detail.
<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>PSTNN network structure diagram, including spatial feature extraction network and temporal feature extraction network, where M represents the number of nodes of LSTM, and N represents the number of nodes of DNN</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13634_2021_763_Fig2_HTML.png" id="MO2"/></fig></p><sec id="Sec17"><title>Spatial feature extraction network</title><p>We used the spatial feature extraction network, including DNN, CNN, and ResNet, based on the GSV feature. GSV is a C * F dimensional super vector connected by the GMM mean value. Assuming that the data of each category conforms to the Gaussian distribution, GMM is a spatial combination of multiple Gaussian distribution functions. GSV features can best represent the differences between various GMM so that GSV features can be regarded as spatial features. In Section <xref rid="Sec18" ref-type="sec">4.1.1</xref>, we describe a DNN-based device source spatial information extraction network. In Section <xref rid="Sec19" ref-type="sec">4.1.2</xref>, we design a CNN-based device source spatial information extraction network. In Section <xref rid="Sec20" ref-type="sec">4.1.3</xref>, we design a residual network for the extraction of device source spatial information.</p><sec id="Sec18"><title>Spatial information feature extraction based on DNN</title><p>DNN is usually used as a spatial feature extraction tool. In the DNN-based spatial feature extraction network, the input layer is a 2496-dimensional GSV feature. The design of multiple hidden layers is mainly to enhance the expressive ability of the model. Of course, the complexity of the multilayer perceptron model will increase. The output layer is the deep representation bottleneck feature based on DNN. To increase the nonlinear fitting ability of the model, we use the RELU activation function. The specific parameters of the DNN-based device source spatial feature extraction network we designed will be explained in Section <xref rid="Sec30" ref-type="sec">6.3</xref>.</p></sec><sec id="Sec19"><title>Spatial information feature extraction based on CNN</title><p>In this section, we design a CNN to extract device source spatial information. Compared with DNN, the CNN performs better on high-dimensional data [<xref ref-type="bibr" rid="CR34">34</xref>]. CNN has fewer parameters, which shortens training time and improves operational efficiency. Its layer properties are more suitable for high-dimensional data. The CNN network structure mainly includes the convolutional layer, pooling layer, and batch normalization layer. The specific parameters of the CNN-based device source spatial feature extraction network we designed in Section <xref rid="Sec30" ref-type="sec">6.3</xref>.</p></sec><sec id="Sec20"><title>Spatial information feature extraction based on ResNet</title><p>In this section, we plan to deepen the network to obtain more prosperous device source spatial information. However, after increasing the number of network layers, the optimization effect worsens. In order to alleviate the problem of gradient explosion and gradient disappearance caused by network deepening [<xref ref-type="bibr" rid="CR35">35</xref>], we design ResNet to extract the spatial information of the device source, the structure of which is shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>. Our residual network consists of four residual blocks. The input and output of each residual block will be connected to form a jump connection. Each residual block contains five layers of networks: four layers of convolution, one layer of pooling, one layer of batch normalization to reduce the risk of overfitting. See Section <xref rid="Sec30" ref-type="sec">6.3</xref> for specific network parameters.
<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>The figure shows the schematic diagram of the residual network and residual block parameters and structure</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13634_2021_763_Fig3_HTML.png" id="MO3"/></fig></p></sec></sec><sec id="Sec21"><title>Temporal feature extraction network</title><p>In order to extract the temporal information from the device source, we designed LSTM and Bi-LSTM networks to extract the device source deep temporal features from MFCC. MFCC is a set of temporal feature vectors obtained by encoding the speech’s physical information (spectral envelope and details). In Section <xref rid="Sec22" ref-type="sec">4.2.1</xref>, we describe the LSTM based device source temporal information extraction network. In Section <xref rid="Sec23" ref-type="sec">4.2.2</xref>, we introduce the Bi-LSTM based device source temporal information extraction network.</p><sec id="Sec22"><title>Temporal information feature extraction based on LSTM</title><p>In order to extract the temporal information from the device source, we design an LSTM + DNN network. The LSTM network refers to [<xref ref-type="bibr" rid="CR36">36</xref>], which can solve the long-term dependence of temporal information and improve the adaptability of the network. The LSTM network can execute the temporal state of each unit through the gate structure, delete or add information, and essentially transmit useful device source information to the next frame. The structure diagram of LSTM + DNN we designed is shown in the following Fig. <xref rid="Fig4" ref-type="fig">4</xref>. The model takes the frame-level speech feature MFCC as input and obtains the output, corresponding to each frame through the 2-layer LSTM. M is the number of LSTM hidden units, and N is the number of DNN hidden units.
<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>The figure is a schematic diagram of LSTM+DNN parameters and structure, where M represents the number of nodes of LSTM, and N represents the number of nodes of DNN</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13634_2021_763_Fig4_HTML.png" id="MO4"/></fig></p></sec><sec id="Sec23"><title>Temporal information feature extraction based on Bi-LSTM</title><p>Similarly, we also designed the Bi-LSTM temporal feature extractor. Bi-LSTM is the abbreviation of bidirectional long and short-term memory and combines forward LSTM and backward LSTM. Compared with the structure of LSTM + DNN, the prediction of Bi-LSTM + DNN is determined by the first inputs and the following inputs, which will be more accurate. However, it is undeniable that compared to LSTM, Bi-LSTM will be more challenging to converge and increase calculation.</p></sec></sec></sec><sec id="Sec24"><title>Attention mechanism and back-end classification network</title><p>This section describes the fusion mechanism for device source spatial information and temporal information and back-end classification network. The task of device source identification based on spatial information and temporal information has been completed, but the task based on the fusion of the two has not been developed. Considering the interdependence between spatial and temporal information in device source identification, in Section <xref rid="Sec25" ref-type="sec">5.1</xref>, we use the attention mechanism to fuse two kinds of information. In Section <xref rid="Sec26" ref-type="sec">5.2</xref>, we use a reduced DNN network for back-end classification network.</p><sec id="Sec25"><title>Attention mechanism</title><p>Usually, using a single feature representation of the data may not be sufficient to complete the classification task. In this paper, we use two feature representations to capture different aspects of the input. The attention mechanism can assign significant weights to two representations, determining the most relevant aspects while ignoring noise and redundancy in the input. The expression of the attention mechanism is a weighted combination of two features and the weight of the attention mechanism. One advantage of the attention mechanism is to evaluate which combination of feature representations is the preferred expression for a specific classification task by checking the weights.</p><p>Essentially, the attention mechanism is a resource allocation mechanism that allocates available resources to more important features. The attention mechanism in this paper uses convolution, pooling, and activation functions (similar to softmax) to construct weights to readjust the feature map. Firstly, since the device source information of spatial is usually low-frequency, convolutional neural and average pooling operations can reduce the difference between adjacent GSVs and smooth features. Secondly, by adding a convolutional neural network, the network can introduce appropriate parameters to learn the best weights.</p><p>As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, deep device source spatial and temporal features are both 1024-dimensional vectors. They will be merged into a 1024*2 matrix for the convolutional layer and the pooling layer to build weights before the attention mechanism. In the process of the attention mechanism, the network learns parameters and generates weight values through the convolutional layer and the pooling layer, and the weight values are multiplied by the temporal feature map and the spatial feature map and merged to obtain the final feature. After the attention mechanism, we obtain the fusion vector feature of the spatial and temporal representation of the device source, and their dimensions are 1024-dimensional vectors. The fusion feature discards redundant information and magnifies the inter-class representation capabilities of the two representation features.
<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Schematic diagram of attention mechanism. N represents the dimension of the vector</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13634_2021_763_Fig5_HTML.png" id="MO5"/></fig></p></sec><sec id="Sec26"><title>Back-end classification network</title><p>In the back-end classification network, we use a DNN network with decreasing neurons. Mainly reduce the storage and calculation cost of parameters or calculation results in the model and increase the fit. The goal is to map feature data to a series of quantitative trim levels. The DNN network with decreasing neurons can effectively reduce parameter redundancy and help deploy deep learning applications. The parameter designed according to our previous work [<xref ref-type="bibr" rid="CR14">14</xref>].</p></sec></sec><sec id="Sec27"><title>Experimental results and discussion</title><p>In this section, we introduce the dataset, baseline system, experimental setup, and experimental results. In order to verify the effectiveness of our work, we design six sets of experiments to verify our contribution: (1) comparison experiment based on UBM Gaussian number, (2) comparison experiment based on back-end classification, (3) network selection comparison experiment based on PSTNN, (4) comparison experiment based on our proposed deep-and-shallow loss weight setting and single loss function, (5) comparison experiment based on the effectiveness of the attention mechanism, and (6) compare the end-to-end model with the GMM-UBM [<xref ref-type="bibr" rid="CR6">6</xref>], GSV-SVM [<xref ref-type="bibr" rid="CR37">37</xref>], BED-SVM [<xref ref-type="bibr" rid="CR29">29</xref>], and previous work [<xref ref-type="bibr" rid="CR14">14</xref>].</p><sec id="Sec28"><title>Dataset description</title><p>To verify the performance of the proposed model, this paper uses the CCNU-Mobile dataset as the experimental dataset. We established the CCNU-Mobile dataset in 2019. The CCNU-Mobile dataset contains 8 different brands and 45 different devices. See Table <xref rid="Tab2" ref-type="table">2</xref> for specific models and brands. In addition, the CCNU-Mobile dataset is recorded in the TIMIT dataset, each mobile device has 642 recording files, and the sample size of the dataset is 45*642 = 28890. To ensure the recording quality, the recording environment is a professional recording studio that is almost completely quiet. The recorded voice data sampling rate is 32Khz, and the quantization rate is 16bit.
<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Brands and models of the CCNU-Mobile dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Brands</p></th><th align="left"><p>Models</p></th></tr></thead><tbody><tr><td align="left"><p>APPLE</p></td><td align="left"><p>iphone6 (4), iphone6s (3), iphoneSE, ipad7, iphone7p, iphoneX, air2(2), air1</p></td></tr><tr><td align="left"><p>HUAWEI</p></td><td align="left"><p>tagal00, nova, novo2s, nova3e, honor7x, honor8(3), honorV8, honor9, honor10, p10, p20</p></td></tr><tr><td align="left"><p>XIAOMI</p></td><td align="left"><p>mi2s, note3, mi5, mi8, mi8se(2), mix2, redmiNote4x, redmi3S</p></td></tr><tr><td align="left"><p>VIVO</p></td><td align="left"><p>y11t,x3f,x7</p></td></tr><tr><td align="left"><p>ZTE</p></td><td align="left"><p>c880a,g719c</p></td></tr><tr><td align="left"><p>SAMSUNG</p></td><td align="left"><p>sphd710,s8</p></td></tr><tr><td align="left"><p>OPPO</p></td><td align="left"><p>r9s</p></td></tr><tr><td align="left"><p>NUBIA</p></td><td align="left"><p>z11</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec29"><title>Experimental setup</title><p>This work divides the dataset into 8:2, each training device contains 514 recordings, and the test device contains 128 recordings. We use a batch normalization layer in the network, which speeds up the convergence speed of the model and, more importantly, alleviates the overfitting problem in the deep network to a certain extent. We have added Dropout to the network. Dropout can effectively alleviate the occurrence of over-fitting and achieve the effect of regularization to a certain extent. We use Adam’s optimization method to optimize all network parameters, and the exponential decay rates of the first and second moment are 0.9 and 0.999. We use Tensorflow and Keras software packages to train and test all network models in this work. The GPU hardware information is RTX TITAN. When extracting MFCC in the experiment, the length of the signal frame is 30 ms, and the overlap time is 15 ms. In order to make the feature better reflect the time domain continuity, we also added the first-order difference and the second-order difference in the MFCC. Therefore, we use 39-dimensional MFCC and TIMIT dataset to train 64-, 128-, and 256-dimensional UBM. GSV uses 64, 128, 256 Gaussian 39-dimensional parameters, and the baseline system uses GMM-UBM [<xref ref-type="bibr" rid="CR6">6</xref>], GSV-SVM [<xref ref-type="bibr" rid="CR37">37</xref>], and BED-SVM models [<xref ref-type="bibr" rid="CR29">29</xref>]. All the experimental evaluation indicators in this paper use classification accuracy (ACC), and the total number of categories is 45.</p></sec><sec id="Sec30"><title>Hyperparameter settings of the network</title><p>This section mainly introduces the parameter composition of the PSTNN network. As shown in Table <xref rid="Tab3" ref-type="table">3</xref>, the SFENN (spatial feature extraction network) we designed includes DNN, CNN, and ResNet. The parameters of each network are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. The input dimension of DNN is 2496, and the three hidden layer nodes are 1024. The CNN network contains three layers of convolution, three layers of pooling, and one layer of full connection. The number of the three-layer convolution kernel is 6, 16, and 40, respectively, and the size of the convolution kernel is 5*5. All pooling layers in this paper adopt maximum pooling. The number of hidden layers of the fully connected layer is 1024. The residual network uses four residual blocks, and each residual block includes four convolutional layers, four Batch Normalization layers, and a pooling layer. The convolution kernel of the convolution layer is 16, and the size of the convolution kernel is 3*3. TFENN (temporal feature extraction network) includes LSTM and Bi-LSTM. LSTM includes two hidden layers and a fully connected layer. The input has the dimension of [128,64,39], where 128 is the batch size, 64 is the number of frames, and 39 is the number of features exacted from speech. The number of units in the first hidden layer is 39, and the number of units in the second hidden layer is 78. Bi-LSTM is similar to LSTM.
<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>PSTNN structure design parameter</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="3"><p>SFENN</p></th><th align="left" colspan="2"><p>TFENN</p></th></tr><tr><th align="left"><p>DNN</p></th><th align="left"><p>CNN</p></th><th align="left"><p>ResNet</p></th><th align="left"><p>LSTM</p></th><th align="left"><p>Bi-LSTM</p></th></tr></thead><tbody><tr><td align="left"><p>Input (2496)</p></td><td align="left"><p>Input (64*39*1)</p></td><td align="left"><p>Input</p></td><td align="left"><p>Input</p></td><td align="left"><p>Input</p></td></tr><tr><td align="left"><p>FC (1024)</p></td><td align="left"><p>Conv (6*5*5)</p></td><td align="left"><p>BN+Relu</p></td><td align="left"><p>LSTM(39)</p></td><td align="left"><p>Bi-LSTM(39)</p></td></tr><tr><td align="left"><p>FC (1024)</p></td><td align="left"><p>pooling</p></td><td align="left"><p>Conv(16*3*3)</p></td><td align="left"><p>LSTM(78)</p></td><td align="left"><p>Bi-LSTM(78)</p></td></tr><tr><td align="left"><p>FC (1024)</p></td><td align="left"><p>Conv (16*5*5)</p></td><td align="left"><p>BN+Relu</p></td><td align="left"><p>FC(1024)</p></td><td align="left"><p>FC(1024)</p></td></tr><tr><td align="left"/><td align="left"><p>pooling</p></td><td align="left"><p>Conv(16*3*3)</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"><p>Conv (40*5*5)</p></td><td align="left"><p>BN+Relu</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"><p>pooling</p></td><td align="left"><p>Conv(16*3*3)</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"><p>FC (1024)</p></td><td align="left"><p>BN+Relu</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"><p>Conv(16*3*3)</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"><p>pooling (Input)</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"><p>Add()</p></td><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p></sec><sec id="Sec31"><title>Comparison experiment based on UBM Gaussian number</title><p>To explore the impact of UBM on the model’s overall performance, first, we used the TIMIT dataset to extract 64, 128, and 256 Gaussian UBM. Then, using the MAP algorithm to extract the GSV features of each recording sample, the MFCC is consistent with Section <xref rid="Sec29" ref-type="sec">6.2</xref>. The PSTNN network is a combination of DNN + Bi-LSTM. Finally, the experimental results are shown in Table <xref rid="Tab4" ref-type="table">4</xref>.
<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Comparison experiment based on the effectiveness of UBM Gaussian number</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>UBM Gaussian number</p></th><th align="left"><p>Network parameter</p></th><th align="left"><p>DNN + Bi-LSTM (ACC)</p></th></tr></thead><tbody><tr><td align="left"><p>64</p></td><td align="left"><p>114765k</p></td><td align="left"><p>97.5%</p></td></tr><tr><td align="left"><p>128</p></td><td align="left"><p>120995k</p></td><td align="left"><p>97.3%</p></td></tr><tr><td align="left"><p>256</p></td><td align="left"><p>133455k</p></td><td align="left"><p>97.0%</p></td></tr></tbody></table></table-wrap></p><p>It can be seen from the experimental results that when the Gaussian number is 64, the model has the best effect, reaching 97.5%, and the network parameter amount is 114765k. It is proved that the increase of the Gaussian number will increase the amount of calculation and cause redundancy to the data. The main reason is that the category of the data set is 45 categories, and 64 Gauss is enough to characterize the similarities and differences of the 45 categories of data.</p></sec><sec id="Sec32"><title>Comparison experiment based on back-end classification</title><p>To verify the impact of the back-end classification network on the model performance, we conducted a comparison experiment of the back-end classification network. The parameters of the back-end classification network are simple softmax classification, 1-layer full connection, 2-layer full connection, and 3-layer full connection.</p><p>The experimental results in Table <xref rid="Tab5" ref-type="table">5</xref> show that when the back-end classification networks are 2-layer and 3-layer fully connection, the model has the best effect, which is 97.5%. It is 0.1% higher than the simple softmax network, so when the back-end classification is 2-layer fully connection, the efficiency is the highest, and it proves that adding an appropriate number of fully connected layers can increase the complexity of the model and improve the learning ability of the network model.
<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>Comparison experiment based on back-end classification</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Back-end classification</p></th><th align="left"><p>DNN + Bi-LSTM (ACC)</p></th></tr></thead><tbody><tr><td align="left"><p>Simple softmax classification</p></td><td align="left"><p>97.4%</p></td></tr><tr><td align="left"><p>1-layer full connection</p></td><td align="left"><p>97.4%</p></td></tr><tr><td align="left"><p>2-layer full connection</p></td><td align="left"><p>97.5%</p></td></tr><tr><td align="left"><p>3-layer full connection</p></td><td align="left"><p>97.5%</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec33"><title>Network selection comparison experiment based on PSTNN</title><p>To optimize our network structure, we design an end-to-end framework comparison experiment based on PSTNN, which compares the networks that extract spatial and temporal features. The weight ratio of deep-and-shallow loss is 0.25:0.5:0.25.</p><p>The comparison results of the PSTNN and the single feature extraction network are shown in Table <xref rid="Tab6" ref-type="table">6</xref>. According to the longitudinal comparison in Table <xref rid="Tab6" ref-type="table">6</xref>, when the training period is 100 epochs, DNN is best used as a spatial extraction network, followed by ResNet, and finally, the CNN model. In the temporal network extractor, LSTM is slightly better than Bi-LSTM. When the training period is 200 epochs, this situation is changed, and the effect of using Bi-LSTM exceeds the effect of LSTM. It proves that Bi-LSTM needs longer training time than LSTM network to achieve convergence and balance. From a horizontal comparison, DNN is used to extract the device source spatial information for the single network model to achieve the best result of 96.6%. The DNN + Bi-LSTM network combination achieves the best results with an accuracy rate of 97.5% for the fusion network model. Compared with another fused network model, the maximum increase is 1.4%. From the comparison of experiment 3 and experiment 11, the ResNet + Bi-LSTM combination is better than the single ResNet spatial feature extraction network, which proves that the fusion network is better than one of the single feature extraction networks. However, from experiment 1 and experiment 11, the effect of a single DNN spatial feature is better than the ResNet + Bi-LSTM combination, which proves that each spatial feature extraction network has different spatial extraction capabilities. At the same time, it also proves that DNN spatial feature extraction ability is much better than ResNet so that the combination of ResNet + Bi-LSTM is also lower than DNN spatial feature extraction effect. All in all, our DNN + Bi-LSTM combination achieves the best fitting effect, which is 27% higher than the single network model. The reason for this problem is that it combines the spatial and temporal information in the device source and further proves that our method is effective.
<table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>Comparison and optimization experiment on network selection in PSTNN</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>End-to-end</p></th><th align="left" colspan="3"><p>SFENN</p></th><th align="left" colspan="2"><p>TFENN</p></th><th align="left" colspan="2"><p>Epoch</p></th></tr><tr><th align="left"/><th align="left"><p>DNN</p></th><th align="left"><p>CNN</p></th><th align="left"><p>ResNet</p></th><th align="left"><p>LSTM</p></th><th align="left"><p>Bi-LSTM</p></th><th align="left"><p>100ep</p></th><th align="left"><p>200ep</p></th></tr></thead><tbody><tr><td align="left"><p>Experiment 1</p></td><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"><p>96.5%</p></td><td align="left"><p>96.6%</p></td></tr><tr><td align="left"><p>Experiment 2</p></td><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"><p>94.5%</p></td><td align="left"><p>94.6%</p></td></tr><tr><td align="left"><p>Experiment 3</p></td><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"><p>94.6%</p></td><td align="left"><p>94.5%</p></td></tr><tr><td align="left"><p>Experiment 4</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>68.8%</p></td><td align="left"><p>69.4%</p></td></tr><tr><td align="left"><p>Experiment 5</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"><p>70.9%</p></td><td align="left"><p>70.7%</p></td></tr><tr><td align="left"><p>Experiment 6</p></td><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>97.3%</p></td><td align="left"><p>97.3%</p></td></tr><tr><td align="left"><p>Experiment 7</p></td><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>96.2%</p></td><td align="left"><p>96.1%</p></td></tr><tr><td align="left"><p>Experiment 8</p></td><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>96.2%</p></td><td align="left"><p>96.2%</p></td></tr><tr><td align="left"><p>Experiment 9</p></td><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"><p>97.2%</p></td><td align="left"><p>97.5%</p></td></tr><tr><td align="left"><p>Experiment 10</p></td><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"><p>96.1%</p></td><td align="left"><p>96.3%</p></td></tr><tr><td align="left"><p>Experiment 11</p></td><td align="left"/><td align="left"/><td align="left"><p>✓</p></td><td align="left"/><td align="left"><p>✓</p></td><td align="left"><p>96.0%</p></td><td align="left"><p>96.2%</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec34"><title>Comparison experiment based on our proposed deep-and-shallow loss</title><p>Weight setting and single loss function To explore the effectiveness of our proposed deep-and-shallow loss by controlling the weight of deep-and-shallow loss, we design several sets of comparative experiments to optimize our model. In addition, to verify the effectiveness of the deep-and-shallow loss, we compare the deep-and-shallow loss with the use of a single cross-entropy loss function. The PSTNN network uses DNN + Bi-LSTM and DNN-LSTM, which have better performance in Section <xref rid="Sec33" ref-type="sec">6.6</xref> experiment, and is trained for 200 epochs. The learning rate benchmark is set to 0.001 and is reduced to 1/10 every 20 cycles.</p><p>Table <xref rid="Tab7" ref-type="table">7</xref> shows the experimental results of end-to-end classification based on deep-and-shallow loss weight distribution. When using deep-and-shallow loss to optimize the network, all the results are higher than 97.1% regardless of the weight of loss. Compared with the network model using a single loss, our proposed deep-and-shallow loss is significantly better than the single loss network model, and the maximum increase accuracy is 1%. It shows that by co-optimizing the shallow loss and deep loss, the network can converge better, and the fit of the network can be increased. By setting up a comparative experiment with different weights of deep-and-shallow loss, we found the optimal weight distribution of our proposed PSTNN. When the weight distribution is 0.25:0.5:0.25 and 0.25:0.25:0.5, our network model achieves the best result, reaching 97.5%. The results show that better results can be obtained when appropriate weights are assigned to the network. It may be because Bi-LSTM is more difficult to train and adapt than DNN and CNN networks, so it needs to directly or indirectly increase the training loss weight.
<table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>Comparison experiment of weight selection based on deep-and-shallow loss and single loss</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Loss function and parameters</p></th><th align="left"><p>Network model</p></th><th align="left"><p>End-to-end (ACC)</p></th></tr></thead><tbody><tr><td align="left"><p>Categorical crossentropy loss</p></td><td align="left"><p>DNN-LSTM</p></td><td align="left"><p>96.5%</p></td></tr><tr><td align="left"/><td align="left"><p>DNN + Bi-LSTM</p></td><td align="left"><p>96.6%</p></td></tr><tr><td align="left"><p>Deep-and-shallow loss (0.25:0.5:0.25)</p></td><td align="left"><p>DNN-LSTM</p></td><td align="left"><p>97.3%</p></td></tr><tr><td align="left"/><td align="left"><p>DNN + Bi-LSTM</p></td><td align="left"><p>97.5%</p></td></tr><tr><td align="left"><p>Deep-and-shallow loss (0.4:0.2:0.4)</p></td><td align="left"><p>DNN-LSTM</p></td><td align="left"><p>97.2%</p></td></tr><tr><td align="left"/><td align="left"><p>DNN + Bi-LSTM</p></td><td align="left"><p>97.2%</p></td></tr><tr><td align="left"><p>Deep-and-shallow loss (0.25:0.25:0.5)</p></td><td align="left"><p>DNN-LSTM</p></td><td align="left"><p>97.1%</p></td></tr><tr><td align="left"/><td align="left"><p>DNN + Bi-LSTM</p></td><td align="left"><p>97.5%</p></td></tr><tr><td align="left"><p>Deep-and-shallow loss (0.2:0.6:0.2)</p></td><td align="left"><p>DNN-LSTM</p></td><td align="left"><p>97.3%</p></td></tr><tr><td align="left"/><td align="left"><p>DNN + Bi-LSTM</p></td><td align="left"><p>97.3%</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec35"><title>The attention mechanism verifies the effectiveness of the combination of temporal and spatial features</title><p>To verify the influence of the attention mechanism on our PSTNN network. We design a comparison experiment between the splicing feature and feature using the attention mechanism. The splicing feature network removes the attention module based on the PSTNN, splice the SFENN and TFENN features before and after. The experimental parameters are consistent with those in Section <xref rid="Sec30" ref-type="sec">6.3</xref>.</p><p>It can be seen from Table <xref rid="Tab8" ref-type="table">8</xref> that 0.2% improves the accuracy of the attention mechanism compared with the accuracy of the simple splicing network structure. It proves that the attention mechanism is effective for different types of feature weight distribution. On the one hand, the convolutional layer in the attention mechanism provides additional learning parameters, thereby improving the fitting ability of the network. On the other hand, the attention mechanism achieves feature selection, which provides greater weight for essential features and leads to better results.
<table-wrap id="Tab8"><label>Table 8</label><caption xml:lang="en"><p>Comparative experiment based on the effectiveness of the attention mechanism</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Network model</p></th><th align="left"><p>End-to-end (ACC)</p></th></tr></thead><tbody><tr><td align="left"><p>DNN + Bi-LSTM + splicing</p></td><td align="left"><p>97.3%</p></td></tr><tr><td align="left"><p>DNN + Bi-LSTM + attention</p></td><td align="left"><p>97.5%</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec36"><title>Compare the end-to-end model with the baseline and previous work</title><p>To verify that the end-to-end model we proposed is better than the traditional model and previous work [<xref ref-type="bibr" rid="CR14">14</xref>], we design an experiment to compare the end-to-end network with BED-SVM, GMM-UBM, GSV-SVM, and previous work. The PSTNN in the end-to-end network contains a combination of DNN + Bi-LSTM, and the parameters are the same as in Section <xref rid="Sec30" ref-type="sec">6.3</xref>. The BED+SVM model is the same as the parameters in [<xref ref-type="bibr" rid="CR29">29</xref>].</p><p>It can be seen from Table <xref rid="Tab9" ref-type="table">9</xref>. The results show that this work is 66% higher than the GMM-UBM model and is 4% higher than the GSV-SVM model, which is 5% higher than the BED-SVM model. It proves that our end-to-end framework performs better than the traditional method, which shows that our end-to-end framework can better play the performance of the entire model compared to the traditional model by jointly optimizing model weights. In addition, The number of features of the work in this paper is lower than the previous work, but the accuracy is indeed improved, which more intuitively proves our contribution compared to the previous work.
<table-wrap id="Tab9"><label>Table 9</label><caption xml:lang="en"><p>Verification of effectiveness based on end-to-end structure</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Model</p></th><th align="left"><p>Features</p></th><th align="left"><p>Classification results (ACC)</p></th><th align="left"><p>Testing time (second)</p></th></tr></thead><tbody><tr><td align="left"><p>GMM-UBM</p></td><td align="left"><p>MFCC</p></td><td align="left"><p>31.5%</p></td><td align="left"><p>0.0677</p></td></tr><tr><td align="left"><p>GSV-SVM</p></td><td align="left"><p>GSV</p></td><td align="left"><p>93.6%</p></td><td align="left"><p>0.0519</p></td></tr><tr><td align="left"><p>BED-SVM</p></td><td align="left"><p>BED</p></td><td align="left"><p>92.5%</p></td><td align="left"><p>0.0026</p></td></tr><tr><td align="left"><p>previous work</p></td><td align="left"><p>MFCC,GSV,I-vector</p></td><td align="left"><p>97.3%</p></td><td align="left"><p>0.0017</p></td></tr><tr><td align="left"><p>this work</p></td><td align="left"><p>MFCC,GSV</p></td><td align="left"><p>97.5%</p></td><td align="left"><p>0.0213</p></td></tr></tbody></table></table-wrap></p><p>In terms of test time complexity, the index of this paper adopts the time of testing a single sample. As can be seen from Table <xref rid="Tab9" ref-type="table">9</xref>, the test time complexity of this paper is ahead of the GMM-UBM model and GSV-SVM model, which is faster than the GMM-UBM model test time. It is 0.046 s faster, which is 0.03 s faster than the GSV-SVM model. The test time complexity of this work is 0.02 s behind the previous work and 0.019 s behind the BED-SVM model. It proves that the work of this paper achieves a better accuracy improvement when the time complexity gap is not significant.</p></sec><sec id="Sec37"><title>Discussion</title><p>In this section, we design six sets of experiments based on our four contributions. The first set of experiments verified the influence of the number of UBM on the entire model. The experiment proved that the 64-dimensional UBM had the best effect. The second set of experiments discussed the impact of the back-end classification network on the model. The experiment proved that the two-layer full connection is the most efficient. The third set of experiments mainly discusses spatial and temporal feature extraction networks and compares them with a single network, and the experiment proves that our proposed combination of spatial and temporal features achieves the best results. The fourth group of experiments mainly conduct experiments on the optimal weight distribution of deep-and-shallow loss and compare it with a single loss. The experimental results show that our deep-and-shallow loss has better performance than a single loss. The fifth set of experiments compares the attention mechanism’s effect, which proves that the attention mechanism achieves the best results. The sixth set of experiments verifies the effectiveness of the end-to-end model from an overall perspective by comparing it with traditional methods and previous work.</p></sec></sec><sec id="Sec38" sec-type="conclusion"><title>Conclusions</title><p>This paper proposes a device source identification method based on an end-to-end framework, which combines spatial and temporal information. First, we propose a fusion system of spatial and temporal features, which combines the spatial and temporal features in device source identification. Secondly, we proposed a deep-and-shallow loss and discussed the optimal weight of these loss combinations. Third, we add the attention mechanism to the fusion of spatial and temporal information. Fourth, we have established an end-to-end device source identification task, using deep-and-shallow optimization to make the system more compact and mobile. In summary, after our experimental exploration, the final solution to the problem in this article is the combination of DNN + Bi-LSTM for recording device source identification.</p><p>Although the work in this paper has achieved good results, the computational cost is relatively high. On the one hand, due to LSTM and Bi-LSTM networks in the temporal feature extraction module, they perform well in processing temporal data. However, their parameters are extensive, resulting in too long training time. On the other hand, the structure of the model is also more complicated. Therefore, future work will study how to improve the accuracy of device source identification while reducing computational costs.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors acknowledge the comments by anonymous reviewers that helped to improve a preliminary version of the paper.</p></ack><sec sec-type="author-contribution"><title>Authors’ contributions</title><p>Equal contribution from all authors. All authors read and approved the final manuscript.</p></sec><sec><title>Funding</title><p>This research was supported by National Natural Science Foundation of China (No.61901165, 61501199), Science and Technology Research Project of Hubei Education Department (No. Q20191406), Hubei Natural Science Foundation (No. 2017CFB683), and self-determined research funds of CCNU from the colleges basic research and operation of MOE (No. CCNU20ZT010).</p></sec><sec sec-type="data-availability"><title>Availability of data and materials</title><p>Please contact authors for data requests.</p></sec><sec sec-type="ethics-statement"><title>Declarations</title><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p>The authors declare that they have no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Narkhede</surname><given-names>M.</given-names></name><name><surname>Patole</surname><given-names>R.</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Wang</surname><given-names>J.</given-names></name><name><surname>Reddy</surname><given-names>G. R. M.</given-names></name><name><surname>Prasad</surname><given-names>V. K.</given-names></name><name><surname>Reddy</surname><given-names>V. S.</given-names></name></person-group><article-title xml:lang="en">Acoustic scene identification for audio authentication</article-title><source>Soft computing and signal processing</source><year>2019</year><publisher-loc>Singapore</publisher-loc><publisher-name>Springer</publisher-name><fpage>593</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1007/978-981-13-3600-3_56</pub-id></mixed-citation></ref><ref id="CR2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maher</surname><given-names>R. C.</given-names></name></person-group><article-title xml:lang="en">Audio forensic examination</article-title><source>IEEE Signal Proc. Mag.</source><year>2009</year><volume>26</volume><fpage>84</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1109/MSP.2008.931080</pub-id></mixed-citation></ref><ref id="CR3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinebach</surname><given-names>M.</given-names></name><name><surname>Dittmann</surname><given-names>J.</given-names></name></person-group><article-title xml:lang="en">Watermarking-based digital audio data authentication</article-title><source>EURASIP J. Adv. Sig. Process.</source><year>2003</year><volume>2003</volume><fpage>1001</fpage><lpage>1015</lpage></mixed-citation></ref><ref id="CR4"><label>4</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Garcia-Romero</surname><given-names>D.</given-names></name><name><surname>Espy-Wilson</surname><given-names>C. Y.</given-names></name></person-group><article-title xml:lang="en">Automatic acquisition device identification from speech recordings</article-title><source>2010 IEEE International Conference on Acoustics, Speech and Signal Processing</source><year>2010</year><publisher-loc>New Jersey</publisher-loc><publisher-name>IEEE</publisher-name><fpage>1806</fpage><lpage>1809</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2010.5495407</pub-id></mixed-citation></ref><ref id="CR5"><label>5</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hadoltikar</surname><given-names>V. A.</given-names></name><name><surname>Ratnaparkhe</surname><given-names>V. R.</given-names></name><name><surname>Kumar</surname><given-names>R.</given-names></name></person-group><article-title xml:lang="en">Optimization of mfcc parameters for mobile phone recognition from audio recordings</article-title><source>2019 3rd International Conference on Electronics, Communication and Aerospace Technology (ICECA)</source><year>2019</year><publisher-loc>New Jersey</publisher-loc><publisher-name>IEEE</publisher-name><fpage>777</fpage><lpage>780</lpage><pub-id pub-id-type="doi">10.1109/ICECA.2019.8822177</pub-id></mixed-citation></ref><ref id="CR6"><label>6</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>L.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Huang</surname><given-names>T.</given-names></name></person-group><article-title xml:lang="en">Automatic cell phone recognition from speech recordings</article-title><source>2014 IEEE China Summit International Conference on Signal and Information Processing (ChinaSIP)</source><year>2014</year><publisher-loc>New Jersey</publisher-loc><publisher-name>IEEE</publisher-name><fpage>621</fpage><lpage>625</lpage><pub-id pub-id-type="doi">10.1109/ChinaSIP.2014.6889318</pub-id></mixed-citation></ref><ref id="CR7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanilci</surname><given-names>C.</given-names></name><name><surname>Ertas</surname><given-names>F.</given-names></name><name><surname>Ertas</surname><given-names>T.</given-names></name><name><surname>Eskidere</surname><given-names>.</given-names></name></person-group><article-title xml:lang="en">Recognition of brand and models of cell-phones from recorded speech signals</article-title><source>IEEE Trans. Inf. Forensic. Secur.</source><year>2012</year><volume>7</volume><fpage>625</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2011.2178403</pub-id></mixed-citation></ref><ref id="CR8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanili</surname><given-names>C.</given-names></name><name><surname>Kinnunen</surname><given-names>T.</given-names></name></person-group><article-title xml:lang="en">Source cell-phone recognition from recorded speech using non-speech segments</article-title><source>Digit. Sig. Process.</source><year>2014</year><volume>35</volume><fpage>75</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.dsp.2014.08.008</pub-id></mixed-citation></ref><ref id="CR9"><label>9</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia-Romero</surname><given-names>D.</given-names></name><name><surname>Espy-Wilson</surname><given-names>C.</given-names></name></person-group><article-title xml:lang="en">Speech forensics: automatic acquisition device identification</article-title><source>J. Acoust. Soc. Am.</source><year>2010</year><volume>127</volume><fpage>2044</fpage><pub-id pub-id-type="doi">10.1121/1.3385386</pub-id></mixed-citation></ref><ref id="CR10"><label>10</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kotropoulos</surname><given-names>C.</given-names></name><name><surname>Samaras</surname><given-names>S.</given-names></name></person-group><article-title xml:lang="en">Mobile phone identification using recorded speech signals</article-title><source>2014 19th International Conference on Digital Signal Processing</source><year>2014</year><publisher-loc>New Jersey</publisher-loc><publisher-name>IEEE</publisher-name><fpage>586</fpage><lpage>591</lpage><pub-id pub-id-type="doi">10.1109/ICDSP.2014.6900732</pub-id></mixed-citation></ref><ref id="CR11"><label>11</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldini</surname><given-names>G.</given-names></name><name><surname>Amerini</surname><given-names>I.</given-names></name></person-group><article-title xml:lang="en">Smartphones identification through the built-in microphones with convolutional neural network</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>158685</fpage><lpage>158696</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2950859</pub-id></mixed-citation></ref><ref id="CR12"><label>12</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Zhang</surname><given-names>Y.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name></person-group><article-title xml:lang="en">Mobile phone clustering from speech recordings using deep representation and spectral clustering</article-title><source>IEEE Trans. Inf. Forensic. Secur.</source><year>2018</year><volume>13</volume><fpage>965</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2017.2774505</pub-id></mixed-citation></ref><ref id="CR13"><label>13</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashraf</surname><given-names>M.</given-names></name><name><surname>Geng</surname><given-names>G.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Ahmad</surname><given-names>F.</given-names></name><name><surname>Abid</surname><given-names>F.</given-names></name></person-group><article-title xml:lang="en">A globally regularized joint neural architecture for music classification</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>220980</fpage><lpage>220989</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2020.3043142</pub-id></mixed-citation></ref><ref id="CR14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>C.</given-names></name><name><surname>Zhu</surname><given-names>D.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Wang</surname><given-names>Z.</given-names></name><name><surname>Zhao</surname><given-names>N.</given-names></name><name><surname>He</surname><given-names>L.</given-names></name></person-group><article-title xml:lang="en">An end-to-end deep source recording device identification system for web media forensics</article-title><source>Int. J. Web Inf. Syst.</source><year>2020</year><volume>16</volume><issue>4</issue><fpage>413</fpage><lpage>425</lpage><pub-id pub-id-type="doi">10.1108/IJWIS-06-2020-0038</pub-id></mixed-citation></ref><ref id="CR15"><label>15</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kraetzer</surname><given-names>C.</given-names></name><name><surname>Oermann</surname><given-names>A.</given-names></name><name><surname>Dittmann</surname><given-names>J.</given-names></name><name><surname>Lang</surname><given-names>A.</given-names></name></person-group><article-title xml:lang="en">Digital audio forensics: a first practical evaluation on microphone and environment classification</article-title><source>Workshop on Multimedia &amp; Security</source><year>2007</year><publisher-loc>New York</publisher-loc><publisher-name>ACM Press</publisher-name><fpage>63</fpage><lpage>74</lpage></mixed-citation></ref><ref id="CR16"><label>16</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>T.</given-names></name><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Yan</surname><given-names>D.</given-names></name><name><surname>Lin</surname><given-names>L.</given-names></name></person-group><article-title xml:lang="en">Source cell-phone identification in the presence of additive noise from CQT domain</article-title><source>Inf. (Switzerland)</source><year>2018</year><volume>9</volume><fpage>205</fpage></mixed-citation></ref><ref id="CR17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eskidere</surname><given-names>m.</given-names></name></person-group><article-title xml:lang="en">Source digital voice recorder identification by wavelet analysis</article-title><source>Int. J. Artif. Intell. Tools</source><year>2016</year><volume>25</volume><fpage>1</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1142/S0218213016500160</pub-id></mixed-citation></ref><ref id="CR18"><label>18</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hanilçi</surname><given-names>C.</given-names></name><name><surname>Ertas</surname><given-names>F.</given-names></name></person-group><article-title xml:lang="en">Optimizing acoustic features for source cell-phone recognition using speech signals</article-title><source>Acm Workshop on Information Hiding &amp; Multimedia Security</source><year>2013</year><publisher-loc>New York</publisher-loc><publisher-name>ACM Press</publisher-name><fpage>141</fpage><lpage>148</lpage><pub-id pub-id-type="doi">10.1145/2482513.2482520</pub-id></mixed-citation></ref><ref id="CR19"><label>19</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Aggarwal</surname><given-names>R.</given-names></name><name><surname>Singh</surname><given-names>S.</given-names></name><name><surname>Roul</surname><given-names>A. K.</given-names></name><name><surname>Khanna</surname><given-names>N.</given-names></name></person-group><article-title xml:lang="en">Cellphone identification using noise estimates from recorded audio</article-title><source>2014 International Conference on Communication and Signal Processing</source><year>2014</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>1218</fpage><lpage>1222</lpage><pub-id pub-id-type="doi">10.1109/ICCSP.2014.6950045</pub-id></mixed-citation></ref><ref id="CR20"><label>20</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>L.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Li</surname><given-names>Y.</given-names></name></person-group><article-title xml:lang="en">Source cell phone matching from speech recordings by sparse representation and kiss metric</article-title><source>2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><year>2016</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>2079</fpage><lpage>2083</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2016.7472043</pub-id></mixed-citation></ref><ref id="CR21"><label>21</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jin</surname><given-names>C.</given-names></name><name><surname>Wang</surname><given-names>R.</given-names></name><name><surname>Yan</surname><given-names>D.</given-names></name></person-group><article-title xml:lang="en">Source smartphone identification by exploiting encoding characteristics of recorded speech</article-title><source>Digit. Investig.</source><year>2019</year><volume>29</volume><fpage>129</fpage><lpage>146</lpage><pub-id pub-id-type="doi">10.1016/j.diin.2019.03.003</pub-id></mixed-citation></ref><ref id="CR22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>W. M.</given-names></name><name><surname>Sturim</surname><given-names>D. E.</given-names></name><name><surname>Reynolds</surname><given-names>D. A.</given-names></name></person-group><article-title xml:lang="en">Support vector machines using gmm supervectors for speaker verification</article-title><source>IEEE Signal Proc. Lett.</source><year>2006</year><volume>13</volume><fpage>308</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1109/LSP.2006.870086</pub-id></mixed-citation></ref><ref id="CR23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>Y.</given-names></name><name><surname>Leung</surname><given-names>F. H. F.</given-names></name></person-group><article-title xml:lang="en">Source microphone recognition aided by a kernel-based projection method</article-title><source>IEEE Trans. Inf. Forensic. Secur.</source><year>2019</year><volume>14</volume><fpage>2875</fpage><lpage>2886</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2019.2911175</pub-id></mixed-citation></ref><ref id="CR24"><label>24</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Li</surname><given-names>X.</given-names></name><name><surname>Feng</surname><given-names>X.</given-names></name><name><surname>Yang</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>A.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name></person-group><article-title xml:lang="en">Mobile phone clustering from acquired speech recordings using deep Gaussian supervector and spectral clustering</article-title><source>2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><year>2017</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>2137</fpage><lpage>2141</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2017.7952534</pub-id></mixed-citation></ref><ref id="CR25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>X.</given-names></name><name><surname>Zhu</surname><given-names>J.</given-names></name><name><surname>Chen</surname><given-names>D.</given-names></name></person-group><article-title xml:lang="en">Subband aware CNN for cell-phone recognition</article-title><source>IEEE Sig. Process Lett.</source><year>2020</year><volume>27</volume><fpage>605</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1109/LSP.2020.2985594</pub-id></mixed-citation></ref><ref id="CR26"><label>26</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>W. M.</given-names></name></person-group><article-title xml:lang="en">Generalized linear discriminant sequence kernels for speaker recognition</article-title><source>2002 IEEE International Conference on Acoustics, Speech, and Signal Processing</source><year>2002</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>161</fpage><lpage>164</lpage></mixed-citation></ref><ref id="CR27"><label>27</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldini</surname><given-names>G.</given-names></name><name><surname>Amerini</surname><given-names>I.</given-names></name><name><surname>Gentile</surname><given-names>C.</given-names></name></person-group><article-title xml:lang="en">Microphone identification using convolutional neural networks</article-title><source>IEEE Sensors Lett.</source><year>2019</year><volume>3</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1109/LSENS.2019.2923590</pub-id></mixed-citation></ref><ref id="CR28"><label>28</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldini</surname><given-names>G.</given-names></name><name><surname>Steri</surname><given-names>G.</given-names></name></person-group><article-title xml:lang="en">A survey of techniques for the identification of mobile phones using the physical fingerprints of the built-in components</article-title><source>IEEE Commun. Surv. Tutorials</source><year>2017</year><volume>19</volume><fpage>1761</fpage><lpage>1789</lpage><pub-id pub-id-type="doi">10.1109/COMST.2017.2694487</pub-id></mixed-citation></ref><ref id="CR29"><label>29</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname><given-names>D.</given-names></name><name><surname>Korus</surname><given-names>P.</given-names></name><name><surname>Huang</surname><given-names>J.</given-names></name></person-group><article-title xml:lang="en">Band energy difference for source attribution in audio forensics</article-title><source>IEEE Trans. Inf. Forensic. Secur.</source><year>2018</year><volume>13</volume><fpage>2179</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1109/TIFS.2018.2812185</pub-id></mixed-citation></ref><ref id="CR30"><label>30</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>L.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name><name><surname>Wu</surname><given-names>J.</given-names></name></person-group><article-title xml:lang="en">Source cell phone verification from speech recordings using sparse representation</article-title><source>Digital Sig. Process</source><year>2016</year><volume>62</volume><fpage>125</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.dsp.2016.10.017</pub-id></mixed-citation></ref><ref id="CR31"><label>31</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zou</surname><given-names>L.</given-names></name><name><surname>He</surname><given-names>Q.</given-names></name><name><surname>Feng</surname><given-names>X.</given-names></name></person-group><article-title xml:lang="en">Cell phone verification from speech recordings using sparse representation</article-title><source>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</source><year>2015</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>1787</fpage><lpage>1791</lpage><pub-id pub-id-type="doi">10.1109/ICASSP.2015.7178278</pub-id></mixed-citation></ref><ref id="CR32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharon</surname><given-names>M.</given-names></name><name><surname>Elad</surname><given-names>M.</given-names></name><name><surname>Bruckstein</surname><given-names>A.</given-names></name></person-group><article-title xml:lang="en">K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation</article-title><source>IEEE Trans. Sig. Process.</source><year>2006</year><volume>54</volume><fpage>4311</fpage><lpage>4322</lpage><pub-id pub-id-type="doi">10.1109/TSP.2006.881199</pub-id></mixed-citation></ref><ref id="CR33"><label>33</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Q.</given-names></name><name><surname>Li</surname><given-names>B.</given-names></name></person-group><article-title xml:lang="en">Discriminative K-SVD for dictionary learning in face recognition</article-title><source>2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</source><year>2010</year><publisher-loc>New York</publisher-loc><publisher-name>IEEE</publisher-name><fpage>2691</fpage><lpage>2698</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2010.5539989</pub-id></mixed-citation></ref><ref id="CR34"><label>34</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>X.</given-names></name><name><surname>Shi</surname><given-names>J.</given-names></name><name><surname>Luo</surname><given-names>P.</given-names></name><name><surname>Wang</surname><given-names>X.</given-names></name><name><surname>Tang</surname><given-names>X.</given-names></name></person-group><article-title xml:lang="en">Spatial as deep: spatial CNN for traffic scene understanding</article-title><source>2018 AAAI Conference on Artificial Intelligence</source><year>2017</year><publisher-loc>Palo Alto</publisher-loc><publisher-name>AAAI</publisher-name><fpage>7276</fpage><lpage>7283</lpage></mixed-citation></ref><ref id="CR35"><label>35</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>He</surname><given-names>K.</given-names></name><name><surname>Zhang</surname><given-names>X.</given-names></name><name><surname>Ren</surname><given-names>S.</given-names></name><name><surname>Sun</surname><given-names>J.</given-names></name></person-group><article-title xml:lang="en">Deep residual learning for image recognition</article-title><source>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</source><year>2016</year><publisher-loc>New Jersey</publisher-loc><publisher-name>IEEE</publisher-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></mixed-citation></ref><ref id="CR36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>Y.</given-names></name><name><surname>Liang</surname><given-names>R.</given-names></name><name><surname>Liang</surname><given-names>Z.</given-names></name><name><surname>Huang</surname><given-names>C.</given-names></name><name><surname>Zou</surname><given-names>C.</given-names></name><name><surname>Schuller</surname><given-names>B.</given-names></name></person-group><article-title xml:lang="en">Speech emotion classification using attention-based LSTM</article-title><source>IEEE/ACM Trans. Audio Speech Lang. Process.</source><year>2019</year><volume>27</volume><fpage>1675</fpage><lpage>1685</lpage><pub-id pub-id-type="doi">10.1109/TASLP.2019.2925934</pub-id></mixed-citation></ref><ref id="CR37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>W. M.</given-names></name><name><surname>Sturim</surname><given-names>D. E.</given-names></name><name><surname>Reynolds</surname><given-names>D. A.</given-names></name></person-group><article-title xml:lang="en">Support vector machines using GMM supervectors for speaker verification</article-title><source>IEEE Sign. Process. Lett.</source><year>2006</year><volume>13</volume><issue>5</issue><fpage>308</fpage><lpage>311</lpage><pub-id pub-id-type="doi">10.1109/LSP.2006.870086</pub-id><comment>https://doi.org/10.1109/LSP.2006.870086</comment></mixed-citation></ref></ref-list></ref-list><glossary><title>Abbreviations</title><def-list><def-item><term>CNN</term><def><p>Convolutional Neural Networks</p></def></def-item><def-item><term>DNN</term><def><p>Deep Neural Networks</p></def></def-item><def-item><term>LSTM</term><def><p>Long Short-Term Memory</p></def></def-item><def-item><term>SVM</term><def><p>Support Vector Machine</p></def></def-item><def-item><term>MFCC</term><def><p>Mel Frequency Cepstral Coefficient</p></def></def-item><def-item><term>BFCC</term><def><p>Bark Frequency Cepstrum Coefficient</p></def></def-item><def-item><term>LPCC</term><def><p>Linear Prediction Cepstrum Coefficient</p></def></def-item><def-item><term>PNCC</term><def><p>Power Normalized Cepstrum Coefficient</p></def></def-item><def-item><term>GSV</term><def><p>Gaussian Super Vector</p></def></def-item><def-item><term>GMM</term><def><p>Gaussian Mixture Model</p></def></def-item><def-item><term>SRC</term><def><p>Sparse Representation-based Classifier</p></def></def-item><def-item><term>RBF</term><def><p>Radial Basis Function kernel</p></def></def-item><def-item><term>GLDS</term><def><p>Generalized Linear Discriminant Sequence kernel</p></def></def-item><def-item><term>PSTNN</term><def><p>Parallel spatial-temporal network</p></def></def-item></def-list></glossary><notes notes-type="Misc"><title>Publisher’s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Engineering</facet-value><facet-value count="1">Quantum Information Technology, Spintronics</facet-value><facet-value count="1">Signal,Image and Speech Processing</facet-value></facet><facet name="keyword"><facet-value count="1">Convolution neural network (CNN)</facet-value><facet-value count="1">Long Short-Term Memory (LSTM)</facet-value><facet-value count="1">Spatial features</facet-value><facet-value count="1">Temporal features</facet-value></facet><facet name="pub"><facet-value count="1">EURASIP Journal on Advances in Signal Processing</facet-value></facet><facet name="year"><facet-value count="1">2021</facet-value></facet><facet name="country"><facet-value count="1">China</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
