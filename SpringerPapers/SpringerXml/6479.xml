<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1038/s41524-022-00734-6</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="review-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">41524</journal-id><journal-id journal-id-type="doi">10.1038/41524.2057-3960</journal-id><journal-title-group><journal-title>npj Computational Materials</journal-title><abbrev-journal-title abbrev-type="publisher">npj Comput Mater</abbrev-journal-title></journal-title-group><issn pub-type="epub">2057-3960</issn><publisher><publisher-name>Nature Publishing Group UK</publisher-name><publisher-loc>London</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s41524-022-00734-6</article-id><article-id pub-id-type="manuscript">734</article-id><article-id pub-id-type="doi">10.1038/s41524-022-00734-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Review Article</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1037</subject></subj-group><subj-group subj-group-type="SubjectPath"><subject>/639/301/1034/1035</subject></subj-group><subj-group subj-group-type="NatureArticleTypeID"><subject>review-article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Recent advances and applications of deep learning methods in materials science</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-9737-8074</contrib-id><name><surname>Choudhary</surname><given-names>Kamal</given-names></name><address><email>kamal.choudhary@nist.gov</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="aff" rid="Aff3">3</xref><xref ref-type="corresp" rid="IDs41524022007346_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-3459-5888</contrib-id><name><surname>DeCost</surname><given-names>Brian</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><contrib contrib-type="author" id="Au3"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8008-7043</contrib-id><name><surname>Chen</surname><given-names>Chi</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au4"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5893-9967</contrib-id><name><surname>Jain</surname><given-names>Anubhav</given-names></name><xref ref-type="aff" rid="Aff6">6</xref></contrib><contrib contrib-type="author" id="Au5"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-5602-180X</contrib-id><name><surname>Tavazza</surname><given-names>Francesca</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au6"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-7898-0059</contrib-id><name><surname>Cohn</surname><given-names>Ryan</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" id="Au7"><name><surname>Park</surname><given-names>Cheol Woo</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><contrib contrib-type="author" id="Au8"><name><surname>Choudhary</surname><given-names>Alok</given-names></name><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author" id="Au9"><name><surname>Agrawal</surname><given-names>Ankit</given-names></name><xref ref-type="aff" rid="Aff9">9</xref></contrib><contrib contrib-type="author" id="Au10"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9734-4998</contrib-id><name><surname>Billinge</surname><given-names>Simon J. L.</given-names></name><xref ref-type="aff" rid="Aff10">10</xref></contrib><contrib contrib-type="author" id="Au11"><name><surname>Holm</surname><given-names>Elizabeth</given-names></name><xref ref-type="aff" rid="Aff7">7</xref></contrib><contrib contrib-type="author" id="Au12"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-5726-2587</contrib-id><name><surname>Ong</surname><given-names>Shyue Ping</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><contrib contrib-type="author" id="Au13"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0003-2248-474X</contrib-id><name><surname>Wolverton</surname><given-names>Chris</given-names></name><xref ref-type="aff" rid="Aff8">8</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.94225.38</institution-id><institution-id institution-id-type="ISNI">000000012158463X</institution-id><institution content-type="org-division">Materials Science and Engineering Division</institution><institution content-type="org-name">National Institute of Standards and Technology</institution></institution-wrap><addr-line content-type="postcode">20899</addr-line><addr-line content-type="city">Gaithersburg</addr-line><addr-line content-type="state">MD</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.421663.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 7432 9327</institution-id><institution content-type="org-name">Theiss Research</institution></institution-wrap><addr-line content-type="postcode">92037</addr-line><addr-line content-type="city">La Jolla</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution content-type="org-name">DeepMaterials LLC</institution></institution-wrap><addr-line content-type="postcode">20906</addr-line><addr-line content-type="city">Silver Spring</addr-line><addr-line content-type="state">MD</addr-line><country country="US">USA</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.94225.38</institution-id><institution-id institution-id-type="ISNI">000000012158463X</institution-id><institution content-type="org-division">Material Measurement Science Division</institution><institution content-type="org-name">National Institute of Standards and Technology</institution></institution-wrap><addr-line content-type="postcode">20899</addr-line><addr-line content-type="city">Gaithersburg</addr-line><addr-line content-type="state">MD</addr-line><country country="US">USA</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.266100.3</institution-id><institution-id institution-id-type="ISNI">0000 0001 2107 4242</institution-id><institution content-type="org-division">Department of NanoEngineering</institution><institution content-type="org-name">University of California San Diego</institution></institution-wrap><addr-line content-type="postcode">92093</addr-line><addr-line content-type="city">San Diego</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff6"><label>6</label><institution-wrap><institution-id institution-id-type="GRID">grid.184769.5</institution-id><institution-id institution-id-type="ISNI">0000 0001 2231 4551</institution-id><institution content-type="org-division">Energy Technologies Area</institution><institution content-type="org-name">Lawrence Berkeley National Laboratory</institution></institution-wrap><addr-line content-type="city">Berkeley</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff7"><label>7</label><institution-wrap><institution-id institution-id-type="GRID">grid.147455.6</institution-id><institution-id institution-id-type="ISNI">0000 0001 2097 0344</institution-id><institution content-type="org-division">Department of Materials Science and Engineering</institution><institution content-type="org-name">Carnegie Mellon University</institution></institution-wrap><addr-line content-type="postcode">15213</addr-line><addr-line content-type="city">Pittsburgh</addr-line><addr-line content-type="state">PA</addr-line><country country="US">USA</country></aff><aff id="Aff8"><label>8</label><institution-wrap><institution-id institution-id-type="GRID">grid.16753.36</institution-id><institution-id institution-id-type="ISNI">0000 0001 2299 3507</institution-id><institution content-type="org-division">Department of Materials Science and Engineering</institution><institution content-type="org-name">Northwestern University</institution></institution-wrap><addr-line content-type="postcode">60208</addr-line><addr-line content-type="city">Evanston</addr-line><addr-line content-type="state">IL</addr-line><country country="US">USA</country></aff><aff id="Aff9"><label>9</label><institution-wrap><institution-id institution-id-type="GRID">grid.16753.36</institution-id><institution-id institution-id-type="ISNI">0000 0001 2299 3507</institution-id><institution content-type="org-division">Department of Electrical and Computer Engineering</institution><institution content-type="org-name">Northwestern University</institution></institution-wrap><addr-line content-type="postcode">60208</addr-line><addr-line content-type="city">Evanston</addr-line><addr-line content-type="state">IL</addr-line><country country="US">USA</country></aff><aff id="Aff10"><label>10</label><institution-wrap><institution-id institution-id-type="GRID">grid.21729.3f</institution-id><institution-id institution-id-type="ISNI">0000000419368729</institution-id><institution content-type="org-division">Department of Applied Physics and Applied Mathematics and the Data Science Institute, Fu Foundation School of Engineering and Applied Sciences</institution><institution content-type="org-name">Columbia University</institution></institution-wrap><addr-line content-type="postcode">10027</addr-line><addr-line content-type="city">New York</addr-line><addr-line content-type="state">NY</addr-line><country country="US">USA</country></aff></contrib-group><author-notes><corresp id="IDs41524022007346_cor1"><label>a</label><email>kamal.choudhary@nist.gov</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>5</day><month>4</month><year>2022</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2022</year></pub-date><volume>8</volume><issue seq="59">1</issue><elocation-id>59</elocation-id><history><date date-type="registration"><day>1</day><month>3</month><year>2022</year></date><date date-type="received"><day>25</day><month>10</month><year>2021</year></date><date date-type="accepted"><day>24</day><month>2</month><year>2022</year></date><date date-type="online"><day>5</day><month>4</month><year>2022</year></date></history><permissions><copyright-statement content-type="compact">© This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply 2022</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Deep learning (DL) is one of the fastest-growing topics in materials data science, with rapidly emerging applications spanning atomistic, image-based, spectral, and textual data modalities. DL allows analysis of unstructured data and automated identification of features. The recent development of large materials databases has fueled the application of DL methods in atomistic prediction in particular. In contrast, advances in image and spectral data have largely leveraged synthetic data enabled by high-quality forward models as well as by generative unsupervised DL methods. In this article, we present a high-level overview of deep learning methods followed by a detailed discussion of recent developments of deep learning in atomistic simulation, materials imaging, spectral analysis, and natural language processing. For each modality we discuss applications involving both theoretical and experimental data, typical modeling approaches with their strengths and limitations, and relevant publicly available software and datasets. We conclude the review with a discussion of recent cross-cutting work related to uncertainty quantification in this field and a brief perspective on limitations, challenges, and potential growth areas for DL methods in materials science.</p></abstract><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Nature Portfolio</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>59</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>The Author(s)</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2022</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>3</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>NonStandardArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/41524_2022_Article_734.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>ReviewPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Materials Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Materials Science, general</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Characterization and Evaluation of Materials</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical and Computational Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Theoretical, Mathematical and Computational Physics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Modeling and Industrial Mathematics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Chemistry and Materials Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">“Processing-structure-property-performance” is the key mantra in Materials Science and Engineering (MSE)<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. The length and time scales of material structures and phenomena vary significantly among these four elements, adding further complexity<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. For instance, structural information can range from detailed knowledge of atomic coordinates of elements to the microscale spatial distribution of phases (microstructure), to fragment connectivity (mesoscale), to images and spectra. Establishing linkages between the above components is a challenging task.</p><p id="Par3">Both experimental and computational techniques are useful to identify such relationships. Due to rapid growth in automation in experimental equipment and immense expansion of computational resources, the size of public materials datasets has seen exponential growth. Several large experimental and computational datasets<sup><xref ref-type="bibr" rid="CR3">3</xref>–<xref ref-type="bibr" rid="CR10">10</xref></sup> have been developed through the Materials Genome Initiative (MGI)<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> and the increasing adoption of Findable, Accessible, Interoperable, Reusable (FAIR)<sup><xref ref-type="bibr" rid="CR12">12</xref></sup> principles. Such an outburst of data requires automated analysis which can be facilitated by machine learning (ML) techniques<sup><xref ref-type="bibr" rid="CR13">13</xref>–<xref ref-type="bibr" rid="CR20">20</xref></sup>.</p><p id="Par4">Deep learning (DL)<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR22">22</xref></sup> is a specialized branch of machine learning (ML). Originally inspired by biological models of computation and cognition in the human brain<sup><xref ref-type="bibr" rid="CR23">23</xref>,<xref ref-type="bibr" rid="CR24">24</xref></sup>, one of DL’s major strengths is its potential to extract higher-level features from the raw input data.</p><p id="Par5">DL applications are rapidly replacing conventional systems in many aspects of our daily lives, for example, in image and speech recognition, web search, fraud detection, email/spam filtering, financial risk modeling, and so on. DL techniques have been proven to provide exciting new capabilities in numerous fields (such as playing Go<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, self-driving cars<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, navigation, chip design, particle physics, protein science, drug discovery, astrophysics, object recognition<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, etc).</p><p id="Par6">Recently DL methods have been outperforming other machine learning techniques in numerous scientific fields, such as chemistry, physics, biology, and materials science<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR28">28</xref>–<xref ref-type="bibr" rid="CR32">32</xref></sup>. DL applications in MSE are still relatively new, and the field has not fully explored its potential, implications, and limitations. DL provides new approaches for investigating material phenomena and has pushed materials scientists to expand their traditional toolset.</p><p id="Par7">DL methods have been shown to act as a complementary approach to physics-based methods for materials design. While large datasets are often viewed as a prerequisite for successful DL applications, techniques such as transfer learning, multi-fidelity modelling, and active learning can often make DL feasible for small datasets as well<sup><xref ref-type="bibr" rid="CR33">33</xref>–<xref ref-type="bibr" rid="CR36">36</xref></sup>.</p><p id="Par8">Traditionally, materials have been designed experimentally using trial and error methods with a strong dose of chemical intuition. In addition to being a very costly and time-consuming approach, the number of material combinations is so huge that it is intractable to study experimentally, leading to the need for empirical formulation and computational methods. While computational approaches (such as density functional theory, molecular dynamics, Monte Carlo, phase-field, finite elements) are much faster and cheaper than experiments, they are still limited by length and time scale constraints, which in turn limits their respective domains of applicability. DL methods can offer substantial speedups compared to conventional scientific computing, and, for some applications, are reaching an accuracy level comparable to physics-based or computational models.</p><p id="Par9">Moreover, entering a new domain of materials science and performing cutting-edge research requires years of education, training, and the development of specialized skills and intuition. Fortunately, we now live in an era of increasingly open data and computational resources. Mature, well-documented DL libraries make DL research much more easily accessible to newcomers than almost any other research field. Testing and benchmarking methodologies such as underfitting/overfitting/cross-validation<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup> are common knowledge, and standards for measuring model performance are well established in the community.</p><p id="Par10">Despite their many advantages, DL methods have disadvantages too, the most significant one being their black-box nature<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> which may hinder physical insights into the phenomena under examination. Evaluating and increasing the interpretability and explainability of DL models remains an active field of research. Generally a DL model has a few thousand to millions of parameters, making model interpretation and direct generation of scientific insight difficult.</p><p id="Par11">Although there are several good recent reviews of ML applications in MSE<sup><xref ref-type="bibr" rid="CR15">15</xref>–<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR39">39</xref>–<xref ref-type="bibr" rid="CR49">49</xref></sup>, DL for materials has been advancing rapidly, warranting a dedicated review to cover the explosion of research in this field. This article discusses some of the basic principles in DL methods and highlights major trends among the recent advances in DL applications for materials science. As the tools and datasets for DL applications in materials keep evolving, we provide a github repository (<ext-link xlink:href="https://github.com/deepmaterials/dlmatreview" ext-link-type="uri">https://github.com/deepmaterials/dlmatreview</ext-link>) that can be updated as new resources are made publicly available.</p></sec><sec id="Sec2"><title>General machine learning concepts</title><p id="Par12">It is beyond the scope of this article to give a detailed hands-on introduction to Deep Learning. There are many materials for this purpose, for example, the free online book “Neural Networks and Deep Learning” by Michael Nielsen (<ext-link xlink:href="http://neuralnetworksanddeeplearning.com" ext-link-type="uri">http://neuralnetworksanddeeplearning.com</ext-link>), Deep Learning by Goodfellow et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>, and multiple online courses at Coursera, Udemy, and so on. Rather, this article aims to motivate materials scientist researchers in the types of problems that are amenable to DL, and to introduce some of the basic concepts, jargon, and materials-specific databases and software (at the time of writing) as a helpful on-ramp to help get started. With this in mind, we begin with a very basic introduction to Deep learning.</p><p id="Par13">Artificial intelligence (AI)<sup><xref ref-type="bibr" rid="CR13">13</xref></sup> is the development of machines and algorithms that mimic human intelligence, for example, by optimizing actions to achieve certain goals. Machine learning (ML) is a subset of AI, and provides the ability to learn without explicitly being programmed for a given dataset such as playing chess, social network recommendation etc. DL, in turn, is the subset of ML that takes inspiration from biological brains and uses multilayer neural networks to solve ML tasks. A schematic of AI-ML-DL context and some of the key application areas of DL in the materials science and engineering field are shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><title>Schematic showing an overview of artificial intelligence (AI), machine learning (ML), and deep learning (DL) methods and its applications in materials science and engineering.</title><p>Deep learning is considered a part of machine learning, which is contained in an umbrella term artificial intelligence.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_734_Fig1_HTML.png"/></fig></p><p id="Par14">Some of the commonly used ML technologies are linear regression, decision trees, and random forest in which generalized models are trained to learn coefficients/weights/parameters for a given dataset (usually structured i.e., on a grid or a spreadsheet).</p><p id="Par15">Applying traditional ML techniques to unstructured data (such as pixels or features from an image, sounds, text, and graphs) is challenging because users have to first extract generalized meaningful representations or features themselves (such as calculating pair-distribution for an atomic structure) and then train the ML models. Hence, the process becomes time-consuming, brittle, and not easily scalable. Here, deep learning (DL) techniques become more important.</p><p id="Par16">DL methods are based on artificial neural networks and allied techniques. According to the “universal approximation theorem”<sup><xref ref-type="bibr" rid="CR50">50</xref>,<xref ref-type="bibr" rid="CR51">51</xref></sup>, neural networks can approximate any function to arbitrary accuracy. However, it is important to note that the theorem doesn’t guarantee that the functions can be learnt easily<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>.</p></sec><sec id="Sec3"><title>Neural networks</title><sec id="Sec4"><title>Perceptron</title><p id="Par17">A perceptron or a single artificial neuron<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> is the building block of artificial neural networks (ANNs) and performs forward propagation of information. For a set of inputs [<italic>x</italic><sub>1</sub>, <italic>x</italic><sub>2</sub>, . . . , <italic>x</italic><sub><italic>m</italic></sub>] to the perceptron, we assign floating number weights (and biases to shift wights) [<italic>w</italic><sub>1</sub>, <italic>w</italic><sub>2</sub>, . . . , <italic>w</italic><sub><italic>m</italic></sub>] and then we multiply them correspondingly together to get a sum of all of them. Some of the common software packages allowing NN trainings are: PyTorch<sup><xref ref-type="bibr" rid="CR54">54</xref></sup>, Tensorflow<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>, and MXNet<sup><xref ref-type="bibr" rid="CR56">56</xref></sup>. Please note that certain commercial equipment, instruments, or materials are identified in this paper in order to specify the experimental procedure adequately. Such identification is not intended to imply recommendation or endorsement by NIST, nor is it intended to imply that the materials or equipment identified are necessarily the best available for the purpose.</p></sec><sec id="Sec5"><title>Activation function</title><p id="Par18">Activation functions (such as sigmoid, hyperbolic tangent (tanh), rectified linear unit (ReLU), leaky ReLU, Swish) are the critical nonlinear components that enable neural networks to compose many small building blocks to learn complex nonlinear functions. For example, the sigmoid activation maps real numbers to the range (0, 1); this activation function is often used in the last layer of binary classifiers to model probabilities. The choice of activation function can affect training efficiency as well as final accuracy<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>.</p></sec><sec id="Sec6"><title>Loss function, gradient descent, and normalization</title><p id="Par19">The weight matrices of a neural network are initialized randomly or obtained from a pre-trained model. These weight matrices are multiplied with the input matrix (or output from a previous layer) and subjected to a nonlinear activation function to yield updated representations, which are often referred to as activations or feature maps. The loss function (also known as an objective function or empirical risk) is calculated by comparing the output of the neural network and the known target value data. Typically, network weights are iteratively updated via stochastic gradient descent algorithms to minimize the loss function until the desired accuracy is achieved. Most modern deep learning frameworks facilitate this by using reverse-mode automatic differentiation<sup><xref ref-type="bibr" rid="CR58">58</xref></sup> to obtain the partial derivatives of the loss function with respect to each network parameter through recursive application of the chain rule. Colloquially, this is also known as back-propagation.</p><p id="Par20">Common gradient descent algorithms include: Stochastic Gradient Descent (SGD), Adam, Adagrad etc. The learning rate is an important parameter in gradient descent. Except for SGD, all other methods use adaptive learning parameter tuning. Depending on the objective such as classification or regression, different loss functions such as Binary Cross Entropy (BCE), Negative Log likelihood (NLLL) or Mean Squared Error (MSE) are used.</p><p id="Par21">The inputs of a neural network are generally scaled i.e., normalized to have zero mean and unit standard deviation. Scaling is also applied to the input of hidden layers (using batch or layer normalization) to improve the stability of ANNs.</p></sec><sec id="Sec7"><title>Epoch and mini-batches</title><p id="Par22">A single pass of the entire training data is called an epoch, and multiple epochs are performed until the weights converge. In DL, datasets are usually large and computing gradients for the entire dataset and network becomes challenging. Hence, the forward passes are done with small subsets of the training data called mini-batches.</p></sec><sec id="Sec8"><title>Underfitting, overfitting, regularization, and early stopping</title><p id="Par23">During an ML training, the dataset is split into training, validation, and test sets. The test set is never used during the training process. A model is said to be underfitting if the model performs poorly on the training set and lacks the capacity to fully learn the training data. A model is said to overfit if the model performs too well on the training data but does not perform well on the validation data. Overfitting is controlled with regularization techniques such as L2 regularization, dropout, and early stopping<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>.</p><p id="Par24">Regularization discourages the model from simply memorizing the training data, resulting in a model that is more generalizable. Overfitting models are often characterized by neurons that have weights with large magnitudes. L2 regularization reduces the possibility of overfitting by adding an additional term to the loss function that penalizes the large weight values, keeping the values of the weights and biases small during training. Another popular regularization is dropout<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> in which we randomly set the activations for an NN layer to zero during training. Similar to bagging<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, the use of dropout brings about the same effect of training a collection of randomly chosen models which prevents the co-adaptations among the neurons, consequently reducing the likelihood of the model from overfitting. In early stopping, further epochs for training are stopped before the model overfits i.e., accuracy on the validation set flattens or decreases.</p></sec></sec><sec id="Sec9"><title>Convolutional neural networks</title><p id="Par25">Convolutional neural networks (CNN)<sup><xref ref-type="bibr" rid="CR61">61</xref></sup> can be viewed as a regularized version of multilayer perceptrons with a strong inductive bias for learning translation-invariant image representations. There are four main components in CNNs: (a) learnable convolution filterbanks, (b) nonlinear activations, (c) spatial coarsening (via pooling or strided convolution), (d) a prediction module, often consisting of fully connected layers that operate on a global instance representation.</p><p id="Par26">In CNNs we use convolution functions with multiple kernels or filters with trainable and shared weights or parameters, instead of general matrix multiplication. These filters/kernels are matrices with a relatively small number of rows and columns that convolve over the input to automatically extract high-level local features in the form of feature maps. The filters slide/convolve (element-wise multiply) across the input with a fixed number of strides to produce the feature map and the information thus learnt is passed to the hidden/fully connected layers. Depending on the input data, these filters can be one, two, or three-dimensional.</p><p id="Par27">Similar to the fully connected NNs, nonlinearities such as ReLU are then applied that allows us to deal with nonlinear and complicated data. The pooling operation preserves spatial invariance, downsamples and reduces the dimension of each feature map obtained after convolution. These downsampling/pooling operations can be of different types such as maximum-pooling, minimum-pooling, average pooling, and sum pooling. After one or more convolutional and pooling layers, the outputs are usually reduced to a one-dimensional global representation. CNNs are especially popular for image data.</p></sec><sec id="Sec10"><title>Graph neural networks</title><sec id="Sec11"><title>Graphs and their variants</title><p id="Par28">Classical CNNs as described above are based on a regular grid Euclidean data (such as 2D grid in images). However, real-life data structures, such as social networks, segments of images, word vectors, recommender systems, and atomic/molecular structures, are usually non-Euclidean. In such cases, graph-based non-Euclidean data structures become especially important.</p><p id="Par29">Mathematically, a graph <italic>G</italic> is defined as a set of nodes/vertices <italic>V</italic>, a set of edges/links, <italic>E</italic> and node features, <italic>X</italic>: <italic>G</italic> = (<italic>V</italic>, <italic>E</italic>, <italic>X</italic>)<sup><xref ref-type="bibr" rid="CR62">62</xref>–<xref ref-type="bibr" rid="CR64">64</xref></sup> and can be used to represent non-Euclidean data. An edge is formed between a pair of two nodes and contains the relation information between the nodes. Each node and edge can have attributes/features associated with it. An adjacency matrix <italic>A</italic> is a square matrix indicating connections between the nodes or not in the form of 1 (connected) and 0 (unconnected). A graph can be of various types such as: undirected/directed, weighted/unweighted, homogeneous/heterogeneous, static/dynamic.</p><p id="Par30">An undirected graph captures symmetric relations between nodes, while a directed one captures asymmetric relations such that <italic>A</italic><italic>i</italic><italic>j</italic> ≠ <italic>A</italic><italic>j</italic><italic>i</italic>. In a weighted graph, each edge is associated with a scalar weight rather than just 1s and 0s. In a homogeneous graph, all the nodes represent instances of the same type, and all the edges capture relations of the same type while in a heterogeneous graph, the nodes and edges can be of different types. Heterogeneous graphs provide an easy interface for managing nodes and edges of different types as well as their associated features. When input features or graph topology vary with time, they are called dynamic graphs otherwise they are considered static. If a node is connected to another node more than once it is termed a multi-graph.</p></sec><sec id="Sec12"><title>Types of GNNs</title><p id="Par31">At present, GNNs are probably the most popular AI method for predicting various materials properties based on structural information<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR65">65</xref>–<xref ref-type="bibr" rid="CR69">69</xref></sup>. Graph neural networks (GNNs) are DL methods that operate on graph domain and can capture the dependence of graphs via message passing between the nodes and edges of graphs. There are two key steps in GNN training: (a) we first aggregate information from neighbors and (b) update the nodes and/or edges. Importantly, aggregation is permutation invariant. Similar to the fully connected NNs, the input node features, X (with embedding matrix) are multiplied with the adjacency matrix and the weight matrices and then multiplied with the nonlinear activation function to provide outputs for the next layer. This method is called the propagation rule.</p><p id="Par32">Based on the propagation rule and aggregation methodology, there could be different variants of GNNs such as Graph convolutional network (GCN)<sup><xref ref-type="bibr" rid="CR70">70</xref></sup>, Graph attention network (GAT)<sup><xref ref-type="bibr" rid="CR71">71</xref></sup>, Relational-GCN<sup><xref ref-type="bibr" rid="CR72">72</xref></sup>, graph recurrent network (GRN)<sup><xref ref-type="bibr" rid="CR73">73</xref></sup>, Graph isomerism network (GIN)<sup><xref ref-type="bibr" rid="CR74">74</xref></sup>, and Line graph neural network (LGNN)<sup><xref ref-type="bibr" rid="CR75">75</xref></sup>. Graph convolutional neural networks are the most popular GNNs.</p></sec></sec><sec id="Sec13"><title>Sequence-to-sequence models</title><p id="Par33">Traditionally, learning from sequential inputs such as text involves generating a fixed-length input from the data. For example, the “bag-of-words” approach simply counts the number of instances of each word in a document and produces a fixed-length vector that is the size of the overall vocabulary.</p><p id="Par34">In contrast, sequence-to-sequence models can take into account sequential/contextual information about each word and produce outputs of arbitrary length. For example, in named entity recognition (NER), an input sequence of words (e.g., a chemical abstract) is mapped to an output sequence of “entities” or categories where every word in the sequence is assigned a category.</p><p id="Par35">An early form of sequence-to-sequence model is the recurrent neural network, or RNN. Unlike the fully connected NN architecture, where there is no connection between hidden nodes in the same layer, but only between nodes in adjacent layers, RNN has feedback connections. Each hidden layer can be unfolded and processed similarly to traditional NNs sharing the same weight matrices. There are multiple types of RNNs, of which the most common ones are: gated recurrent unit recurrent neural network (GRURNN), long short-term memory (LSTM) network, and clockwork RNN (CW-RNN)<sup><xref ref-type="bibr" rid="CR76">76</xref></sup>.</p><p id="Par36">However, all such RNNs suffer from some drawbacks, including: (i) difficulty of parallelization and therefore difficulty in training on large datasets and (ii) difficulty in preserving long-range contextual information due to the “vanishing gradient” problem. Nevertheless, as we will later describe, LSTMs have been successfully applied to various NER problems in the materials domain.</p><p id="Par37">More recently, sequence-to-sequence models based on a “transformer” architecture, such as Google’s Bidirectional Encoder Representations from Transformers (BERT) model<sup><xref ref-type="bibr" rid="CR77">77</xref></sup>, have helped address some of the issues of traditional RNNs. Rather than passing a state vector that is iterated word-by-word, such models use an attention mechanism to allow access to all previous words simultaneously without explicit time steps. This mechanism facilitates parallelization and also better preserves long-term context.</p></sec><sec id="Sec14"><title>Generative models</title><p id="Par38">While the above DL frameworks are based on supervised machine learning (i.e., we know the target or ground truth data such as in classification and regression) and discriminative (i.e., learn differentiating features between various datasets), many AI tasks are based on unsupervised (such as clustering) and are generative (i.e., aim to learn underlying distributions)<sup><xref ref-type="bibr" rid="CR78">78</xref></sup>.</p><p id="Par39">Generative models are used to (a) generate data samples similar to the training set with variations i.e., augmentation and for synthetic data, (b) learn good generalized latent features, (c) guide mixed reality applications such as virtual try-on. There are various types of generative models, of which the most common are: (a) variational encoders (VAE), which explicitly define and learn likelihood of data, (b) Generative adversarial networks (GAN), which learn to directly generate samples from model’s distribution, without defining any density function.</p><p id="Par40">A VAE model has two components: namely encoder and decoder. A VAE’s encoder takes input from a target distribution and compresses it into a low-dimensional latent space. Then the decoder takes that latent space representation and reproduces the original image. Once the network is trained, we can generate latent space representations of various images, and interpolate between these before forwarding them through the decoder which produces new images. A VAE is similar to a principal component analysis (PCA) but instead of linear data assumption in PCA, VAEs work in nonlinear domain. A GAN model also has two components: namely generator, and discriminator. GAN’s generator generates fake/synthetic data that could fool the discriminator. Its discriminator tries to distinguish fake data from real ones. This process is also termed as “min-max two-player game.” We note that VAE models learn the hidden state distributions during the training process, while GAN’s hidden state distributions are predefined. Rather GAN generators serve to generate images that could fool the discriminator. These techniques are widely used for images and spectra and have also been recently applied to atomic structures.</p></sec><sec id="Sec15"><title>Deep reinforcement learning</title><p id="Par41">Reinforcement learning (RL) deals with tasks in which a computational agent learns to make decisions by trial and error. Deep RL uses DL into the RL framework, allowing agents to make decisions from unstructured input data<sup><xref ref-type="bibr" rid="CR79">79</xref></sup>. In traditional RL, Markov decision process (MDP) is used in which an agent at every timestep takes action to receive a scalar reward and transitions to the next state according to system dynamics to learn policy in order to maximize returns. However, in deep RL, the states are high-dimensional (such as continuous images or spectra) which act as an input to DL methods. DRL architectures can be either model-based or model-free.</p></sec><sec id="Sec16"><title>Scientific machine learning</title><p id="Par42">The nascent field of scientific machine learning (SciML)<sup><xref ref-type="bibr" rid="CR80">80</xref></sup> is creating new opportunities across all paradigms of machine learning, and deep learning in particular. SciML is focused on creating ML systems that incorporate scientific knowledge and physical principles, either directly in the specific form of the model or indirectly through the optimization algorithms used for training. This offers potential improvements in sample and training complexity, robustness (particularly under extrapolation), and model interpretability. One prominent theme can be found in ref. <sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. Such implementations usually involve applying multiple physics-based constraints while training a DL model<sup><xref ref-type="bibr" rid="CR81">81</xref>–<xref ref-type="bibr" rid="CR83">83</xref></sup>. One of the key challenges of universal function approximation is that a NN can quickly learn spurious features that have nothing to do with the features that a researcher could be actually interested in, within the data. In this sense, physics-based regularization can assist. Physics-based deep learning can also aid in inverse design problems, a challenging but important task<sup><xref ref-type="bibr" rid="CR84">84</xref>,<xref ref-type="bibr" rid="CR85">85</xref></sup>. On the flip side, deep Learning using Graph Neural Nets and symbolic regression (stochastically building symbolic expressions) has even been used to “discover” symbolic equations from data that capture known (and unknown) physics behind the data<sup><xref ref-type="bibr" rid="CR86">86</xref></sup>, i.e., to deep learn a physics model rather than to use a physics model to constrain DL.</p></sec><sec id="Sec17"><title>Overview of applications</title><p id="Par43">Some aspects of successful DL application that require materials-science-specific considerations are:<list list-type="order"><list-item><p id="Par44">acquiring large, balanced, and diverse datasets (often on the order of 10,000 data points or more),</p></list-item><list-item><p id="Par45">determing an appropriate DL approach and suitable vector or graph representation of the input samples, and</p></list-item><list-item><p id="Par46">selecting appropriate performance metrics relevant to scientific goals.</p></list-item></list></p><p id="Par47">In the following sections we discuss some of the key areas of materials science in which DL has been applied with available links to repositories and datasets that help in the reproducibility and extensibility of the work. In this review we categorize materials science applications at a high level by the type of input data considered: 11 atomistic, 12 stoichiometric, 13 spectral, 14 image, and 15 text. We summarize prevailing machine learning tasks and their impact on materials research and development within each broad materials data modality.</p></sec><sec id="Sec18"><title>Applications in atomistic representations</title><p id="Par48">In this section, we provide a few examples of solving materials science problems with DL methods trained on atomistic data. The atomic structure of material usually consists of atomic coordinates and atomic composition information of material. An arbitrary number of atoms and types of elements in a system poses a challenge to apply traditional ML algorithms for atomistic predictions. DL-based methods are an obvious strategy to tackle this problem. There have been several previous attempts to represent crystals and molecules using fixed-size descriptors such as Coulomb matrix<sup><xref ref-type="bibr" rid="CR87">87</xref>–<xref ref-type="bibr" rid="CR89">89</xref></sup>, classical force field inspired descriptors (CFID)<sup><xref ref-type="bibr" rid="CR90">90</xref>–<xref ref-type="bibr" rid="CR92">92</xref></sup>, pair-distribution function (PRDF), Voronoi tessellation<sup><xref ref-type="bibr" rid="CR93">93</xref>–<xref ref-type="bibr" rid="CR95">95</xref></sup>. Recently graph neural network methods have been shown to surpass previous hand-crafted feature set<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>.</p><p id="Par49">DL for atomistic materials applications include: (a) force-field development, (b) direct property predictions, (c) materials screening. In addition to the above points, we also elucidate upon some of the recent generative adversarial network and complimentary methods to atomistic aproaches.</p><sec id="Sec19"><title>Databases and software libraries</title><p id="Par50">In Table <xref rid="Tab1" ref-type="table">1</xref> we provide some of the commonly used datasets used for atomistic DL models for molecules, solids, and proteins. We note that the computational methods used for different datasets are different and many of them are continuously evolving. Generally it takes years to generate such databases using conventional methods such as density functional theory; in contrast, DL methods can be used to make predictions with much reduced computational cost and reasonable accuracy.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Databases and software for DL atomistic design (‘k’, ‘mil’ = thousand, million).</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="4"><p>Databases</p></th></tr><tr><th><p>DB name</p></th><th><p>Datasize</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>JARVIS-DFT</p></td><td><p>56k</p></td><td><p><ext-link xlink:href="https://jarvis.nist.gov/jarvisdft/" ext-link-type="uri">https://jarvis.nist.gov/jarvisdft/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR3">3</xref></sup></p></td></tr><tr><td><p>JARVIS-FF</p></td><td><p>2.5k</p></td><td><p><ext-link xlink:href="https://jarvis.nist.gov/jarvisff/" ext-link-type="uri">https://jarvis.nist.gov/jarvisff/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR3">3</xref></sup></p></td></tr><tr><td><p>MP</p></td><td><p>144k</p></td><td><p><ext-link xlink:href="https://materialsproject.org/" ext-link-type="uri">https://materialsproject.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></p></td></tr><tr><td><p>OQMD</p></td><td><p>816k</p></td><td><p><ext-link xlink:href="http://oqmd.org/" ext-link-type="uri">http://oqmd.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR4">4</xref></sup></p></td></tr><tr><td><p>AFLOW</p></td><td><p>3.5mil</p></td><td><p><ext-link xlink:href="http://www.aflowlib.org/" ext-link-type="uri">http://www.aflowlib.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR6">6</xref></sup></p></td></tr><tr><td><p>QM9</p></td><td><p>134k</p></td><td><p><ext-link xlink:href="http://quantum-machine.org/datasets/" ext-link-type="uri">http://quantum-machine.org/datasets/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR7">7</xref></sup></p></td></tr><tr><td><p>ANI</p></td><td><p>20mil</p></td><td><p><ext-link xlink:href="https://github.com/isayev/ANI1_dataset" ext-link-type="uri">https://github.com/isayev/ANI1_dataset</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR96">96</xref></sup></p></td></tr><tr><td><p>MD17</p></td><td><p>1mil</p></td><td><p><ext-link xlink:href="http://quantum-machine.org/datasets" ext-link-type="uri">http://quantum-machine.org/datasets</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR308">308</xref></sup></p></td></tr><tr><td><p>Tox21</p></td><td><p>760k</p></td><td><p><ext-link xlink:href="https://tox21.gov/resources/" ext-link-type="uri">https://tox21.gov/resources/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR309">309</xref></sup></p></td></tr><tr><td><p>CCCBDB</p></td><td><p>2069</p></td><td><p><ext-link xlink:href="https://cccbdb.nist.gov/" ext-link-type="uri">https://cccbdb.nist.gov/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR310">310</xref></sup></p></td></tr><tr><td><p>HOPV15</p></td><td><p>350</p></td><td><p><ext-link xlink:href="10.6084/m9.figshare.1610063" ext-link-type="doi">https://doi.org/10.6084/m9.figshare.1610063</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR311">311</xref></sup></p></td></tr><tr><td><p>C2DB</p></td><td><p>4000</p></td><td><p><ext-link xlink:href="https://cmr.fysik.dtu.dk/c2db/c2db.html" ext-link-type="uri">https://cmr.fysik.dtu.dk/c2db/c2db.html</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR312">312</xref></sup></p></td></tr><tr><td><p>FreeSolv</p></td><td><p>504</p></td><td><p><ext-link xlink:href="https://github.com/MobleyLab/FreeSolv" ext-link-type="uri">https://github.com/MobleyLab/FreeSolv</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR313">313</xref></sup></p></td></tr><tr><td><p>NOMAD</p></td><td><p>11mil</p></td><td><p><ext-link xlink:href="https://nomad-lab.eu/prod/rae/gui/search" ext-link-type="uri">https://nomad-lab.eu/prod/rae/gui/search</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR8">8</xref></sup></p></td></tr><tr><td><p>OPTIMADE</p></td><td><p>18mil</p></td><td><p><ext-link xlink:href="http://www.optimade.org/providers-dashboard/" ext-link-type="uri">http://www.optimade.org/providers-dashboard/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR314">314</xref></sup></p></td></tr><tr><td><p><italic>Open catalyst</italic></p></td><td/><td/><td/></tr><tr><td><p>project</p></td><td><p>1.2mil</p></td><td><p><ext-link xlink:href="https://opencatalystproject.org" ext-link-type="uri">https://opencatalystproject.org</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR315">315</xref></sup></p></td></tr><tr><td><p>MatBench</p></td><td><p>200k</p></td><td><p><ext-link xlink:href="https://matbench.materialsproject.org/" ext-link-type="uri">https://matbench.materialsproject.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR316">316</xref></sup></p></td></tr><tr><td><p>MCloud</p></td><td><p>22mil</p></td><td><p><ext-link xlink:href="https://www.materialscloud.org/home#statistics" ext-link-type="uri">https://www.materialscloud.org/home#statistics</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR317">317</xref></sup></p></td></tr><tr><td><p>CoreMOF</p></td><td><p>163k</p></td><td><p><ext-link xlink:href="https://mof.tech.northwestern.edu/" ext-link-type="uri">https://mof.tech.northwestern.edu/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR318">318</xref></sup></p></td></tr><tr><td><p>QMOF</p></td><td><p>22k</p></td><td><p><ext-link xlink:href="https://github.com/arosen93/QMOF" ext-link-type="uri">https://github.com/arosen93/QMOF</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR124">124</xref></sup></p></td></tr><tr><td><p>PDB</p></td><td><p>183k</p></td><td><p><ext-link xlink:href="https://www.rcsb.org/" ext-link-type="uri">https://www.rcsb.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR319">319</xref></sup></p></td></tr><tr><td><p>PDBBind</p></td><td><p>23k</p></td><td><p><ext-link xlink:href="http://www.pdbbind.org.cn/" ext-link-type="uri">http://www.pdbbind.org.cn/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR9">9</xref></sup></p></td></tr><tr><td><p>MOAD</p></td><td><p>39k</p></td><td><p><ext-link xlink:href="http://www.bindingmoad.org/" ext-link-type="uri">http://www.bindingmoad.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR320">320</xref></sup></p></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th colspan="4"><p>Software packages</p></th></tr><tr><th><p>Model name</p></th><th><p>Applications</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>ALIGNN</p></td><td><p>Mol, Sol</p></td><td><p><ext-link xlink:href="https://github.com/usnistgov/alignn" ext-link-type="uri">https://github.com/usnistgov/alignn</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR65">65</xref></sup></p></td></tr><tr><td><p>SchNetPack</p></td><td><p>Mol, Sol</p></td><td><p><ext-link xlink:href="https://github.com/atomistic-machine-learning" ext-link-type="uri">https://github.com/atomistic-machine-learning</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR69">69</xref></sup></p></td></tr><tr><td><p>CGCNN</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/txie-93/cgcnn" ext-link-type="uri">https://github.com/txie-93/cgcnn</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR67">67</xref></sup></p></td></tr><tr><td><p>MEGNet</p></td><td><p>Mol, Sol</p></td><td><p><ext-link xlink:href="https://github.com/materialsvirtuallab/megnet" ext-link-type="uri">https://github.com/materialsvirtuallab/megnet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR33">33</xref></sup></p></td></tr><tr><td><p>DimeNet</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/klicperajo/dimenet" ext-link-type="uri">https://github.com/klicperajo/dimenet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR68">68</xref></sup></p></td></tr><tr><td><p>MPNN</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/priba/nmp_qc" ext-link-type="uri">https://github.com/priba/nmp_qc</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR108">108</xref></sup></p></td></tr><tr><td><p>MatDeepLearn</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/vxfung/MatDeepLearn" ext-link-type="uri">https://github.com/vxfung/MatDeepLearn</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR321">321</xref></sup></p></td></tr><tr><td><p>GATGCNN</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/superlouis/GATGNN" ext-link-type="uri">https://github.com/superlouis/GATGNN</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR322">322</xref></sup></p></td></tr><tr><td><p>ANI</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/isayev/ASE_ANI" ext-link-type="uri">https://github.com/isayev/ASE_ANI</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR96">96</xref></sup></p></td></tr><tr><td><p>Amp</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://bitbucket.org/andrewpeterson/amp" ext-link-type="uri">https://bitbucket.org/andrewpeterson/amp</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR323">323</xref></sup></p></td></tr><tr><td><p>TensorMol</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/jparkhill/TensorMol" ext-link-type="uri">https://github.com/jparkhill/TensorMol</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR324">324</xref></sup></p></td></tr><tr><td><p>TorchMD</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/torchmd/torchmd" ext-link-type="uri">https://github.com/torchmd/torchmd</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR325">325</xref></sup></p></td></tr><tr><td><p>PROPhet</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/biklooost/PROPhet" ext-link-type="uri">https://github.com/biklooost/PROPhet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR326">326</xref></sup></p></td></tr><tr><td><p>DeepMD</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/deepmodeling/deepmd-kit" ext-link-type="uri">https://github.com/deepmodeling/deepmd-kit</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR101">101</xref>,<xref ref-type="bibr" rid="CR327">327</xref></sup></p></td></tr><tr><td><p>ænet</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/atomisticnet/aenet" ext-link-type="uri">https://github.com/atomisticnet/aenet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR328">328</xref></sup></p></td></tr><tr><td><p>E3NN</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/e3nn/e3nn" ext-link-type="uri">https://github.com/e3nn/e3nn</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR329">329</xref></sup></p></td></tr><tr><td><p><italic>Neural</italic></p></td><td/><td/><td/></tr><tr><td><p>fingerprint</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/HIPS/neural-fingerprint" ext-link-type="uri">https://github.com/HIPS/neural-fingerprint</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR330">330</xref></sup></p></td></tr><tr><td><p>DeepChemSt.</p></td><td><p>Mol</p></td><td><p><ext-link xlink:href="https://github.com/MingCPU/DeepChemStable" ext-link-type="uri">https://github.com/MingCPU/DeepChemStable</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR331">331</xref></sup></p></td></tr><tr><td><p>MoleculeNet</p></td><td><p>Mol, Sol</p></td><td><p><ext-link xlink:href="https://github.com/deepchem/deepchem" ext-link-type="uri">https://github.com/deepchem/deepchem</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR332">332</xref></sup></p></td></tr><tr><td><p>dgl-lifesci</p></td><td><p>Prot</p></td><td><p><ext-link xlink:href="https://github.com/awslabs/dgl-lifesci" ext-link-type="uri">https://github.com/awslabs/dgl-lifesci</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR66">66</xref></sup></p></td></tr><tr><td><p>gnina</p></td><td><p>Prot</p></td><td><p><ext-link xlink:href="https://github.com/gnina/gnina" ext-link-type="uri">https://github.com/gnina/gnina</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR110">110</xref></sup></p></td></tr></tbody></table></table-wrap></p><p id="Par51">Table <xref rid="Tab1" ref-type="table">1</xref> we provide DL software packages used for atomistic materials design. The type of models includes general property (GP) predictors and interatomic force fields (FF). The models have been demonstrated in molecules (Mol), solid-state materials (Sol), or proteins (Prot). For some force fields, high-performance large-scale implementations (LSI) that leverage paralleling computing exist. Some of these methods mainly used interatomic distances to build graphs while others use distances as well as bond-angle information. Recently, including bond angle within GNN has been shown to drastically improve the performance with comparable computational timings.</p></sec><sec id="Sec20"><title>Force-field development</title><p id="Par52">The first application includes the development of DL-based force fields (FF)<sup><xref ref-type="bibr" rid="CR96">96</xref>,<xref ref-type="bibr" rid="CR97">97</xref></sup>/interatomic potentials. Some of the major advantages of such applications are that they are very fast (on the order of hundreds to thousands times<sup><xref ref-type="bibr" rid="CR64">64</xref></sup>) for making predictions and solving the tenuous development of FFs, but the disadvantage is they still require a large dataset using computationally expensive methods to train.</p><p id="Par53">Models such as Behler-Parrinello neural network (BPNN) and its variants<sup><xref ref-type="bibr" rid="CR98">98</xref>,<xref ref-type="bibr" rid="CR99">99</xref></sup> are used for developing interatomic potentials that can be used beyond just 0 K temperature and time-dependent behavior using molecular dynamics simulations such as for nanoparticles<sup><xref ref-type="bibr" rid="CR100">100</xref></sup>. Such FF models have been developed for molecular systems, such as water, methane, and other organic molecules<sup><xref ref-type="bibr" rid="CR99">99</xref>,<xref ref-type="bibr" rid="CR101">101</xref></sup> as well as solids such as silicon<sup><xref ref-type="bibr" rid="CR98">98</xref></sup>, sodium<sup><xref ref-type="bibr" rid="CR102">102</xref></sup>, graphite<sup><xref ref-type="bibr" rid="CR103">103</xref></sup>, and titania (<italic>T</italic><italic>i</italic><italic>O</italic><sub>2</sub>)<sup><xref ref-type="bibr" rid="CR104">104</xref></sup>.</p><p id="Par54">While the above works are mainly based on NNs, there has also been the development of graph neural network force-field (GNNFF) framework<sup><xref ref-type="bibr" rid="CR105">105</xref>,<xref ref-type="bibr" rid="CR106">106</xref></sup> that bypasses both computational bottlenecks. GNNFF can predict atomic forces directly using automatically extracted structural features that are not only translationally invariant, but rotationally-covariant to the coordinate space of the atomic positions, i.e., the features and hence the predicted force vectors rotate the same way as the rotation of coordinates. In addition to the development of pure NN-based FFs, there have also been recent developments of combining traditional FFs such as bond-order potentials with NNs and ReaxFF with message passing neural network (MPNN) that can help mitigate the NNs issue for extrapolation<sup><xref ref-type="bibr" rid="CR82">82</xref>,<xref ref-type="bibr" rid="CR107">107</xref></sup>.</p></sec><sec id="Sec21"><title>Direct property prediction from atomistic configurations</title><p id="Par55">DL methods can be used to establish a structure-property relationship between atomic structure and their properties with high accuracy<sup><xref ref-type="bibr" rid="CR28">28</xref>,<xref ref-type="bibr" rid="CR108">108</xref></sup>. Models such as SchNet, crystal graph convolutional neural network (CGCNN), improved crystal graph convolutional neural network (iCGCNN), directional message passing neural network (DimeNet), atomistic line graph neural network (ALIGNN) and materials graph neural network (MEGNet) shown in Table <xref rid="Tab1" ref-type="table">1</xref> have been used to predict up to 50 properties of crystalline and molecular materials. These property datasets are usually obtained from ab-initio calculations. A schematic of such models shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. While SchNet, CGCNN, MEGNet are primarily based on atomic distances, iCGCNN, DimeNet, and ALIGNN models capture many-body interactions using GCNN.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><title>Schematic representations of an atomic structure as a graph.</title><p><bold>a</bold> CGCNN model in which crystals are converted to graphs with nodes representing atoms in the unit cell and edges representing atom connections. Nodes and edges are characterized by vectors corresponding to the atoms and bonds in the crystal, respectively [Reprinted with permission from ref. <sup><xref ref-type="bibr" rid="CR67">67</xref></sup> Copyright 2019 American Physical Society], <bold>b</bold> ALIGNN<sup><xref ref-type="bibr" rid="CR65">65</xref></sup> model in which the convolution layer alternates between message passing on the bond graph and its bond-angle line graph. <bold>c</bold> MEGNet in which the initial graph is represented by the set of atomic attributes, bond attributes and global state attributes [Reprinted with permission from ref. <sup><xref ref-type="bibr" rid="CR33">33</xref></sup> Copyright 2019 American Chemical Society] model, <bold>d</bold> iCGCNN model in which multiple edges connect a node to neighboring nodes to show the number of Voronoi neighbors [Reprinted with permission from ref. <sup><xref ref-type="bibr" rid="CR122">122</xref></sup> Copyright 2019 American Physical Society].</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_734_Fig2_HTML.png"/></fig></p><p id="Par56">Some of these properties include formation energies, electronic bandgaps, solar-cell efficiency, topological spin-orbit spillage, dielectric constants, piezoelectric constants, 2D exfoliation energies, electric field gradients, elastic modulus, Seebeck coefficients, power factors, carrier effective masses, highest occupied molecular orbital, lowest unoccupied molecular orbital, energy gap, zero-point vibrational energy, dipole moment, isotropic polarizability, electronic spatial extent, internal energy.</p><p id="Par57">For instance, the current state-of-the-art mean absolute error for formation energy for solids at 0 K is 0.022 eV/atom as obtained by the ALIGNN model<sup><xref ref-type="bibr" rid="CR65">65</xref></sup>. DL is also heavily being used for predicting catalytic behavior of materials such as the Open Catalyst Project<sup><xref ref-type="bibr" rid="CR109">109</xref></sup> which is driven by the DL methods materials design. There is an ongoing effort to continuously improve the models. Usually energy-based models such as formation and total energies are more accurate than electronic property-based models such as bandgaps and power factors.</p><p id="Par58">In addition to molecules and solids, property predictions models have also been used for bio-materials such as proteins, which can be viewed as large molecules. There have been several efforts for predicting protein-based properties, such as binding affinity<sup><xref ref-type="bibr" rid="CR66">66</xref></sup> and docking predictions<sup><xref ref-type="bibr" rid="CR110">110</xref></sup>.</p><p id="Par59">There have also been several applications for identifying reasonable chemical space using DL methods such as autoencoders<sup><xref ref-type="bibr" rid="CR111">111</xref></sup> and reinforcement learning<sup><xref ref-type="bibr" rid="CR112">112</xref>–<xref ref-type="bibr" rid="CR114">114</xref></sup> for inverse materials design. Inverse materials design with techniques such as GAN deals with finding chemical compounds with suitable properties and act as complementary to forward prediction models. While such concepts have been widely applied to molecular systems,<sup><xref ref-type="bibr" rid="CR115">115</xref></sup>, recently these methods have been applied to solids as well<sup><xref ref-type="bibr" rid="CR116">116</xref>–<xref ref-type="bibr" rid="CR120">120</xref></sup>.</p></sec><sec id="Sec22"><title>Fast materials screening</title><p id="Par60">DFT-based high-throughput methods are usually limited to a few thousands of compounds and take a long time for calculations, DL-based methods can aid this process and allow much faster predictions. DL-based property prediction models mentioned above can be used for pre-screening chemical compounds. Hence, DL-based tools can be viewed as a pre-screening tool for traditional methods such as DFT. For example, Xie et al. used CGCNN model to screen stable perovskite materials<sup><xref ref-type="bibr" rid="CR67">67</xref></sup> as well hierarchical visualization of materials space<sup><xref ref-type="bibr" rid="CR121">121</xref></sup>. Park et al.<sup><xref ref-type="bibr" rid="CR122">122</xref></sup> used iCGCNN to screen <italic>T</italic><italic>h</italic><italic>C</italic><italic>r</italic><sub>2</sub><italic>S</italic><italic>i</italic><sub>2</sub>-type materials. Lugier et al. used DL methods to predict thermoelectric properties<sup><xref ref-type="bibr" rid="CR123">123</xref></sup>. Rosen et al.<sup><xref ref-type="bibr" rid="CR124">124</xref></sup> used graph neural network models to predict the bandgaps of metal-organic frameworks. DL for molecular materials has been used to predict technologically important properties such as aqueous solubility<sup><xref ref-type="bibr" rid="CR125">125</xref></sup> and toxicity<sup><xref ref-type="bibr" rid="CR126">126</xref></sup>.</p><p id="Par61">It should be noted that the full atomistic representations and the associated DL models are only possible if the crystal structure and atom positions are available. In practice, the precise atom positions are only available from DFT structural relaxations or experiments, and are one of the goals for materials discovery instead of the starting point. Hence, alternative methods have been proposed to bypass the necessity for atom positions in building DL models. For example, Jain and Bligaard<sup><xref ref-type="bibr" rid="CR127">127</xref></sup> proposed the atomic position-independent descriptors and used a CNN model to learn the energies of crystals. Such descriptors include information based only on the symmetry (e.g., space group and Wyckoff position). In principle, the method can be applied universally in all crystals. Nevertheless, the model errors tend to be much higher than graph-based models. Similar coarse-grained representation using Wyckoff representation was also used by Goodall et al.<sup><xref ref-type="bibr" rid="CR128">128</xref></sup>. Alternatively, Zuo et al.<sup><xref ref-type="bibr" rid="CR129">129</xref></sup> started from the hypothetical structures without precise atom positions, and used a Bayesian optimization method coupled with a MEGNet energy model as an energy evaluator to perform direct structural relaxation. Applying the Bayesian optimization with symmetry relaxation (BOWSR) algorithm successfully discovered ReWB (Pca2<sub>1</sub>) and MoWC<sub>2</sub> (P6<sub>3</sub>/mmc) hard materials, which were then experimentally synthesized.</p></sec></sec><sec id="Sec23"><title>Applications in chemical formula and segment representations</title><p id="Par62">One of the earliest applications for DL included SMILES for molecules, elemental fractions and chemical descriptors for solids, and sequence of protein names as descriptors. Such descriptors lack explicit inclusion of atomic structure information but are still useful for various pre-screening applications for both theoretical and experimental data.</p><sec id="Sec24"><title>SMILES and fragment representation</title><p id="Par63">The simplified molecular-input line-entry system (SMILES) is a method to represent elemental and bonding for molecular structures using short American Standard Code for Information Interchange (ASCII) strings. SMILES can express structural differences including the chirality of compounds, making it more useful than a simply chemical formula. A SMILES string is a simple grid-like (1-D grid) structure that can represent molecular sequences such as DNA, macromolecules/polymers, protein sequences also<sup><xref ref-type="bibr" rid="CR130">130</xref>,<xref ref-type="bibr" rid="CR131">131</xref></sup>. In addition to the chemical constituents as in the chemical formula, bondings (such as double and triple bondings) are represented by special symbols (such as ’=’ and ’#’). The presence of a branch point indicated using a left-hand bracket “(” while the right-hand bracket “)” indicates that all the atoms in that branch have been taken into account. SMILES strings are represented as a distributed representation termed a SMILES feature matrix (as a sparse matrix), and then we can apply DL to the matrix similar to image data. The length of the SMILES matrix is generally kept fixed (such as 400) during training and in addition to the SMILES multiple elemental attributes and bonding attributes (such as chirality, aromaticity) can be used. Key DL tasks for molecules include (a) novel molecule design, (b) molecule screening.</p><p id="Par64">Novel molecules with target properties can designed using VAE, GAN and RNN based methods<sup><xref ref-type="bibr" rid="CR132">132</xref>–<xref ref-type="bibr" rid="CR134">134</xref></sup>. These DL-generated molecules might not be physically valid, but the goal is to train the model to learn the patterns in SMILES strings such that the output resembles valid molecules. Then chemical intuitions can be further used to screen the molecules. DL for SMILES can also be used for molecularscreening such as to predict molecular toxicity. Some of the common SMILES datasets are: ZINC<sup><xref ref-type="bibr" rid="CR135">135</xref></sup>, Tox21<sup><xref ref-type="bibr" rid="CR136">136</xref></sup>, and PubChem<sup><xref ref-type="bibr" rid="CR137">137</xref></sup>.</p><p id="Par65">Due to the limitations to enforce the generation of valid molecular structures from SMILES, fragment-based models are developed such as DeepFrag and DeepFrag-K<sup><xref ref-type="bibr" rid="CR138">138</xref>,<xref ref-type="bibr" rid="CR139">139</xref></sup>. In fragment-based models, a ligand/receptor complex is removed and then a DL model is trained to predict the most suitable fragment substituent. A set of useful tools for SMILES and fragment representations are provided in Table <xref rid="Tab2" ref-type="table">2</xref>.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Software to apply DL to chemical formula, SMILES, and fragment representations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="3"><p>Chemical formula</p></th></tr><tr><th><p>Model name</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>MatMiner</p></td><td><p><ext-link xlink:href="https://github.com/hackingmaterials/matminer" ext-link-type="uri">https://github.com/hackingmaterials/matminer</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR151">151</xref></sup></p></td></tr><tr><td><p>MagPie</p></td><td><p><ext-link xlink:href="https://bitbucket.org/wolverton/magpie" ext-link-type="uri">https://bitbucket.org/wolverton/magpie</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR150">150</xref></sup></p></td></tr><tr><td><p>DScribe</p></td><td><p><ext-link xlink:href="https://github.com/SINGROUP/dscribe" ext-link-type="uri">https://github.com/SINGROUP/dscribe</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR158">158</xref></sup></p></td></tr><tr><td><p>ElemNet</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/ElemNet" ext-link-type="uri">https://github.com/NU-CUCIS/ElemNet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR141">141</xref></sup></p></td></tr><tr><td><p>IRNet</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/IRNet" ext-link-type="uri">https://github.com/NU-CUCIS/IRNet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR152">152</xref>,<xref ref-type="bibr" rid="CR153">153</xref></sup></p></td></tr><tr><td><p>Roost</p></td><td><p><ext-link xlink:href="https://github.com/CompRhys/roost" ext-link-type="uri">https://github.com/CompRhys/roost</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR154">154</xref></sup></p></td></tr><tr><td><p>CrabNet</p></td><td><p><ext-link xlink:href="https://github.com/anthony-wang/CrabNet" ext-link-type="uri">https://github.com/anthony-wang/CrabNet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR333">333</xref></sup></p></td></tr><tr><td><p>CFID-Chem</p></td><td><p><ext-link xlink:href="https://github.com/usnistgov/jarvis/" ext-link-type="uri">https://github.com/usnistgov/jarvis/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR90">90</xref></sup></p></td></tr><tr><td><p>Atom2vec</p></td><td><p><ext-link xlink:href="https://github.com/idocx/Atom2Vec" ext-link-type="uri">https://github.com/idocx/Atom2Vec</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR334">334</xref></sup></p></td></tr><tr><td><p>CrossPropertyTL</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/CrossPropertyTL" ext-link-type="uri">https://github.com/NU-CUCIS/CrossPropertyTL</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR157">157</xref></sup></p></td></tr><tr><td colspan="3"><p><italic>SMILES and fragments</italic></p></td></tr><tr><td><p>DeepSMILES</p></td><td><p><ext-link xlink:href="https://github.com/baoilleach/deepsmiles" ext-link-type="uri">https://github.com/baoilleach/deepsmiles</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR335">335</xref></sup></p></td></tr><tr><td><p>ChemicalVAE</p></td><td><p><ext-link xlink:href="https://github.com/aspuru-guzik-group/chemical_vae" ext-link-type="uri">https://github.com/aspuru-guzik-group/chemical_vae</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR336">336</xref></sup></p></td></tr><tr><td><p>CVAE</p></td><td><p><ext-link xlink:href="https://github.com/jaechanglim/CVAE" ext-link-type="uri">https://github.com/jaechanglim/CVAE</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR133">133</xref></sup></p></td></tr><tr><td><p>DeepChem</p></td><td><p><ext-link xlink:href="https://github.com/deepchem/deepchem" ext-link-type="uri">https://github.com/deepchem/deepchem</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR332">332</xref></sup></p></td></tr><tr><td><p>DeepFRAG</p></td><td><p><ext-link xlink:href="https://git.durrantlab.pitt.edu/jdurrant/deepfrag/" ext-link-type="uri">https://git.durrantlab.pitt.edu/jdurrant/deepfrag/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR337">337</xref></sup></p></td></tr><tr><td><p>DeepFRAG-k</p></td><td><p><ext-link xlink:href="https://github.com/yaohangli/DeepFragK/" ext-link-type="uri">https://github.com/yaohangli/DeepFragK/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR338">338</xref></sup></p></td></tr><tr><td><p>CheMixNet</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/CheMixNet" ext-link-type="uri">https://github.com/NU-CUCIS/CheMixNet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR339">339</xref></sup></p></td></tr><tr><td><p>SINet</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/SINet" ext-link-type="uri">https://github.com/NU-CUCIS/SINet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR340">340</xref></sup></p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec25"><title>Chemical formula representation</title><p id="Par66">There are several ways of using the chemical formula-based representations for building ML/DL models, beginning with a simple vector of raw elemental fractions<sup><xref ref-type="bibr" rid="CR140">140</xref>,<xref ref-type="bibr" rid="CR141">141</xref></sup> or of weight percentages of alloying compositions<sup><xref ref-type="bibr" rid="CR142">142</xref>–<xref ref-type="bibr" rid="CR145">145</xref></sup>, as well as more sophisticated hand-crafted descriptors or physical attributes to add known chemistry knowledge (e.g., electronegativity, valency, etc. of constituent elements) to the feature representations<sup><xref ref-type="bibr" rid="CR146">146</xref>–<xref ref-type="bibr" rid="CR151">151</xref></sup>. Statistical and mathematical operations such as average, max, min, median, mode, and exponentiation can be carried out on elemental properties of the constituent elements to get a set of descriptors for a given compound. The number of such composition-based features can range from a few dozens to a few hundreds. One of the commonly used representations that have been shown to work for a variety of different use-cases is the materials agnostic platform for informatics and exploration (MagPie)<sup><xref ref-type="bibr" rid="CR150">150</xref></sup>. All these composition-based representations can be used with both traditional ML methods such as Random Forest as well as DL.</p><p id="Par67">It is relevant to note that ElemNet<sup><xref ref-type="bibr" rid="CR141">141</xref></sup>, which is a 17-layer neural network composed of fully connected layers and uses only raw elemental fractions as input, was found to significantly outperform traditional ML methods such as Random Forest, even when they were allowed to use more sophisticated physical attributes based on MagPie as input. Although no periodic table information was provided to the model, it was found to self-learn some interesting chemistry, like groups (element similarity) and charge balance (element interaction). It was also able to predict phase diagrams on unseen materials systems, underscoring the power of DL for representation learning directly from raw inputs without explicit feature extraction. Further increasing the depth of the network was found to adversely affect the model accuracy due to the vanishing gradient problem. To address this issue, Jha et al.<sup><xref ref-type="bibr" rid="CR152">152</xref></sup> developed IRNet, which uses individual residual learning to allow a smoother flow of gradients and enable deeper learning for cases where big data is available. IRNet models were tested on a variety of big and small materials datasets, such as OQMD, AFLOW, Materials Project, JARVIS, using different vector-based materials representations (element fractions, MagPie, structural) and were found to not only successfully alleviate the vanishing gradient problem and enable deeper learning, but also lead to significantly better model accuracy as compared to plain deep neural networks and traditional ML techniques for a given input materials representation in the presence of big data<sup><xref ref-type="bibr" rid="CR153">153</xref></sup>. Further, graph-based methods such as Roost<sup><xref ref-type="bibr" rid="CR154">154</xref></sup> have also been developed which can outperform many similar techniques.</p><p id="Par68">Such methods have been used for diverse DFT datasets mentioned above in Table <xref rid="Tab1" ref-type="table">1</xref> as well as experimental datasets such as SuperCon<sup><xref ref-type="bibr" rid="CR155">155</xref>,<xref ref-type="bibr" rid="CR156">156</xref></sup> for quick pre-screening applications. In terms of applications, they have been applied for predicting properties such as formation energy<sup><xref ref-type="bibr" rid="CR141">141</xref></sup>, bandgap, and magnetization<sup><xref ref-type="bibr" rid="CR152">152</xref></sup>, superconducting temperatures<sup><xref ref-type="bibr" rid="CR156">156</xref></sup>, bulk, and shear modulus<sup><xref ref-type="bibr" rid="CR153">153</xref></sup>. They have also been used for transfer learning across datasets for enhanced predictive accuracy on small data<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>, even for different source and target properties<sup><xref ref-type="bibr" rid="CR157">157</xref></sup>, which is especially useful to build predictive models for target properties for which big source datasets may not be readily available.</p><p id="Par69">There have been libraries of such descriptors developed such as MatMiner<sup><xref ref-type="bibr" rid="CR151">151</xref></sup> and DScribe<sup><xref ref-type="bibr" rid="CR158">158</xref></sup>. Some examples of such models are given in Table <xref rid="Tab2" ref-type="table">2</xref>. Such representations are especially useful for experimental datasets such as those for superconducting materials where the atomic structure is not tabulated. However, these representations cannot distinguish different polymorphs of a system with different point groups and space groups. It has been recently shown that although composition-based representations can help build ML/DL models to predict some properties like formation energy with remarkable accuracy, it does not necessarily translate to accurate predictions of other properties such as stability, when compared to DFT’s own accuracy<sup><xref ref-type="bibr" rid="CR159">159</xref></sup>.</p></sec></sec><sec id="Sec26"><title>Spectral models</title><p id="Par70">When electromagnetic radiation hits materials, the interaction between the radiation and matter measured as a function of the wavelength or frequency of the radiation produces a spectroscopic signal. By studying spectroscopy, researchers can gain insights into the materials’ composition, structural, and dynamic properties. Spectroscopic techniques are foundational in materials characterization. For instance, X-ray diffraction (XRD) has been used to characterize the crystal structure of materials for more than a century. Spectroscopic analysis can involve fitting quantitative physical models (for example, Rietveld refinement) or more empirical approaches such as fitting linear combinations of reference spectra, such as with x-ray absorption near-edge spectroscopy (XANES). Both approaches require a high degree of researcher expertise through careful design of experiments; specification, revision, and iterative fitting of physical models; or the availability of template spectra of known materials. In recent years, with the advances in high-throughput experiments and computational data, spectroscopic data has multiplied, giving opportunities for researchers to learn from the data and potentially displace the conventional methods in analyzing such data. This section covers emerging DL applications in various modes of spectroscopic data analysis, aiming to offer practice examples and insights. Some of the applications are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><title>Example applications of deep learning for spectral data.</title><p><bold>a</bold> Predicting structure information from the X-ray diffraction<sup><xref ref-type="bibr" rid="CR374">374</xref></sup>, Reprinted according to the terms of the CC-BY license<sup><xref ref-type="bibr" rid="CR374">374</xref></sup>. Copyright 2020. <bold>b</bold> Predicting catalysis properties from computational electronic density of states data. Reprinted according to the terms of the CC-BY license<sup><xref ref-type="bibr" rid="CR202">202</xref></sup>. Copyright 2021.</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_734_Fig3_HTML.png"/></fig></p><sec id="Sec27"><title>Databases and software libraries</title><p id="Par71">Currently, large-scale and element-diverse spectral data mainly exist in computational databases. For example, in ref. <sup><xref ref-type="bibr" rid="CR160">160</xref></sup>, the authors calculated the infrared spectra, piezoelectric tensor, Born effective charge tensor, and dielectric response as a part of the JARVIS-DFT DFPT database. The Materials Project has established the largest computational X-ray absorption database (XASDb), covering the K-edge X-ray near-edge fine structure (XANES)<sup><xref ref-type="bibr" rid="CR161">161</xref>,<xref ref-type="bibr" rid="CR162">162</xref></sup> and the L-edge XANES<sup><xref ref-type="bibr" rid="CR163">163</xref></sup> of a large number of material structures. The database currently hosts more than 400,000 K-edge XANES site-wise spectra and 90,000 L-edge XANES site-wise spectra of many compounds in the Materials Project. There are considerably fewer experimental XAS spectra, being on the order of hundreds, as seen in the EELSDb and the XASLib. Collecting large experimental spectra databases that cover a wide range of elements is a challenging task. Collective efforts focused on curating data extracted from different sources, as found in the RRUFF Raman, XRD and chemistry database<sup><xref ref-type="bibr" rid="CR164">164</xref></sup>, the open Raman database<sup><xref ref-type="bibr" rid="CR165">165</xref></sup>, and the SOP spectra library<sup><xref ref-type="bibr" rid="CR166">166</xref></sup>. However, data consistency is not guaranteed. It is also now possible for contributors to share experimental data in a Materials Project curated database, MPContribs<sup><xref ref-type="bibr" rid="CR167">167</xref></sup>. This database is supported by the US Department of Energy (DOE) providing some expectation of persistence. Entries can be kept private or published and are linked to the main materials project computational databases. There is an ongoing effort to capture data from DOE-funded synchrotron light sources (<ext-link xlink:href="https://lightsources.materialsproject.org/" ext-link-type="uri">https://lightsources.materialsproject.org/</ext-link>) into MPContribs in the future.</p><p id="Par72">Recent advances in sources, detectors, and experimental instrumentation have made high-throughput measurements of experimental spectra possible, giving rise to new possibilities for spectral data generation and modeling. Such examples include the HTEM database<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> that contains 50,000 optical absorption spectra and the UV-Vis database of 180,000 samples from the Joint Center for Artificial Photosynthesis. Some of the common spectra databases for spectra data are shown in Table <xref rid="Tab3" ref-type="table">3</xref>. There are beginning to appear cloud-based software as a service platforms for high-throughput data analysis, for example, pair-distribution function (PDF) in the cloud (<ext-link xlink:href="https://pdfitc.org" ext-link-type="uri">https://pdfitc.org</ext-link>)<sup><xref ref-type="bibr" rid="CR168">168</xref></sup> which are backed by structured databases, where data can be kept private or made public. This transition to the cloud from data analysis software installed and run locally on a user’s computer will facilitate the sharing and reuse of data by the community.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Databases and software packages for applying DL methods for spectra data.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th/><th><p>Databases</p></th><th/></tr><tr><th><p>DB name</p></th><th><p>Datasize</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>MP XAS-DB</p></td><td><p>490k</p></td><td><p><ext-link xlink:href="https://materialsproject.org/" ext-link-type="uri">https://materialsproject.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR162">162</xref>,<xref ref-type="bibr" rid="CR163">163</xref></sup></p></td></tr><tr><td><p>JV Dielectric function</p></td><td><p>16k</p></td><td><p><ext-link xlink:href="http://jarvis.nist.gov/jarvisdft" ext-link-type="uri">http://jarvis.nist.gov/jarvisdft</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR341">341</xref></sup></p></td></tr><tr><td><p>JV Infrared</p></td><td><p>5k</p></td><td><p><ext-link xlink:href="http://jarvis.nist.gov/jarvisdft" ext-link-type="uri">http://jarvis.nist.gov/jarvisdft</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR160">160</xref></sup></p></td></tr><tr><td><p>RRUFF</p></td><td><p>3527</p></td><td><p><ext-link xlink:href="https://rruff.info" ext-link-type="uri">https://rruff.info</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR164">164</xref></sup></p></td></tr><tr><td><p>ICDD XRD</p></td><td><p>108k</p></td><td><p><ext-link xlink:href="https://www.icdd.com/pdf-product-summary/" ext-link-type="uri">https://www.icdd.com/pdf-product-summary/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR342">342</xref></sup></p></td></tr><tr><td><p>ICSD XRD</p></td><td><p>150k</p></td><td><p><ext-link xlink:href="https://icsd.nist.gov/" ext-link-type="uri">https://icsd.nist.gov/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR343">343</xref></sup></p></td></tr><tr><td><p>COD XRD</p></td><td><p>480k</p></td><td><p><ext-link xlink:href="http://www.crystallography.net/cod/" ext-link-type="uri">http://www.crystallography.net/cod/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR344">344</xref></sup></p></td></tr><tr><td><p>MP XRD</p></td><td><p>140k</p></td><td><p><ext-link xlink:href="https://materialsproject.org/" ext-link-type="uri">https://materialsproject.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR5">5</xref></sup></p></td></tr><tr><td><p>JV XRD</p></td><td><p>60k</p></td><td><p><ext-link xlink:href="https://jarvis.nist.gov/jarvisdft/" ext-link-type="uri">https://jarvis.nist.gov/jarvisdft/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR3">3</xref></sup></p></td></tr><tr><td><p>MPContribs</p></td><td><p>–</p></td><td><p><ext-link xlink:href="https://mpcontribs.org/" ext-link-type="uri">https://mpcontribs.org/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR167">167</xref></sup></p></td></tr><tr><td><p>Raman OpenDB</p></td><td><p>1k</p></td><td><p><ext-link xlink:href="https://solsa.crystallography.net/rod/index.php" ext-link-type="uri">https://solsa.crystallography.net/rod/index.php</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR165">165</xref></sup></p></td></tr><tr><td><p>Chem. Web</p></td><td><p>1k</p></td><td><p><ext-link xlink:href="https://webbook.nist.gov/chemistry/" ext-link-type="uri">https://webbook.nist.gov/chemistry/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR345">345</xref></sup></p></td></tr><tr><td><p>PDFitc XPD</p></td><td><p>–</p></td><td><p><ext-link xlink:href="https://pdfitc.org" ext-link-type="uri">https://pdfitc.org</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR168">168</xref></sup></p></td></tr><tr><td><p>SDBS</p></td><td><p>35k</p></td><td><p><ext-link xlink:href="http://sdbs.riodb.aist.go.jp/sdbs/cgi-bin/cre_index.cgi" ext-link-type="uri">http://sdbs.riodb.aist.go.jp/sdbs/cgi-bin/cre_index.cgi</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR346">346</xref></sup></p></td></tr><tr><td><p>NMRShiftDB</p></td><td><p>44k</p></td><td><p><ext-link xlink:href="https://nmrshiftdb.nmr.uni-koeln.de/" ext-link-type="uri">https://nmrshiftdb.nmr.uni-koeln.de/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR347">347</xref></sup></p></td></tr><tr><td><p>SpectraBase</p></td><td><p>–</p></td><td><p><ext-link xlink:href="https://spectrabase.com/" ext-link-type="uri">https://spectrabase.com/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR347">347</xref></sup></p></td></tr><tr><td><p>SOP</p></td><td><p>325</p></td><td><p><ext-link xlink:href="https://soprano.kikirpa.be/index.php?lib=sop" ext-link-type="uri">https://soprano.kikirpa.be/index.php?lib=sop</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR166">166</xref></sup></p></td></tr><tr><td><p>HTEM</p></td><td><p>140k</p></td><td><p><ext-link xlink:href="https://htem.nrel.gov/" ext-link-type="uri">https://htem.nrel.gov/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR10">10</xref></sup></p></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th/><th/><th><p>Software packages</p></th><th/></tr><tr><th><p>Software name</p></th><th><p>Type</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>DOSNet</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/vxfung/DOSnet" ext-link-type="uri">https://github.com/vxfung/DOSnet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR348">348</xref></sup></p></td></tr><tr><td><p>Mat2Spec</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/gomes-lab/H-CLMP" ext-link-type="uri">https://github.com/gomes-lab/H-CLMP</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR349">349</xref></sup></p></td></tr><tr><td><p>PCA-CGCNN</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/kihoon-bang/PCA-CGCNN" ext-link-type="uri">https://github.com/kihoon-bang/PCA-CGCNN</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR350">350</xref></sup></p></td></tr><tr><td><p>autoXRD</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/PV-Lab/autoXRD" ext-link-type="uri">https://github.com/PV-Lab/autoXRD</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR177">177</xref></sup></p></td></tr><tr><td><p>PDFitc XPD</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://pdfitc.org" ext-link-type="uri">https://pdfitc.org</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR168">168</xref></sup></p></td></tr><tr><td><p>DRNets</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/gomes-lab/DRNets-Nature-Machine-Intelligence" ext-link-type="uri">https://github.com/gomes-lab/DRNets-Nature-Machine-Intelligence</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR351">351</xref></sup></p></td></tr><tr><td><p>HCLMP</p></td><td><p>Sol</p></td><td><p><ext-link xlink:href="https://github.com/gomes-lab/H-CLMP" ext-link-type="uri">https://github.com/gomes-lab/H-CLMP</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR349">349</xref></sup></p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec28"><title>Applications</title><p id="Par73">Due to the widespread deployment of XRD across many materials technologies, XRD spectra became one of the first test grounds for DL models. Phase identification from XRD can be mapped into a classification task (assuming all phases are known) or an unsupervised clustering task. Unlike the traditional analysis of XRD data, where the spectra are treated as convolved, discrete peak positions and intensities, DL methods treat the data as a continuous pattern similar to an image. Unfortunately, a significant number of experimental XRD datasets in one place are not readily available at the moment. Nevertheless, extensive, high-quality crystal structure data makes creating simulated XRD trivial.</p><p id="Par74">Park et al.<sup><xref ref-type="bibr" rid="CR169">169</xref></sup> calculated 150,000 XRD patterns from the Inorganic Crystal Structure Database (ICSD) structural database<sup><xref ref-type="bibr" rid="CR170">170</xref></sup> and then used CNN models to predict structural information from the simulated XRD patterns. The accuracies of the CNN models reached 81.14%, 83.83%, and 94.99% for space-group, extinction-group, and crystal-system classifications, respectively.</p><p id="Par75">Liu et al.<sup><xref ref-type="bibr" rid="CR95">95</xref></sup> obtained similar accuracies by using a CNN for classifying atomic pair-distribution function (PDF) data into space groups. The PDF is obtained by Fourier transforming XRD into real space and is particularly useful for studying the local and nanoscale structure of materials. In the case of the PDF, models were trained, validated, and tested on simulated data from the ICSD. However, the trained model showed excellent performance when given experimental data, something that can be a challenge in XRD data because of the different resolutions and line-shapes of the diffraction data depending on specifics of the sample and experimental conditions. The PDF seems to be more robust against these aspects.</p><p id="Par76">Similarly, Zaloga et al.<sup><xref ref-type="bibr" rid="CR171">171</xref></sup> also used the ICSD database for XRD pattern generation and CNN models to classify crystals. The models achieved 90.02% and 79.82% accuracy for crystal systems and space groups, respectively.</p><p id="Par77">It should be noted that the ICSD database contains many duplicates, and such duplicates should be filtered out to avoid information leakage. There is also a large difference in the number of structures represented in each space group (the label) in the database resulting in data normalization challenges.</p><p id="Par78">Lee et al.<sup><xref ref-type="bibr" rid="CR172">172</xref></sup> developed a CNN model for phase identification from samples consisting of a mixture of several phases in a limited chemical space relevant for battery materials. The training data are mixed patterns consisting of 1,785,405 synthetic XRD patterns from the Sr-Li-Al-O phase space. The resulting CNN can not only identify the phases but also predict the compound fraction in the mixture. A similar CNN was utilized by Wang et al.<sup><xref ref-type="bibr" rid="CR173">173</xref></sup> for fast identification of metal-organic frameworks (MOFs), where experimental spectral noise was extracted and then synthesized into the theoretical XRD for training data augmentation.</p><p id="Par79">An alternative idea was proposed by Dong et al.<sup><xref ref-type="bibr" rid="CR174">174</xref></sup>. Instead of recognizing only phases from the CNN, a proposed “parameter quantification network” (PQ-Net) was able to extract physico-chemical information. The PQ-Net yields accurate predictions for scale factors, crystallite size, and lattice parameters for simulated and experimental XRD spectra. The work by Aguiar et al.<sup><xref ref-type="bibr" rid="CR175">175</xref></sup> took a step further and proposed a modular neural network architecture that enables the combination of diffraction patterns and chemistry data and provided a ranked list of predictions. The ranked list predictions provide user flexibility and overcome some aspects of overconfidence in model predictions. In practical applications, AI-driven XRD identification can be beneficial for high-throughput materials discovery, as shown by Maffettone et al.<sup><xref ref-type="bibr" rid="CR176">176</xref></sup>. In their work, an ensemble of 50 CNN models was trained on synthetic data reproducing experimental variations (missing peaks, broadening, peaking shifting, noises). The model ensemble is capable of predicting the probability of each category label. A similar data augmentation idea was adopted by Oviedo et al.<sup><xref ref-type="bibr" rid="CR177">177</xref></sup>, where experimental XRD data for 115 thin-film metal-halides were measured, and CNN models trained on the augmented XRD data achieved accuracies of 93% and 89% for classifying dimensionality and space group, respectively.</p><p id="Par80">Although not a DL method, an unsupervised machine learning approach, non-negative matrix factorization (NMF), is showing great promise for yielding chemically relevant XRD spectra from time- or spatially-dependent sets of diffraction patterns. NMF is closely related to principle component analysis in that it takes a set of patterns as a matrix and then compresses the data by reducing the dimensionality by finding the most important components. In NMF a constraint is applied that all the components and their weights must be strictly positive. This often corresponds to a real physical situation (for example, spectra tend to be positive, as are the weights of chemical constituents). As a result, it appears that the mathematical decomposition often results in interpretable, physically meaningful, components and weights, as shown by Liu et al. for PDF data<sup><xref ref-type="bibr" rid="CR178">178</xref></sup>. An extension of this showed that in a spatially resolved study, NMF could be used to extract chemically resolved differential PDFs (similar to the information in EXAFS) from non-chemically resolved PDF measurements<sup><xref ref-type="bibr" rid="CR179">179</xref></sup>. NMF is very quick and easy to apply and can be applied to just about any set of spectra. It is likely to become widely used and is being implemented in the PDFitc.org website to make it more accessible to potential users.</p><p id="Par81">Other than XRD, the XAS, Raman, and infrared spectra, also contain rich structure-dependent spectroscopic information about the material. Unlike XRD, where relatively simple theories and equations exist to relate structures to the spectral patterns, the relationships between general spectra and structures are somewhat elusive. This difficulty has created a higher demand for machine learning models to learn structural information from other spectra.</p><p id="Par82">For instance, the case of X-ray absorption spectroscopy (XAS), including the X-ray absorption near-edge spectroscopy (XANES) and extended X-ray absorption fine structure (EXAFS), is usually used to analyze the structural information on an atomic level. However, the high signal-to-noise XANES region has no equation for data fitting. DL modeling of XAS data is fascinating and offers unprecedented insights. Timoshenko et al. used neural networks to predict the coordination numbers of Pt<sup><xref ref-type="bibr" rid="CR180">180</xref></sup> and Cu<sup><xref ref-type="bibr" rid="CR181">181</xref></sup> in nanoclusters from the XANES. Aside from the high accuracies, the neural network also offers high prediction speed and new opportunities for quantitative XANES analysis. Timoshenko et al.<sup><xref ref-type="bibr" rid="CR182">182</xref></sup> further carried out a novel analysis of EXAFS using DL. Although EXAFS analysis has an explicit equation to fit, the study is limited to the first few coordination shells and on relatively ordered materials. Timoshenko et al.<sup><xref ref-type="bibr" rid="CR182">182</xref></sup> first transformed the EXAFS data into 2D maps with a wavelet transform and then supplied the 2D data to a neural network model. The model can instantly predict relatively long-range radial distribution functions, offering in situ local structure analysis of materials. The advent of high-throughput XAS databases has recently unveiled more possibilities for machine learning models to be deployed using XAS data. For example, Zheng et al.<sup><xref ref-type="bibr" rid="CR161">161</xref></sup> used an ensemble learning method to match and fast search new spectra in the XASDb. Later, the same authors showed that random forest models outperform DL models such as MLPs or CNNs in directly predicting atomic environment labels from the XANES spectra<sup><xref ref-type="bibr" rid="CR183">183</xref></sup>. Similar approaches were also adopted by Torrisi et al.<sup><xref ref-type="bibr" rid="CR184">184</xref></sup> In practical applications, Andrejevic et al.<sup><xref ref-type="bibr" rid="CR185">185</xref></sup> used the XASDb data together with the topological materials database. They constructed CNN models to classify the topology of materials from the XANES and symmetry group inputs. The model correctly predicted 81% topological and 80% trivial cases and achieved 90% accuracy in material classes containing certain elements.</p><p id="Par83">Raman, infrared, and other vibrational spectroscopies provide structural fingerprints and are usually used to discriminate and estimate the concentration of components in a mixture. For example, Madden et al.<sup><xref ref-type="bibr" rid="CR186">186</xref></sup> have used neural network models to predict the concentration of illicit materials in a mixture using the Raman spectra. Interestingly, several groups have independently found that DL models outperform chemometrics analysis in vibrational spectroscopies<sup><xref ref-type="bibr" rid="CR187">187</xref>,<xref ref-type="bibr" rid="CR188">188</xref></sup>. For learning vibrational spectra, the number of training spectra is usually less than or on the order of the number of features (intensity points), and the models can easily overfit. Hence, dimensional reduction strategies are commonly used to compress the information dimension using, for example, principal component analysis (PCA)<sup><xref ref-type="bibr" rid="CR189">189</xref>,<xref ref-type="bibr" rid="CR190">190</xref></sup>. DL approaches do not have such concerns and offer elegant and unified solutions. For example, Liu et al.<sup><xref ref-type="bibr" rid="CR191">191</xref></sup> applied CNN models to the Raman spectra in the RRUFF spectral database and show that CNN models outperform classical machine learning models such as SVM in classification tasks. More DL applications in vibrational spectral analysis can be found in a recent review by Yang et al.<sup><xref ref-type="bibr" rid="CR192">192</xref></sup>.</p><p id="Par84">Although most current DL work focuses on the inverse problem, i.e., predicting structural information from the spectra, some innovative approaches also solve the forward problems by predicting the spectra from the structure. In this case, the spectroscopy data can be viewed simply as a high-dimensional material property of the structure. This is most common in molecular science, where predicting the infrared spectra<sup><xref ref-type="bibr" rid="CR193">193</xref></sup>, molecular excitation spectra<sup><xref ref-type="bibr" rid="CR194">194</xref></sup>, is of particular interest. In the early 2000s, Selzer et al.<sup><xref ref-type="bibr" rid="CR193">193</xref></sup> and Kostka et al.<sup><xref ref-type="bibr" rid="CR195">195</xref></sup> attempted predicting the infrared spectra directly from the molecular structural descriptors using neural networks. Non-DL models can also perform such tasks to a reasonable accuracy<sup><xref ref-type="bibr" rid="CR196">196</xref></sup>. For DL models, Chen et al.<sup><xref ref-type="bibr" rid="CR197">197</xref></sup> used a Euclidean neural network (E(3)NN) to predict the phonon density of state (DOS) spectra<sup><xref ref-type="bibr" rid="CR198">198</xref></sup> from atom positions and element types. The E(3)NN model captures symmetries of the crystal structures, with no need to perform data augmentation to achieve target invariances. Hence the E(3)NN model is extremely data-efficient and can give reliable DOS spectra prediction and heat capacity using relatively sparse data of 1200 calculation results on 65 elements. A similar idea was also used to predict the XAS spectra. Carbone et al.<sup><xref ref-type="bibr" rid="CR199">199</xref></sup> used a message passing neural network (MPNN) to predict the O and N K-edge XANES spectra from the molecular structures in the QM9 database<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. The training XANES data were generated using the FEFF package<sup><xref ref-type="bibr" rid="CR200">200</xref></sup>. The trained MPNN model reproduced all prominent peaks in the predicted XANES, and 90% of the predicted peaks are within 1 eV of the FEFF calculations. Similarly, Rankine et al.<sup><xref ref-type="bibr" rid="CR201">201</xref></sup> started from the two-body radial distribution function (RDC) and used a deep neural network model to predict the Fe K-edge XANES spectra for arbitrary local environments.</p><p id="Par85">In addition to learn the structure-spectra or spectra-structure relationships, a few works have also explored the possibility of relating spectra to other material properties in a non-trivial way. The DOSnet proposed by Fung et al.<sup><xref ref-type="bibr" rid="CR202">202</xref></sup> (Fig. <xref rid="Fig3" ref-type="fig">3</xref>b) uses the electronic DOS spectra calculated from DFT as inputs to a CNN model to predict the adsorption energies of H, C, N, O, S and their hydrogenated counterparts, CH, CH<sub>2</sub>, CH<sub>3</sub>, NH, OH, and SH, on bimetallic alloy surfaces. This approach extends the previous d-band theory<sup><xref ref-type="bibr" rid="CR203">203</xref></sup>, where only the d-band center, a scalar, was used to correlate with the adsorption energy on transition metals. Similarly, Kaundinya et al.<sup><xref ref-type="bibr" rid="CR204">204</xref></sup> used Atomistic Line Graph Neural Network (ALIGNN) to predict DOS for 56,000 materials in the JARVIS-DFT database using a direct discretized spectrum (D-ALIGNN), and a compressed low-dimensional representation using an autoencoder (AE-ALIGNN). Stein et al.<sup><xref ref-type="bibr" rid="CR205">205</xref></sup> tried to learn the mapping between the image and the UV-vis spectrum of the material using the conditional variational encoder (cVAE) with neural network models as the backbone. Such models can generate the UV-vis spectrum directly from a simple material image, offering much faster material characterizations. Predicting gas adsorption isotherms for direct air capture (DAC) are also an important application of spectra-based DL models. There have been several important works<sup><xref ref-type="bibr" rid="CR206">206</xref>,<xref ref-type="bibr" rid="CR207">207</xref></sup> for CO<sub>2</sub> capture with high-performance metal-organic frameworks (MOFs) which are important for mitigating climate change issues.</p></sec></sec><sec id="Sec29"><title>Image-based models</title><p id="Par86">Computer vision is often credited as precipitating the current wave of mainstream DL applications a decade ago<sup><xref ref-type="bibr" rid="CR208">208</xref></sup>. Naturally, materials researchers have developed a broad portfolio of applications of computer vision for accelerating and improving image-based material characterization techniques. High-level microscopy vision tasks can be organized as follows: image classification (and material property regression), auto-tuning experimental imaging hyperparameters, pixelwise learning (e.g., semantic segmentation), super-resolution imaging, object/entity recognition, localization, and tracking, microstructure representation learning.</p><p id="Par87">Often these tasks generalize across many different imaging modalities, spanning optical microscopy (OM), scanning electron microscopy (SEM) techniques, scanning probe microscopy (SPM, as in scanning tunneling microscopy (STM) or atomic force microscopy (AFM), and transmission electron microscopy (TEM) variants, including scanning transmission electron microscopy (STEM).</p><p id="Par88">The images obtained with these techniques range from capturing local atomic to mesoscale structures (microstructure), the distribution and type of defects, and their dynamics which are critically linked to the functionality and performance of the materials. Over the past few decades, atomic-scale imaging has become widespread and near-routine due to aberration-corrected STEM<sup><xref ref-type="bibr" rid="CR209">209</xref></sup>. The collection of large image datasets is increasingly presenting an analysis bottleneck in the materials characterization pipeline, and the immediate need for automated image analysis becomes important. Non-DL image analysis methods have driven tremendous progress in quantitative microscopy, but often image processing pipelines are brittle and require too much manual identification of image features to be broadly applicable. Thus, DL is currently the most promising solution for high-performance, high-throughput automated analysis of image datasets. For a good overview of applications in microstructure characterization specifically, see<sup><xref ref-type="bibr" rid="CR210">210</xref></sup>.</p><sec id="Sec30"><title>Databases and software libraries</title><p id="Par89">Image datasets for materials can come from either experiments or simulations. Software libraries mentioned above can be used to generate images such as STM/STEM. Images can also be obtained from the literature. A few common examples for image datasets are shown below in Table <xref rid="Tab4" ref-type="table">4</xref>. Recently, there has been a rapid development in the field of image learning tasks for materials leading to several useful packages. We list some of them in Table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Databases and software packages for applying DL methods for image applications.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th><p>Databases</p></th><th/></tr><tr><th><p>DB Name</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>JARVIS-STM</p></td><td><p><ext-link xlink:href="https://jarvis.nist.gov/jarvisstm" ext-link-type="uri">https://jarvis.nist.gov/jarvisstm</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR217">217</xref></sup></p></td></tr><tr><td><p>atomagined</p></td><td><p><ext-link xlink:href="https://github.com/MaterialEyes/atomagined" ext-link-type="uri">https://github.com/MaterialEyes/atomagined</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR352">352</xref></sup></p></td></tr><tr><td><p>deep damage</p></td><td><p><ext-link xlink:href="https://git.rwth-aachen.de/Sandra.Korte.Kerzel/DeepDamage" ext-link-type="uri">https://git.rwth-aachen.de/Sandra.Korte.Kerzel/DeepDamage</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR230">230</xref></sup></p></td></tr><tr><td><p>NanoSEM</p></td><td><p><ext-link xlink:href="10.1038/sdata.2018.172" ext-link-type="doi">https://doi.org/10.1038/sdata.2018.172</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR353">353</xref></sup></p></td></tr><tr><td><p>UHCSDB</p></td><td><p><ext-link xlink:href="http://hdl.handle.net/11256/940" ext-link-type="uri">http://hdl.handle.net/11256/940</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR223">223</xref></sup></p></td></tr><tr><td><p>UHCS micro. DB</p></td><td><p><ext-link xlink:href="http://hdl.handle.net/11256/964" ext-link-type="uri">http://hdl.handle.net/11256/964</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR224">224</xref></sup></p></td></tr><tr><td><p>SmBFO</p></td><td><p><ext-link xlink:href="https://drive.google.com/" ext-link-type="uri">https://drive.google.com/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR354">354</xref></sup></p></td></tr><tr><td><p>Diffranet</p></td><td><p><ext-link xlink:href="https://github.com/arturluis/diffranet" ext-link-type="uri">https://github.com/arturluis/diffranet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR355">355</xref></sup></p></td></tr><tr><td><p>Peregrine v2021-03</p></td><td><p><ext-link xlink:href="10.13139/ORNLNCCS/1779073" ext-link-type="doi">https://doi.org/10.13139/ORNLNCCS/1779073</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR356">356</xref></sup></p></td></tr><tr><td><p>Warwick electron microscopy data</p></td><td><p><ext-link xlink:href="https://github.com/Jeffrey-Ede/datasets/wiki" ext-link-type="uri">https://github.com/Jeffrey-Ede/datasets/wiki</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR357">357</xref></sup></p></td></tr><tr><td><p>Powder bed anamoly</p></td><td><p><ext-link xlink:href="https://www.osti.gov/biblio/1779073" ext-link-type="uri">https://www.osti.gov/biblio/1779073</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR356">356</xref></sup></p></td></tr></tbody></table><table frame="hsides" rules="groups"><thead><tr><th/><th><p>Software packages</p></th><th/></tr><tr><th><p>Package Name</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>PyCroscopy</p></td><td><p><ext-link xlink:href="https://github.com/pycroscopy/pycroscopy" ext-link-type="uri">https://github.com/pycroscopy/pycroscopy</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR358">358</xref></sup></p></td></tr><tr><td><p>Prismatic</p></td><td><p><ext-link xlink:href="https://github.com/prism-em/prismatic" ext-link-type="uri">https://github.com/prism-em/prismatic</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR352">352</xref></sup></p></td></tr><tr><td><p>AtomVision</p></td><td><p><ext-link xlink:href="https://github.com/usnistgov/atomvision" ext-link-type="uri">https://github.com/usnistgov/atomvision</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR217">217</xref></sup></p></td></tr><tr><td><p>py4DSTEM</p></td><td><p><ext-link xlink:href="https://github.com/py4dstem/py4DSTEM" ext-link-type="uri">https://github.com/py4dstem/py4DSTEM</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR359">359</xref></sup></p></td></tr><tr><td><p>abTEM</p></td><td><p><ext-link xlink:href="https://github.com/jacobjma/abTEM" ext-link-type="uri">https://github.com/jacobjma/abTEM</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR360">360</xref></sup></p></td></tr><tr><td><p>QSTEM</p></td><td><p><ext-link xlink:href="https://github.com/QSTEM/QSTEM" ext-link-type="uri">https://github.com/QSTEM/QSTEM</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR361">361</xref></sup></p></td></tr><tr><td><p>MuSTEM</p></td><td><p><ext-link xlink:href="https://github.com/HamishGBrown/MuSTEM" ext-link-type="uri">https://github.com/HamishGBrown/MuSTEM</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR362">362</xref></sup></p></td></tr><tr><td><p>MuSTEM</p></td><td><p><ext-link xlink:href="https://github.com/HamishGBrown/MuSTEM" ext-link-type="uri">https://github.com/HamishGBrown/MuSTEM</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR362">362</xref></sup></p></td></tr><tr><td><p>AICrystallographer</p></td><td><p><ext-link xlink:href="https://github.com/pycroscopy/AICrystallographer" ext-link-type="uri">https://github.com/pycroscopy/AICrystallographer</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR363">363</xref></sup></p></td></tr><tr><td><p>AtomAI</p></td><td><p><ext-link xlink:href="https://github.com/pycroscopy/atomai" ext-link-type="uri">https://github.com/pycroscopy/atomai</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR363">363</xref></sup></p></td></tr><tr><td><p>EM-net</p></td><td><p><ext-link xlink:href="https://github.com/cellsmb/EM-net" ext-link-type="uri">https://github.com/cellsmb/EM-net</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR364">364</xref></sup></p></td></tr><tr><td><p>NionSwift</p></td><td><p><ext-link xlink:href="https://github.com/nion-software/nionswift" ext-link-type="uri">https://github.com/nion-software/nionswift</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR365">365</xref></sup></p></td></tr><tr><td><p>EENCM</p></td><td><p><ext-link xlink:href="https://github.com/ceright1/Prediction-material-property" ext-link-type="uri">https://github.com/ceright1/Prediction-material-property</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR366">366</xref></sup></p></td></tr><tr><td><p>DefectSegNet</p></td><td><p><ext-link xlink:href="https://github.com/rajatsainju/DefectSegNet" ext-link-type="uri">https://github.com/rajatsainju/DefectSegNet</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR229">229</xref></sup></p></td></tr><tr><td><p>AMPIS</p></td><td><p><ext-link xlink:href="https://github.com/rccohn/AMPIS" ext-link-type="uri">https://github.com/rccohn/AMPIS</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR235">235</xref></sup></p></td></tr><tr><td><p>partial-STEM</p></td><td><p><ext-link xlink:href="https://github.com/Jeffrey-Ede/partial-STEM/tree/1.0.0" ext-link-type="uri">https://github.com/Jeffrey-Ede/partial-STEM/tree/1.0.0</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR237">237</xref></sup></p></td></tr><tr><td><p>ZeroCostDL4Mic</p></td><td><p><ext-link xlink:href="https://github.com/HenriquesLab/ZeroCostDL4Mic" ext-link-type="uri">https://github.com/HenriquesLab/ZeroCostDL4Mic</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR367">367</xref></sup></p></td></tr><tr><td><p>EBSD indexing</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/EBSD-indexing" ext-link-type="uri">https://github.com/NU-CUCIS/EBSD-indexing</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR219">219</xref></sup></p></td></tr><tr><td><p>PADNet-XRD</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/PADNet-XRD" ext-link-type="uri">https://github.com/NU-CUCIS/PADNet-XRD</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR368">368</xref></sup></p></td></tr><tr><td><p>DKACNN</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/DKACNN" ext-link-type="uri">https://github.com/NU-CUCIS/DKACNN</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR221">221</xref></sup></p></td></tr><tr><td><p>PlasticityDL</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/PlasticityDL" ext-link-type="uri">https://github.com/NU-CUCIS/PlasticityDL</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR222">222</xref></sup></p></td></tr><tr><td><p>HomogenizationDL</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/HomogenizationDL" ext-link-type="uri">https://github.com/NU-CUCIS/HomogenizationDL</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR241">241</xref></sup></p></td></tr><tr><td><p>LocalizationDL</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/LocalizationDL" ext-link-type="uri">https://github.com/NU-CUCIS/LocalizationDL</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR243">243</xref></sup></p></td></tr><tr><td><p>MDGAN</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/MDGAN" ext-link-type="uri">https://github.com/NU-CUCIS/MDGAN</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR248">248</xref></sup></p></td></tr><tr><td><p>MDN-GAN</p></td><td><p><ext-link xlink:href="https://github.com/NU-CUCIS/MDN-GAN" ext-link-type="uri">https://github.com/NU-CUCIS/MDN-GAN</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR249">249</xref></sup></p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec31"><title>Applications in image classification and regression</title><p id="Par90">DL for images can be used to automatically extract information from images or transform images into a more useful state. The benefits of automated image analysis include higher throughput, better consistency of measurements compared to manual analysis, and even the ability to measure signals in images that humans cannot detect. The benefits of altering images include image super-resolution, denoising, inferring 3D structure from 2D images, and more. Examples of the applications of each task are summarized below.</p></sec><sec id="Sec32"><title>Image classification and regression</title><p id="Par91">Classification and regression are the processes of predicting one or more values associated with an image. In the context of DL the only difference between the two methods is that the outputs of classification are discrete while the outputs of regression models are continuous. The same network architecture may be used for both classification and regression by choosing the appropriate activation function (i.e., linear for regression or Softmax for classification) for the output of the network. Due to its simplicity image classification is one of the most established DL techniques available in the materials science literature. Nonetheless, this technique remains an area of active research.</p><p id="Par92">Modarres et al. applied DL with transfer learning to automatically classify SEM images of different material systems<sup><xref ref-type="bibr" rid="CR211">211</xref></sup>. They demonstrated how a single approach can be used to identify a wide variety of features and material systems such as particles, fibers, Microelectromechanical systems (MEMS) devices, and more. The model achieved 90% accuracy on a test set. Misclassifications resulted from images containing objects from multiple classes, which is an inherent limitation of single-class classification. More advanced techniques such as those described in subsequent sections can be applied to avoid these limitations. Additionally, they developed a system to deploy the trained model at scale to process thousands of images in parallel. This approach is essential for large-scale, high-throughput experiments or industrial applications of classification. ImageNet-based deep transfer learning has also been successfully applied for crack detection in macroscale materials images<sup><xref ref-type="bibr" rid="CR212">212</xref>,<xref ref-type="bibr" rid="CR213">213</xref></sup>, as well as for property prediction on small, noisy, and heterogeneous industrial datasets<sup><xref ref-type="bibr" rid="CR214">214</xref>,<xref ref-type="bibr" rid="CR215">215</xref></sup>.</p><p id="Par93">DL has also been applied to characterize the symmetries of simulated measurements of samples. In ref. <sup><xref ref-type="bibr" rid="CR216">216</xref></sup>, Ziletti et al. obtained a large database of perfect crystal structures, introduced defects into the perfect lattices, and simulated diffraction patterns for each structure. DL models were trained to identify the space group of each diffraction patterns. The model achieved high classification performance, even on crystals with significant numbers of defects, surpassing the performance of conventional algorithms for detecting symmetries from diffraction patterns.</p><p id="Par94">DL has also been applied to classify symmetries in simulated STM measurements of 2D material systems<sup><xref ref-type="bibr" rid="CR217">217</xref></sup>. DFT was used to generate simulated STM images for a variety of material systems. A convolutional neural network was trained to identify which of the five 2D Bravais lattices each material belonged to using the simulated STM image as input. The model achieved an average F1 score of around 0.9 for each lattice type.</p><p id="Par95">DL has also been used to improve the analysis of electron backscatter diffraction (EBSD) data, with Liu et al.<sup><xref ref-type="bibr" rid="CR218">218</xref></sup> presenting one of the first DL-based solution for EBSD indexing capable of taking an EBSD image as input and predicting the three Euler angles representing the orientation that would have led to the given EBSD pattern. However, they considered the three Euler angles to be independent of each other, creating separate CNNs for each angle, although the three angles should be considered together. Jha et al.<sup><xref ref-type="bibr" rid="CR219">219</xref></sup> built upon that work to train a single DL model to predict the three Euler angles in simulated EBSD patterns of polycrystalline Ni while directly minimizing the misorientation angle between the true and predicted orientations. When tested on experimental EBSD patterns, the model achieved 16% lower disorientation error than dictionary-based indexing. Similarly, Kaufman et al. trained a CNN to predict the corresponding space group for a given diffraction pattern<sup><xref ref-type="bibr" rid="CR220">220</xref></sup>. This enables EBSD to be used for phase identification in samples where the existing phases are unknown, providing a faster or more cost-effective method of characterizing than X-ray or neutron diffraction. The results from these studies demonstrate the promise of applying DL to improve the performance and utility of EBSD experiments.</p><p id="Par96">Recently, DL has also been to learn crystal plasticity using images of strain profiles as input<sup><xref ref-type="bibr" rid="CR221">221</xref>,<xref ref-type="bibr" rid="CR222">222</xref></sup>. The work in ref. <sup><xref ref-type="bibr" rid="CR221">221</xref></sup> used domain knowledge integration in the form of two-point auto-correlation to enhance the predictive accuracy, while<sup><xref ref-type="bibr" rid="CR222">222</xref></sup> applied residual learning to learn crystal plasticity at nanoscale. It used strain profiles of materials of varying sample widths ranging from 2 μm down to 62.5 nm obtained from discrete dislocation dynamics to build a deep residual network capable of identifying prior deformation history of the sample as low, medium, or high. Compared to the correlation function-based method (68.24% accuracy), the DL model was found to be significantly more accurate (92.48%) and also capable of predicting stress-strain curves of test samples. This work additionally used saliency maps to try to interpret the developed DL model.</p></sec><sec id="Sec33"><title>Pixelwise learning</title><p id="Par97">DL can also be applied to generate one or more predictions for every pixel in an image. This can provide more detailed information about the size, position, orientation, and morphology of features of interest in images. Thus, pixelwise learning has been a significant area of focus with many recent studies appearing in materials science literature.</p><p id="Par98">Azimi et al. applied an ensemble of fully convolutional neural networks to segment martensite, tempered martensite, bainite, and pearlite in SEM images of carbon steels. Their model achieved 94% accuracy, demonstrating a significant improvement over previous efforts to automate the segmentation of different phases in SEM images. Decost, Francis, and Holm applied PixelNet to segment microstructural constituents in the UltraHigh Carbon Steel Database<sup><xref ref-type="bibr" rid="CR223">223</xref>,<xref ref-type="bibr" rid="CR224">224</xref></sup>. In contrast to fully convolutional neural networks, which encode and decode visual signals using a series of convolution layers, PixelNet constructs “hypercolumns”, or concatenations of feature representations corresponding to each pixel at different layers in a neural network. The hypercolumns are treated as individual feature vectors, which can then be classified using any typical classification approach, like a multilayer perceptron. This approach achieved phase segmentation precision and recall scores of 86.5% and 86.5%, respectively. Additionally, this approach was used to segment spheroidite particles in the matrix, achieving precision and recall scores of 91.1% and 91.1%, respectively.</p><p id="Par99">Pixelwise DL has also been applied to automatically segment dislocations in Ni superalloys<sup><xref ref-type="bibr" rid="CR210">210</xref></sup>. Dislocations are visually similar to <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma -{\gamma }^{\prime}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_734_Article_IEq1.gif"/></alternatives></inline-formula> and dislocation in Ni superalloys. With limited training data, a single segmentation model could not distinguish between these features. To overcome this, a second model was trained to generate a coarse mask corresponding to the deformed region in the material. Overlaying this mask with predictions from the first model selects the dislocations, enabling them to be distinguished from <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\gamma -{\gamma }^{\prime}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="41524_2022_734_Article_IEq2.gif"/></alternatives></inline-formula> interfaces.</p><p id="Par100">Stan, Thompson, and Voorhees applied Pixelwise DL to characterize dendritic growth from serial sectioning and synchrotron computed tomography data<sup><xref ref-type="bibr" rid="CR225">225</xref></sup>. Both of these techniques generate large amounts of data, making manual analysis impractical. Conventional image processing approaches, utilizing thresholding, edge detectors, or other hand-crafted filters, cannot effectively deal with noise, contrast gradients, and other artifacts that are present in the data. Despite having a small training set of labeled images, SegNet automatically segmented these images with much higher performance.</p></sec><sec id="Sec34"><title>Object/entity recognition, localization, and tracking</title><p id="Par101">Object detection or localization is needed when individual instances of recognized objects in a given image need to be distinguished from each other. In cases where instances do not overlap each other by a significant amount, individual instances can be resolved through post-processing of semantic segmentation outputs. This technique has been applied extensively to detect individual atoms and defects in microstructural images.</p><p id="Par102">Madsen et al. applied pixelwise DL to detect atoms in simulated atomic-resolution TEM images of graphene<sup><xref ref-type="bibr" rid="CR226">226</xref></sup>. A neural network was trained to detect the presence of each atom as well as predict its column height. Pixelwise results are used as seeds for watershed segmentation to achieve instance-level detection. Analysis of the arrangement of the atoms led to the autonomous characterization of defects in the lattice structure of the material. Interestingly, despite being trained only on simulations, the model successfully detected atomic positions in experimental images.</p><p id="Par103">Maksov et al. demonstrated atomistic defect recognition and tracking across sequences of atomic-resolution STEM images of WS<sub>2</sub><sup><xref ref-type="bibr" rid="CR227">227</xref></sup>. The lattice structure and defects existing in the first frame were characterized through a physics-based approach utilizing Fourier transforms. The positions of atoms and defects in the first frame were used to train a segmentation model. Despite only using the first frame for training, the model successfully identified and tracked defects in the subsequent frames for each sequence, even when the lattice underwent significant deformation. Similarly, Yang et al.<sup><xref ref-type="bibr" rid="CR228">228</xref></sup> used U-net architecture (as shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>) to detect vacancies and dopants in WSe<sub>2</sub> in STEM images with model accuracy of up to 98%. They classified the possible atomic sites based on experimental observations into five different types: tungsten, vanadium substituting for tungsten, selenium with no vacancy, mono-vacancy of selenium, and di-vacancy of selenium.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><title>Deep-learning-based algorithm for atomic site classification.</title><p><bold>a</bold> Deep neural networks U-Net model constructed for quantification analysis of annular dark-field in the scanning transmission electron microscope (ADF-STEM) image of V-WSe<sub>2</sub>. <bold>b</bold> Examples of training dataset for deep learning of atom segmentation model for five different species. <bold>c</bold> Pixel-level accuracy of the atom segmentation model as a function of training epoch. <bold>d</bold> Measurement accuracy of the segmentation model compared with human-based measurements. Scale bars are 1 nm [Reprinted according to the terms of the CC-BY license ref. <sup><xref ref-type="bibr" rid="CR228">228</xref></sup>].</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_734_Fig4_HTML.png"/></fig></p><p id="Par104">Roberts et al. developed DefectSegNet to automatically identify defects in transmission and STEM images of steel including dislocations, precipitates, and voids<sup><xref ref-type="bibr" rid="CR229">229</xref></sup>. They provide detailed information on the model’s design, training, and evaluation. They also compare measurements generated from the model to manual measurements performed by several different human experts, demonstrating that the measurements generated by DL are quantitatively more accurate and consistent.</p><p id="Par105">Kusche et al. applied DL to localize defects in panoramic SEM images of dual-phase steel<sup><xref ref-type="bibr" rid="CR230">230</xref></sup>. Manual thresholding was applied to identify dark defects against the brighter matrix. Regions containing defects were classified via two neural networks. The first neural network distinguished between inclusions and ductile damage in the material. The second classified the type of ductile damage (i.e., notching, martensite cracking, etc.) Each defect was also segmented via a watershed algorithm to obtain detailed information on its size, position, and morphology.</p><p id="Par106">Applying DL to localize defects and atomic structures is a popular area in materials science research. Thus, several other recent studies on these applications can be found in the literature<sup><xref ref-type="bibr" rid="CR231">231</xref>–<xref ref-type="bibr" rid="CR234">234</xref></sup>.</p><p id="Par107">In the above examples pixelwise DL, or classification models are combined with image analysis to distinguish individual instances of detected objects. However, when several adjacent objects of the same class touch or overlap each other in the image, this approach will falsely detect them to be a single, larger object. In this case, DL models designed for the detection or instance segmentation can be used to resolve overlapping instances. In one such study, Cohn and Holm applied DL for instance-level segmentation of individual particles and satellites in dense powder images<sup><xref ref-type="bibr" rid="CR235">235</xref></sup>. Segmenting each particle allows for computer vision to generate detailed size and morphology information which can be used to supplement experimental powder characterization for additive manufacturing. Additionally, overlaying the powder and satellite masks yielded the first method for quantifying the satellite content of powder samples, which cannot be measured experimentally.</p></sec><sec id="Sec35"><title>Super-resolution imaging and auto-tuning experimental parameters</title><p id="Par108">The studies listed so far focus on automating the analysis of existing data after it has been collected experimentally. However, DL can also be applied during experiments to improve the quality of the data itself. This can reduce the time for data collection or improve the amount of information captured in each image. Super-resolution and other DL techniques can also be applied in situ to autonomously adjust experimental parameters.</p><p id="Par109">Recording high-resolution electron microscope images often require large dwell times, limiting the throughput of microscopy experiments. Additionally, during imaging, interactions between the electron beam and a microscopy sample can result in undesirable effects, including charging of non-conductive samples and damage to sensitive samples. Thus, there is interest in using DL to artificially increase the resolution of images without introducing these artifacts. One method of interest is applying generative adversarial networks (GANs) for this application.</p><p id="Par110">De Haan et al. recorded SEM images of the same regions of interest in carbon samples containing gold nanoparticles at two resolutions<sup><xref ref-type="bibr" rid="CR236">236</xref></sup>. Low-resolution images recorded were used as inputs to a GAN. The corresponding images with twice the resolution were used as the ground truth. After training the GAN reduced the number of undetected gaps between nanoparticles from 13.9 to 3.7%, indicating that super-resolution was successful. Thus, applying DL led to a four-fold reduction of the interaction time between the electron beam and the sample.</p><p id="Par111">Ede and Beanland collected a dataset of STEM images of different samples<sup><xref ref-type="bibr" rid="CR237">237</xref></sup>. Images were subsampled with spiral and ‘jittered’ grid masks to obtain partial images with resolutions reduced by a factor up to 100. A GAN was trained to reconstruct full images from their corresponding partial images. The results indicated that despite a significant reduction in the sampling area, this approach successfully reconstructed high-resolution images with relatively small errors.</p><p id="Par112">DL has also been applied to automated tip conditioning for SPM experiments. Rashidi and Wolkow trained a model to detect artifacts in SPM measurements resulting from degradation in tip quality<sup><xref ref-type="bibr" rid="CR238">238</xref></sup>. Using an ensemble of convolutional neural networks resulted in 99% accuracy. After detecting that a tip has degraded, the SPM was configured to automatically recondition the tip in situ until the network indicated that the atomic sharpness of the tip has been restored. Monitoring and reconditioning the tip is the most time and labor-intensive part of conducting SPM experiments. Thus, automating this process through DL can increase the throughput and decrease the cost of collecting data through SPM.</p><p id="Par113">In addition to materials characterization, DL can be applied to autonomously adjust parameters during manufacturing. Scime et al. mounted a camera to multiple 3D printers<sup><xref ref-type="bibr" rid="CR239">239</xref></sup>. Images of the build plate were recorded throughout the printing process. A dynamic segmentation convolutional neural network was trained to recognize defects such as recoater streaking, incomplete spreading, spatter, porosity, and others. The trained model achieved high performance and was transferable to multiple printers from three different methods of additive manufacturing. This work is the first step to enabling smart additive manufacturing machines that can correct defects and adjust parameters during printing.</p><p id="Par114">There is also growing interest in establishing instruments and laboratories for autonomous experimentation. Eppel et al. trained multiple models to detect chemicals, materials, and transparent vessels in a chemistry lab setting<sup><xref ref-type="bibr" rid="CR240">240</xref></sup>. This study provides a rigorous analysis of several different approaches for scene understanding. Models were trained to characterize laboratory scenes with different methods including semantic segmentation and instance segmentation, both with and without overlapping instances. The models successfully detected individual vessels and materials in a variety of settings. Finer-grained understanding of the contents of vessels, such as segmentation of individual phases in multi-phase systems, was limited, outlining the path for future work in this area. The results represent an important step towards realizing automated experimentation for laboratory-scale experiments.</p></sec><sec id="Sec36"><title>Microstructure representation learning</title><p id="Par115">Materials microstructure is often represented in the form of multi-phase high-dimensional 2D/3D images and thus can readily leverage image-based DL methods to learn robust, low-dimensional microstructure representations, which can subsequently be used for building predictive and generative models to learn forward and inverse structure-property linkages, which are typically studied across different length scales (multi-scale modeling). In this context, homogenization and localization refer to the transfer of information from lower length scales to higher length scales and vice-versa. DL using customized CNNs has been used both for homogenization, i.e., predicting the macroscale property of material given its microstructure information<sup><xref ref-type="bibr" rid="CR221">221</xref>,<xref ref-type="bibr" rid="CR241">241</xref>,<xref ref-type="bibr" rid="CR242">242</xref></sup>, as well as for localization, i.e., predicting the strain distribution across a given microstructure for a loading condition<sup><xref ref-type="bibr" rid="CR243">243</xref></sup>.</p><p id="Par116">Transfer learning has also been widely used for analyzing materials microstructure images; methods for improving the use of transfer learning to materials science applications remain an area of active research. Goetz et al. investigated the use of unsupervised domain adaptation as an alternative to simply fine-tuning a pre-trained model<sup><xref ref-type="bibr" rid="CR244">244</xref></sup>. In this technique a model is first trained on a labeled dataset in the source domain. Next, a discriminator model is used to train the model to generate domain-agnostic features. Compared to simple fine-tuning, unsupervised domain adaptation improved the performance of classification and segmentation neural networks on materials science datasets. However, it was determined that the highest performance was achieved when the source domain was more visually similar to the target (for example, using a different set of microstructural images instead of ImageNet.) This highlights the utility of establishing large, publicly available datasets of annotated images in materials science.</p><p id="Par117">Kitaraha and Holm used the output of an intermediate layer of a pre-trained convolutional neural network as a feature representation for images of steel surface defects and Inconnel fracture surfaces<sup><xref ref-type="bibr" rid="CR245">245</xref></sup>. Images were classified by defect type or fracture surface orientation using unsupervised DL. Even though no labeled data was used to train the neural network or the unsupervised classifier, the model found natural decision boundaries that achieved a classification performance of 98% and 88% for the defect classes and fracture surface orientations, respectively. Visualization of the representations through principal component analysis (PCA) and t-distributed stochastic neighborhood embedding (t-SNE) provided qualitative insights into the representations. Although the detailed physical interpretation of the representations is still a distant goal, this study provides tools for investigating patterns in visual signals contained in image-based datasets in materials science.</p><p id="Par118">Larmuseau et al. investigated the use of triplet networks to obtain consistent representations for visually similar images of materials<sup><xref ref-type="bibr" rid="CR246">246</xref></sup>. Triplet networks are trained with three images at a time. The first image, the reference, is classified by the network. The second image, called the positive, is another image with the same class label. The last image, called the negative, is an image from a separate class. During training the loss function includes errors in predicting the class of the reference image, the difference in representations of the reference and positive images, and the similarity in representations of the reference and negative images. This process allows the network to learn consistent representations for images in the same class while distinguishing images from different classes. The triple network outperformed an ordinary convolutional neural network trained for image classification on the same dataset.</p><p id="Par119">In addition to investigating representations used to analyze existing images, DL can generate synthetic images of materials systems. Generative Adversarial Networks (GANs) are currently the predominant method for synthetic microstructure generation. GANs consist of a generator, which creates a synthetic microstructure image, and a discriminator, which attempts to predict if a given input image is real or synthetic. With careful application, GANs can be a powerful tool for microstructure representation learning and design.</p><p id="Par120">Yang and Li et al.<sup><xref ref-type="bibr" rid="CR247">247</xref>,<xref ref-type="bibr" rid="CR248">248</xref></sup> developed a GAN-based model for learning a low-dimensional embedding of microstructures, which could then be easily sampled and used with the generator of the GAN model to generate realistic, statistically similar microstructure images, thus enabling microstructural materials design. The model was able to capture complex, nonlinear microstructure characteristics and learn the mapping between the latent design variables and microstructures. In order to close the loop, the method was combined with a Bayesian optimization approach to design microstructures with optimal optical absorption performance. The discovered microstructures were found to have up to 17% better property than randomly sampled microstructures. The unique architecture of their GAN model also facilitated generator scalability to generate arbitrary-sized microstructure images and discriminator transferability to build structure-property prediction models. Yang et al.<sup><xref ref-type="bibr" rid="CR249">249</xref></sup> recently combined GANs with MDNs (mixture density networks) to enable inverse modeling in microstructural materials design, i.e., generate the microstructure for a given desired property.</p><p id="Par121">Hsu et al. constructed a GAN to generate 3D synthetic solid oxide fuel cell microstructures<sup><xref ref-type="bibr" rid="CR250">250</xref></sup>. These microstructures were compared to other synthetic microstructures generated by DREAM.3D as well as experimentally observed microstructures measured via sectioning and imaging with PFIB-SEM. Synthetic microstructures generated from the GAN were observed to qualitatively show better agreement to the experimental microstructures than the DREAM.3D microstructures, as evidenced by the more realistic phase connectivity and lower amount of agglomeration of solid phases. Additionally, a statistical analysis of various features such as volume fraction, particle size, and several other quantities demonstrated that the GAN microstructures were quantitatively more similar to the real microstructures than the DREAM.3D microstructures.</p><p id="Par122">In a similar study, Chun et al. generated synthetic microstructures of high energy materials using a GAN<sup><xref ref-type="bibr" rid="CR251">251</xref></sup>. Once again, a synthetic microstructure generated via GAN showed better qualitative visual similarity to an experimentally observed microstructure compared to a synthetic microstructure generated via a transfer learning approach, with sharper phase boundaries and fewer computational artifacts. Additionally, a statistical analysis of the void size, aspect ratio, and orientation distributions indicated that the GAN produced microstructures that were quantitatively more similar to real materials.</p><p id="Par123">Applications of DL to microstructure representation learning can help researchers improve the performance of predictive models used for the applications listed above. Additionally, using generative models can generate more realistic simulated microstructures. This can help researchers develop more accurate models for predicting material properties and performance without needing to synthesize and process these materials, significantly increasing the throughput of materials selection and screening experiments.</p></sec><sec id="Sec37"><title>Mesoscale modeling applications</title><p id="Par124">In addition to image-based characterization, deep learning methods are increasingly used in mesoscale modeling. Dai et al.<sup><xref ref-type="bibr" rid="CR252">252</xref></sup> trained a GNN successfully trained to predict magnetostriction in a wide range of synthetic polycrystalline systems with around 10% prediction error. The microstructure is represented by a graph where each node corresponds to a single grain, and the edges between nodes indicate an interface between neighboring grains. Five node features (3 Euler angles, volume, and the number of neighbors) were associated with each grain. The GNN outperformed other machine learning approaches for property prediction of polycrystalline materials by accounting for interactions between neighboring grains.</p><p id="Par125">Similarly, Cohn and Holm present preliminary work applying GNNs to predict the occurrence of abnormal grain growth (AGG) in Monte Carlo simulations of microstructure evolution<sup><xref ref-type="bibr" rid="CR253">253</xref></sup>. AGG appears to be stochastic, making it notoriously difficult to predict, control, and even observe experimentally in some materials. AGG has been reproduced in Monte Carlo simulations of material systems, but a model that can predict which initial microstructures will undergo AGG has not been established before. A dataset of Monte Carlo simulations was created using SPPARKS<sup><xref ref-type="bibr" rid="CR254">254</xref>,<xref ref-type="bibr" rid="CR255">255</xref></sup>. A microstructure GNN was trained to predict AGG in individual simulations, with 75% classification accuracy. In comparison, an image-based only achieved 60% accuracy. The GNN also provided physical insight to understanding AGG and indicated that only 2 neighborhood shells are needed to achieve the maximum performance achieved in the study. These early results motivate additional work on applying GNNs to predict the occurrence in both simulated and real materials during processing.</p></sec></sec><sec id="Sec38"><title>Natural language processing</title><p id="Par126">Most of the existing knowledge in the materials domain is currently unavailable as structured information and only exists as unstructured text, tables, or images in various publications. There exists a great opportunity to use natural language processing (NLP) techniques to convert text to structured data or to directly learn and make inferences from the text information. However, as a relatively new field within materials science, many challenges remain unsolved in this domain, such as resolving dependencies between words and phrases across multiple sentences and paragraphs.</p><sec id="Sec39"><title>Datasets for NLP</title><p id="Par127">Datasets relevant to natural language processing include peer-reviewed journal articles, articles published on preprint servers such as arXiv or ChemRxiv, patents, and online material such as Wikipedia. Unfortunately, accessing or parsing most such datasets remains difficult. Peer-reviewed journal articles are typically subject to copyright restrictions and thus difficult to obtain, especially in the large numbers required for machine learning. Many publishers now offer text and data mining (TDM) agreements that can be signed online, allowing at least a limited, restricted amount of work to be performed. However, gaining access to the full text of many publications still typically requires strict and dedicated agreements with each publisher. The major advantage of working with publishers is that they have often already converted the articles from a document format such as PDF into an easy-to-parse format such as HyperText Markup Language (HTML). In contrast, articles on preprint servers and patents are typically available with fewer restrictions, but are commonly available only as PDF files. It remains difficult to properly parse text from PDF files in a reliable manner, even when the text is embedded in the PDF. Therefore, new tools that can easily and automatically convert such content into well-structured HTML format with few residual errors would likely have a major impact on the field. Finally, online sources of information such as Wikipedia can serve as another type of data source. However, such online sources are often more difficult to verify in terms of accuracy and also do not contain as much domain-specific information as the research literature.</p></sec><sec id="Sec40"><title>Software libraries for NLP</title><p id="Par128">Applying NLP to a raw dataset involves multiple steps. These steps include retrieving the data, various forms of “pre-processing” (sentence and word tokenization, word stemming and lemmatization, featurization such as word vectors or part of speech tagging), and finally machine learning for information extraction (e.g., named entity recognition, entity-relationship modeling, question and answer, or others). Multiple software libraries exist to aid in materials NLP, as described in Table <xref rid="Tab5" ref-type="table">5</xref>. We note that although many of these steps can in theory be performed by general-purpose NLP libraries such as NLTK<sup><xref ref-type="bibr" rid="CR256">256</xref></sup>, SpaCy<sup><xref ref-type="bibr" rid="CR257">257</xref></sup>, or AllenNLP<sup><xref ref-type="bibr" rid="CR258">258</xref></sup>, the specialized nature of chemistry and materials science text (including the presence of complex chemical formulas) often leads to errors. For example, researchers have developed specialized codes to perform preprocessing that better detect chemical formulas (and not split them into separate tokens or apply stemming/lemmatization to them) and scientific phrases and notation such as oxidation states or symbols for physical units.<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>Software packages for applying DL to natural language processing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th><p>Software name</p></th><th><p>Link</p></th><th><p>Ref.</p></th></tr></thead><tbody><tr><td><p>Borges</p></td><td><p><ext-link xlink:href="https://github.com/CederGroupHub/Borges" ext-link-type="uri">https://github.com/CederGroupHub/Borges</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR270">270</xref></sup></p></td></tr><tr><td><p>ChemDataExtractor</p></td><td><p><ext-link xlink:href="http://chemdataextractor.org" ext-link-type="uri">http://chemdataextractor.org</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR262">262</xref></sup></p></td></tr><tr><td><p>ChemicalTagger</p></td><td><p><ext-link xlink:href="https://github.com/BlueObelisk/chemicaltagger" ext-link-type="uri">https://github.com/BlueObelisk/chemicaltagger</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR369">369</xref></sup></p></td></tr><tr><td><p>ChemListem</p></td><td><p><ext-link xlink:href="https://bitbucket.org/rscapplications/chemlistem/" ext-link-type="uri">https://bitbucket.org/rscapplications/chemlistem/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR370">370</xref></sup></p></td></tr><tr><td><p>ChemSpot</p></td><td><p><ext-link xlink:href="https://github.com/rockt/ChemSpot" ext-link-type="uri">https://github.com/rockt/ChemSpot</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR371">371</xref></sup></p></td></tr><tr><td><p>LBNLP</p></td><td><p><ext-link xlink:href="https://github.com/lbnlp/lbnlp" ext-link-type="uri">https://github.com/lbnlp/lbnlp</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR268">268</xref></sup></p></td></tr><tr><td><p>mat2vec</p></td><td><p><ext-link xlink:href="https://github.com/materialsintelligence/mat2vec" ext-link-type="uri">https://github.com/materialsintelligence/mat2vec</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR259">259</xref></sup></p></td></tr><tr><td><p>MaterialsParser</p></td><td><p><ext-link xlink:href="https://github.com/CederGroupHub/MaterialParser" ext-link-type="uri">https://github.com/CederGroupHub/MaterialParser</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR271">271</xref></sup></p></td></tr><tr><td><p>OSCAR4</p></td><td><p><ext-link xlink:href="https://github.com/BlueObelisk/oscar4" ext-link-type="uri">https://github.com/BlueObelisk/oscar4</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR372">372</xref></sup></p></td></tr><tr><td><p>Synthesis Project</p></td><td><p><ext-link xlink:href="https://www.synthesisproject.org" ext-link-type="uri">https://www.synthesisproject.org</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR272">272</xref></sup></p></td></tr><tr><td><p>tmChem</p></td><td><p><ext-link xlink:href="https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/tmchem/" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/research/bionlp/Tools/tmchem/</ext-link></p></td><td><p><sup><xref ref-type="bibr" rid="CR373">373</xref></sup></p></td></tr></tbody></table></table-wrap></p><p id="Par129">Similarly, chemistry-specific codes for extracting entities are better at extracting the names of chemical elements (e.g., recognizing that “He” likely represents helium and not a male pronoun) and abbreviations for chemical formulas. Finally, word embeddings that convert words such as “manganese” into numerical vectors for further data mining are more informative when trained specifically on materials science text versus more generic texts, even when the latter datasets are larger<sup><xref ref-type="bibr" rid="CR259">259</xref></sup>. Thus, domain-specific tools for NLP are required in nearly all aspects of the pipeline. The main exception is that the architecture of the specific neural network models used for information extraction (e.g., LSTM, BERT, or architectures used to generate word embeddings such as word2vec or GloVe) are typically not modified specifically for the materials domain. Thus, much of the materials and chemistry-centric work currently regards data retrieval and appropriate preprocessing. A longer discussion of this topic, with specific examples, can be found in refs. <sup><xref ref-type="bibr" rid="CR260">260</xref>,<xref ref-type="bibr" rid="CR261">261</xref></sup>.</p></sec><sec id="Sec41"><title>Applications</title><p id="Par130">NLP methods for materials have been applied for information extraction and search (particularly as applied to synthesis prediction) as well as materials discovery. As the domain is rapidly growing, we suggest dedicated reviews on this topic by Olivetti et al.<sup><xref ref-type="bibr" rid="CR261">261</xref></sup> and Kononova et al.<sup><xref ref-type="bibr" rid="CR260">260</xref></sup> for more information.</p><p id="Par131">One of the major uses of NLP methods is to extract datasets from the text in published studies. Conventionally, such datasets required manual entry of datasets by researchers combing the literature, a laborious and time-consuming process. Recently, software tools such as ChemDataExtractor<sup><xref ref-type="bibr" rid="CR262">262</xref></sup> and other methods<sup><xref ref-type="bibr" rid="CR263">263</xref></sup> based on more conventional machine learning and rule-based approaches have enabled automated or semi-automated extraction of datasets such as Curie and Néel magnetic phase transition temperatures<sup><xref ref-type="bibr" rid="CR264">264</xref></sup>, battery properties<sup><xref ref-type="bibr" rid="CR265">265</xref></sup>, UV-vis spectra<sup><xref ref-type="bibr" rid="CR266">266</xref></sup>, and surface and pore characteristics of metal-organic frameworks<sup><xref ref-type="bibr" rid="CR267">267</xref></sup>. In the past few years, DL approaches such as LSTMs and transformer-based models have been employed to extract various categories of information<sup><xref ref-type="bibr" rid="CR268">268</xref></sup>, and in particular materials synthesis information<sup><xref ref-type="bibr" rid="CR269">269</xref>–<xref ref-type="bibr" rid="CR271">271</xref></sup> from text sources. Such data have been used to predict synthesis maps for titania nanotubes<sup><xref ref-type="bibr" rid="CR272">272</xref></sup>, various binary and ternary oxides<sup><xref ref-type="bibr" rid="CR273">273</xref></sup>, and perovskites<sup><xref ref-type="bibr" rid="CR274">274</xref></sup>.</p><p id="Par132">Databases based on natural language processing have also been used to train machine learning models to identify materials with useful functional properties, such as the recent discovery of the large magnetocaloric properties of HoBe<sub>2</sub><sup><xref ref-type="bibr" rid="CR275">275</xref></sup>. Similarly, Cooper et al.<sup><xref ref-type="bibr" rid="CR276">276</xref></sup> demonstrated a “design to device approach” for designing dye-sensitized solar cells that are co-sensitized with two dyes<sup><xref ref-type="bibr" rid="CR276">276</xref></sup>. This study used automated text mining to compile a list of candidate dyes for the application along with measured properties such as maximum absorption wavelengths and extinction coefficients. The resulting list of 9431 dyes extracted from the literature was downselected to 309 candidates using various criteria such as molecular structure and ability to absorb in the solar spectrum. These candidates were evaluated for suitable combinations for co-sensitization, yielding 33 dyes that were further downselected using density functional theory calculations and experimental constraints. The resulting 5 dyes were evaluated experimentally, both individually and in combinations, resulting in a combination of dyes that not only outperformed any of the individual dyes but demonstrated performance comparable to existing standard material. This study demonstrates the possibility of using literature-based extraction to identify materials candidates for new applications from the vast body of published work, which may have never tested those materials for the desired application.</p><p id="Par133">It is even possible that natural language processing can directly make materials predictions without intermediary models. In a study reported by Tshitoyan et al.<sup><xref ref-type="bibr" rid="CR259">259</xref></sup> (as shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>), word embeddings (i.e., numerical vectors representing distinct words) trained on materials science literature could directly predict materials applications through a simple dot product between the trained embedding for a composition word (such as PbTe) and an application words (such as thermoelectrics). The researchers demonstrated that such an approach, if applied in the past using historical data, may have subsequently predicted many recently reported thermoelectric materials; they also presented a list of potentially interesting thermoelectric compositions using the known literature at the time. Since then, several of these predictions have been tested either computationally<sup><xref ref-type="bibr" rid="CR277">277</xref>–<xref ref-type="bibr" rid="CR282">282</xref></sup> or experimentally<sup><xref ref-type="bibr" rid="CR283">283</xref></sup> as potential thermoelectrics. Such approaches have recently been applied to search for understudied areas of metallocene catalysis<sup><xref ref-type="bibr" rid="CR284">284</xref></sup>, although challenges still remain in such direct approaches to materials prediction.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><title>A schematic showing the application of skip-gram variation of Word2vec for predicting context words.</title><p><bold>a</bold> Network for training word embeddings for natural language processing application. A one-hot encoded vector at left represents each distinct word in the corpus; the role of a hidden layer is to predict the probability of neighboring words in the corpus. This network structure trains a relatively small hidden layer of 100–200 neurons to contain information on the context of words in the entire corpus, with the result that similar words end up with similar hidden layer weights (word embeddings). Such word embeddings can transform wordsin text form into numerical vectors that may be useful for a variety of applications. <bold>b</bold> projection of word embeddings for various materials science words, as trained on a corpus scientific abstracts, into two dimensions using principle components analysis. Without any explicit training, the word embeddings naturally preserve relationships between chemical formulas, their common oxides, and their ground state structures. [Reprinted according to the terms of the CC-BY license ref. <sup><xref ref-type="bibr" rid="CR259">259</xref></sup>].</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/41524_2022_734_Fig5_HTML.png"/></fig></p></sec></sec><sec id="Sec42"><title>Uncertainty quantification</title><p id="Par134">Uncertainty quantification (UQ) is an essential step in evaluating the robustness of DL. Specifically, DL models have been criticized for lack of robustness, interpretability, and reliability and the addition of carefully quantified uncertainties would go a long way towards addressing such shortcomings. While most of the focus in the DL field currently goes into developing new algorithms or training networks to high accuracy, there is increasing attention to UQ, as exemplified by the detailed review of Abdar et al.<sup><xref ref-type="bibr" rid="CR285">285</xref></sup>. However, determining the uncertainty associated with DL predictions is still challenging and far from a completely solved problem.</p><p id="Par135">The main drawback to estimating UQ when performing DL is the fact that most of the currently available UQ implementations do not work for arbitrary, off-the-shelf models, without retraining or redesigning. Bayesian NNs are the exception; however, they require significant modifications to the training procedure, are computationally expensive compared to non-Bayesian NNs, and become increasingly inefficient the larger the datasize gets. A considerable fraction of the current research in DL UQ focuses exactly on such an issue: how to evaluate uncertainty without requiring computationally expensive retraining or DL code modifications. An example of such an effort is the work of Mi et al.<sup><xref ref-type="bibr" rid="CR286">286</xref></sup>, where three scalable methods are explored, to evaluate the variance of output from trained NN, without requiring any amount of retraining. Another example is Teye, Azizpour, and Smith’s exploration of the use of batch normalization as a way to approximate inference in Bayesian models<sup><xref ref-type="bibr" rid="CR287">287</xref></sup>.</p><p id="Par136">Before reviewing the most common methods used to evaluate uncertainty in DL, let us briefly point out key reasons to add UQ to DL modeling. Reaching high accuracy when training DL models implicitly assume the availability of a sufficiently large and diverse training dataset. Unfortunately, this rarely occurs in material discovery applications<sup><xref ref-type="bibr" rid="CR288">288</xref></sup>. ML/DL models are prone to perform poorly on extrapolation<sup><xref ref-type="bibr" rid="CR289">289</xref></sup>. It is also extremely difficult for ML/DL models to recognize ambiguous samples<sup><xref ref-type="bibr" rid="CR290">290</xref></sup>. In general, determining the amount of data necessary to train a DL to achieve the required accuracy is a challenging problem. Careful evaluation of the uncertainty associated with DL predictions would not only increase reliability in predicted results but would also provide guidance on estimating the needed training dataset size as well as suggesting what new data should be added to reach the target accuracy (uncertainty-guided decision). Zhang, Kailkhura, and Han’s work emphasizes how including a UQ-motivated reject option into the DL model substantially improves the performance of the remaining material data<sup><xref ref-type="bibr" rid="CR288">288</xref></sup>. Such a reject option is associated with the detection of out-of-distribution samples, which is only possible through UQ analysis of the predicted results.</p><p id="Par137">Two different uncertainty types are associated with each ML prediction: epistemic uncertainty and aleatory uncertainty. Epistemic uncertainty is related to insufficient training data in part of the input domain. As mentioned above, while DL is very effective at interpolation tasks, they can have more difficulty in extrapolation. Therefore, it is vital to quantify the lack of accuracy due to localized, insufficient training data. The aleatory uncertainty, instead, is related to parameters not included in the model. It relates to the possibility of training on data that our DL perceives as very similar but that are associated with different outputs because of missing features in the model. Ideally, we would like UQ methodologies to distinguish and quantify both types of uncertainties separately.</p><p id="Par138">The most common approaches to evaluate uncertainty using DL are Dropout methods, Deep Ensemble methods, Quantile regression, and Gaussian Processes. Dropout methods are commonly used to avoid overfitting. In this type of approach, network nodes are disabled randomly during training, resulting in the evaluation of a different subset of the network at each training step. When a similar randomization procedure is also applied to the prediction procedure, the methodology becomes Monte-Carlo dropout<sup><xref ref-type="bibr" rid="CR291">291</xref></sup>. Repeating such randomization multiple times produces a distribution over the outputs, from which mean and variance are determined for each prediction. Another example of using a dropout approach to approximate Bayesian inference in deep Gaussian processes is the work of Gal and Ghahramani<sup><xref ref-type="bibr" rid="CR292">292</xref></sup>.</p><p id="Par139">Deep ensemble methodologies<sup><xref ref-type="bibr" rid="CR293">293</xref>–<xref ref-type="bibr" rid="CR296">296</xref></sup> combine deep learning modelling with ensemble learning. Ensemble methods utilize multiple models and different random initializations to improve predictability. Because of the multiple predictions, statistical distributions of the outputs are generated. Combining such results into a Gaussian distribution, confidence intervals are obtained through variance evaluation. Such a multi-model strategy allows the evaluation of aleatory uncertainty when sufficient training data are provided. For areas without sufficient data, the predicted mean and variance will not be accurate, but the expectation is that a very large variance should be estimated, clearly indicating non-trustable predictions. Monte-Carlo Dropout and Deep Ensembles approaches can be combined to further improve confidence in the predicted outputs.</p><p id="Par140">Quantile regression can be utilized with DL<sup><xref ref-type="bibr" rid="CR297">297</xref></sup>. In this approach, the loss function is used in a way that allows to predict for the chosen quantile a (between 0 and 1). A choice of <italic>a</italic> = 0.5 corresponds to evaluating the Mean Absolute Error (MAE) and predicting the median of the distribution. Predicting for two more quantile values (amin and amax) determines confidence intervals of width amax − amin. For instance, predicting for amin = 0.1 and amax = 0.8 produces confidence intervals covering 70% of the population. The largest drawback of using quantile to estimate prediction intervals is the need to run the model three times, one for each quantile needed. However, a recent implementation in TensorFlow allows to simultaneously obtain multiple quantiles in one run.</p><p id="Par141">Lastly, Gaussian Processes (GP) can be used within a DL approach as well and have the side benefit of providing UQ information at no extra cost. Gaussian processes are a family of infinite-dimensional multivariate Gaussian distributions completely specified by a mean function and a flexible kernel function (prior distribution). By optimizing such functions to fit the training data, the posterior distribution is determined, which is later used to predict outputs for inputs not included in the training set. Because the prior is a Gaussian process, the posterior distribution is Gaussian as well<sup><xref ref-type="bibr" rid="CR298">298</xref></sup>, thus providing mean and variance information for each predicted data. However, in practice standard kernels under-perform<sup><xref ref-type="bibr" rid="CR299">299</xref></sup>. In 2016, Wilson et al.<sup><xref ref-type="bibr" rid="CR300">300</xref></sup> suggested processing inputs through a neural network prior to a Gaussian process model. This procedure could extract high-level patterns and features, but required careful design and optimization. In general, Deep Gaussian processes improve the performance of Gaussian processes by mapping the inputs through multiple Gaussian process ‘layers’. Several groups have followed this avenue and further perfected such an approach (ref. <sup><xref ref-type="bibr" rid="CR299">299</xref></sup> and references within). A common drawback of Bayesian methods is a prohibitive computational cost if dealing with large datasets<sup><xref ref-type="bibr" rid="CR292">292</xref></sup>.</p></sec><sec id="Sec43"><title>Limitations and challenges</title><p id="Par142">Although DL methods have various fascinating opportunities for materials design, they have several limitations and there is much room to improve. Reliability and quality assessment of datasets used in DL tasks are challenging because there is either a lack of ground truth data, or there are not enough metrics for global comparison, or datasets using similar or identical set-ups may not be reproducible<sup><xref ref-type="bibr" rid="CR301">301</xref></sup>. This poses an important challenge in relying upon DL-based prediction.</p><p id="Par143">Material representations based on chemical formula alone by definition do not consider structure, which on the one hand makes them more amenable to work for new compounds for which structure information may not be available, but on the other hand, makes it impossible for them to capture phenomena such as phase transitions. Properties of materials depend sensitively on structure to the extent that their properties can be quite opposite depending on the atomic arrangement, like a diamond (hard, wide-band-gap insulator) and graphite (soft, semi-metal). It is thus not a surprise that chemical formula-based methods may not be adequate in some cases<sup><xref ref-type="bibr" rid="CR159">159</xref></sup>.</p><p id="Par144">Atomistic graph-based predictions, although considered a full atomistic description, are tested on bulk materials only and not for defective systems or for multi-dimensional phases of space exploration such as using genetic algorithms. In general, this underscores that the input features must be predictive for the output labels and not be missing some key information. Although atomistic graph neural network models such as atomistic line graph neural network (ALIGNN) have achieved remarkable accuracy compared to previous atomistic based models, the model errors still need to be further brought down to reach something resembling deep learning ‘chemical-accuracies.’</p><p id="Par145">In terms of images and spectra, the experimental data are too noisy most of the time and require much manipulation before applying DL. In contrast, theory-based simulated data represent an alternate path forward but may not capture realistic scenarios such as the presence of structured noise<sup><xref ref-type="bibr" rid="CR217">217</xref></sup>.</p><p id="Par146">Uncertainty quantification for deep learning for materials science is important, yet only a few works have been published in this field. To alleviate the black-box<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> nature of the DL methods, a package such as GNNExplainer<sup><xref ref-type="bibr" rid="CR302">302</xref></sup> has been tried in the context of the material. Such attempts at greater interpretability will be important moving forward to gain the trust of the materials community.</p><p id="Par147">While training-validation-test split strategies were primarily designed in DL for image classification tasks with a certain number of classes, the same for regression models in materials science may not be the best approach. This is because it is possible that during the training the model is seeing a material very similar to the test set material and in reality it is difficult to generalize the model. Best practices need to be developed for data split, normalization, and augmentation to avoid such issues<sup><xref ref-type="bibr" rid="CR289">289</xref></sup>.</p><p id="Par148">Finally, we note an important technological challenge is to make a closed-loop autonomous materials design and synthesis process<sup><xref ref-type="bibr" rid="CR303">303</xref>,<xref ref-type="bibr" rid="CR304">304</xref></sup> that can include both machine learning and experimental components in a self-driving laboratory<sup><xref ref-type="bibr" rid="CR305">305</xref></sup>. For an overview of early proof of principle attempts see<sup><xref ref-type="bibr" rid="CR306">306</xref></sup>. For example, in an autonomous synthesis experiment the oxidation state of copper (and therefore the oxide phase) was varied in a sample of copper oxide by automatically flowing more oxidizing or more reducing gas over the sample and monitoring the charge state of the copper using XANES. An algorithmic decision policy was then used to automatically change the gas composition for a subsequent experiment based on the prior experiments, with no human in the loop, in such a way as to autonomously move towards a target copper oxidation state<sup><xref ref-type="bibr" rid="CR307">307</xref></sup>. This simple proof of principle experiment provides just a glimpse of what is possible moving forward.</p></sec></body><back><ack><title>Acknowledgements</title><p>Contributions from K.C. were supported by the financial assistance award 70NANB19H117 from the U.S. Department of Commerce, National Institute of Standards and Technology. E.A.H. and R.C. (CMU) were supported by the National Science Foundation under grant CMMI-1826218 and the Air Force D3OM2S Center of Excellence under agreement FA8650-19-2-5209. A.J., C.C., and S.P.O. were supported by the Materials Project, funded by the U.S. Department of Energy, Office of Science, Office of Basic Energy Sciences, Materials Sciences and Engineering Division under contract no. DE-AC02-05-CH11231: Materials Project program KC23MP. S.J.L.B. was supported by the U.S. National Science Foundation through grant DMREF-1922234. A.A. and A.C. were supported by NIST award 70NANB19H005 and NSF award CMMI-2053929.</p></ack><sec sec-type="author-contribution"><title>Author contributions</title><p>The authors contributed equally to the search as well as analysis of the literature and writing of the manuscript.</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The data from new figures are available on reasonable request from the corresponding author. Data from other publishers are not available from the corresponding author of this work but may be available by reaching the corresponding author of the cited work.</p></sec><sec sec-type="data-availability"><title>Code availability</title><p>Software packages mentioned in the article (whichever made available by the authors) can be found at <ext-link xlink:href="https://github.com/deepmaterials/dlmatreview" ext-link-type="uri">https://github.com/deepmaterials/dlmatreview</ext-link>. Software for other packages can be obtained by reaching the corresponding author of the cited work.</p></sec><sec sec-type="ethics-statement"><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par149">The authors declare no competing interests.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Callister, W. D. et al. <italic>Materials Science and Engineering: An Introduction</italic> (Wiley, 2021).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Saito, T. <italic>Computational Materials Design,</italic> Vol. 34 (Springer Science &amp; Business Media, 2013).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">The joint automated repository for various integrated simulations (jarvis) for data-driven materials design</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41524-020-00440-1</pub-id></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kirklin</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">The open quantum materials database (oqmd): assessing the accuracy of dft formation energies</article-title><source>npj Comput. Mater.</source><year>2015</year><volume>1</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/npjcompumats.2015.10</pub-id></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Commentary: The materials project: A materials genome approach to accelerating materials innovation</article-title><source>APL Mater.</source><year>2013</year><volume>1</volume><fpage>011002</fpage><pub-id pub-id-type="doi">10.1063/1.4812323</pub-id></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Curtarolo</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Aflow: An automatic framework for high-throughput materials discovery</article-title><source>Comput. Mater. Sci.</source><year>2012</year><volume>58</volume><fpage>218</fpage><lpage>226</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XksVyktL8%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2012.02.005</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramakrishnan</surname><given-names>R</given-names></name><name><surname>Dral</surname><given-names>PO</given-names></name><name><surname>Rupp</surname><given-names>M</given-names></name><name><surname>Von Lilienfeld</surname><given-names>OA</given-names></name></person-group><article-title xml:lang="en">Quantum chemistry structures and properties of 134 kilo molecules</article-title><source>Sci. Data</source><year>2014</year><volume>1</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/sdata.2014.22</pub-id></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Draxl</surname><given-names>C</given-names></name><name><surname>Scheffler</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Nomad: The fair concept for big data-driven materials science</article-title><source>MRS Bull.</source><year>2018</year><volume>43</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="doi">10.1557/mrs.2018.208</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Fang</surname><given-names>X</given-names></name><name><surname>Lu</surname><given-names>Y</given-names></name><name><surname>Yang</surname><given-names>C-Y</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">The pdbbind database: methodologies and updates</article-title><source>J. Med. Chem.</source><year>2005</year><volume>48</volume><fpage>4111</fpage><lpage>4119</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2MXktlKisbg%3D</pub-id><pub-id pub-id-type="doi">10.1021/jm048957q</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zakutayev</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">An open experimental database for exploring inorganic materials</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.53</pub-id></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Pablo</surname><given-names>JJ</given-names></name><etal/></person-group><article-title xml:lang="en">New frontiers for the materials genome initiative</article-title><source>npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1038/s41524-019-0173-4</pub-id></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>MD</given-names></name><etal/></person-group><article-title xml:lang="en">The fair guiding principles for sci. data management and stewardship</article-title><source>Sci. Data</source><year>2016</year><volume>3</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Friedman, J. et al. <italic>The Elements of Statistical Learning,</italic> Vol. 1 (Springer series in statistics New York, 2001).</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science</article-title><source>APL Mater.</source><year>2016</year><volume>4</volume><fpage>053208</fpage><pub-id pub-id-type="doi">10.1063/1.4946894</pub-id></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vasudevan</surname><given-names>RK</given-names></name><etal/></person-group><article-title xml:lang="en">Materials science in the artificial intelligence age: high-throughput library generation, machine learning, and a pathway from correlations to the underpinning physics</article-title><source>MRS Commun.</source><year>2019</year><volume>9</volume><fpage>821</fpage><lpage>838</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvVKisr3P</pub-id><pub-id pub-id-type="doi">10.1557/mrc.2019.95</pub-id></mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>J</given-names></name><name><surname>Marques</surname><given-names>MR</given-names></name><name><surname>Botti</surname><given-names>S</given-names></name><name><surname>Marques</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Recent advances and applications of machine learning in solid-state materials science</article-title><source>npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1038/s41524-019-0221-0</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Butler</surname><given-names>KT</given-names></name><name><surname>Davies</surname><given-names>DW</given-names></name><name><surname>Cartwright</surname><given-names>H</given-names></name><name><surname>Isayev</surname><given-names>O</given-names></name><name><surname>Walsh</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Machine learning for molecular and materials science</article-title><source>Nature</source><year>2018</year><volume>559</volume><fpage>547</fpage><lpage>555</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtl2jt7vL</pub-id><pub-id pub-id-type="doi">10.1038/s41586-018-0337-2</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Deep dive into machine learning models for protein engineering</article-title><source>J. Chem. Inf. Model.</source><year>2020</year><volume>60</volume><fpage>2773</fpage><lpage>2790</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXmtl2ltr8%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.0c00073</pub-id></mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schleder</surname><given-names>GR</given-names></name><name><surname>Padilha</surname><given-names>AC</given-names></name><name><surname>Acosta</surname><given-names>CM</given-names></name><name><surname>Costa</surname><given-names>M</given-names></name><name><surname>Fazzio</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">From dft to machine learning: recent approaches to materials science–a review</article-title><source>J. Phys. Mater.</source><year>2019</year><volume>2</volume><fpage>032001</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFGhtrjJ</pub-id><pub-id pub-id-type="doi">10.1088/2515-7639/ab084b</pub-id></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep materials informatics: applications of deep learning in materials science</article-title><source>MRS Commun.</source><year>2019</year><volume>9</volume><fpage>779</fpage><lpage>792</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvVKisrzN</pub-id><pub-id pub-id-type="doi">10.1557/mrc.2019.73</pub-id></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Goodfellow, I., Bengio, Y. &amp; Courville, A. <italic>Deep Learning</italic> (MIT Press, 2016).</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Deep learning</article-title><source>Nature</source><year>2015</year><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXht1WlurzP</pub-id><pub-id pub-id-type="doi">10.1038/nature14539</pub-id></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCulloch</surname><given-names>WS</given-names></name><name><surname>Pitts</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">A logical calculus of the ideas immanent in nervous activity</article-title><source>Bull. Math. Biophys.</source><year>1943</year><volume>5</volume><fpage>115</fpage><lpage>133</lpage><pub-id pub-id-type="doi">10.1007/BF02478259</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">The perceptron: A probabilistic model for information storage and organization in the brain</article-title><source>Psychol. Rev.</source><year>1958</year><volume>65</volume><fpage>386</fpage><lpage>408</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DyaG1M%2FjtFCmtw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1037/h0042519</pub-id></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibney</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Google ai algorithm masters ancient game of go</article-title><source>Nat. News</source><year>2016</year><volume>529</volume><fpage>445</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28Xhs12ju78%3D</pub-id><pub-id pub-id-type="doi">10.1038/529445a</pub-id></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Ramos, S., Gehrig, S., Pinggera, P., Franke, U. &amp; Rother, C. Detecting unexpected obstacles for self-driving cars: Fusing deep learning and geometric modeling. in <italic>2017 IEEE Intelligent Vehicles Symposium (IV)</italic>, 1025–1032 (IEEE, 2017).</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Buduma, N. &amp; Locascio, N. <italic>Fundamentals of deep learning: Designing next-generation machine intelligence algorithms</italic> (O’Reilly Media, Inc., O’Reilly, 2017).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kearnes</surname><given-names>S</given-names></name><name><surname>McCloskey</surname><given-names>K</given-names></name><name><surname>Berndl</surname><given-names>M</given-names></name><name><surname>Pande</surname><given-names>V</given-names></name><name><surname>Riley</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Molecular graph convolutions: moving beyond fingerprints</article-title><source>J. Computer Aided Mol. Des.</source><year>2016</year><volume>30</volume><fpage>595</fpage><lpage>608</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsVSlsb%2FF</pub-id><pub-id pub-id-type="doi">10.1007/s10822-016-9938-8</pub-id></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albrecht</surname><given-names>T</given-names></name><name><surname>Slabaugh</surname><given-names>G</given-names></name><name><surname>Alonso</surname><given-names>E</given-names></name><name><surname>Al-Arif</surname><given-names>SMR</given-names></name></person-group><article-title xml:lang="en">Deep learning for single-molecule science</article-title><source>Nanotechnology</source><year>2017</year><volume>28</volume><fpage>423001</fpage><pub-id pub-id-type="doi">10.1088/1361-6528/aa8334</pub-id></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname><given-names>M</given-names></name><name><surname>Su</surname><given-names>F</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Su</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Deep learning analysis on microscopic imaging in materials science</article-title><source>Mater. Today Nano</source><year>2020</year><volume>11</volume><fpage>100087</fpage><pub-id pub-id-type="doi">10.1016/j.mtnano.2020.100087</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Agrawal, A., Gopalakrishnan, K. &amp; Choudhary, A. In <italic>Handbook on Big Data and Machine Learning in the Physical Sciences: Volume 1. Big Data Methods in Experimental Materials Discovery</italic> World Scientific Series on Emerging Technologies, 205–230 (“World Scientific, 2020).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Erdmann, M., Glombitza, J., Kasieczka, G. &amp; Klemradt, U. <italic>Deep Learning for Physics Research</italic> (World Scientific, 2021).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Graph networks as a universal machine learning framework for molecules and crystals</article-title><source>Chem. Mater.</source><year>2019</year><volume>31</volume><fpage>3564</fpage><lpage>3572</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXntFaqt7g%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.9b01294</pub-id></mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Enhancing materials property prediction by leveraging computational and experimental data using deep transfer learning</article-title><source>Nat. Commun</source><year>2019</year><volume>10</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitFKis73F</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13297-w</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cubuk</surname><given-names>ED</given-names></name><name><surname>Sendek</surname><given-names>AD</given-names></name><name><surname>Reed</surname><given-names>EJ</given-names></name></person-group><article-title xml:lang="en">Screening billions of candidates for solid lithium-ion conductors: a transfer learning approach for small data</article-title><source>J. Chem. Phys.</source><year>2019</year><volume>150</volume><fpage>214701</fpage><pub-id pub-id-type="doi">10.1063/1.5093220</pub-id></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Ye</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Learning properties of ordered and disordered materials from multi-fidelity data</article-title><source>Nat. Comput. Sci.</source><year>2021</year><volume>1</volume><fpage>46</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1038/s43588-020-00002-x</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Artrith</surname><given-names>N</given-names></name><etal/></person-group><article-title xml:lang="en">Best practices in machine learning for chemistry</article-title><source>Nat. Chem.</source><year>2021</year><volume>13</volume><fpage>505</fpage><lpage>508</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXht1WqtrbL</pub-id><pub-id pub-id-type="doi">10.1038/s41557-021-00716-z</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holm</surname><given-names>EA</given-names></name></person-group><article-title xml:lang="en">In defense of the black box</article-title><source>Science</source><year>2019</year><volume>364</volume><fpage>26</fpage><lpage>27</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsVOhs7bP</pub-id><pub-id pub-id-type="doi">10.1126/science.aax0162</pub-id></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname><given-names>T</given-names></name><name><surname>Kusne</surname><given-names>AG</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Machine learning in materials science: Recent progress and emerging applications</article-title><source>Rev. Comput. Chem.</source><year>2016</year><volume>29</volume><fpage>186</fpage><lpage>273</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXht1arsbbP</pub-id></mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning in materials science</article-title><source>InfoMat</source><year>2019</year><volume>1</volume><fpage>338</fpage><lpage>358</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsVKnt7jK</pub-id><pub-id pub-id-type="doi">10.1002/inf2.12028</pub-id></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning in materials genome initiative: a review</article-title><source>J. Mater. Sci. Technol.</source><year>2020</year><volume>57</volume><fpage>113</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1016/j.jmst.2020.01.067</pub-id></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>AY-T</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning for materials scientists: an introductory guide toward best practices</article-title><source>Chem. Mater.</source><year>2020</year><volume>32</volume><fpage>4954</fpage><lpage>4965</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXpslyntrs%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c01907</pub-id></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morgan</surname><given-names>D</given-names></name><name><surname>Jacobs</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Opportunities and challenges for machine learning in materials science</article-title><source>Annu. Rev. Mater. Res.</source><year>2020</year><volume>50</volume><fpage>71</fpage><lpage>103</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXoslegsbs%3D</pub-id><pub-id pub-id-type="doi">10.1146/annurev-matsci-070218-010015</pub-id></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himanen</surname><given-names>L</given-names></name><name><surname>Geurts</surname><given-names>A</given-names></name><name><surname>Foster</surname><given-names>AS</given-names></name><name><surname>Rinke</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Data-driven materials science: status, challenges, and perspectives</article-title><source>Adv. Sci.</source><year>2019</year><volume>6</volume><fpage>1900808</fpage><pub-id pub-id-type="doi">10.1002/advs.201900808</pub-id></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Rajan, K. <italic>Informatics for materials science and engineering: data-driven discovery for accelerated experimentation and application</italic> (Butterworth-Heinemann, 2013).</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montáns</surname><given-names>FJ</given-names></name><name><surname>Chinesta</surname><given-names>F</given-names></name><name><surname>Gómez-Bombarelli</surname><given-names>R</given-names></name><name><surname>Kutz</surname><given-names>JN</given-names></name></person-group><article-title xml:lang="en">Data-driven modeling and learning in science and engineering</article-title><source>Comptes Rendus Mécanique</source><year>2019</year><volume>347</volume><fpage>845</fpage><lpage>855</lpage><pub-id pub-id-type="doi">10.1016/j.crme.2019.11.009</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aykol</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">The materials research platform: defining the requirements from user stories</article-title><source>Matter</source><year>2019</year><volume>1</volume><fpage>1433</fpage><lpage>1438</lpage><pub-id pub-id-type="doi">10.1016/j.matt.2019.10.024</pub-id></mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanev</surname><given-names>V</given-names></name><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>Kusne</surname><given-names>AG</given-names></name><name><surname>Paglione</surname><given-names>J</given-names></name><name><surname>Takeuchi</surname><given-names>I</given-names></name></person-group><article-title xml:lang="en">Artificial intelligence for search and discovery of quantum materials</article-title><source>Commun. Mater.</source><year>2021</year><volume>2</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s43246-021-00209-z</pub-id></mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">A critical review of machine learning of energy materials</article-title><source>Adv. Energy Mater.</source><year>2020</year><volume>10</volume><fpage>1903242</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhslens7o%3D</pub-id><pub-id pub-id-type="doi">10.1002/aenm.201903242</pub-id></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cybenko</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Approximation by superpositions of a sigmoidal function</article-title><source>Math. Control Signals Syst.</source><year>1989</year><volume>2</volume><fpage>303</fpage><lpage>314</lpage><pub-id pub-id-type="doi">10.1007/BF02551274</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="other">Kidger, P. &amp; Lyons, T. <italic>Universal approximation with deep narrow networks</italic>. in <italic>Conference on learning theory</italic>, 2306–2327 (PMLR, 2020).</mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>HW</given-names></name><name><surname>Tegmark</surname><given-names>M</given-names></name><name><surname>Rolnick</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Why does deep and cheap learning work so well?</article-title><source>J. Stat. Phys.</source><year>2017</year><volume>168</volume><fpage>1223</fpage><lpage>1247</lpage><pub-id pub-id-type="doi">10.1007/s10955-017-1836-5</pub-id></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Minsky, M. &amp; Papert, S. A. <italic>Perceptrons: An introduction to computational geometry</italic> (MIT press, 2017).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Pytorch: An imperative style, high-performance deep learning library</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2019</year><volume>32</volume><fpage>8026</fpage><lpage>8037</lpage></mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Abadi et al., TensorFlow: A system for large-scale machine learning. arXiv:1605.08695, Preprint at <ext-link xlink:href="https://arxiv.org/abs/1605.08695" ext-link-type="uri">https://arxiv.org/abs/1605.08695</ext-link> (2006).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Chen, T. et al. Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1512.01274" ext-link-type="uri">https://arxiv.org/abs/1512.01274</ext-link> (2015).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Nwankpa, C., Ijomah, W., Gachagan, A. &amp; Marshall, S. Activation functions: comparison of trends in practice and research for deep learning. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1811.03378" ext-link-type="uri">https://arxiv.org/abs/1811.03378</ext-link> (2018).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baydin</surname><given-names>AG</given-names></name><name><surname>Pearlmutter</surname><given-names>BA</given-names></name><name><surname>Radul</surname><given-names>AA</given-names></name><name><surname>Siskind</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Automatic differentiation in machine learning: a survey</article-title><source>J. Machine Learn. Res.</source><year>2018</year><volume>18</volume><fpage>1</fpage><lpage>43</lpage></mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I. &amp; Salakhutdinov, R. R. Improving neural networks by preventing co-adaptation of feature detectors. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1207.0580" ext-link-type="uri">https://arxiv.org/abs/1207.0580</ext-link> (2012).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breiman</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Bagging predictors</article-title><source>Machine Learn.</source><year>1996</year><volume>24</volume><fpage>123</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1007/BF00058655</pub-id></mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">LeCun, Y. et al. <italic>The Handbook of Brain Theory and Neural Networks</italic> vol. 3361 (MIT press Cambridge, MA, USA 1995).</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="other">Wilson, R. J. <italic>Introduction to Graph Theory</italic> (Pearson Education India, 1979).</mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">West, D. B. et al. <italic>Introduction to Graph Theory</italic> Vol. 2 (Prentice hall Upper Saddle River, 2001).</mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Wang, M. et al. Deep graph library: A graph-centric, highly-performant package for graph neural networks. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1909.01315" ext-link-type="uri">https://arxiv.org/abs/1909.01315</ext-link> (2019).</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>DeCost</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Atomistic line graph neural network for improved materials property predictions</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00650-1</pub-id></mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="other">Li, M. et al. Dgl-lifesci: An open-source toolkit for deep learning on graphs in life science. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2106.14232" ext-link-type="uri">https://arxiv.org/abs/2106.14232</ext-link> (2021).</mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Crystal graph convolutional neural networks for an accurate and interpretable prediction of material properties</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>145301</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSnu7c%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.145301</pub-id></mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="other">Klicpera, J., Groß, J. &amp; Günnemann, S. Directional message passing for molecular graphs. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2003.03123" ext-link-type="uri">https://arxiv.org/abs/2003.03123</ext-link> (2020).</mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schutt</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Schnetpack: A deep learning toolbox for atomistic systems</article-title><source>J. Chem. Theory Comput.</source><year>2018</year><volume>15</volume><fpage>448</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1021/acs.jctc.8b00908</pub-id></mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Kipf, T. N. &amp; Welling, M. Semi-supervised classification with graph convolutional networks. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1609.02907" ext-link-type="uri">https://arxiv.org/abs/1609.02907</ext-link> (2016).</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Veličković, P. et al. Graph attention networks. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1710.10903" ext-link-type="uri">https://arxiv.org/abs/1710.10903</ext-link> (2017).</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Schlichtkrull, M. et al. Modeling relational data with graph convolutional networks. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1703.06103" ext-link-type="uri">https://arxiv.org/abs/1703.06103</ext-link> (2017).</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Song, L., Zhang, Y., Wang, Z. &amp; Gildea, D. <italic>A graph-to-sequence model for AMR-to-text generation</italic>. In <italic>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</italic>, 1616–1626 (Association for Computational Linguistics, 2018).</mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Xu, K., Hu, W., Leskovec, J. &amp; Jegelka, S. How powerful are graph neural networks? <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1810.00826" ext-link-type="uri">https://arxiv.org/abs/1810.00826</ext-link> (2018).</mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="other">Chen, Z., Li, X. &amp; Bruna, J. Supervised community detection with line graph neural networks. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1705.08415" ext-link-type="uri">https://arxiv.org/abs/1705.08415</ext-link> (2017).</mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jing</surname><given-names>Y</given-names></name><name><surname>Bian</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Xie</surname><given-names>X-QS</given-names></name></person-group><article-title xml:lang="en">Deep learning for drug design: an artificial intelligence paradigm for drug discovery in the big data era</article-title><source>AAPS J.</source><year>2018</year><volume>20</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXmslKrtLY%3D</pub-id></mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Devlin, J., Chang, M.-W., Lee, K. &amp; Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1810.04805" ext-link-type="uri">https://arxiv.org/abs/1810.04805</ext-link> (2018).</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">De Cao, N. &amp; Kipf, T. Molgan: An implicit generative model for small molecular graphs. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1805.11973" ext-link-type="uri">https://arxiv.org/abs/1805.11973</ext-link> (2018).</mixed-citation></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>T</given-names></name><name><surname>Abbasi</surname><given-names>M</given-names></name><name><surname>Ribeiro</surname><given-names>B</given-names></name><name><surname>Arrais</surname><given-names>JP</given-names></name></person-group><article-title xml:lang="en">Diversity oriented deep reinforcement learning for targeted molecule generation</article-title><source>J. Cheminformatics</source><year>2021</year><volume>13</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1186/s13321-021-00498-z</pub-id></mixed-citation></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Baker, N. et al. Workshop report on basic research needs for scientific machine learning: core technologies for artificial intelligence. <italic>Tech. Rep</italic>. <ext-link xlink:href="10.2172/1478744" ext-link-type="doi">https://doi.org/10.2172/1478744</ext-link>. (2019).</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chan</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Rapid 3d nanoscale coherent imaging via physics-aware deep learning</article-title><source>Appl. Phys. Rev.</source><year>2021</year><volume>8</volume><fpage>021407</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhtFaqsLrK</pub-id><pub-id pub-id-type="doi">10.1063/5.0031486</pub-id></mixed-citation></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pun</surname><given-names>GP</given-names></name><name><surname>Batra</surname><given-names>R</given-names></name><name><surname>Ramprasad</surname><given-names>R</given-names></name><name><surname>Mishin</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Physically informed artificial neural networks for atomistic modeling of materials</article-title><source>Nat. Commun.</source><year>2019</year><volume>10</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtVGgsrvN</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-10343-5</pub-id></mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="other">Onken, D. et al. A neural network approach for high-dimensional optimal control. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2104.03270" ext-link-type="uri">https://arxiv.org/abs/2104.03270</ext-link> (2021).</mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zunger</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Inverse design in search of materials with target functionalities</article-title><source>Nat. Rev. Chem.</source><year>2018</year><volume>2</volume><fpage>1</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1038/s41570-018-0121</pub-id></mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Nie</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>S</given-names></name><name><surname>Pan</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Generative models for inverse design of inorganic solid materials</article-title><source>J. Mater. Inform.</source><year>2021</year><volume>1</volume><fpage>4</fpage></mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="other">Cranmer, M. et al. Discovering symbolic models from deep learning with inductive biases. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2006.11287" ext-link-type="uri">https://arxiv.org/abs/2006.11287</ext-link> (2020).</mixed-citation></ref><ref id="CR87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rupp</surname><given-names>M</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name><name><surname>Von Lilienfeld</surname><given-names>OA</given-names></name></person-group><article-title xml:lang="en">Fast and accurate modeling of molecular atomization energies with machine learning</article-title><source>Phys. Rev. Lett.</source><year>2012</year><volume>108</volume><fpage>058301</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.108.058301</pub-id></mixed-citation></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartók</surname><given-names>AP</given-names></name><name><surname>Kondor</surname><given-names>R</given-names></name><name><surname>Csányi</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">On representing chemical environments</article-title><source>Phys. Rev. B</source><year>2013</year><volume>87</volume><fpage>184115</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.87.184115</pub-id></mixed-citation></ref><ref id="CR89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faber</surname><given-names>FA</given-names></name><etal/></person-group><article-title xml:lang="en">Prediction errors of molecular machine learning models lower than hybrid dft error</article-title><source>J. Chem. Theory Comput.</source><year>2017</year><volume>13</volume><fpage>5255</fpage><lpage>5264</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhsFWhu7vL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jctc.7b00577</pub-id></mixed-citation></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>DeCost</surname><given-names>B</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Machine learning with force-field-inspired descriptors for materials: Fast screening and mapping energy landscape</article-title><source>Phys. Rev. Mater.</source><year>2018</year><volume>2</volume><fpage>083801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltVKltb0%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.2.083801</pub-id></mixed-citation></ref><ref id="CR91"><label>91.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>Garrity</surname><given-names>KF</given-names></name><name><surname>Ghimire</surname><given-names>NJ</given-names></name><name><surname>Anand</surname><given-names>N</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">High-throughput search for magnetic topological materials using spin-orbit spillage, machine learning, and experiments</article-title><source>Phys. Rev. B</source><year>2021</year><volume>103</volume><fpage>155131</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhtVyitLfK</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevB.103.155131</pub-id></mixed-citation></ref><ref id="CR92"><label>92.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><name><surname>Garrity</surname><given-names>KF</given-names></name><name><surname>Tavazza</surname><given-names>F</given-names></name></person-group><article-title xml:lang="en">Data-driven discovery of 3d and 2d thermoelectric materials</article-title><source>J. Phys. Condens. Matter</source><year>2020</year><volume>32</volume><fpage>475501</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXisF2ltb7I</pub-id><pub-id pub-id-type="doi">10.1088/1361-648X/aba06b</pub-id></mixed-citation></ref><ref id="CR93"><label>93.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Including crystal structure attributes in machine learning models of formation energies via voronoi tessellations</article-title><source>Phys. Rev. B</source><year>2017</year><volume>96</volume><fpage>024104</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.96.024104</pub-id></mixed-citation></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Isayev</surname><given-names>O</given-names></name><etal/></person-group><article-title xml:lang="en">Universal fragment descriptors for predicting properties of inorganic crystals</article-title><source>Nat. Commun.</source><year>2017</year><volume>8</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ncomms15679</pub-id></mixed-citation></ref><ref id="CR95"><label>95.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C-H</given-names></name><name><surname>Tao</surname><given-names>Y</given-names></name><name><surname>Hsu</surname><given-names>D</given-names></name><name><surname>Du</surname><given-names>Q</given-names></name><name><surname>Billinge</surname><given-names>SJ</given-names></name></person-group><article-title xml:lang="en">Using a machine learning approach to determine the space group of a structure from the atomic pair distribution function</article-title><source>Acta Crystallogr. Sec. A</source><year>2019</year><volume>75</volume><fpage>633</fpage><lpage>643</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXht1ymtrnF</pub-id><pub-id pub-id-type="doi">10.1107/S2053273319005606</pub-id></mixed-citation></ref><ref id="CR96"><label>96.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JS</given-names></name><name><surname>Isayev</surname><given-names>O</given-names></name><name><surname>Roitberg</surname><given-names>AE</given-names></name></person-group><article-title xml:lang="en">Ani-1: an extensible neural network potential with dft accuracy at force field computational cost</article-title><source>Chem. Sci.</source><year>2017</year><volume>8</volume><fpage>3192</fpage><lpage>3203</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXitlGnsrs%3D</pub-id><pub-id pub-id-type="doi">10.1039/C6SC05720A</pub-id></mixed-citation></ref><ref id="CR97"><label>97.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behler</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Atom-centered symmetry functions for constructing high-dimensional neural network potentials</article-title><source>J. Chem. Phys.</source><year>2011</year><volume>134</volume><fpage>074106</fpage><pub-id pub-id-type="doi">10.1063/1.3553717</pub-id></mixed-citation></ref><ref id="CR98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behler</surname><given-names>J</given-names></name><name><surname>Parrinello</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Generalized neural-network representation of high-dimensional potential-energy surfaces</article-title><source>Phys. Rev. Lett.</source><year>2007</year><volume>98</volume><fpage>146401</fpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.98.146401</pub-id></mixed-citation></ref><ref id="CR99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>TW</given-names></name><name><surname>Finkler</surname><given-names>JA</given-names></name><name><surname>Goedecker</surname><given-names>S</given-names></name><name><surname>Behler</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">A fourth-generation high-dimensional neural network potential with accurate electrostatics including non-local charge transfer</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhsl2qtrg%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-20427-2</pub-id></mixed-citation></ref><ref id="CR100"><label>100.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinreich</surname><given-names>J</given-names></name><name><surname>Romer</surname><given-names>A</given-names></name><name><surname>Paleico</surname><given-names>ML</given-names></name><name><surname>Behler</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Properties of alpha-brass nanoparticles. 1. neural network potential energy surface</article-title><source>J. Phys. Chem C</source><year>2020</year><volume>124</volume><fpage>12682</fpage><lpage>12695</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXms1Olsr0%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpcc.0c00559</pub-id></mixed-citation></ref><ref id="CR101"><label>101.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>E</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Deepmd-kit: A deep learning package for many-body potential energy representation and molecular dynamics</article-title><source>Computer Phys. Commun.</source><year>2018</year><volume>228</volume><fpage>178</fpage><lpage>184</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXmt1ajsrk%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.cpc.2018.03.016</pub-id></mixed-citation></ref><ref id="CR102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eshet</surname><given-names>H</given-names></name><name><surname>Khaliullin</surname><given-names>RZ</given-names></name><name><surname>Kühne</surname><given-names>TD</given-names></name><name><surname>Behler</surname><given-names>J</given-names></name><name><surname>Parrinello</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Ab initio quality neural-network potential for sodium</article-title><source>Phys. Rev. B</source><year>2010</year><volume>81</volume><fpage>184107</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.81.184107</pub-id></mixed-citation></ref><ref id="CR103"><label>103.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaliullin</surname><given-names>RZ</given-names></name><name><surname>Eshet</surname><given-names>H</given-names></name><name><surname>Kühne</surname><given-names>TD</given-names></name><name><surname>Behler</surname><given-names>J</given-names></name><name><surname>Parrinello</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Graphite-diamond phase coexistence study employing a neural-network mapping of the ab initio potential energy surface</article-title><source>Phys. Rev. B</source><year>2010</year><volume>81</volume><fpage>100103</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.81.100103</pub-id></mixed-citation></ref><ref id="CR104"><label>104.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Artrith</surname><given-names>N</given-names></name><name><surname>Urban</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for tio2</article-title><source>Comput. Mater. Sci.</source><year>2016</year><volume>114</volume><fpage>135</fpage><lpage>150</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtFGisA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2015.11.047</pub-id></mixed-citation></ref><ref id="CR105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>CW</given-names></name><etal/></person-group><article-title xml:lang="en">Accurate and scalable graph neural network force field and molecular dynamics with direct force architecture</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXlsVOktLc%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41524-021-00543-3</pub-id></mixed-citation></ref><ref id="CR106"><label>106.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chmiela</surname><given-names>S</given-names></name><name><surname>Sauceda</surname><given-names>HE</given-names></name><name><surname>Müller</surname><given-names>K-R</given-names></name><name><surname>Tkatchenko</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Towards exact molecular dynamics simulations with machine-learned force fields</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhvVSrtrnL</pub-id><pub-id pub-id-type="doi">10.1038/s41467-018-06169-2</pub-id></mixed-citation></ref><ref id="CR107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>L-Y</given-names></name><etal/></person-group><article-title xml:lang="en">Reaxff-mpnn machine learning potential: a combination of reactive force field and message passing neural networks</article-title><source>Phys. Chem. Chem. Phys.</source><year>2021</year><volume>23</volume><fpage>19457</fpage><lpage>19464</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhslCntbzP</pub-id><pub-id pub-id-type="doi">10.1039/D1CP01656C</pub-id></mixed-citation></ref><ref id="CR108"><label>108.</label><mixed-citation publication-type="other">Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O. &amp; Dahl, G. E. Neural message passing for quantum chemistry. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1704.01212" ext-link-type="uri">https://arxiv.org/abs/1704.01212</ext-link> (2017).</mixed-citation></ref><ref id="CR109"><label>109.</label><mixed-citation publication-type="other">Zitnick, C. L. et al. An introduction to electrocatalyst design using machine learning for renewable energy storage. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2010.09435" ext-link-type="uri">https://arxiv.org/abs/2010.09435</ext-link> (2020).</mixed-citation></ref><ref id="CR110"><label>110.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNutt</surname><given-names>AT</given-names></name><etal/></person-group><article-title xml:lang="en">Gnina 1 molecular docking with deep learning</article-title><source>J. Cheminformatics</source><year>2021</year><volume>13</volume><fpage>1</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1186/s13321-021-00522-2</pub-id></mixed-citation></ref><ref id="CR111"><label>111.</label><mixed-citation publication-type="other">Jin, W., Barzilay, R. &amp; Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. in <italic>International conference on machine learning</italic>, 2323–2332 (PMLR, 2018).</mixed-citation></ref><ref id="CR112"><label>112.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olivecrona</surname><given-names>M</given-names></name><name><surname>Blaschke</surname><given-names>T</given-names></name><name><surname>Engkvist</surname><given-names>O</given-names></name><name><surname>Chen</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Molecular de-novo design through deep reinforcement learning</article-title><source>J. Cheminformatics</source><year>2017</year><volume>9</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1186/s13321-017-0235-x</pub-id></mixed-citation></ref><ref id="CR113"><label>113.</label><mixed-citation publication-type="other">You, J., Liu, B., Ying, R., Pande, V. &amp; Leskovec, J. Graph convolutional policy network for goal-directed molecular graph generation. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1806.02473" ext-link-type="uri">https://arxiv.org/abs/1806.02473</ext-link> (2018).</mixed-citation></ref><ref id="CR114"><label>114.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Putin</surname><given-names>E</given-names></name><etal/></person-group><article-title xml:lang="en">Reinforced adversarial neural computer for de novo molecular design</article-title><source>J. Chem. Inf. Model.</source><year>2018</year><volume>58</volume><fpage>1194</fpage><lpage>1204</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXpsVChtrs%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.7b00690</pub-id></mixed-citation></ref><ref id="CR115"><label>115.</label><mixed-citation publication-type="other">Sanchez-Lengeling, B., Outeiral, C., Guimaraes, G. L. &amp; Aspuru-Guzik, A. Optimizing distributions over molecular space. an objective-reinforced generative adversarial network for inverse-design chemistry (organic). <italic>ChemRxiv</italic><ext-link xlink:href="10.26434/chemrxiv.5309668.v3" ext-link-type="doi">https://doi.org/10.26434/chemrxiv.5309668.v3</ext-link> (2017).</mixed-citation></ref><ref id="CR116"><label>116.</label><mixed-citation publication-type="other">Nouira, A., Sokolovska, N. &amp; Crivello, J.-C. Crystalgan: learning to discover crystallographic structures with generative adversarial networks. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1810.11203" ext-link-type="uri">https://arxiv.org/abs/1810.11203</ext-link> (2018).</mixed-citation></ref><ref id="CR117"><label>117.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Long</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Constrained crystals deep convolutional generative adversarial network for the inverse design of crystal structures</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>66</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhvFShtb3L</pub-id><pub-id pub-id-type="doi">10.1038/s41524-021-00526-4</pub-id></mixed-citation></ref><ref id="CR118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noh</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Inverse design of solid-state materials via a continuous representation</article-title><source>Matter</source><year>2019</year><volume>1</volume><fpage>1370</fpage><lpage>1384</lpage><pub-id pub-id-type="doi">10.1016/j.matt.2019.08.017</pub-id></mixed-citation></ref><ref id="CR119"><label>119.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Noh</surname><given-names>J</given-names></name><name><surname>Gu</surname><given-names>GH</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name><name><surname>Jung</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Generative adversarial networks for crystal structure prediction</article-title><source>ACS Central Sci.</source><year>2020</year><volume>6</volume><fpage>1412</fpage><lpage>1420</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhtlCku77P</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.0c00426</pub-id></mixed-citation></ref><ref id="CR120"><label>120.</label><mixed-citation publication-type="other">Long, T. et al. Inverse design of crystal structures for multicomponent systems. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2104.08040" ext-link-type="uri">https://arxiv.org/abs/2104.08040</ext-link> (2021).</mixed-citation></ref><ref id="CR121"><label>121.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>T</given-names></name><name><surname>Grossman</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">Hierarchical visualization of materials space with graph convolutional neural networks</article-title><source>J. Chem. Phys.</source><year>2018</year><volume>149</volume><fpage>174111</fpage><pub-id pub-id-type="doi">10.1063/1.5047803</pub-id></mixed-citation></ref><ref id="CR122"><label>122.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>CW</given-names></name><name><surname>Wolverton</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Developing an improved crystal graph convolutional neural network framework for accelerated materials discovery</article-title><source>Phys. Rev. Mater.</source><year>2020</year><volume>4</volume><fpage>063801</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFKis7fP</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.4.063801</pub-id></mixed-citation></ref><ref id="CR123"><label>123.</label><mixed-citation publication-type="other">Laugier, L. et al. Predicting thermoelectric properties from crystal graphs and material descriptors-first application for functional materials. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1811.06219" ext-link-type="uri">https://arxiv.org/abs/1811.06219</ext-link> (2018).</mixed-citation></ref><ref id="CR124"><label>124.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname><given-names>AS</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning the quantum-chemical properties of metal–organic frameworks for accelerated materials discovery</article-title><source>Matter</source><year>2021</year><volume>4</volume><fpage>1578</fpage><lpage>1597</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhsl2qurrI</pub-id><pub-id pub-id-type="doi">10.1016/j.matt.2021.02.015</pub-id></mixed-citation></ref><ref id="CR125"><label>125.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lusci</surname><given-names>A</given-names></name><name><surname>Pollastri</surname><given-names>G</given-names></name><name><surname>Baldi</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules</article-title><source>J. Chem. Inf. Model.</source><year>2013</year><volume>53</volume><fpage>1563</fpage><lpage>1575</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3sXpvVGht7g%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci400187y</pub-id></mixed-citation></ref><ref id="CR126"><label>126.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning for drug-induced liver injury</article-title><source>J. Chem. Inf. Model.</source><year>2015</year><volume>55</volume><fpage>2085</fpage><lpage>2093</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXhs1ShsLvO</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.5b00238</pub-id></mixed-citation></ref><ref id="CR127"><label>127.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jain</surname><given-names>A</given-names></name><name><surname>Bligaard</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Atomic-position independent descriptor for machine learning of material properties</article-title><source>Phys. Rev. B</source><year>2018</year><volume>98</volume><fpage>214112</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.98.214112</pub-id></mixed-citation></ref><ref id="CR128"><label>128.</label><mixed-citation publication-type="other">Goodall, R. E., Parackal, A. S., Faber, F. A., Armiento, R. &amp; Lee, A. A. Rapid discovery of novel materials by coordinate-free coarse graining. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2106.11132" ext-link-type="uri">https://arxiv.org/abs/2106.11132</ext-link> (2021).</mixed-citation></ref><ref id="CR129"><label>129.</label><mixed-citation publication-type="other">Zuo, Y. et al. Accelerating Materials Discovery with Bayesian Optimization and Graph Deep Learning. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2104.10242" ext-link-type="uri">https://arxiv.org/abs/2104.10242</ext-link> (2021).</mixed-citation></ref><ref id="CR130"><label>130.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>T-S</given-names></name><etal/></person-group><article-title xml:lang="en">Bigsmiles: a structurally-based line notation for describing macromolecules</article-title><source>ACS Central Sci.</source><year>2019</year><volume>5</volume><fpage>1523</fpage><lpage>1531</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhslCntb7E</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.9b00476</pub-id></mixed-citation></ref><ref id="CR131"><label>131.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyagi</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Cancerppd: a database of anticancer peptides and proteins</article-title><source>Nucleic Acids Res.</source><year>2015</year><volume>43</volume><fpage>D837</fpage><lpage>D843</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhtV2isbvM</pub-id><pub-id pub-id-type="doi">10.1093/nar/gku892</pub-id></mixed-citation></ref><ref id="CR132"><label>132.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krenn</surname><given-names>M</given-names></name><name><surname>Häse</surname><given-names>F</given-names></name><name><surname>Nigam</surname><given-names>A</given-names></name><name><surname>Friederich</surname><given-names>P</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Self-referencing embedded strings (selfies): a 100% robust molecular string representation</article-title><source>Machine Learn. Sci. Technol.</source><year>2020</year><volume>1</volume><fpage>045024</fpage><pub-id pub-id-type="doi">10.1088/2632-2153/aba947</pub-id></mixed-citation></ref><ref id="CR133"><label>133.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>J</given-names></name><name><surname>Ryu</surname><given-names>S</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Kim</surname><given-names>WY</given-names></name></person-group><article-title xml:lang="en">Molecular generative model based on conditional variational autoencoder for de novo molecular design</article-title><source>J. Cheminformatics</source><year>2018</year><volume>10</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1186/s13321-018-0286-7</pub-id></mixed-citation></ref><ref id="CR134"><label>134.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krasnov</surname><given-names>L</given-names></name><name><surname>Khokhlov</surname><given-names>I</given-names></name><name><surname>Fedorov</surname><given-names>MV</given-names></name><name><surname>Sosnin</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Transformer-based artificial neural networks for the conversion between chemical notations</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41598-021-94082-y</pub-id></mixed-citation></ref><ref id="CR135"><label>135.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Irwin</surname><given-names>JJ</given-names></name><name><surname>Sterling</surname><given-names>T</given-names></name><name><surname>Mysinger</surname><given-names>MM</given-names></name><name><surname>Bolstad</surname><given-names>ES</given-names></name><name><surname>Coleman</surname><given-names>RG</given-names></name></person-group><article-title xml:lang="en">Zinc: a free tool to discover chemistry for biology</article-title><source>J. Chem. Inf. Model.</source><year>2012</year><volume>52</volume><fpage>1757</fpage><lpage>1768</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XmvFGnsrg%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci3001277</pub-id></mixed-citation></ref><ref id="CR136"><label>136.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dix</surname><given-names>DJ</given-names></name><etal/></person-group><article-title xml:lang="en">The toxcast program for prioritizing toxicity testing of environmental chemicals</article-title><source>Toxicol. Sci.</source><year>2007</year><volume>95</volume><fpage>5</fpage><lpage>12</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD28XhtlChtLjK</pub-id><pub-id pub-id-type="doi">10.1093/toxsci/kfl103</pub-id></mixed-citation></ref><ref id="CR137"><label>137.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Pubchem 2019 update: improved access to chemical data</article-title><source>Nucleic Acids Res.</source><year>2019</year><volume>47</volume><fpage>D1102</fpage><lpage>D1109</lpage><pub-id pub-id-type="doi">10.1093/nar/gky1033</pub-id></mixed-citation></ref><ref id="CR138"><label>138.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirohara</surname><given-names>M</given-names></name><name><surname>Saito</surname><given-names>Y</given-names></name><name><surname>Koda</surname><given-names>Y</given-names></name><name><surname>Sato</surname><given-names>K</given-names></name><name><surname>Sakakibara</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Convolutional neural network based on smiles representation of compounds for detecting chemical motif</article-title><source>BMC Bioinformatics</source><year>2018</year><volume>19</volume><fpage>83</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1186/s12859-018-2523-5</pub-id></mixed-citation></ref><ref id="CR139"><label>139.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gómez-Bombarelli</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Automatic chemical design using a data-driven continuous representation of molecules</article-title><source>ACS Central Sci.</source><year>2018</year><volume>4</volume><fpage>268</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id></mixed-citation></ref><ref id="CR140"><label>140.</label><mixed-citation publication-type="other">Liu, R. et al. <italic>Deep learning for chemical compound stability prediction</italic>. In <italic>Proceedings of ACM SIGKDD workshop on large-scale deep learning for data mining (DL-KDD)</italic>, 1–7. <ext-link xlink:href="https://rosanneliu.com/publication/kdd/" ext-link-type="uri">https://rosanneliu.com/publication/kdd/</ext-link> (ACM SIGKDD, 2016).</mixed-citation></ref><ref id="CR141"><label>141.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Elemnet: Deep learning the chem. mater. from only elemental composition</article-title><source>Sci. Rep.</source><year>2018</year><volume>8</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41598-018-35934-y</pub-id></mixed-citation></ref><ref id="CR142"><label>142.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters</article-title><source>Integr. Mater. Manuf. Innov.</source><year>2014</year><volume>3</volume><fpage>90</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1186/2193-9772-3-8</pub-id></mixed-citation></ref><ref id="CR143"><label>143.</label><mixed-citation publication-type="other">Agrawal, A. &amp; Choudhary, A. <italic>A fatigue strength predictor for steels using ensemble data mining: steel fatigue strength predictor</italic>. In <italic>Proceedings of the 25th ACM International on Conference on information and knowledge management</italic>, 2497–2500. <ext-link xlink:href="10.1145/2983323.2983343" ext-link-type="doi">https://doi.org/10.1145/2983323.2983343</ext-link> (2016).</mixed-citation></ref><ref id="CR144"><label>144.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">An online tool for predicting fatigue strength of steel alloys based on ensemble data mining</article-title><source>Int. J. Fatigue</source><year>2018</year><volume>113</volume><fpage>389</fpage><lpage>400</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXpslaitbk%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.ijfatigue.2018.04.017</pub-id></mixed-citation></ref><ref id="CR145"><label>145.</label><mixed-citation publication-type="other">Agrawal, A., Saboo, A., Xiong, W., Olson, G. &amp; Choudhary, A. <italic>Martensite start temperature predictor for steels using ensemble data mining</italic>. in <italic>2019 IEEE International Conference on Data Science and Advanced Analytics (DSAA)</italic>, 521–530 (IEEE, 2019).</mixed-citation></ref><ref id="CR146"><label>146.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredig</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Combinatorial screening for new materials in unconstrained composition space with machine learning</article-title><source>Phys. Rev. B</source><year>2014</year><volume>89</volume><fpage>094104</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.89.094104</pub-id></mixed-citation></ref><ref id="CR147"><label>147.</label><mixed-citation publication-type="other">Agrawal, A., Meredig, B., Wolverton, C. &amp; Choudhary, A. <italic>A formation energy predictor for crystalline materials using ensemble data mining</italic>. in <italic>2016 IEEE 16th International Conference on Data Mining Workshops (ICDMW)</italic>, 1276–1279 (IEEE, 2016).</mixed-citation></ref><ref id="CR148"><label>148.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furmanchuk</surname><given-names>A</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Predictive analytics for crystalline materials: bulk modulus</article-title><source>RSC Adv.</source><year>2016</year><volume>6</volume><fpage>95246</fpage><lpage>95251</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsFOrsLnO</pub-id><pub-id pub-id-type="doi">10.1039/C6RA19284J</pub-id></mixed-citation></ref><ref id="CR149"><label>149.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furmanchuk</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Prediction of seebeck coefficient for compounds without restriction to fixed stoichiometry: A machine learning approach</article-title><source>J. Comput. Chem.</source><year>2018</year><volume>39</volume><fpage>191</fpage><lpage>202</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhsFOitbjM</pub-id><pub-id pub-id-type="doi">10.1002/jcc.25067</pub-id></mixed-citation></ref><ref id="CR150"><label>150.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name><name><surname>Wolverton</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">A general-purpose machine learning framework for predicting properties of inorganic materials</article-title><source>npj Comput. Mater.</source><year>2016</year><volume>2</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/npjcompumats.2016.28</pub-id></mixed-citation></ref><ref id="CR151"><label>151.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Matminer: An open source toolkit for materials data mining</article-title><source>Comput. Mater. Sci.</source><year>2018</year><volume>152</volume><fpage>60</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1016/j.commatsci.2018.05.018</pub-id></mixed-citation></ref><ref id="CR152"><label>152.</label><mixed-citation publication-type="other">Jha, D. et al. <italic>Irnet: A general purpose deep residual regression framework for materials discovery</italic>. In <italic>Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</italic>, 2385–2393. <ext-link xlink:href="https://arxiv.org/abs/1907.03222" ext-link-type="uri">https://arxiv.org/abs/1907.03222</ext-link> (2019).</mixed-citation></ref><ref id="CR153"><label>153.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Enabling deeper learning on big data for materials informatics applications</article-title><source>Sci. Rep.</source><year>2021</year><volume>11</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-021-83193-1</pub-id></mixed-citation></ref><ref id="CR154"><label>154.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodall</surname><given-names>RE</given-names></name><name><surname>Lee</surname><given-names>AA</given-names></name></person-group><article-title xml:lang="en">Predicting materials properties without crystal structure: Deep representation learning from stoichiometry</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-19964-7</pub-id></mixed-citation></ref><ref id="CR155"><label>155.</label><mixed-citation publication-type="other">NIMS. <italic>Superconducting material database (supercon)</italic>. <ext-link xlink:href="https://supercon.nims.go.jp/" ext-link-type="uri">https://supercon.nims.go.jp/</ext-link> (2021).</mixed-citation></ref><ref id="CR156"><label>156.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanev</surname><given-names>V</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning modeling of superconducting critical temperature</article-title><source>npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41524-018-0085-8</pub-id></mixed-citation></ref><ref id="CR157"><label>157.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>V</given-names></name><etal/></person-group><article-title xml:lang="en">Cross-property deep transfer learning framework for enhanced predictive analytics on small materials data</article-title><source>Nat. Commun</source><year>2021</year><volume>12</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-26921-5</pub-id></mixed-citation></ref><ref id="CR158"><label>158.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Himanen</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Dscribe: Library of descriptors for machine learning in materials science</article-title><source>Computer Phys. Commun.</source><year>2020</year><volume>247</volume><fpage>106949</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhvV2itrzI</pub-id><pub-id pub-id-type="doi">10.1016/j.cpc.2019.106949</pub-id></mixed-citation></ref><ref id="CR159"><label>159.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartel</surname><given-names>CJ</given-names></name><etal/></person-group><article-title xml:lang="en">A critical examination of compound stability predictions from machine-learned formation energies</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41524-020-00362-y</pub-id></mixed-citation></ref><ref id="CR160"><label>160.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput density functional perturbation theory and machine learning predictions of infrared, piezoelectric, and dielectric responses</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41524-020-0337-2</pub-id></mixed-citation></ref><ref id="CR161"><label>161.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Automated generation and ensemble-learned matching of X-ray absorption spectra</article-title><source>npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>1</fpage><lpage>9</lpage></mixed-citation></ref><ref id="CR162"><label>162.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathew</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">High-throughput computational x-ray absorption spectroscopy</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.151</pub-id></mixed-citation></ref><ref id="CR163"><label>163.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Database of ab initio l-edge x-ray absorption near edge structure</article-title><source>Sci. Data</source><year>2021</year><volume>8</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41597-021-00936-5</pub-id></mixed-citation></ref><ref id="CR164"><label>164.</label><mixed-citation publication-type="other">Lafuente, B., Downs, R. T., Yang, H. &amp; Stone, N. In <italic>Highlights in mineralogical crystallography</italic> 1–30 (De Gruyter (O), 2015).</mixed-citation></ref><ref id="CR165"><label>165.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>El Mendili</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Raman open database: first interconnected raman–x-ray diffraction open-access resource for material identification</article-title><source>J. Appl. Crystallogr.</source><year>2019</year><volume>52</volume><fpage>618</fpage><lpage>625</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtFGqtr3F</pub-id><pub-id pub-id-type="doi">10.1107/S1600576719004229</pub-id></mixed-citation></ref><ref id="CR166"><label>166.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fremout</surname><given-names>W</given-names></name><name><surname>Saverwyns</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Identification of synthetic organic pigments: the role of a comprehensive digital raman spectral library</article-title><source>J. Raman Spectrosc.</source><year>2012</year><volume>43</volume><fpage>1536</fpage><lpage>1544</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhtVymtrbL</pub-id><pub-id pub-id-type="doi">10.1002/jrs.4054</pub-id></mixed-citation></ref><ref id="CR167"><label>167.</label><mixed-citation publication-type="other">Huck, P. &amp; Persson, K. A. <italic>Mpcontribs: user contributed data to the materials project database</italic>. <ext-link xlink:href="https://docs.mpcontribs.org/" ext-link-type="uri">https://docs.mpcontribs.org/</ext-link> (2019).</mixed-citation></ref><ref id="CR168"><label>168.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">A cloud platform for atomic pair distribution function analysis: Pdfitc</article-title><source>Acta Crystallogr. A</source><year>2021</year><volume>77</volume><fpage>2</fpage><lpage>6</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXjt1SqsA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1107/S2053273320013066</pub-id></mixed-citation></ref><ref id="CR169"><label>169.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>WB</given-names></name><etal/></person-group><article-title xml:lang="en">Classification of crystal structure using a convolutional neural network</article-title><source>IUCrJ</source><year>2017</year><volume>4</volume><fpage>486</fpage><lpage>494</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhtFegurbK</pub-id><pub-id pub-id-type="doi">10.1107/S205225251700714X</pub-id></mixed-citation></ref><ref id="CR170"><label>170.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hellenbrandt</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">The Inorganic Crystal Structure Database (ICSD)—present and future</article-title><source>Crystallogr. Rev.</source><year>2004</year><volume>10</volume><fpage>17</fpage><lpage>22</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2cXjt1ahsrY%3D</pub-id><pub-id pub-id-type="doi">10.1080/08893110410001664882</pub-id></mixed-citation></ref><ref id="CR171"><label>171.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zaloga</surname><given-names>AN</given-names></name><name><surname>Stanovov</surname><given-names>VV</given-names></name><name><surname>Bezrukova</surname><given-names>OE</given-names></name><name><surname>Dubinin</surname><given-names>PS</given-names></name><name><surname>Yakimov</surname><given-names>IS</given-names></name></person-group><article-title xml:lang="en">Crystal symmetry classification from powder X-ray diffraction patterns using a convolutional neural network</article-title><source>Mater. Today Commun.</source><year>2020</year><volume>25</volume><fpage>101662</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvFaqtLzJ</pub-id><pub-id pub-id-type="doi">10.1016/j.mtcomm.2020.101662</pub-id></mixed-citation></ref><ref id="CR172"><label>172.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>J-W</given-names></name><name><surname>Park</surname><given-names>WB</given-names></name><name><surname>Lee</surname><given-names>JH</given-names></name><name><surname>Singh</surname><given-names>SP</given-names></name><name><surname>Sohn</surname><given-names>K-S</given-names></name></person-group><article-title xml:lang="en">A deep-learning technique for phase identification in multiphase inorganic compounds using synthetic XRD powder patterns</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXlslWjsQ%3D%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41467-019-13749-3</pub-id></mixed-citation></ref><ref id="CR173"><label>173.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">Rapid identification of X-ray diffraction patterns based on very limited data by interpretable convolutional neural networks</article-title><source>J. Chem. Inf. Model.</source><year>2020</year><volume>60</volume><fpage>2004</fpage><lpage>2011</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXls12iurs%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.0c00020</pub-id></mixed-citation></ref><ref id="CR174"><label>174.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dong</surname><given-names>H</given-names></name><etal/></person-group><article-title xml:lang="en">A deep convolutional neural network for real-time full profile analysis of big powder diffraction data</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00542-4</pub-id></mixed-citation></ref><ref id="CR175"><label>175.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aguiar</surname><given-names>JA</given-names></name><name><surname>Gong</surname><given-names>ML</given-names></name><name><surname>Tasdizen</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Crystallographic prediction from diffraction and chemistry data for higher throughput classification using machine learning</article-title><source>Comput. Mater. Sci.</source><year>2020</year><volume>173</volume><fpage>109409</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitlWqur%2FE</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2019.109409</pub-id></mixed-citation></ref><ref id="CR176"><label>176.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maffettone</surname><given-names>PM</given-names></name><etal/></person-group><article-title xml:lang="en">Crystallography companion agent for high-throughput materials discovery</article-title><source>Nat. Comput. Sci.</source><year>2021</year><volume>1</volume><fpage>290</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1038/s43588-021-00059-2</pub-id></mixed-citation></ref><ref id="CR177"><label>177.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oviedo</surname><given-names>F</given-names></name><etal/></person-group><article-title xml:lang="en">Fast and interpretable classification of small X-ray diffraction datasets using data augmentation and deep neural networks</article-title><source>npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXpvV2isbk%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41524-019-0196-x</pub-id></mixed-citation></ref><ref id="CR178"><label>178.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>C-H</given-names></name><etal/></person-group><article-title xml:lang="en">Validation of non-negative matrix factorization for rapid assessment of large sets of atomic pair-distribution function (pdf) data</article-title><source>J. Appl. Crystallogr.</source><year>2021</year><volume>54</volume><fpage>768</fpage><lpage>775</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXht1Cmt7zL</pub-id><pub-id pub-id-type="doi">10.1107/S160057672100265X</pub-id></mixed-citation></ref><ref id="CR179"><label>179.</label><mixed-citation publication-type="other">Rakita, Y. et al. Studying heterogeneities in local nanostructure with scanning nanostructure electron microscopy (snem). <italic>arXiv</italic><ext-link xlink:href="https://arxiv.org/abs/2110.03589" ext-link-type="uri">https://arxiv.org/abs/2110.03589</ext-link> (2021).</mixed-citation></ref><ref id="CR180"><label>180.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timoshenko</surname><given-names>J</given-names></name><name><surname>Lu</surname><given-names>D</given-names></name><name><surname>Lin</surname><given-names>Y</given-names></name><name><surname>Frenkel</surname><given-names>AI</given-names></name></person-group><article-title xml:lang="en">Supervised machine-learning-based determination of three-dimensional structure of metallic nanoparticles</article-title><source>J. Phys. Chem Lett.</source><year>2017</year><volume>8</volume><fpage>5091</fpage><lpage>5098</lpage><pub-id pub-id-type="doi">10.1021/acs.jpclett.7b02364</pub-id></mixed-citation></ref><ref id="CR181"><label>181.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timoshenko</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Subnanometer substructures in nanoassemblies formed from clusters under a reactive atmosphere revealed using machine learning</article-title><source>J. Phys. Chem C</source><year>2018</year><volume>122</volume><fpage>21686</fpage><lpage>21693</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhsF2gsrnE</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpcc.8b07952</pub-id></mixed-citation></ref><ref id="CR182"><label>182.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Timoshenko</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Neural network approach for characterizing structural transformations by X-ray absorption fine structure spectroscopy</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>225502</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltVynsrg%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.225502</pub-id></mixed-citation></ref><ref id="CR183"><label>183.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Ong</surname><given-names>SP</given-names></name></person-group><article-title xml:lang="en">Random forest models for accurate identification of coordination environments from X-ray absorption near-edge structure</article-title><source>Patterns</source><year>2020</year><volume>1</volume><fpage>100013</fpage><pub-id pub-id-type="doi">10.1016/j.patter.2020.100013</pub-id></mixed-citation></ref><ref id="CR184"><label>184.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torrisi</surname><given-names>SB</given-names></name><etal/></person-group><article-title xml:lang="en">Random forest machine learning models for interpretable X-ray absorption near-edge structure spectrum-property relationships</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41524-020-00376-6</pub-id></mixed-citation></ref><ref id="CR185"><label>185.</label><mixed-citation publication-type="other">Andrejevic, N., Andrejevic, J., Rycroft, C. H. &amp; Li, M. Machine learning spectral indicators of topology. <italic>arXiv</italic> preprint at <ext-link xlink:href="https://arxiv.org/abs/2003.00994" ext-link-type="uri">https://arxiv.org/abs/2003.00994</ext-link> (2020).</mixed-citation></ref><ref id="CR186"><label>186.</label><mixed-citation publication-type="other">Madden, M. G. &amp; Ryder, A. G. <italic>Machine learning methods for quantitative analysis of raman spectroscopy data</italic>. in <italic>Opto-Ireland 2002: Optics and Photonics Technologies and Applications</italic>, Vol. 4876, 1130–1139 (International Society for Optics and Photonics, 2003).</mixed-citation></ref><ref id="CR187"><label>187.</label><mixed-citation publication-type="other">Conroy, J., Ryder, A. G., Leger, M. N., Hennessey, K. &amp; Madden, M. G. <italic>Qualitative and quantitative analysis of chlorinated solvents using Raman spectroscopy and machine learning</italic>. in Opto-Ireland 2005: Optical Sensing and Spectroscopy, Vol. 5826, 131–142 (International Society for Optics and Photonics, 2005).</mixed-citation></ref><ref id="CR188"><label>188.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acquarelli</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Convolutional neural networks for vibrational spectroscopic data analysis</article-title><source>Anal. Chim. Acta</source><year>2017</year><volume>954</volume><fpage>22</fpage><lpage>31</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhtFGitw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.aca.2016.12.010</pub-id></mixed-citation></ref><ref id="CR189"><label>189.</label><mixed-citation publication-type="other">O’Connell, M.-L., Howley, T., Ryder, A. G., Leger, M. N. &amp; Madden, M. G. <italic>Classification of a target analyte in solid mixtures using principal component analysis, support vector machines, and Raman spectroscopy</italic>. in <italic>Opto-Ireland 2005: Optical Sensing and Spectroscopy</italic>, Vol. 5826, 340–350 (International Society for Optics and Photonics, 2005).</mixed-citation></ref><ref id="CR190"><label>190.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Chen</surname><given-names>Q</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Fang</surname><given-names>CH</given-names></name></person-group><article-title xml:lang="en">Qualitative identification of tea categories by near infrared spectroscopy and support vector machine</article-title><source>J. Pharm. Biomed. Anal.</source><year>2006</year><volume>41</volume><fpage>1198</fpage><lpage>1204</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD28XltFOkt7g%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.jpba.2006.02.053</pub-id></mixed-citation></ref><ref id="CR191"><label>191.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Deep convolutional neural networks for Raman spectrum recognition: a unified solution</article-title><source>Analyst</source><year>2017</year><volume>142</volume><fpage>4067</fpage><lpage>4074</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhsFOhsb3N</pub-id><pub-id pub-id-type="doi">10.1039/C7AN01371J</pub-id></mixed-citation></ref><ref id="CR192"><label>192.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning for vibrational spectral analysis: Recent progress and a practical guide</article-title><source>Anal. Chim. Acta</source><year>2019</year><volume>1081</volume><fpage>6</fpage><lpage>17</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtFOrurjK</pub-id><pub-id pub-id-type="doi">10.1016/j.aca.2019.06.012</pub-id></mixed-citation></ref><ref id="CR193"><label>193.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Selzer</surname><given-names>P</given-names></name><name><surname>Gasteiger</surname><given-names>J</given-names></name><name><surname>Thomas</surname><given-names>H</given-names></name><name><surname>Salzer</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Rapid access to infrared reference spectra of arbitrary organic compounds: scope and limitations of an approach to the simulation of infrared spectra by neural networks</article-title><source>Chem. Euro. J.</source><year>2000</year><volume>6</volume><fpage>920</fpage><lpage>927</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3cXhvVKhs7o%3D</pub-id><pub-id pub-id-type="doi">10.1002/(SICI)1521-3765(20000303)6:5&lt;920::AID-CHEM920&gt;3.0.CO;2-W</pub-id></mixed-citation></ref><ref id="CR194"><label>194.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning spectroscopy: neural networks for molecular excitation spectra</article-title><source>Adv. Sci.</source><year>2019</year><volume>6</volume><fpage>1801367</fpage><pub-id pub-id-type="doi">10.1002/advs.201801367</pub-id></mixed-citation></ref><ref id="CR195"><label>195.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kostka</surname><given-names>T</given-names></name><name><surname>Selzer</surname><given-names>P</given-names></name><name><surname>Gasteiger</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">A combined application of reaction prediction and infrared spectra simulation for the identification of degradation products of s-triazine herbicides</article-title><source>Chemistry</source><year>2001</year><volume>7</volume><fpage>2254</fpage><lpage>2260</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3MXktVajsLo%3D</pub-id><pub-id pub-id-type="doi">10.1002/1521-3765(20010518)7:10&lt;2254::AID-CHEM2254&gt;3.0.CO;2-#</pub-id></mixed-citation></ref><ref id="CR196"><label>196.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmoud</surname><given-names>CB</given-names></name><name><surname>Anelli</surname><given-names>A</given-names></name><name><surname>Csányi</surname><given-names>G</given-names></name><name><surname>Ceriotti</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">Learning the electronic density of states in condensed matter</article-title><source>Phys. Rev. B</source><year>2020</year><volume>102</volume><fpage>235130</fpage><pub-id pub-id-type="doi">10.1103/PhysRevB.102.235130</pub-id></mixed-citation></ref><ref id="CR197"><label>197.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Direct prediction of phonon density of states with Euclidean neural networks</article-title><source>Adv. Sci.</source><year>2021</year><volume>8</volume><fpage>2004214</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXisV2qu7jF</pub-id><pub-id pub-id-type="doi">10.1002/advs.202004214</pub-id></mixed-citation></ref><ref id="CR198"><label>198.</label><mixed-citation publication-type="other">Kong, S. et al. Density of states prediction for materials discovery via contrastive learning from probabilistic embeddings. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2110.11444" ext-link-type="uri">https://arxiv.org/abs/2110.11444</ext-link> (2021).</mixed-citation></ref><ref id="CR199"><label>199.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carbone</surname><given-names>MR</given-names></name><name><surname>Topsakal</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>D</given-names></name><name><surname>Yoo</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Machine-learning X-ray absorption spectra to quantitative accuracy</article-title><source>Phys. Rev. Lett.</source><year>2020</year><volume>124</volume><fpage>156401</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXptF2gtbk%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.124.156401</pub-id></mixed-citation></ref><ref id="CR200"><label>200.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rehr</surname><given-names>JJ</given-names></name><name><surname>Kas</surname><given-names>JJ</given-names></name><name><surname>Vila</surname><given-names>FD</given-names></name><name><surname>Prange</surname><given-names>MP</given-names></name><name><surname>Jorissen</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Parameter-free calculations of X-ray spectra with FEFF9</article-title><source>Phys. Chem. Chem. Phys.</source><year>2010</year><volume>12</volume><fpage>5503</fpage><lpage>5513</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3cXmt1KqtrY%3D</pub-id><pub-id pub-id-type="doi">10.1039/b926434e</pub-id></mixed-citation></ref><ref id="CR201"><label>201.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rankine</surname><given-names>CD</given-names></name><name><surname>Madkhali</surname><given-names>MMM</given-names></name><name><surname>Penfold</surname><given-names>TJ</given-names></name></person-group><article-title xml:lang="en">A deep neural network for the rapid prediction of X-ray absorption spectra</article-title><source>J. Phys. Chem A</source><year>2020</year><volume>124</volume><fpage>4263</fpage><lpage>4270</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXosF2gtrY%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpca.0c03723</pub-id></mixed-citation></ref><ref id="CR202"><label>202.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname><given-names>V</given-names></name><name><surname>Hu</surname><given-names>G</given-names></name><name><surname>Ganesh</surname><given-names>P</given-names></name><name><surname>Sumpter</surname><given-names>BG</given-names></name></person-group><article-title xml:lang="en">Machine learned features from density of states for accurate adsorption energy prediction</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXnsVOjtQ%3D%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41467-020-20342-6</pub-id></mixed-citation></ref><ref id="CR203"><label>203.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammer</surname><given-names>B</given-names></name><name><surname>Nørskov</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Theoretical surface science and catalysis-calculations and concepts</article-title><source>Adv. Catal. Impact Surface Sci. Catal.</source><year>2000</year><volume>45</volume><fpage>71</fpage><lpage>129</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3cXlslajurc%3D</pub-id><pub-id pub-id-type="doi">10.1016/S0360-0564(02)45013-4</pub-id></mixed-citation></ref><ref id="CR204"><label>204.</label><mixed-citation publication-type="other">Kaundinya, P. R., Choudhary, K. &amp; Kalidindi, S. R. Prediction of the electron density of states for crystalline compounds with atomistic line graph neural networks (alignn). <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2201.08348" ext-link-type="uri">https://arxiv.org/abs/2201.08348</ext-link> (2022).</mixed-citation></ref><ref id="CR205"><label>205.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>HS</given-names></name><name><surname>Soedarmadji</surname><given-names>E</given-names></name><name><surname>Newhouse</surname><given-names>PF</given-names></name><name><surname>Guevarra</surname><given-names>D</given-names></name><name><surname>Gregoire</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Synthesis, optical imaging, and absorption spectroscopy data for 179072 metal oxides</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume><pub-id pub-id-type="doi">10.1038/s41597-019-0019-4</pub-id></mixed-citation></ref><ref id="CR206"><label>206.</label><mixed-citation publication-type="other">Choudhary, A. et al. Graph neural network predictions of metal organic framework co2 adsorption properties. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2112.10231" ext-link-type="uri">https://arxiv.org/abs/2112.10231</ext-link> (2021).</mixed-citation></ref><ref id="CR207"><label>207.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>R</given-names></name><name><surname>Biong</surname><given-names>A</given-names></name><name><surname>Gómez-Gualdrón</surname><given-names>DA</given-names></name></person-group><article-title xml:lang="en">Adsorption isotherm predictions for multiple molecules in mofs using the same deep learning model</article-title><source>J. Chem. Theory Comput.</source><year>2020</year><volume>16</volume><fpage>1271</fpage><lpage>1283</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXnsl2gtA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jctc.9b00940</pub-id></mixed-citation></ref><ref id="CR208"><label>208.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title xml:lang="en">Imagenet classification with deep convolutional neural networks</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2012</year><volume>25</volume><fpage>1097</fpage><lpage>1105</lpage></mixed-citation></ref><ref id="CR209"><label>209.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Varela</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Materials characterization in the aberration-corrected scanning transmission electron microscope</article-title><source>Annu. Rev. Mater. Res.</source><year>2005</year><volume>35</volume><fpage>539</fpage><lpage>569</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2MXpsFGjtL8%3D</pub-id><pub-id pub-id-type="doi">10.1146/annurev.matsci.35.102103.090513</pub-id></mixed-citation></ref><ref id="CR210"><label>210.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holm</surname><given-names>EA</given-names></name><etal/></person-group><article-title xml:lang="en">Overview: Computer vision and machine learning for microstructural characterization and analysis</article-title><source>Metal. Mater Trans. A</source><year>2020</year><volume>51</volume><fpage>5985</fpage><lpage>5999</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvFCgs7bM</pub-id><pub-id pub-id-type="doi">10.1007/s11661-020-06008-4</pub-id></mixed-citation></ref><ref id="CR211"><label>211.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Modarres</surname><given-names>MH</given-names></name><etal/></person-group><article-title xml:lang="en">Neural network for nanoscience scanning electron microscope image recognition</article-title><source>Sci. Rep.</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhsFWgtbfF</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-13565-z</pub-id></mixed-citation></ref><ref id="CR212"><label>212.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopalakrishnan</surname><given-names>K</given-names></name><name><surname>Khaitan</surname><given-names>SK</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep convolutional neural networks with transfer learning for computer vision-based data-driven pavement distress detection</article-title><source>Construct. Build. Mater.</source><year>2017</year><volume>157</volume><fpage>322</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1016/j.conbuildmat.2017.09.110</pub-id></mixed-citation></ref><ref id="CR213"><label>213.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopalakrishnan</surname><given-names>K</given-names></name><name><surname>Gholami</surname><given-names>H</given-names></name><name><surname>Vidyadharan</surname><given-names>A</given-names></name><name><surname>Choudhary</surname><given-names>A</given-names></name><name><surname>Agrawal</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Crack damage detection in unmanned aerial vehicle images of civil infrastructure using pre-trained deep learning model</article-title><source>Int. J. Traffic Transp. Eng</source><year>2018</year><volume>8</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.7708/ijtte.2018.8(1).01</pub-id></mixed-citation></ref><ref id="CR214"><label>214.</label><mixed-citation publication-type="other">Yang, Z. et al. <italic>Data-driven insights from predictive analytics on heterogeneous experimental data of industrial magnetic materials</italic>. In <italic>IEEE International Conference on Data Mining Workshops (ICDMW)</italic>, 806–813. <ext-link xlink:href="10.1109/ICDMW.2019.00119" ext-link-type="doi">https://doi.org/10.1109/ICDMW.2019.00119</ext-link> (IEEE Computer Society, 2019).</mixed-citation></ref><ref id="CR215"><label>215.</label><mixed-citation publication-type="other">Yang, Z. et al. <italic>Heterogeneous feature fusion based machine learning on shallow-wide and heterogeneous-sparse industrial datasets</italic>. In <italic>25th International Conference on Pattern Recognition Workshops, ICPR 2020</italic>, 566–577. <ext-link xlink:href="10.1007/978-3-030-68799-1_41" ext-link-type="doi">https://doi.org/10.1007/978-3-030-68799-1_41</ext-link> (Springer Science and Business Media Deutschland GmbH, 2021).</mixed-citation></ref><ref id="CR216"><label>216.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziletti</surname><given-names>A</given-names></name><name><surname>Kumar</surname><given-names>D</given-names></name><name><surname>Scheffler</surname><given-names>M</given-names></name><name><surname>Ghiringhelli</surname><given-names>LM</given-names></name></person-group><article-title xml:lang="en">Insightful classification of crystal structures using deep learning</article-title><source>Nat. Commun.</source><year>2018</year><volume>9</volume><pub-id pub-id-type="doi">10.1038/s41467-018-05169-6</pub-id></mixed-citation></ref><ref id="CR217"><label>217.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Computational scanning tunneling microscope image database</article-title><source>Sci. Data</source><year>2021</year><volume>8</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41597-021-00824-y</pub-id></mixed-citation></ref><ref id="CR218"><label>218.</label><mixed-citation publication-type="other">Liu, R., Agrawal, A., Liao, W.-k., Choudhary, A. &amp; De Graef, M. <italic>Materials discovery: Understanding polycrystals from large-scale electron patterns</italic>. in <italic>2016 IEEE International Conference on Big Data (Big Data)</italic>, 2261–2269 (IEEE, 2016).</mixed-citation></ref><ref id="CR219"><label>219.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jha</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Extracting grain orientations from EBSD patterns of polycrystalline materials using convolutional neural networks</article-title><source>Microsc. Microanal.</source><year>2018</year><volume>24</volume><fpage>497</fpage><lpage>502</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhvFeqs77M</pub-id><pub-id pub-id-type="doi">10.1017/S1431927618015131</pub-id></mixed-citation></ref><ref id="CR220"><label>220.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufmann</surname><given-names>K</given-names></name><name><surname>Zhu</surname><given-names>C</given-names></name><name><surname>Rosengarten</surname><given-names>AS</given-names></name><name><surname>Vecchio</surname><given-names>KS</given-names></name></person-group><article-title xml:lang="en">Deep neural network enabled space group identification in EBSD</article-title><source>Microsc. Microanal.</source><year>2020</year><volume>26</volume><fpage>447</fpage><lpage>457</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXht1OhtbjE</pub-id><pub-id pub-id-type="doi">10.1017/S1431927620001506</pub-id></mixed-citation></ref><ref id="CR221"><label>221.</label><mixed-citation publication-type="other">Yang, Z. et al. <italic>Deep learning based domain knowledge integration for small datasets: Illustrative applications in materials informatics</italic>. in <italic>2019 International Joint Conference on Neural Networks (IJCNN)</italic>, 1–8 (IEEE, 2019).</mixed-citation></ref><ref id="CR222"><label>222.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Learning to predict crystal plasticity at the nanoscale: Deep residual networks and size effects in uniaxial compression discrete dislocation simulations</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>14</lpage></mixed-citation></ref><ref id="CR223"><label>223.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decost</surname><given-names>BL</given-names></name><etal/></person-group><article-title xml:lang="en">Uhcsdb: Ultrahigh carbon steel micrograph database</article-title><source>Integr. Mater. Manuf. Innov.</source><year>2017</year><volume>6</volume><fpage>197</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1007/s40192-017-0097-0</pub-id></mixed-citation></ref><ref id="CR224"><label>224.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Decost</surname><given-names>BL</given-names></name><name><surname>Lei</surname><given-names>B</given-names></name><name><surname>Francis</surname><given-names>T</given-names></name><name><surname>Holm</surname><given-names>EA</given-names></name></person-group><article-title xml:lang="en">High throughput quantitative metallography for complex microstructures using deep learning: a case study in ultrahigh carbon steel</article-title><source>Microsc. Microanal.</source><year>2019</year><volume>25</volume><fpage>21</fpage><lpage>29</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXkslajsL0%3D</pub-id><pub-id pub-id-type="doi">10.1017/S1431927618015635</pub-id></mixed-citation></ref><ref id="CR225"><label>225.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stan</surname><given-names>T</given-names></name><name><surname>Thompson</surname><given-names>ZT</given-names></name><name><surname>Voorhees</surname><given-names>PW</given-names></name></person-group><article-title xml:lang="en">Optimizing convolutional neural networks to perform semantic segmentation on large materials imaging datasets: X-ray tomography and serial sectioning</article-title><source>Materials Characterization</source><year>2020</year><volume>160</volume><fpage>110119</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhtVOksr0%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.matchar.2020.110119</pub-id></mixed-citation></ref><ref id="CR226"><label>226.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">A deep learning approach to identify local structures in atomic-resolution transmission electron microscopy images</article-title><source>Adv. Theory Simulations</source><year>2018</year><volume>1</volume><fpage>1800037</fpage><pub-id pub-id-type="doi">10.1002/adts.201800037</pub-id></mixed-citation></ref><ref id="CR227"><label>227.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maksov</surname><given-names>A</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning analysis of defect and phase evolution during electron beam-induced transformations in ws 2</article-title><source>npj Comput. Mater.</source><year>2019</year><volume>5</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXptFyqtLo%3D</pub-id><pub-id pub-id-type="doi">10.1038/s41524-019-0152-9</pub-id></mixed-citation></ref><ref id="CR228"><label>228.</label><mixed-citation publication-type="other">Yang, S.-H. et al. Deep learning-assisted quantification of atomic dopants and defects in 2d materials. <italic>Adv. Sci.</italic><ext-link xlink:href="10.1002/advs.202101099" ext-link-type="doi">https://doi.org/10.1002/advs.202101099</ext-link> (2021).</mixed-citation></ref><ref id="CR229"><label>229.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roberts</surname><given-names>G</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning for semantic segmentation of defects in advanced stem images of steels</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-019-49105-0</pub-id></mixed-citation></ref><ref id="CR230"><label>230.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusche</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Large-area, high-resolution characterisation and classification of damage mechanisms in dual-phase steel using deep learning</article-title><source>PLoS ONE</source><year>2019</year><volume>14</volume><fpage>e0216493</fpage><pub-id pub-id-type="doi">10.1371/journal.pone.0216493</pub-id></mixed-citation></ref><ref id="CR231"><label>231.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vlcek</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Learning from imperfections: predicting structure and thermodynamics from atomic imaging of fluctuations</article-title><source>ACS Nano</source><year>2019</year><volume>13</volume><fpage>718</fpage><lpage>727</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXkslOkuw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/acsnano.8b07980</pub-id></mixed-citation></ref><ref id="CR232"><label>232.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziatdinov</surname><given-names>M</given-names></name><name><surname>Maksov</surname><given-names>A</given-names></name><name><surname>Kalinin</surname><given-names>SV</given-names></name></person-group><article-title xml:lang="en">Learning surface molecular structures via machine vision</article-title><source>npj Comput. Mater.</source><year>2017</year><volume>3</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXht1Cjs7jN</pub-id><pub-id pub-id-type="doi">10.1038/s41524-017-0038-7</pub-id></mixed-citation></ref><ref id="CR233"><label>233.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ovchinnikov</surname><given-names>OS</given-names></name><etal/></person-group><article-title xml:lang="en">Detection of defects in atomic-resolution images of materials using cycle analysis</article-title><source>Adv. Struct. Chem. Imaging</source><year>2020</year><volume>6</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXlvFylt7k%3D</pub-id><pub-id pub-id-type="doi">10.1186/s40679-020-00070-x</pub-id></mixed-citation></ref><ref id="CR234"><label>234.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Field</surname><given-names>KG</given-names></name><name><surname>Morgan</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Automated defect analysis in electron microscopic images</article-title><source>npj Comput. Mater.</source><year>2018</year><volume>4</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41524-018-0093-8</pub-id></mixed-citation></ref><ref id="CR235"><label>235.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohn</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Instance segmentation for direct measurements of satellites in metal powders and automated microstructural characterization from image data</article-title><source>JOM</source><year>2021</year><volume>73</volume><fpage>2159</fpage><lpage>2172</lpage><pub-id pub-id-type="doi">10.1007/s11837-021-04713-y</pub-id></mixed-citation></ref><ref id="CR236"><label>236.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Haan</surname><given-names>K</given-names></name><name><surname>Ballard</surname><given-names>ZS</given-names></name><name><surname>Rivenson</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>Y</given-names></name><name><surname>Ozcan</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Resolution enhancement in scanning electron microscopy using deep learning</article-title><source>Sci. Rep.</source><year>2019</year><volume>9</volume><fpage>1</fpage><lpage>7</lpage></mixed-citation></ref><ref id="CR237"><label>237.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ede</surname><given-names>JM</given-names></name><name><surname>Beanland</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Partial scanning transmission electron microscopy with deep learning</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-65261-0</pub-id></mixed-citation></ref><ref id="CR238"><label>238.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashidi</surname><given-names>M</given-names></name><name><surname>Wolkow</surname><given-names>RA</given-names></name></person-group><article-title xml:lang="en">Autonomous scanning probe microscopy in situ tip conditioning through machine learning</article-title><source>ACS Nano</source><year>2018</year><volume>12</volume><fpage>5185</fpage><lpage>5189</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXpvVCnsb4%3D</pub-id><pub-id pub-id-type="doi">10.1021/acsnano.8b02208</pub-id></mixed-citation></ref><ref id="CR239"><label>239.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scime</surname><given-names>L</given-names></name><name><surname>Siddel</surname><given-names>D</given-names></name><name><surname>Baird</surname><given-names>S</given-names></name><name><surname>Paquit</surname><given-names>V</given-names></name></person-group><article-title xml:lang="en">Layer-wise anomaly detection and classification for powder bed additive manufacturing processes: A machine-agnostic algorithm for real-time pixel-wise semantic segmentation</article-title><source>Addit. Manufact.</source><year>2020</year><volume>36</volume><fpage>101453</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXlvFw%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.addma.2020.101453</pub-id></mixed-citation></ref><ref id="CR240"><label>240.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eppel</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Bismuth</surname><given-names>M</given-names></name><name><surname>Aspuru-Guzik</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Computer vision for recognition of materials and vessels in chemistry lab settings and the Vector-LabPics Data Set</article-title><source>ACS Central Sci.</source><year>2020</year><volume>6</volume><fpage>1743</fpage><lpage>1752</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhslyisrzM</pub-id><pub-id pub-id-type="doi">10.1021/acscentsci.0c00460</pub-id></mixed-citation></ref><ref id="CR241"><label>241.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning approaches for mining structure-property linkages in high contrast composites from simulation datasets</article-title><source>Comput. Mater. Sci.</source><year>2018</year><volume>151</volume><fpage>278</fpage><lpage>287</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXpvFGgs7g%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2018.05.014</pub-id></mixed-citation></ref><ref id="CR242"><label>242.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cecen</surname><given-names>A</given-names></name><name><surname>Dai</surname><given-names>H</given-names></name><name><surname>Yabansu</surname><given-names>YC</given-names></name><name><surname>Kalidindi</surname><given-names>SR</given-names></name><name><surname>Song</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Material structure-property linkages using three-dimensional convolutional neural networks</article-title><source>Acta Mater.</source><year>2018</year><volume>146</volume><fpage>76</fpage><lpage>84</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXltVemsw%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.actamat.2017.11.053</pub-id></mixed-citation></ref><ref id="CR243"><label>243.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Establishing structure-property localization linkages for elastic deformation of three-dimensional high contrast composites using deep learning approaches</article-title><source>Acta Mater.</source><year>2019</year><volume>166</volume><fpage>335</fpage><lpage>345</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXnslyqug%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.actamat.2018.12.045</pub-id></mixed-citation></ref><ref id="CR244"><label>244.</label><mixed-citation publication-type="other">Goetz, A. et al. Addressing materials’ microstructure diversity using transfer learning. <italic>arXiv</italic>. arXiv-2107. <ext-link xlink:href="https://arxiv.org/abs/2107.13841" ext-link-type="uri">https://arxiv.org/abs/2107.13841</ext-link> (2021).</mixed-citation></ref><ref id="CR245"><label>245.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitahara</surname><given-names>AR</given-names></name><name><surname>Holm</surname><given-names>EA</given-names></name></person-group><article-title xml:lang="en">Microstructure cluster analysis with transfer learning and unsupervised learning</article-title><source>Integr. Mater. Manuf. Innov.</source><year>2018</year><volume>7</volume><fpage>148</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1007/s40192-018-0116-9</pub-id></mixed-citation></ref><ref id="CR246"><label>246.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larmuseau</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Compact representations of microstructure images using triplet networks</article-title><source>npj Comput. Mater. 2020 6:1</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage></mixed-citation></ref><ref id="CR247"><label>247.</label><mixed-citation publication-type="other">Li, X. et al. <italic>A deep adversarial learning methodology for designing microstructural material systems</italic>. in <italic>International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</italic>, Vol. 51760, V02BT03A008 (American Society of Mechanical Engineers, 2018).</mixed-citation></ref><ref id="CR248"><label>248.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">Microstructural materials design via deep adversarial learning methodology</article-title><source>J. Mech. Des.</source><year>2018</year><volume>140</volume><fpage>111416</fpage><pub-id pub-id-type="doi">10.1115/1.4041371</pub-id></mixed-citation></ref><ref id="CR249"><label>249.</label><mixed-citation publication-type="other">Yang, Z. et al. A general framework combining generative adversarial networks and mixture density networks for inverse modeling in microstructural materials design. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2101.10553" ext-link-type="uri">https://arxiv.org/abs/2101.10553</ext-link> (2021).</mixed-citation></ref><ref id="CR250"><label>250.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hsu</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Microstructure generation via generative adversarial network for heterogeneous, topologically complex 3d materials</article-title><source>JOM</source><year>2020</year><volume>73</volume><fpage>90</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1007/s11837-020-04484-y</pub-id></mixed-citation></ref><ref id="CR251"><label>251.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chun</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning for synthetic microstructure generation in a materials-by-design framework for heterogeneous energetic materials</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-70149-0</pub-id></mixed-citation></ref><ref id="CR252"><label>252.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dai</surname><given-names>M</given-names></name><name><surname>Demirel</surname><given-names>MF</given-names></name><name><surname>Liang</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>J-M</given-names></name></person-group><article-title xml:lang="en">Graph neural networks for an accurate and interpretable prediction of the properties of polycrystalline materials</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00574-w</pub-id></mixed-citation></ref><ref id="CR253"><label>253.</label><mixed-citation publication-type="other">Cohn, R. &amp; Holm, E. Neural message passing for predicting abnormal grain growth in Monte Carlo simulations of microstructural evolution. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2110.09326v1" ext-link-type="uri">https://arxiv.org/abs/2110.09326v1</ext-link> (2021).</mixed-citation></ref><ref id="CR254"><label>254.</label><mixed-citation publication-type="other">Plimpton, S. et al. <italic>SPPARKS Kinetic Monte Carlo Simulator</italic>. <ext-link xlink:href="https://spparks.github.io/index.html" ext-link-type="uri">https://spparks.github.io/index.html</ext-link>. (2021).</mixed-citation></ref><ref id="CR255"><label>255.</label><mixed-citation publication-type="other">Plimpton, S. et al. Crossing the mesoscale no-man’s land via parallel kinetic Monte Carlo. <italic>Tech. Rep</italic>. <ext-link xlink:href="10.2172/966942" ext-link-type="doi">https://doi.org/10.2172/966942</ext-link> (2009).</mixed-citation></ref><ref id="CR256"><label>256.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xue</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">Steven bird, evan klein and edward loper. natural language processing with python. oreilly media, inc.2009. isbn: 978-0-596-51649-9</article-title><source>Nat. Lang. Eng.</source><year>2010</year><volume>17</volume><fpage>419</fpage><lpage>424</lpage><pub-id pub-id-type="doi">10.1017/S1351324910000306</pub-id></mixed-citation></ref><ref id="CR257"><label>257.</label><mixed-citation publication-type="other">Honnibal, M. &amp; Montani, I. spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing. <ext-link xlink:href="10.5281/zenodo.3358113" ext-link-type="doi">https://doi.org/10.5281/zenodo.3358113</ext-link> (2017).</mixed-citation></ref><ref id="CR258"><label>258.</label><mixed-citation publication-type="other">Gardner, M. et al. Allennlp: A deep semantic natural language processing platform. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1803.07640" ext-link-type="uri">https://arxiv.org/abs/1803.07640</ext-link> (2018).</mixed-citation></ref><ref id="CR259"><label>259.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tshitoyan</surname><given-names>V</given-names></name><etal/></person-group><article-title xml:lang="en">Unsupervised word embeddings capture latent knowledge from materials science literature</article-title><source>Nature</source><year>2019</year><volume>571</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhtlamurrK</pub-id><pub-id pub-id-type="doi">10.1038/s41586-019-1335-8</pub-id></mixed-citation></ref><ref id="CR260"><label>260.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kononova</surname><given-names>O</given-names></name><etal/></person-group><article-title xml:lang="en">Opportunities and challenges of text mining in aterials research</article-title><source>iScience</source><year>2021</year><volume>24</volume><fpage>102155</fpage><pub-id pub-id-type="doi">10.1016/j.isci.2021.102155</pub-id></mixed-citation></ref><ref id="CR261"><label>261.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olivetti</surname><given-names>EA</given-names></name><etal/></person-group><article-title xml:lang="en">Data-driven materials research enabled by natural language processing and information extraction</article-title><source>Appl. Phys. Rev.</source><year>2020</year><volume>7</volume><fpage>041317</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXis1Olu77L</pub-id><pub-id pub-id-type="doi">10.1063/5.0021106</pub-id></mixed-citation></ref><ref id="CR262"><label>262.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Swain</surname><given-names>MC</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Chemdataextractor: a toolkit for automated extraction of chemical information from the scientific literature</article-title><source>J. Chem. Inf. Model.</source><year>2016</year><volume>56</volume><fpage>1894</fpage><lpage>1904</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsFKjsr%2FK</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.6b00207</pub-id></mixed-citation></ref><ref id="CR263"><label>263.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Text mining metal–organic framework papers</article-title><source>J. Chem. Inf. Model.</source><year>2018</year><volume>58</volume><fpage>244</fpage><lpage>251</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhvFGmtrjE</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.7b00608</pub-id></mixed-citation></ref><ref id="CR264"><label>264.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Court</surname><given-names>CJ</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Auto-generated materials database of curie and néel temperatures via semi-supervised relationship extraction</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.111</pub-id></mixed-citation></ref><ref id="CR265"><label>265.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">A database of battery materials auto-generated using chemdataextractor</article-title><source>Sci. Data</source><year>2020</year><volume>7</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41597-020-00602-2</pub-id></mixed-citation></ref><ref id="CR266"><label>266.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beard</surname><given-names>EJ</given-names></name><name><surname>Sivaraman</surname><given-names>G</given-names></name><name><surname>Vázquez-Mayagoitia</surname><given-names>Á</given-names></name><name><surname>Vishwanath</surname><given-names>V</given-names></name><name><surname>Cole</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Comparative dataset of experimental and computational attributes of uv/vis absorption spectra</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41597-019-0306-0</pub-id></mixed-citation></ref><ref id="CR267"><label>267.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tayfuroglu</surname><given-names>O</given-names></name><name><surname>Kocak</surname><given-names>A</given-names></name><name><surname>Zorlu</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">In silico investigation into h2 uptake in mofs: combined text/data mining and structural calculations</article-title><source>Langmuir</source><year>2019</year><volume>36</volume><fpage>119</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1021/acs.langmuir.9b03618</pub-id></mixed-citation></ref><ref id="CR268"><label>268.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weston</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Named entity recognition and normalization applied to large-scale information extraction from the materials science literature</article-title><source>J. Chem. Inf. Model.</source><year>2019</year><volume>59</volume><fpage>3692</fpage><lpage>3702</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXhsVOhsL7M</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00470</pub-id></mixed-citation></ref><ref id="CR269"><label>269.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaucher</surname><given-names>AC</given-names></name><etal/></person-group><article-title xml:lang="en">Automated extraction of chemical synthesis actions from experimental procedures</article-title><source>Nat. Commun.</source><year>2020</year><volume>11</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-17266-6</pub-id></mixed-citation></ref><ref id="CR270"><label>270.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>He</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Similarity of precursors in solid-state synthesis as text-mined from scientific literature</article-title><source>Chem. Mater.</source><year>2020</year><volume>32</volume><fpage>7861</fpage><lpage>7873</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhs1aqsLnE</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c02553</pub-id></mixed-citation></ref><ref id="CR271"><label>271.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kononova</surname><given-names>O</given-names></name><etal/></person-group><article-title xml:lang="en">Text-mined dataset of inorganic materials synthesis recipes</article-title><source>Sci. Data</source><year>2019</year><volume>6</volume><fpage>1</fpage><lpage>11</lpage></mixed-citation></ref><ref id="CR272"><label>272.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>E</given-names></name><etal/></person-group><article-title xml:lang="en">Materials synthesis insights from scientific literature via text extraction and machine learning</article-title><source>Chem. Mater.</source><year>2017</year><volume>29</volume><fpage>9436</fpage><lpage>9444</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs1Kqu7fJ</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.7b03500</pub-id></mixed-citation></ref><ref id="CR273"><label>273.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>E</given-names></name><name><surname>Huang</surname><given-names>K</given-names></name><name><surname>Jegelka</surname><given-names>S</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Virtual screening of inorganic materials synthesis parameters with deep learning</article-title><source>npj Comput. Mater.</source><year>2017</year><volume>3</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXht1CjsLzP</pub-id><pub-id pub-id-type="doi">10.1038/s41524-017-0055-6</pub-id></mixed-citation></ref><ref id="CR274"><label>274.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>E</given-names></name><etal/></person-group><article-title xml:lang="en">Inorganic materials synthesis planning with literature-trained neural networks</article-title><source>J. Chem. Inf. Model.</source><year>2020</year><volume>60</volume><fpage>1194</fpage><lpage>1201</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXksVentA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.9b00995</pub-id></mixed-citation></ref><ref id="CR275"><label>275.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Castro</surname><given-names>PB</given-names></name><etal/></person-group><article-title xml:lang="en">Machine-learning-guided discovery of the gigantic magnetocaloric effect in hob 2 near the hydrogen liquefaction temperature</article-title><source>NPG Asia Mater.</source><year>2020</year><volume>12</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/s41427-020-0214-y</pub-id></mixed-citation></ref><ref id="CR276"><label>276.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cooper</surname><given-names>CB</given-names></name><etal/></person-group><article-title xml:lang="en">Design-to-device approach affords panchromatic co-sensitized solar cells</article-title><source>Adv. Energy Mater.</source><year>2019</year><volume>9</volume><fpage>1802820</fpage><pub-id pub-id-type="doi">10.1002/aenm.201802820</pub-id></mixed-citation></ref><ref id="CR277"><label>277.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>X</given-names></name><name><surname>Dai</surname><given-names>Z</given-names></name><name><surname>Zhao</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>J</given-names></name><name><surname>Meng</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Low lattice thermal conductivity and excellent thermoelectric behavior in li3sb and li3bi</article-title><source>J. Phys. Condens. Matter</source><year>2018</year><volume>30</volume><fpage>425401</fpage><pub-id pub-id-type="doi">10.1088/1361-648X/aade17</pub-id></mixed-citation></ref><ref id="CR278"><label>278.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Ultralow lattice thermal conductivity and electronic properties of monolayer 1t phase semimetal site2 and snte2</article-title><source>Phys. E</source><year>2019</year><volume>108</volume><fpage>53</fpage><lpage>59</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXisF2lurbO</pub-id><pub-id pub-id-type="doi">10.1016/j.physe.2018.12.004</pub-id></mixed-citation></ref><ref id="CR279"><label>279.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jong</surname><given-names>U-G</given-names></name><name><surname>Yu</surname><given-names>C-J</given-names></name><name><surname>Kye</surname><given-names>Y-H</given-names></name><name><surname>Hong</surname><given-names>S-N</given-names></name><name><surname>Kim</surname><given-names>H-G</given-names></name></person-group><article-title xml:lang="en">Manifestation of the thermoelectric properties in ge-based halide perovskites</article-title><source>Phys. Rev. Mater.</source><year>2020</year><volume>4</volume><fpage>075403</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitVCktbzN</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevMaterials.4.075403</pub-id></mixed-citation></ref><ref id="CR280"><label>280.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamamoto</surname><given-names>K</given-names></name><name><surname>Narita</surname><given-names>G</given-names></name><name><surname>Yamasaki</surname><given-names>J</given-names></name><name><surname>Iikubo</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">First-principles study of thermoelectric properties of mixed iodide perovskite cs (b, b’) i3 (b, b’= ge, sn, and pb)</article-title><source>J. Phys. Chem. Solids</source><year>2020</year><volume>140</volume><fpage>109372</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXit1Ortb8%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.jpcs.2020.109372</pub-id></mixed-citation></ref><ref id="CR281"><label>281.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viennois</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Anisotropic low-energy vibrational modes as an effect of cage geometry in the binary barium silicon clathrate b a 24 s i 100</article-title><source>Phys. Rev. B</source><year>2020</year><volume>101</volume><fpage>224302</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFChs7bI</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevB.101.224302</pub-id></mixed-citation></ref><ref id="CR282"><label>282.</label><mixed-citation publication-type="other">Haque, E. Effect of electron-phonon scattering, pressure and alloying on the thermoelectric performance of tmcu _3 ch _4(tm= v, nb, ta; ch= s, se, te). <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2010.08461" ext-link-type="uri">https://arxiv.org/abs/2010.08461</ext-link> (2020).</mixed-citation></ref><ref id="CR283"><label>283.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yahyaoglu</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Phase-transition-enhanced thermoelectric transport in rickardite mineral cu3–x te2</article-title><source>Chem. Mater.</source><year>2021</year><volume>33</volume><fpage>1832</fpage><lpage>1841</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXks1yjsbk%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.chemmater.0c04839</pub-id></mixed-citation></ref><ref id="CR284"><label>284.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ho</surname><given-names>D</given-names></name><name><surname>Shkolnik</surname><given-names>AS</given-names></name><name><surname>Ferraro</surname><given-names>NJ</given-names></name><name><surname>Rizkin</surname><given-names>BA</given-names></name><name><surname>Hartman</surname><given-names>RL</given-names></name></person-group><article-title xml:lang="en">Using word embeddings in abstracts to accelerate metallocene catalysis polymerization research</article-title><source>Computers Chem. Eng.</source><year>2020</year><volume>141</volume><fpage>107026</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsVCitLrN</pub-id><pub-id pub-id-type="doi">10.1016/j.compchemeng.2020.107026</pub-id></mixed-citation></ref><ref id="CR285"><label>285.</label><mixed-citation publication-type="other">Abdar, M. et al. A review of uncertainty quantification in deep learning: techniques, applications and challenges. <italic>Inf. Fusion</italic>. <bold>76</bold>, 243–297 (2021).</mixed-citation></ref><ref id="CR286"><label>286.</label><mixed-citation publication-type="other">Mi, Lu, et al. Training-free uncertainty estimation for dense regression: Sensitivityas a surrogate. <italic>arXiv</italic>. preprint at arXiv:1910.04858. <ext-link xlink:href="https://arxiv.org/abs/1910.04858" ext-link-type="uri">https://arxiv.org/abs/1910.04858</ext-link> (2019).</mixed-citation></ref><ref id="CR287"><label>287.</label><mixed-citation publication-type="other">Teye, M., Azizpour, H. &amp; Smith, K. <italic>Bayesian uncertainty estimation for batch normalized deep networks</italic>. in <italic>International Conference on Machine Learning</italic>, 4907–4916 (PMLR, 2018).</mixed-citation></ref><ref id="CR288"><label>288.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Kailkhura</surname><given-names>B</given-names></name><name><surname>Han</surname><given-names>TY-J</given-names></name></person-group><article-title xml:lang="en">Leveraging uncertainty from deep learning for trustworthy material discovery workflows</article-title><source>ACS Omega</source><year>2021</year><volume>6</volume><fpage>12711</fpage><lpage>12721</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhtVSgtrrE</pub-id><pub-id pub-id-type="doi">10.1021/acsomega.1c00975</pub-id></mixed-citation></ref><ref id="CR289"><label>289.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meredig</surname><given-names>B</given-names></name><etal/></person-group><article-title xml:lang="en">Can machine learning identify the next high-temperature superconductor? examining extrapolation performance for materials discovery</article-title><source>Mol. Syst. Des. Eng.</source><year>2018</year><volume>3</volume><fpage>819</fpage><lpage>825</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhsFaqs7vE</pub-id><pub-id pub-id-type="doi">10.1039/C8ME00012C</pub-id></mixed-citation></ref><ref id="CR290"><label>290.</label><mixed-citation publication-type="other">Zhang, J., Kailkhura, B. &amp; Han, T. Y.-J. <italic>Mix-n-match: Ensemble and compositional methods for uncertainty calibration in deep learning</italic>. in <italic>International Conference on Machine Learning</italic>, 11117–11128 (PMLR, 2020).</mixed-citation></ref><ref id="CR291"><label>291.</label><mixed-citation publication-type="other">Seoh, R. Qualitative analysis of monte carlo dropout. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2007.01720" ext-link-type="uri">https://arxiv.org/abs/2007.01720</ext-link> (2020).</mixed-citation></ref><ref id="CR292"><label>292.</label><mixed-citation publication-type="other">Gal, Y. &amp; Ghahramani, Z. <italic>Dropout as a bayesian approximation: Representing model uncertainty in deep learning</italic>. in <italic>international conference on machine learning</italic>, 1050–1059 (PMLR, 2016).</mixed-citation></ref><ref id="CR293"><label>293.</label><mixed-citation publication-type="other">Jain, S., Liu, G., Mueller, J. &amp; Gifford, D. <italic>Maximizing overall diversity for improved uncertainty estimates in deep ensembles</italic>. In <italic>Proceedings of the AAAI Conference on Artificial Intelligence</italic>, <bold>34</bold>, 4264–4271. <ext-link xlink:href="10.1609/aaai.v34i04.5849" ext-link-type="doi">https://doi.org/10.1609/aaai.v34i04.5849</ext-link> (2020).</mixed-citation></ref><ref id="CR294"><label>294.</label><mixed-citation publication-type="other">Ganaie, M. et al. Ensemble deep learning: a review. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2104.02395" ext-link-type="uri">https://arxiv.org/abs/2104.02395</ext-link> (AAAI Technical Track: Machine Learning, 2021).</mixed-citation></ref><ref id="CR295"><label>295.</label><mixed-citation publication-type="other">Fort, S., Hu, H. &amp; Lakshminarayanan, B. Deep ensembles: a loss landscape perspective. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1912.02757" ext-link-type="uri">https://arxiv.org/abs/1912.02757</ext-link> (2019).</mixed-citation></ref><ref id="CR296"><label>296.</label><mixed-citation publication-type="other">Lakshminarayanan, B., Pritzel, A. &amp; Blundell, C. Simple and scalable predictive uncertainty estimation using deep ensembles. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1612.01474" ext-link-type="uri">https://arxiv.org/abs/1612.01474</ext-link> (2016).</mixed-citation></ref><ref id="CR297"><label>297.</label><mixed-citation publication-type="other">Moon, S. J., Jeon, J.-J., Lee, J. S. H. &amp; Kim, Y. Learning multiple quantiles with neural networks. <italic>J. Comput. Graph. Stat.</italic><bold>30</bold>, 1–11. <ext-link xlink:href="10.1080/10618600.2021.1909601" ext-link-type="doi">https://doi.org/10.1080/10618600.2021.1909601</ext-link> (2021).</mixed-citation></ref><ref id="CR298"><label>298.</label><mixed-citation publication-type="other">Rasmussen, C. E. <italic>Summer School on Machine Learning</italic>, 63–71 (Springer, 2003).</mixed-citation></ref><ref id="CR299"><label>299.</label><mixed-citation publication-type="other">Hegde, P., Heinonen, M., Lähdesmäki, H. &amp; Kaski, S. Deep learning with differential gaussian process flows. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/1810.04066" ext-link-type="uri">https://arxiv.org/abs/1810.04066</ext-link> (2018).</mixed-citation></ref><ref id="CR300"><label>300.</label><mixed-citation publication-type="other">Wilson, A. G., Hu, Z., Salakhutdinov, R. &amp; Xing, E. P. Deep kernel learning. in <italic>Artificial intelligence and statistics</italic>, 370–378 (PMLR, 2016).</mixed-citation></ref><ref id="CR301"><label>301.</label><mixed-citation publication-type="other">Hegde, V. I. et al. Reproducibility in high-throughput density functional theory: a comparison of aflow, materials project, and oqmd. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2007.01988" ext-link-type="uri">https://arxiv.org/abs/2007.01988</ext-link> (2020).</mixed-citation></ref><ref id="CR302"><label>302.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ying</surname><given-names>R</given-names></name><name><surname>Bourgeois</surname><given-names>D</given-names></name><name><surname>You</surname><given-names>J</given-names></name><name><surname>Zitnik</surname><given-names>M</given-names></name><name><surname>Leskovec</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Gnnexplainer: Generating explanations for graph neural networks</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2019</year><volume>32</volume><fpage>9240</fpage></mixed-citation></ref><ref id="CR303"><label>303.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roch</surname><given-names>LM</given-names></name><etal/></person-group><article-title xml:lang="en">Chemos: orchestrating autonomous experimentation</article-title><source>Sci. Robot.</source><year>2018</year><volume>3</volume><fpage>eaat5559</fpage><pub-id pub-id-type="doi">10.1126/scirobotics.aat5559</pub-id></mixed-citation></ref><ref id="CR304"><label>304.</label><mixed-citation publication-type="other">Szymanski, N. et al. Toward autonomous design and synthesis of novel inorganic materials. <italic>Mater. Horiz.</italic><bold>8</bold>, 2169–2198. <ext-link xlink:href="10.1039/D1MH00495F" ext-link-type="doi">https://doi.org/10.1039/D1MH00495F</ext-link> (2021).</mixed-citation></ref><ref id="CR305"><label>305.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>BP</given-names></name><etal/></person-group><article-title xml:lang="en">Self-driving laboratory for accelerated discovery of thin-film materials</article-title><source>Sci. Adv.</source><year>2020</year><volume>6</volume><fpage>eaaz8867</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFGhsL3J</pub-id><pub-id pub-id-type="doi">10.1126/sciadv.aaz8867</pub-id></mixed-citation></ref><ref id="CR306"><label>306.</label><mixed-citation publication-type="other">Stach, E. A. et al. Autonomous experimentation systems for materials development: a community perspective. <italic>Matter</italic><ext-link xlink:href="https://www.cell.com/matter/fulltext/S2590-2385(21)00306-4" ext-link-type="uri">https://www.cell.com/matter/fulltext/S2590-2385(21)00306-4</ext-link> (2021).</mixed-citation></ref><ref id="CR307"><label>307.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rakita</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Active reaction control of cu redox state based on real-time feedback from <italic>i</italic>n situ synchrotron measurements</article-title><source>J. Am. Chem. Soc.</source><year>2020</year><volume>142</volume><fpage>18758</fpage><lpage>18762</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFagtbvO</pub-id><pub-id pub-id-type="doi">10.1021/jacs.0c09418</pub-id></mixed-citation></ref><ref id="CR308"><label>308.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chmiela</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Machine learning of accurate energy-conserving molecular force fields</article-title><source>Sci. Adv.</source><year>2017</year><volume>3</volume><fpage>e1603015</fpage><pub-id pub-id-type="doi">10.1126/sciadv.1603015</pub-id></mixed-citation></ref><ref id="CR309"><label>309.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>RS</given-names></name><etal/></person-group><article-title xml:lang="en">The us federal tox21 program: a strategic and operational plan for continued leadership</article-title><source>Altex</source><year>2018</year><volume>35</volume><fpage>163</fpage><pub-id pub-id-type="doi">10.14573/altex.1803011</pub-id></mixed-citation></ref><ref id="CR310"><label>310.</label><mixed-citation publication-type="other">Russell Johnson, N. <italic>Nist computational chemistry comparison and benchmark database</italic>. In <italic>The 4th Joint Meeting of the US Sections of the Combustion Institute</italic>. <ext-link xlink:href="https://ci.confex.com/ci/2005/techprogram/P1309.HTM" ext-link-type="uri">https://ci.confex.com/ci/2005/techprogram/P1309.HTM</ext-link> (2005).</mixed-citation></ref><ref id="CR311"><label>311.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopez</surname><given-names>SA</given-names></name><etal/></person-group><article-title xml:lang="en">The harvard organic photovoltaic dataset</article-title><source>Sci. Data</source><year>2016</year><volume>3</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/sdata.2016.86</pub-id></mixed-citation></ref><ref id="CR312"><label>312.</label><mixed-citation publication-type="other">Johnson, R. D. et al. <italic>Nist computational chemistry comparison and benchmark database</italic>. <ext-link xlink:href="http://srdata.nist.gov/cccbdb" ext-link-type="uri">http://srdata.nist.gov/cccbdb</ext-link> (2006).</mixed-citation></ref><ref id="CR313"><label>313.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobley</surname><given-names>DL</given-names></name><name><surname>Guthrie</surname><given-names>JP</given-names></name></person-group><article-title xml:lang="en">Freesolv: a database of experimental and calculated hydration free energies, with input files</article-title><source>J. Computer Aided Mol. Des.</source><year>2014</year><volume>28</volume><fpage>711</fpage><lpage>720</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXpvVeitb0%3D</pub-id><pub-id pub-id-type="doi">10.1007/s10822-014-9747-x</pub-id></mixed-citation></ref><ref id="CR314"><label>314.</label><mixed-citation publication-type="other">Andersen, C. W. et al. Optimade: an api for exchanging materials data. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2103.02068" ext-link-type="uri">https://arxiv.org/abs/2103.02068</ext-link> (2021).</mixed-citation></ref><ref id="CR315"><label>315.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chanussot</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Open catalyst 2020 (oc20) dataset and community challenges</article-title><source>ACS Catal.</source><year>2021</year><volume>11</volume><fpage>6059</fpage><lpage>6072</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhtVSksbnJ</pub-id><pub-id pub-id-type="doi">10.1021/acscatal.0c04525</pub-id></mixed-citation></ref><ref id="CR316"><label>316.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Ganose</surname><given-names>A</given-names></name><name><surname>Dopp</surname><given-names>D</given-names></name><name><surname>Jain</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Benchmarking materials property prediction methods: the matbench test set and automatminer reference algorithm</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>10</lpage></mixed-citation></ref><ref id="CR317"><label>317.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talirz</surname><given-names>L</given-names></name><etal/></person-group><article-title xml:lang="en">Materials cloud, a platform for open computational science</article-title><source>Sci. Data</source><year>2020</year><volume>7</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41597-020-00637-5</pub-id></mixed-citation></ref><ref id="CR318"><label>318.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>YG</given-names></name><etal/></person-group><article-title xml:lang="en">Advances, updates, and analytics for the computation-ready, experimental metal–organic framework database: Core mof 2019</article-title><source>J. Chem. Eng. Data</source><year>2019</year><volume>64</volume><fpage>5985</fpage><lpage>5998</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXitVOmtrrL</pub-id><pub-id pub-id-type="doi">10.1021/acs.jced.9b00835</pub-id></mixed-citation></ref><ref id="CR319"><label>319.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussman</surname><given-names>JL</given-names></name><etal/></person-group><article-title xml:lang="en">Protein data bank (pdb): database of three-dimensional structural information of biological macromolecules</article-title><source>Acta Crystallogr. Sec. D Biol. Crystallogr.</source><year>1998</year><volume>54</volume><fpage>1078</fpage><lpage>1084</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DyaK1M7ptVyjtA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1107/S0907444998009378</pub-id></mixed-citation></ref><ref id="CR320"><label>320.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benson</surname><given-names>ML</given-names></name><etal/></person-group><article-title xml:lang="en">Binding moad, a high-quality protein–ligand database</article-title><source>Nucleic Acids Res.</source><year>2007</year><volume>36</volume><fpage>D674</fpage><lpage>D678</lpage><pub-id pub-id-type="doi">10.1093/nar/gkm911</pub-id></mixed-citation></ref><ref id="CR321"><label>321.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname><given-names>V</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Juarez</surname><given-names>E</given-names></name><name><surname>Sumpter</surname><given-names>BG</given-names></name></person-group><article-title xml:lang="en">Benchmarking graph neural networks for materials chemistry</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1038/s41524-021-00554-0</pub-id></mixed-citation></ref><ref id="CR322"><label>322.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louis</surname><given-names>S-Y</given-names></name><etal/></person-group><article-title xml:lang="en">Graph convolutional neural networks with global attention for improved materials property prediction</article-title><source>Phys. Chem. Chem. Phys.</source><year>2020</year><volume>22</volume><fpage>18141</fpage><lpage>18148</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhsFSmu7bL</pub-id><pub-id pub-id-type="doi">10.1039/D0CP01474E</pub-id></mixed-citation></ref><ref id="CR323"><label>323.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khorshidi</surname><given-names>A</given-names></name><name><surname>Peterson</surname><given-names>AA</given-names></name></person-group><article-title xml:lang="en">Amp: A modular approach to machine learning in atomistic simulations</article-title><source>Computer Phys. Commun.</source><year>2016</year><volume>207</volume><fpage>310</fpage><lpage>324</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28Xptl2ht7k%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.cpc.2016.05.010</pub-id></mixed-citation></ref><ref id="CR324"><label>324.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yao</surname><given-names>K</given-names></name><name><surname>Herr</surname><given-names>JE</given-names></name><name><surname>Toth</surname><given-names>DW</given-names></name><name><surname>Mckintyre</surname><given-names>R</given-names></name><name><surname>Parkhill</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">The tensormol-0.1 model chemistry: a neural network augmented with long-range physics</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>2261</fpage><lpage>2269</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhtFOitLY%3D</pub-id><pub-id pub-id-type="doi">10.1039/C7SC04934J</pub-id></mixed-citation></ref><ref id="CR325"><label>325.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doerr</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Torchmd: A deep learning framework for molecular simulations</article-title><source>J. Chem. Theory Comput.</source><year>2021</year><volume>17</volume><fpage>2355</fpage><lpage>2363</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXmsFWgs7k%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jctc.0c01343</pub-id></mixed-citation></ref><ref id="CR326"><label>326.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolb</surname><given-names>B</given-names></name><name><surname>Lentz</surname><given-names>LC</given-names></name><name><surname>Kolpak</surname><given-names>AM</given-names></name></person-group><article-title xml:lang="en">Discovering charge density functionals and structure-property relationships with prophet: A general framework for coupling machine learning and first-principles methods</article-title><source>Sci. Rep.</source><year>2017</year><volume>7</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhslGls7%2FI</pub-id><pub-id pub-id-type="doi">10.1038/s41598-017-01251-z</pub-id></mixed-citation></ref><ref id="CR327"><label>327.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Han</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Car</surname><given-names>R</given-names></name><name><surname>Weinan</surname><given-names>E</given-names></name></person-group><article-title xml:lang="en">Deep potential molecular dynamics: a scalable model with the accuracy of quantum mechanics</article-title><source>Phys. Rev. Lett.</source><year>2018</year><volume>120</volume><fpage>143001</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXltFSksrg%3D</pub-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.120.143001</pub-id></mixed-citation></ref><ref id="CR328"><label>328.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Artrith</surname><given-names>N</given-names></name><name><surname>Urban</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">An implementation of artificial neural-network potentials for atomistic materials simulations: Performance for tio2</article-title><source>Comput. Mater. Sci.</source><year>2016</year><volume>114</volume><fpage>135</fpage><lpage>150</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhtFGisA%3D%3D</pub-id><pub-id pub-id-type="doi">10.1016/j.commatsci.2015.11.047</pub-id></mixed-citation></ref><ref id="CR329"><label>329.</label><mixed-citation publication-type="other">Geiger, M. et al. <italic>e3nn/e3nn: 2021-06-21</italic>. <ext-link xlink:href="10.5281/zenodo.5006322" ext-link-type="doi">https://doi.org/10.5281/zenodo.5006322</ext-link> (2021).</mixed-citation></ref><ref id="CR330"><label>330.</label><mixed-citation publication-type="other">Duvenaud, D. K. et al. <italic>Convolutional networks on graphs for learning molecular fingerprints</italic> (eds. Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M. &amp; Garnett, R.) in <italic>Adv. Neural Inf. Process. Syst. 28</italic> 2224–2232 (Curran Associates, Inc., 2015).</mixed-citation></ref><ref id="CR331"><label>331.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title xml:lang="en">Deepchemstable: Chemical stability prediction with an attention-based graph convolution network</article-title><source>J. Chem. Inf. Model.</source><year>2019</year><volume>59</volume><fpage>1044</fpage><lpage>1049</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXjtVSgsbo%3D</pub-id><pub-id pub-id-type="doi">10.1021/acs.jcim.8b00672</pub-id></mixed-citation></ref><ref id="CR332"><label>332.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><etal/></person-group><article-title xml:lang="en">MoleculeNet: A benchmark for molecular machine learning</article-title><source>Chem. Sci.</source><year>2018</year><volume>9</volume><fpage>513</fpage><lpage>530</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhslChtrbO</pub-id><pub-id pub-id-type="doi">10.1039/C7SC02664A</pub-id></mixed-citation></ref><ref id="CR333"><label>333.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>AY-T</given-names></name><name><surname>Kauwe</surname><given-names>SK</given-names></name><name><surname>Murdock</surname><given-names>RJ</given-names></name><name><surname>Sparks</surname><given-names>TD</given-names></name></person-group><article-title xml:lang="en">Compositionally restricted attention-based network for materials property predictions</article-title><source>npj Comput. Mater.</source><year>2021</year><volume>7</volume><fpage>77</fpage><pub-id pub-id-type="doi">10.1038/s41524-021-00545-1</pub-id></mixed-citation></ref><ref id="CR334"><label>334.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>Q</given-names></name><etal/></person-group><article-title xml:lang="en">Learning atoms for materials discovery</article-title><source>Proc. Natl Acad. Sci. USA</source><year>2018</year><volume>115</volume><fpage>E6411</fpage><lpage>E6417</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhvFCmsbrF</pub-id><pub-id pub-id-type="doi">10.1073/pnas.1801181115</pub-id></mixed-citation></ref><ref id="CR335"><label>335.</label><mixed-citation publication-type="other">O’Boyle, N. &amp; Dalke, A. Deepsmiles: An adaptation of smiles for use in machine-learning of chemical structures. <italic>ChemRxiv</italic><ext-link xlink:href="10.26434/chemrxiv.7097960.v1" ext-link-type="doi">https://doi.org/10.26434/chemrxiv.7097960.v1</ext-link> (2018).</mixed-citation></ref><ref id="CR336"><label>336.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gómez-Bombarelli</surname><given-names>R</given-names></name><etal/></person-group><article-title xml:lang="en">Automatic chemical design using a data-driven continuous representation of molecules</article-title><source>ACS Central Sci.</source><year>2018</year><volume>4</volume><fpage>268</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1021/acscentsci.7b00572</pub-id></mixed-citation></ref><ref id="CR337"><label>337.</label><mixed-citation publication-type="other">Green, H., Koes, D. R. &amp; Durrant, J. D. Deepfrag: a deep convolutional neural network for fragment-based lead optimization. <italic>Chem. Sci.</italic><bold>12</bold>, 8036–8047. <ext-link xlink:href="10.1039/D1SC00163A" ext-link-type="doi">https://doi.org/10.1039/D1SC00163A</ext-link> (2021).</mixed-citation></ref><ref id="CR338"><label>338.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elhefnawy</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Deepfrag-k: a fragment-based deep learning approach for protein fold recognition</article-title><source>BMC Bioinformatics</source><year>2020</year><volume>21</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXisVCmu7bF</pub-id><pub-id pub-id-type="doi">10.1186/s12859-020-3504-z</pub-id></mixed-citation></ref><ref id="CR339"><label>339.</label><mixed-citation publication-type="other">Paul, A. et al. Chemixnet: Mixed dnn architectures for predicting chemical properties using multiple molecular representations. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/1811.08283" ext-link-type="uri">https://arxiv.org/abs/1811.08283</ext-link> (2018).</mixed-citation></ref><ref id="CR340"><label>340.</label><mixed-citation publication-type="other">Paul, A. et al. <italic>Transfer learning using ensemble neural networks for organic solar cell screening</italic>. in <italic>2019 International Joint Conference on Neural Networks (IJCNN)</italic>, 1–8 (IEEE, 2019).</mixed-citation></ref><ref id="CR341"><label>341.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Choudhary</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">Computational screening of high-performance optoelectronic materials using optb88vdw and tb-mbj formalisms</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.82</pub-id></mixed-citation></ref><ref id="CR342"><label>342.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong-Ng</surname><given-names>W</given-names></name><name><surname>McMurdie</surname><given-names>H</given-names></name><name><surname>Hubbard</surname><given-names>C</given-names></name><name><surname>Mighell</surname><given-names>AD</given-names></name></person-group><article-title xml:lang="en">Jcpds-icdd research associateship (cooperative program with nbs/nist)</article-title><source>J. Res. Natl Inst. Standards Technol.</source><year>2001</year><volume>106</volume><fpage>1013</fpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD38XktlGqt7c%3D</pub-id><pub-id pub-id-type="doi">10.6028/jres.106.052</pub-id></mixed-citation></ref><ref id="CR343"><label>343.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belsky</surname><given-names>A</given-names></name><name><surname>Hellenbrandt</surname><given-names>M</given-names></name><name><surname>Karen</surname><given-names>VL</given-names></name><name><surname>Luksch</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">New developments in the inorganic crystal structure database (icsd): accessibility in support of materials research and design</article-title><source>Acta Crystallogr. Sec. B Struct. Sci.</source><year>2002</year><volume>58</volume><fpage>364</fpage><lpage>369</lpage><pub-id pub-id-type="doi">10.1107/S0108768102006948</pub-id></mixed-citation></ref><ref id="CR344"><label>344.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gražulis</surname><given-names>S</given-names></name><etal/></person-group><article-title xml:lang="en">Crystallography Open Database—an open-access collection of crystal structures</article-title><source>J. Appl. Crystallogr.</source><year>2009</year><volume>42</volume><fpage>726</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1107/S0021889809016690</pub-id></mixed-citation></ref><ref id="CR345"><label>345.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Linstrom</surname><given-names>PJ</given-names></name><name><surname>Mallard</surname><given-names>WG</given-names></name></person-group><article-title xml:lang="en">The nist chemistry webbook: a chemical data resource on the internet</article-title><source>J. Chem. Eng. Data</source><year>2001</year><volume>46</volume><fpage>1059</fpage><lpage>1063</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3MXhvVKisrg%3D</pub-id><pub-id pub-id-type="doi">10.1021/je000236i</pub-id></mixed-citation></ref><ref id="CR346"><label>346.</label><mixed-citation publication-type="other">Saito, T. et al. Spectral database for organic compounds (sdbs). (National Institute of Advanced Industrial Science and Technology (AIST), 2006).</mixed-citation></ref><ref id="CR347"><label>347.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinbeck</surname><given-names>C</given-names></name><name><surname>Krause</surname><given-names>S</given-names></name><name><surname>Kuhn</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Nmrshiftdb constructing a free chemical information system with open-source components</article-title><source>J. Chem. inf. Computer Sci.</source><year>2003</year><volume>43</volume><fpage>1733</fpage><lpage>1739</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3sXotlaltrc%3D</pub-id><pub-id pub-id-type="doi">10.1021/ci0341363</pub-id></mixed-citation></ref><ref id="CR348"><label>348.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fung</surname><given-names>V</given-names></name><name><surname>Hu</surname><given-names>G</given-names></name><name><surname>Ganesh</surname><given-names>P</given-names></name><name><surname>Sumpter</surname><given-names>BG</given-names></name></person-group><article-title xml:lang="en">Machine learned features from density of states for accurate adsorption energy prediction</article-title><source>Nat. Commun.</source><year>2021</year><volume>12</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-020-20342-6</pub-id></mixed-citation></ref><ref id="CR349"><label>349.</label><mixed-citation publication-type="other">Kong, S., Guevarra, D., Gomes, C. P. &amp; Gregoire, J. M. Materials representation and transfer learning for multi-property prediction. <italic>arXiv</italic>. <ext-link xlink:href="https://arxiv.org/abs/2106.02225" ext-link-type="uri">https://arxiv.org/abs/2106.02225</ext-link> (2021).</mixed-citation></ref><ref id="CR350"><label>350.</label><mixed-citation publication-type="other">Bang, K., Yeo, B. C., Kim, D., Han, S. S. &amp; Lee, H. M. Accelerated mapping of electronic density of states patterns of metallic nanoparticles via machine-learning. <italic>Sci. Rep</italic>. <bold>11</bold>, 1–11 (2021).</mixed-citation></ref><ref id="CR351"><label>351.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>D</given-names></name><etal/></person-group><article-title xml:lang="en">Automating crystal-structure phase mapping by combining deep learning with constraint reasoning</article-title><source>Nat. Machine Intell.</source><year>2021</year><volume>3</volume><fpage>812</fpage><lpage>822</lpage><pub-id pub-id-type="doi">10.1038/s42256-021-00384-1</pub-id></mixed-citation></ref><ref id="CR352"><label>352.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ophus</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">A fast image simulation algorithm for scanning transmission electron microscopy</article-title><source>Adv. Struct. Chem. imaging</source><year>2017</year><volume>3</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1186/s40679-017-0046-1</pub-id></mixed-citation></ref><ref id="CR353"><label>353.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aversa</surname><given-names>R</given-names></name><name><surname>Modarres</surname><given-names>MH</given-names></name><name><surname>Cozzini</surname><given-names>S</given-names></name><name><surname>Ciancio</surname><given-names>R</given-names></name><name><surname>Chiusole</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">The first annotated set of scanning electron microscopy images for nanoscience</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/sdata.2018.172</pub-id></mixed-citation></ref><ref id="CR354"><label>354.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ziatdinov</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Causal analysis of competing atomistic mechanisms in ferroelectric materials from high-resolution scanning transmission electron microscopy data</article-title><source>npj Comput. Mater.</source><year>2020</year><volume>6</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1038/s41524-020-00396-2</pub-id></mixed-citation></ref><ref id="CR355"><label>355.</label><mixed-citation publication-type="other">Souza, A. L. F. et al. Deepfreak: Learning crystallography diffraction patterns with automated machine learning. arXiv. <ext-link xlink:href="http://arxiv.org/abs/1904.11834" ext-link-type="uri">http://arxiv.org/abs/1904.11834</ext-link> (2019).</mixed-citation></ref><ref id="CR356"><label>356.</label><mixed-citation publication-type="other">Scime, L. et al. Layer-wise imaging dataset from powder bed additive manufacturing processes for machine learning applications (peregrine v2021-03). <italic>Tech. Rep</italic>. <ext-link xlink:href="https://www.osti.gov/biblio/1779073" ext-link-type="uri">https://www.osti.gov/biblio/1779073</ext-link> (2021).</mixed-citation></ref><ref id="CR357"><label>357.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ede</surname><given-names>JM</given-names></name><name><surname>Beanland</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Partial scanning transmission electron microscopy with deep learning</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-65261-0</pub-id></mixed-citation></ref><ref id="CR358"><label>358.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somnath</surname><given-names>S</given-names></name><name><surname>Smith</surname><given-names>CR</given-names></name><name><surname>Laanait</surname><given-names>N</given-names></name><name><surname>Vasudevan</surname><given-names>RK</given-names></name><name><surname>Jesse</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Usid and pycroscopy–open source frameworks for storing and analyzing imaging and spectroscopy data</article-title><source>Microsc. Microanal.</source><year>2019</year><volume>25</volume><fpage>220</fpage><lpage>221</lpage><pub-id pub-id-type="doi">10.1017/S1431927619001831</pub-id></mixed-citation></ref><ref id="CR359"><label>359.</label><mixed-citation publication-type="other">Savitzky, B. H. et al. py4dstem: A software package for multimodal analysis of four-dimensional scanning transmission electron microscopy datasets. <italic>arXiv.</italic><ext-link xlink:href="https://arxiv.org/abs/2003.09523" ext-link-type="uri">https://arxiv.org/abs/2003.09523</ext-link> (2020).</mixed-citation></ref><ref id="CR360"><label>360.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Madsen</surname><given-names>J</given-names></name><name><surname>Susi</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">The abtem code: transmission electron microscopy from first principles</article-title><source>Open Res. Euro.</source><year>2021</year><volume>1</volume><fpage>24</fpage><pub-id pub-id-type="doi">10.12688/openreseurope.13015.1</pub-id></mixed-citation></ref><ref id="CR361"><label>361.</label><mixed-citation publication-type="other">Koch, C. T. <italic>Determination of core structure periodicity and point defect density along dislocations</italic>. (Arizona State University, 2002).</mixed-citation></ref><ref id="CR362"><label>362.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>LJ</given-names></name><etal/></person-group><article-title xml:lang="en">Modelling the inelastic scattering of fast electrons</article-title><source>Ultramicroscopy</source><year>2015</year><volume>151</volume><fpage>11</fpage><lpage>22</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2cXhvVOks7bO</pub-id><pub-id pub-id-type="doi">10.1016/j.ultramic.2014.10.011</pub-id></mixed-citation></ref><ref id="CR363"><label>363.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maxim</surname><given-names>Z</given-names></name><name><surname>Jesse</surname><given-names>S</given-names></name><name><surname>Sumpter</surname><given-names>BG</given-names></name><name><surname>Kalinin</surname><given-names>SV</given-names></name><name><surname>Dyck</surname><given-names>O</given-names></name></person-group><article-title xml:lang="en">Tracking atomic structure evolution during directed electron beam induced si-atom motion in graphene via deep machine learning</article-title><source>Nanotechnology</source><year>2020</year><volume>32</volume><fpage>035703</fpage><pub-id pub-id-type="doi">10.1088/1361-6528/abb8a6</pub-id></mixed-citation></ref><ref id="CR364"><label>364.</label><mixed-citation publication-type="other">Khadangi, A., Boudier, T. &amp; Rajagopal, V. <italic>Em-net: Deep learning for electron microscopy image segmentation</italic>. in <italic>2020 25th International Conference on Pattern Recognition (ICPR)</italic>, 31–38 (IEEE, 2021).</mixed-citation></ref><ref id="CR365"><label>365.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname><given-names>C</given-names></name><etal/></person-group><article-title xml:lang="en">Nion swift: Open source image processing software for instrument control, data acquisition, organization, visualization, and analysis using python</article-title><source>Microsc. Microanal.</source><year>2019</year><volume>25</volume><fpage>122</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1017/S143192761900134X</pub-id></mixed-citation></ref><ref id="CR366"><label>366.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>J</given-names></name><name><surname>Tiong</surname><given-names>LCO</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Han</surname><given-names>SS</given-names></name></person-group><article-title xml:lang="en">Deep learning-based prediction of material properties using chemical compositions and diffraction patterns as experimentally accessible inputs</article-title><source>J. Phys. Chem Lett.</source><year>2021</year><volume>12</volume><fpage>8376</fpage><lpage>8383</lpage><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhvVOhur7P</pub-id><pub-id pub-id-type="doi">10.1021/acs.jpclett.1c02305</pub-id></mixed-citation></ref><ref id="CR367"><label>367.</label><mixed-citation publication-type="other">Von Chamier, L. et al. Zerocostdl4mic: an open platform to simplify access and use of deep-learning in microscopy. <italic>BioRxiv.</italic><ext-link xlink:href="https://www.biorxiv.org/content/10.1101/2020.03.20.000133v4" ext-link-type="uri">https://www.biorxiv.org/content/10.1101/2020.03.20.000133v4</ext-link> (2020).</mixed-citation></ref><ref id="CR368"><label>368.</label><mixed-citation publication-type="other">Jha, D. et al. <italic>Peak area detection network for directly learning phase regions from raw x-ray diffraction patterns</italic>. in <italic>2019 International Joint Conference on Neural Networks (IJCNN)</italic>, 1–8 (IEEE, 2019).</mixed-citation></ref><ref id="CR369"><label>369.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawizy</surname><given-names>L</given-names></name><name><surname>Jessop</surname><given-names>DM</given-names></name><name><surname>Adams</surname><given-names>N</given-names></name><name><surname>Murray-Rust</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Chemicaltagger: A tool for semantic text-mining in chemistry</article-title><source>J. Cheminformatics</source><year>2011</year><volume>3</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1186/1758-2946-3-17</pub-id></mixed-citation></ref><ref id="CR370"><label>370.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbett</surname><given-names>P</given-names></name><name><surname>Boyle</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Chemlistem: chemical named entity recognition using recurrent neural networks</article-title><source>J. Cheminformatics</source><year>2018</year><volume>10</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1186/s13321-018-0313-8</pub-id></mixed-citation></ref><ref id="CR371"><label>371.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rocktäschel</surname><given-names>T</given-names></name><name><surname>Weidlich</surname><given-names>M</given-names></name><name><surname>Leser</surname><given-names>U</given-names></name></person-group><article-title xml:lang="en">Chemspot: a hybrid system for chemical named entity recognition</article-title><source>Bioinformatics</source><year>2012</year><volume>28</volume><fpage>1633</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bts183</pub-id></mixed-citation></ref><ref id="CR372"><label>372.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessop</surname><given-names>DM</given-names></name><name><surname>Adams</surname><given-names>SE</given-names></name><name><surname>Willighagen</surname><given-names>EL</given-names></name><name><surname>Hawizy</surname><given-names>L</given-names></name><name><surname>Murray-Rust</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Oscar4: a flexible architecture for chemical text-mining</article-title><source>J. Cheminformatics</source><year>2011</year><volume>3</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1186/1758-2946-3-41</pub-id></mixed-citation></ref><ref id="CR373"><label>373.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leaman</surname><given-names>R</given-names></name><name><surname>Wei</surname><given-names>C-H</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">tmchem: a high performance approach for chemical named entity recognition and normalization</article-title><source>J. Cheminformatics</source><year>2015</year><volume>7</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S3</pub-id></mixed-citation></ref><ref id="CR374"><label>374.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>Y</given-names></name><etal/></person-group><article-title xml:lang="en">Symmetry prediction and knowledge discovery from X-ray diffraction patterns using an interpretable machine learning approach</article-title><source>Sci. Rep.</source><year>2020</year><volume>10</volume><pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXis1WltLjP</pub-id><pub-id pub-id-type="doi">10.1038/s41598-020-77474-4</pub-id></mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><p><bold>Publisher’s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Characterization and Evaluation of Materials</facet-value><facet-value count="1">Computational Intelligence</facet-value><facet-value count="1">Materials Science</facet-value><facet-value count="1">Materials Science, general</facet-value><facet-value count="1">Mathematical and Computational Engineering</facet-value><facet-value count="1">Mathematical Modeling and Industrial Mathematics</facet-value><facet-value count="1">Theoretical, Mathematical and Computational Physics</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">npj Computational Materials</facet-value></facet><facet name="year"><facet-value count="1">2022</facet-value></facet><facet name="country"><facet-value count="1">United States</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
