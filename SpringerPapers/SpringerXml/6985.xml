<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1007/s12559-020-09773-x</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="review-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">12559</journal-id><journal-id journal-id-type="doi">10.1007/12559.1866-9964</journal-id><journal-title-group><journal-title>Cognitive Computation</journal-title><abbrev-journal-title abbrev-type="publisher">Cogn Comput</abbrev-journal-title></journal-title-group><issn pub-type="ppub">1866-9956</issn><issn pub-type="epub">1866-9964</issn><publisher><publisher-name>Springer US</publisher-name><publisher-loc>New York</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s12559-020-09773-x</article-id><article-id pub-id-type="manuscript">9773</article-id><article-id pub-id-type="doi">10.1007/s12559-020-09773-x</article-id><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Deep Learning in Mining Biological Data</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-2037-8348</contrib-id><name><surname>Mahmud</surname><given-names>Mufti</given-names></name><address><email>mufti.mahmud@ntu.ac.uk</email><email>muftimahmud@gmail.com</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff5">5</xref><xref ref-type="corresp" rid="IDs1255902009773x_cor1">a</xref></contrib><contrib contrib-type="author" corresp="yes" id="Au2"><name><surname>Kaiser</surname><given-names>M. Shamim</given-names></name><address><email>mskaiser@juniv.edu</email></address><xref ref-type="aff" rid="Aff2">2</xref><xref ref-type="corresp" rid="IDs1255902009773x_cor2">b</xref></contrib><contrib contrib-type="author" id="Au3"><name><surname>McGinnity</surname><given-names>T. Martin</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" id="Au4"><name><surname>Hussain</surname><given-names>Amir</given-names></name><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.12361.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 0727 0669</institution-id><institution content-type="org-division">Department of Computer Science</institution><institution content-type="org-name">Nottingham Trent University</institution></institution-wrap><addr-line content-type="street">Clifton</addr-line><addr-line content-type="postcode">NG11 8NS</addr-line><addr-line content-type="city">Nottingham</addr-line><country country="GB">UK</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="GRID">grid.411808.4</institution-id><institution-id institution-id-type="ISNI">0000 0001 0664 5967</institution-id><institution content-type="org-division">Institute of Information Technology</institution><institution content-type="org-name">Jahangirnagar University</institution></institution-wrap><addr-line content-type="postcode">1342</addr-line><addr-line content-type="city">Dhaka</addr-line><addr-line content-type="state">Savar</addr-line><country country="BD">Bangladesh</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.12641.30</institution-id><institution-id institution-id-type="ISNI">0000000105519715</institution-id><institution content-type="org-division">Intelligent Systems Research Centre</institution><institution content-type="org-name">Ulster University</institution></institution-wrap><addr-line content-type="postcode">BT48 7JL</addr-line><addr-line content-type="city">Derry</addr-line><addr-line content-type="state">Northern Ireland</addr-line><country country="GB">UK</country></aff><aff id="Aff4"><label>4</label><institution-wrap><institution content-type="org-division">School of Computing </institution><institution content-type="org-name">Edinburgh</institution></institution-wrap><addr-line content-type="postcode">EH11 4BN</addr-line><addr-line content-type="city">Edinburgh</addr-line><country country="GB">UK</country></aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.12361.37</institution-id><institution-id institution-id-type="ISNI">0000 0001 0727 0669</institution-id><institution content-type="org-division">Medical Technology Innovation Facility</institution><institution content-type="org-name">Nottingham Trent University</institution></institution-wrap><addr-line content-type="postcode">NG11 8NS</addr-line><addr-line content-type="city">Clifton, Nottingham</addr-line><country country="GB">UK</country></aff></contrib-group><author-notes><fn fn-type="con"><p>M. Mahmud and M.S. Kaiser are joint first authors.</p></fn><corresp id="IDs1255902009773x_cor1"><label>a</label><email>mufti.mahmud@ntu.ac.uk</email><email>muftimahmud@gmail.com</email></corresp><corresp id="IDs1255902009773x_cor2"><label>b</label><email>mskaiser@juniv.edu</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>5</day><month>1</month><year>2021</year></pub-date><pub-date date-type="pub" publication-format="print"><month>1</month><year>2021</year></pub-date><volume>13</volume><issue seq="1">1</issue><fpage>1</fpage><lpage>33</lpage><history><date date-type="registration"><day>29</day><month>9</month><year>2020</year></date><date date-type="received"><day>1</day><month>5</month><year>2020</year></date><date date-type="accepted"><day>28</day><month>9</month><year>2020</year></date><date date-type="online"><day>5</day><month>1</month><year>2021</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2020</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">Recent technological advancements in data acquisition tools allowed life scientists to acquire multimodal data from different biological application domains. Categorized in three broad types (i.e. images, signals, and sequences), these data are huge in amount and complex in nature. Mining such enormous amount of data for pattern recognition is a big challenge and requires sophisticated data-intensive machine learning techniques. Artificial neural network-based learning systems are well known for their pattern recognition capabilities, and lately their deep architectures—known as deep learning (DL)—have been successfully applied to solve many complex pattern recognition problems. To investigate how DL—especially its different architectures—has contributed and been utilized in the mining of biological data pertaining to those three types, a meta-analysis has been performed and the resulting resources have been critically analysed. Focusing on the use of DL to analyse patterns in data from diverse biological domains, this work investigates different DL architectures’ applications to these data. This is followed by an exploration of available open access data sources pertaining to the three data types along with popular open-source DL tools applicable to these data. Also, comparative investigations of these tools from qualitative, quantitative, and benchmarking perspectives are provided. Finally, some open research challenges in using DL to mine biological data are outlined and a number of possible future perspectives are put forward.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Brain–Machine Interfaces</kwd><kwd>Bioimaging</kwd><kwd>Deep learning performance comparison</kwd><kwd>Medical imaging</kwd><kwd>Omics</kwd><kwd>Open access data sources</kwd><kwd>Open-source tools</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>Nottingham Trent University</institution></institution-wrap></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>6</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>13</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Springer Science+Business Media, LLC, part of Springer Nature</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2020</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>9</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>29</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/12559_2020_Article_9773.pdf</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-online-date-day</meta-name><meta-value>20</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-year</meta-name><meta-value>2021</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-month</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-print-date-day</meta-name><meta-value>20</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>ReviewPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Computer Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Artificial Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computation by Abstract Devices</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Artificial Intelligence</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Biology/Bioinformatics</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Computer Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The pursuit of understanding human behaviours, along with the various pathologies, their early diagnosis and finding cures, has driven the life sciences research in the last two centuries [<xref ref-type="bibr" rid="CR1">1</xref>]. This accelerated the development of cutting edge tools and technologies that allow scientists to study holistically the biological systems as well as dig down, in an unprecedented manner, to the molecular details of the living organisms [<xref ref-type="bibr" rid="CR2">2</xref>, <xref ref-type="bibr" rid="CR3">3</xref>]. Increasing technological sophistication has presented scientists with novel tools for DNA sequencing [<xref ref-type="bibr" rid="CR4">4</xref>], gene expression [<xref ref-type="bibr" rid="CR5">5</xref>], bioimaging [<xref ref-type="bibr" rid="CR6">6</xref>], neuroimaging [<xref ref-type="bibr" rid="CR7">7</xref>], and body/brain–machine interfaces [<xref ref-type="bibr" rid="CR8">8</xref>].<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>The ecosystem of modern data analytics using advanced machine learning methods with specific focus on application of DL to biological data mining. The biological data coming from various sources (e.g. sequence data from the <italic>Omics</italic>, various images from the <italic>[Medical/Bio]-Imaging</italic>, and signals from the <italic>[Brain/Body]–Machine Interfaces</italic>) are mined using DL with suitable architectures tailored for specific applications</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig1_HTML.png" id="MO1"/></fig></p><p id="Par3">These innovative approaches to study living organisms produce huge amount of data [<xref ref-type="bibr" rid="CR9">9</xref>] and create a situation often referred as ‘Data Deluge’ [<xref ref-type="bibr" rid="CR10">10</xref>]. Depending on the target application and experimentation, these biological big data can be characterized by their inherent characteristics of being <italic>hierarchical</italic> (i.e. data coming from different levels of a biological system—from molecules to cells to tissues to systems), <italic>heterogeneous</italic> (i.e. data acquired by different acquisition methods—from genetics to physiology to pathology to imaging), <italic>dynamic</italic> (i.e. data changes as a function of time), and <italic>complex</italic> (i.e. data describing nonlinear biological processes) [<xref ref-type="bibr" rid="CR11">11</xref>]. These intrinsic characteristics of biological big data posed an enormous challenge to data scientists to identify patterns and analyse them to infer meaningful conclusions from these data [<xref ref-type="bibr" rid="CR12">12</xref>]. The challenges have triggered the development of rational, reliable, reusable, rigorous, and robust software tools [<xref ref-type="bibr" rid="CR11">11</xref>] using machine learning (ML)-based methods to facilitate recognition, classification, and prediction of patterns in the biological big data [<xref ref-type="bibr" rid="CR13">13</xref>].</p><p id="Par4">Based on how a method learns from the data, the ML techniques can be broadly categorized into <italic>supervised</italic> and <italic>unsupervised</italic> approaches. In <italic>supervised</italic> learning, objects in a pool are classified using a set of known annotations or attributes or features, i.e. a <italic>supervised</italic> algorithm learns the pattern(s) from a limited number of annotated training data and then classifies the remaining testing data using the acquired knowledge. Instead, in the <italic>unsupervised</italic> learning, pattern(s) are first defined from a subset of the unknown data and then the remaining data are classified based on the defined patterns, i.e. an <italic>unsupervised</italic> algorithm first defines pattern(s) among the objects in a pool of data with unknown annotations or attributes or features, and then uses the acquired knowledge to classify the remaining data. In addition, there is another category called <italic>reinforcement</italic> learning which is out of the scope of this work, but allows an agent to improve its experience and knowledge by learning iteratively through interacting with its environment.</p><p id="Par5">Since the 1950s, many methods pertaining to both the learning paradigms (i.e. <italic>supervised</italic> and <italic>unsupervised</italic>) have been proposed. The popular methods in the <italic>supervised</italic> domain include: ANN [<xref ref-type="bibr" rid="CR14">14</xref>] and its variants (e.g. Backpropagation [<xref ref-type="bibr" rid="CR15">15</xref>], Hopfield Networks [<xref ref-type="bibr" rid="CR16">16</xref>], Boltzmann Machines [<xref ref-type="bibr" rid="CR17">17</xref>], Restricted Boltzmann Machines [<xref ref-type="bibr" rid="CR18">18</xref>], Spiking Neural Networks [<xref ref-type="bibr" rid="CR19">19</xref>], etc.), Bayesian Statistics [<xref ref-type="bibr" rid="CR20">20</xref>], Support Vector Machines [<xref ref-type="bibr" rid="CR21">21</xref>] and other linear classifiers [<xref ref-type="bibr" rid="CR22">22</xref>] (e.g. Fisher’s Linear Discriminant [<xref ref-type="bibr" rid="CR23">23</xref>], Regressors [<xref ref-type="bibr" rid="CR24">24</xref>], Naive Bayes Classifier [<xref ref-type="bibr" rid="CR25">25</xref>], etc.), k-Nearest Neighbours [<xref ref-type="bibr" rid="CR26">26</xref>], Hidden Markov Model [<xref ref-type="bibr" rid="CR27">27</xref>], and Decision Trees [<xref ref-type="bibr" rid="CR28">28</xref>]. Popular <italic>unsupervised</italic> methods include: Autoencoders [<xref ref-type="bibr" rid="CR29">29</xref>], Expectation–Maximization [<xref ref-type="bibr" rid="CR30">30</xref>], Information Bottleneck [<xref ref-type="bibr" rid="CR31">31</xref>], Self-Organizing Maps [<xref ref-type="bibr" rid="CR32">32</xref>], Association Rules [<xref ref-type="bibr" rid="CR33">33</xref>], Hierarchical Clustering [<xref ref-type="bibr" rid="CR34">34</xref>], k-Means [<xref ref-type="bibr" rid="CR35">35</xref>], Fuzzy Clustering [<xref ref-type="bibr" rid="CR36">36</xref>], and Density-based Clustering [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>] (e.g. Ordering Points To Identify the Clustering Structure [<xref ref-type="bibr" rid="CR39">39</xref>]). Many of these methods have been successfully applied to data coming from various biological sources.</p><p id="Par6">For the sake of simplicity, the vast amount of biological data coming from the diverse application domains have been categorized to a few broad data types. These data types include <italic>Sequences</italic> (data generated by Omics technologies, e.g. [gen/transcript/epigen/prote/metabol]omics [<xref ref-type="bibr" rid="CR40">40</xref>]), <italic>Images</italic> (data generated by [bio/medical/clinical/health] imaging techniques containing [sub-]cellular and diagnostic images), and <italic>Signals</italic> (electrical signals generated by the brain and the muscles and acquired using appropriate sensors at the [Brain/Body]–Machine Interfaces or BMI). Each of these data types originating at diverse biological application domains have witnessed major contributions from the specified ML methods and their variants (see for <italic>Sequences</italic> [<xref ref-type="bibr" rid="CR41">41</xref>], <italic>images</italic> [<xref ref-type="bibr" rid="CR42">42</xref>–<xref ref-type="bibr" rid="CR44">44</xref>], and <italic>signals</italic> [<xref ref-type="bibr" rid="CR45">45</xref>–<xref ref-type="bibr" rid="CR47">47</xref>]).<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>Application of different DL models to biological data. <bold>a</bold> Wordcloud generated using author keywords extracted from research papers published between January 2011 and March 2020 which mentioned analysis of biological data (images, signals and sequences) using DL techniques and indexed in the Scopus database. The keywords were pruned to highlight the analysis methods. <bold>b</bold> Distribution of published papers mentioning the usage of top 10 techniques. The colours of the individual pies match the colours in the wordcloud. Legend—CNN: Convolutional Neural Network, FCN: Fully Connected Network, DA[E]: Deep Autoencoder, TRL: Transfer Learning, RNN: Recurrent Neural Network (including Long Short-Term Memory or LSTM), ANN: Artificial Neural Network, GAN: Generative Adversarial Network, DNN: Deep Neural Network, DBN: Deep Belief Network, DBM: Deep Boltzmann Machine</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig2_HTML.png" id="MO2"/></fig></p><p id="Par7">In recent years, DL methods are potentially reshaping the future of ML and AI [<xref ref-type="bibr" rid="CR48">48</xref>]. It is worthy to mention here that, from a broader perspective, ML has been applied to a range of tasks including anomaly detection [<xref ref-type="bibr" rid="CR49">49</xref>, <xref ref-type="bibr" rid="CR50">50</xref>, <xref ref-type="bibr" rid="CR278">278</xref>, <xref ref-type="bibr" rid="CR283">283</xref>, <xref ref-type="bibr" rid="CR290">290</xref>], biological data mining [<xref ref-type="bibr" rid="CR51">51</xref>, <xref ref-type="bibr" rid="CR52">52</xref>], detection of coronavirus [<xref ref-type="bibr" rid="CR53">53</xref>, <xref ref-type="bibr" rid="CR54">54</xref>], disease detection and patient management [<xref ref-type="bibr" rid="CR55">55</xref>–<xref ref-type="bibr" rid="CR57">57</xref>, <xref ref-type="bibr" rid="CR277">277</xref>, <xref ref-type="bibr" rid="CR279">279</xref>–<xref ref-type="bibr" rid="CR282">282</xref>, <xref ref-type="bibr" rid="CR284">284</xref>, <xref ref-type="bibr" rid="CR286">286</xref>, <xref ref-type="bibr" rid="CR287">287</xref>, <xref ref-type="bibr" rid="CR289">289</xref>, <xref ref-type="bibr" rid="CR291">291</xref>], education [<xref ref-type="bibr" rid="CR58">58</xref>], natural language processing [<xref ref-type="bibr" rid="CR59">59</xref>, <xref ref-type="bibr" rid="CR285">285</xref>, <xref ref-type="bibr" rid="CR288">288</xref>], and price prediction [<xref ref-type="bibr" rid="CR60">60</xref>]. Despite notable popularity and applicability to diverse disciplines [<xref ref-type="bibr" rid="CR61">61</xref>], there exists no comprehensive review which focuses on pattern recognition in biological data and provides pointers to the various biological data sources and DL tools, and the performances of those tools [<xref ref-type="bibr" rid="CR51">51</xref>].</p><p id="Par8">Also, considering the ecosystem of modern data analysis using advanced ML techniques (such as DL), providing information about methods application only partially covers the components of this ecosystem (see the various components of the ecosystem in Fig. <xref rid="Fig1" ref-type="fig">1</xref>). The remaining components of the ecosystem include open access data sources and open-source toolboxes and libraries which are used in developing the individual methods. It is therefore of paramount importance to have a complete understanding of the availability of datasets and their characteristics, the capabilities and options offered by the libraries, and how they compare with each other in different execution environments such as central processing unit (CPU) and graphical processing unit (GPU). The current paper’s novelty lies in being first of its kind to cover comprehensively the complete ecosystem of modern data analysis using advanced ML technique, i.e., DL.</p><p id="Par9">Therefore, with the above aim, this review provides—a brief overview on DL concepts and their applications to various biological data types; a list of available open access data repositories offering data for method development; and a list of existing open-source libraries and frameworks which can be utilized to harness the power of these techniques along with their relative and performance comparison. Towards the end, some open issues are identified and some speculative future perspectives are outlined.</p><p id="Par10">The remainder of the article is organized as follows: Section <xref rid="Sec2" ref-type="sec">2</xref> provides the conceptual overview and introduces the reader to the underlying theory of DL; Section <xref rid="Sec11" ref-type="sec">3</xref> describes the applications; Section <xref rid="Sec15" ref-type="sec">4</xref> lists the open-source data repositories; Section <xref rid="Sec35" ref-type="sec">5</xref> presents the popular open-source DL tools; and Sections <xref rid="Sec53" ref-type="sec">6</xref> and <xref rid="Sec58" ref-type="sec">7</xref> compare the most popular tools from relative and performance perspectives. Section <xref rid="Sec59" ref-type="sec">8</xref> presents the reader with some of the open issues and hints on the future perspectives, and finally, the article is concluded in Section <xref rid="Sec60" ref-type="sec">9</xref>.</p><p id="Par11"><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Keypoints and applications of different deep learning architectures</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Architecture</p></th><th align="left"><p>Pros.</p></th><th align="left"><p>Cons.</p></th></tr></thead><tbody><tr><td align="left"><p><fig id="Figa" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figa_HTML.png" position="anchor" id="MO3"/></fig></p></td><td align="left"><p>- DNN can learn high-level feature representation and apply transfer learning.</p><p>- It can be used for healthcare and visual recognition.</p></td><td align="left"><p>- It requires substantial volume of training data.</p><p>- Significant computational power is required.</p><p>- The learning process is slow.</p></td></tr><tr><td align="left"><p><fig id="Figb" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figb_HTML.png" position="anchor" id="MO4"/></fig></p></td><td align="left"><p>- Graphical model, undirected links across a set of visible nodes and a set of hidden nodes.</p><p>- Used mainly for dimensionality reduction and classification.</p></td><td align="left"><p>- High time complexity for interference than DBN. ↵ - Learning information does not reach to the lower layer.</p><p>- Tends to overfit.</p></td></tr><tr><td align="left"><p><fig id="Figc" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figc_HTML.png" position="anchor" id="MO5"/></fig></p></td><td align="left"><p>- Easy to code and works sufficiently well for just a few layers.</p><p>- High performance gain by adding layers compared to multilayer perceptron.</p><p>- Robust in classification.</p></td><td align="left"><p>- It can be trained greedily, one layer at a time.</p><p>- Hard to deduce posterior distribution for configurations of hidden causes.</p></td></tr><tr><td align="left"><p><fig id="Figd" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figd_HTML.png" position="anchor" id="MO6"/></fig></p></td><td align="left"><p>- Learn data encoding, reconstruction and generation at same time.</p><p>- Training is stable without label data.</p><p>- Variant: sparse, denoising and contractive DA.</p></td><td align="left"><p>- Requires pretraining stage due to the chances of vanishing error.</p><p>- Each application requires redesigned and retrained the model.</p><p>- The DA is sensitive to input errors.</p></td></tr><tr><td align="left"><p><fig id="Fige" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fige_HTML.png" position="anchor" id="MO7"/></fig></p></td><td align="left"><p>- The main benefit is data augmentation.</p><p>- GAN performs unsupervised learning. </p><p>- GAN learns density distributions of data.</p></td><td align="left"><p>- Difficult to train as optimizing loss function is hard and requires a lot of trial and error.</p></td></tr><tr><td align="left"><p><fig id="Figf" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figf_HTML.png" position="anchor" id="MO8"/></fig></p></td><td align="left"><p>- It can process inputs of any length.</p><p>- RNN can use internal memory and performs well for stream time series data.</p></td><td align="left"><p>- Computation is slow and training can be difficult.</p><p>- Processing long sequences is difficult.</p><p>- Prone to problems such as exploding and gradient vanishing.</p></td></tr><tr><td align="left"><p><fig id="Figg" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Figg_HTML.png" position="anchor" id="MO9"/></fig></p></td><td align="left"><p>- CNN can capture hierarchical information.</p><p>- CNN can share pretrained weight which is required for transfer learning.</p><p>- Requires less connection compared to DNN.</p></td><td align="left"><p>- Large labelled dataset is required for training.</p><p>- The working mechanism of CNN is not clear.</p></td></tr></tbody></table><table-wrap-foot><p>Legend: <italic>DA</italic> Deep Autoencoder, <italic>DBN</italic> Deep Belief Network, <italic>RNN</italic> Recurrent Neural Network, <italic>DNN</italic> Deep Neural Network, <italic>DBM</italic> Deep Boltzmann Machine, <italic>CNN</italic> Convolutional Neural Network.</p></table-wrap-foot></table-wrap></p></sec><sec id="Sec2"><title>Overview of Deep Learning</title><p id="Par12">In DL the data representations are learned with increasing abstraction levels, i.e., at each level more abstract representations are learned by defining them in terms of less abstract representations at lower levels [<xref ref-type="bibr" rid="CR62">62</xref>]. Through this hierarchical learning process, a system can learn complex representations directly from the raw data [<xref ref-type="bibr" rid="CR63">63</xref>].</p><p id="Par13">Though many DL architectures have been proposed in the literature for various applications, there has been a consistent preference to use particular variants for biological data. As shown in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, the most popular models have been identified as—Deep Neural Network (DNN), Deep Boltzmann Machine (DBM) and Deep Belief Network (DBN), Deep Autoencoder (DA), Generative Adversarial Network (GAN), Recurrent Neural Network (RNN, including LSTM), and Convolutional Neural Network (CNN). Each of these models’ architectures and their respective pros and cons are listed in Table <xref rid="Tab1" ref-type="table">1</xref>. The following subsections introduce each of these most frequently used DL architectures in mining biological data.</p><sec id="Sec3"><title>Deep Neural Network (DNN)</title><p id="Par14">A DNN [<xref ref-type="bibr" rid="CR64">64</xref>] is inspired by the brain’s multilevel visual processing mechanism starting with the cortical area ‘V1’ and then to area ‘V2’, and so on [<xref ref-type="bibr" rid="CR65">65</xref>]. Mimicking this, the traditional artificial neural network or NN is extended with additional hidden layers containing nonlinear computational units in each of these hidden layers to learn a subset of the given representations. Despite its successful usage in a range of different applications, the main drawback has been the slow and cumbersome training process [<xref ref-type="bibr" rid="CR66">66</xref>].</p></sec><sec id="Sec4"><title>[Restricted] Boltzmann Machines ([R]BM)</title><p id="Par15">[R]BM represents specific probability distributions through a undirected probabilistic generative model [<xref ref-type="bibr" rid="CR67">67</xref>]. Considered as a nonlinear feature detector, [R]BM is trained based on optimizing its parameters for a set of given observations to obtain the best possible fit of the probability distribution through a Markov chain Monte Carlo method known as Gibbs sampling [<xref ref-type="bibr" rid="CR68">68</xref>, <xref ref-type="bibr" rid="CR69">69</xref>]. With symmetrical connections among subsequent units in multiple hidden layers, BM has only one visible layer. The main drawback of the standard BM is that, the learning process is computationally expensive and quite slow. Due to this, a BM requires a long period to reach equilibrium statistics [<xref ref-type="bibr" rid="CR62">62</xref>]. However, this learning inefficiency can be solved by forming a bipartite graph (i.e. restricting to have one hidden layer and one visible layer) [<xref ref-type="bibr" rid="CR67">67</xref>]. To extend this shallow architecture to a deep one, multiple RBMs as unitary learning elements are stacked together and this yields the following two DL architectures.</p><sec id="Sec5"><title>Deep Boltzmann Machine (DBM)</title><p id="Par16">DBM [<xref ref-type="bibr" rid="CR70">70</xref>] is a stack of undirected RBMs which supports a feedback mechanism among the layers to facilitate inference from higher-level units to propagate to lower-level units. This allows an input to be alternatively interpreted through concurrent competition at all levels of the model. Despite this powerful inference mechanism, estimating model parameters from data remains a challenge and cannot be solved using traditional gradient-based methods (e.g., persistent contrastive divergence [<xref ref-type="bibr" rid="CR71">71</xref>]) [<xref ref-type="bibr" rid="CR70">70</xref>]. Though this learning problem is overcome by pretraining each RBM in a layerwise greedy fashion, with outputs of the hidden variables from lower layers as input to upper layers [<xref ref-type="bibr" rid="CR67">67</xref>], the time complexity remains high and the approach may not be suitable for large training datasets [<xref ref-type="bibr" rid="CR72">72</xref>].</p></sec><sec id="Sec6"><title>Deep Belief Network (DBN)</title><p id="Par17">DBN [<xref ref-type="bibr" rid="CR73">73</xref>], in contrast to the DBM, is formed by stacking several RBMs together in a way that one RBM’s latent layer is linked to the next RBM’s visible layer. As the top two layers of DBN are undirected, the connections are downward directed to its immediate lower layer [<xref ref-type="bibr" rid="CR73">73</xref>, <xref ref-type="bibr" rid="CR74">74</xref>]. Thus, the DBN is a hybrid model with the first two layers as a undirected graphical model and the rest being directed generative model. The different layers are learned in a layerwise greedy fashion and fine-tuned based on required output [<xref ref-type="bibr" rid="CR75">75</xref>]; however, the training procedure is computationally demanding.</p></sec></sec><sec id="Sec7"><title>Deep Autoencoder (DA)</title><p id="Par18">DA is a DL architecture [<xref ref-type="bibr" rid="CR76">76</xref>] obtained by stacking a number of data-driven Autoencoders which are unsupervised elements. DA is also known as DAE and is designed to reduce data dimension by automatically projecting incoming representations to a lower-dimensional space than that of the input. In an Autoencoder, equal amounts of units are used in the input/output layers and less units in the hidden layers. (Non)linear transformations are embodied in the hidden layer units to encode the given input into smaller dimensions [<xref ref-type="bibr" rid="CR77">77</xref>]. Despite the fact that it requires a pretraining stage and suffers from a vanishing error, this architecture is popular for its data compression capability and has many variants, e.g. Denoising Autoencoder [<xref ref-type="bibr" rid="CR76">76</xref>], Sparse Autoencoder [<xref ref-type="bibr" rid="CR78">78</xref>], Variational Autoencoder [<xref ref-type="bibr" rid="CR79">79</xref>], and Contractive Autoencoder [<xref ref-type="bibr" rid="CR80">80</xref>].</p></sec><sec id="Sec8"><title>Generative Adversarial Network (GAN)</title><p id="Par19">GAN [<xref ref-type="bibr" rid="CR81">81</xref>] is an effective generative model. Generative models perform an unsupervised learning task, where they automatically discover and learn existing patterns in data and then use that knowledge to generate new examples of the learnt pattern as if they were drawn from the original dataset. Using GAN, the problem is seen as a supervised learning problem with two strands: (i) the generator, which generates new examples as trained, and (ii) the discriminator, which classifies generated examples to two classes (real or fake). These generator and discriminator models are trained together in a zero-sum game (i.e. in an adversarial fashion) such that the examples generated by the generator model maximize the loss of the discriminator model [<xref ref-type="bibr" rid="CR82">82</xref>, <xref ref-type="bibr" rid="CR83">83</xref>].</p></sec><sec id="Sec9"><title>Recurrent Neural Network (RNN)</title><p id="Par20">The RNN architecture [<xref ref-type="bibr" rid="CR84">84</xref>] is designed to detect spatio-temporal alignments in streams of data [<xref ref-type="bibr" rid="CR85">85</xref>]. Unlike feedforward NN which performs computations unidirectionally from input to output, an RNN computes the current state’s output depending on the outputs of the previous states. Due to this ‘memory’-like property, despite learning problems related to vanishing and exploding gradients, RNN has gained popularity in many fields involving streaming data (e.g. text mining, time series, genomes, financial, etc.). In recent years, two main variants, bidirectional RNN (BRNN) [<xref ref-type="bibr" rid="CR86">86</xref>] and Long Short-Term Memory (LSTM) [<xref ref-type="bibr" rid="CR87">87</xref>], have also been applied [<xref ref-type="bibr" rid="CR48">48</xref>, <xref ref-type="bibr" rid="CR88">88</xref>, <xref ref-type="bibr" rid="CR89">89</xref>].</p></sec><sec id="Sec10"><title>Convolutional Neural Network (CNN)</title><p id="Par21">CNN [<xref ref-type="bibr" rid="CR90">90</xref>] is a multilayer NN model [<xref ref-type="bibr" rid="CR91">91</xref>] which has gained popularity in analysing image-based data. Inspired by the neurobiology of the visual cortex, the CNN consists of convolutional layer(s) containing a set of learnable filter banks and followed by fully connected layer(s). These filter banks convolve with the input data and pass the results to activation functions (e.g. ReLU, Sigmoid, and Tanh). There also exist subsampling steps in between these layers. The CNN outperforms DNNs, which as they do not scale well with multidimensional locally correlated input data. To address the scaling problem of DNNs, the CNN approach has been quite successful in analysing datasets with a high number of nodes and parameters (e.g. images). As the images are ‘stationary,’ convolution filters (CF) can easily learn data-driven kernels. Applying such CF along with a suitable pooling function reduces the features that are supplied to the fully connected network to classify. However, in case of large datasets even this can be daunting and can be solved using sparsely connected networks. Some of the popular CNN configurations include AlexNet [<xref ref-type="bibr" rid="CR92">92</xref>], VGGNet [<xref ref-type="bibr" rid="CR93">93</xref>] GoogLeNet [<xref ref-type="bibr" rid="CR94">94</xref>], etc. (see Table <xref rid="Tab2" ref-type="table">2</xref> for a complete list of CNN’s variations with relevant details).<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Keypoints of different deep CNN architectures</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Architecture</p></th><th align="left"><p>Network Design</p></th><th align="left"><p>Parameters</p></th><th align="left"><p>Key points</p></th></tr></thead><tbody><tr><td align="left"><p>LeNet (1998)</p></td><td align="left"><p>LeNet-5 is the first CNN architecture with 2 convolutional and 3 fully connected layers. </p></td><td align="left"><p>0.06 million</p></td><td align="left"><p>- Feedforward NN.</p><p>- Connection between layers is sparse to reduce computational complexity. </p></td></tr><tr><td align="left"><p>AlexNet (2012)</p></td><td align="left"><p>AlexNet has 8 layers and consists of 5 convolutional and 3 fully connected layers.</p></td><td align="left"><p>60 million</p></td><td align="left"><p>- Deeper than the LeNet and aliasing artifacts in the learned feature maps due to large filter size.</p></td></tr><tr><td align="left"><p>VGG-16 (2014)</p></td><td align="left"><p>VGG-16 has 13 convolutional layers (and max pooling layers) and 2 fully connected layers followed by 1 output layer with softmax activation. </p></td><td align="left"><p>138 million</p></td><td align="left"><p>- Roughly twice deeper network can be designed compared to the AlexNet.</p><p>- A deeper variant of VGG-16 is VGG-19.</p><p>- Computationally expensive and cannot be used with low resource systems.</p></td></tr><tr><td align="left"><p>Inception-v1 (2014)</p></td><td align="left"><p> Also known as GoogleNet, it has 22 layers with parameters (or 27 when pooling layers are included). Towards the end, it employs an average pooling. </p></td><td align="left"><p>5 million</p></td><td align="left"><p>- It uses sparse connections to overcome redundant information problem and omits irrelevant feature maps.</p><p>- High accuracy with a reduced computational cost.</p><p>- It's heterogeneous topology requires customization. </p></td></tr><tr><td align="left"><p>Inception-v3 (2015)</p></td><td align="left"><p>Inception-v3 has 48 layers with a number of inception modules (each consisting of pooling layers and convolutional filters with activation functions), concatenation layers and fully connected layer(s) along with dropout and softmax.</p></td><td align="left"><p>23 million</p></td><td align="left"><p>- It increases accuracy and reduces computational complexity in comparison to Inception-v1.</p><p>- Reduces representational bottleneck.</p><p>- Replaces large size filters with smaller ones.</p><p>- It's architecture is complex and lacks homogeneity.</p></td></tr><tr><td align="left"><p>ResNet-50 (2015)</p></td><td align="left"><p>ResNet-50 has 50 layers with initial convolutional and max-pooling layers, and final average pooling and fully connected layers. In between, there are 3, 4, 6 and 3 residual blocks separated in 4 stages where each block contains 3 convolutional layers.</p></td><td align="left"><p> 25.5 million</p></td><td align="left"><p>- It provides an accelerated training speed.↵ -Reduces the effect of Vanishing Gradient Problem.</p><p>- Classifies images with high accuracy.</p></td></tr><tr><td align="left"><p>Xception (2016)</p></td><td align="left"><p>The Xception architecture has 36 convolutional layers forming the feature extraction base of the network. The 36 convolutional layers are structured into 14 modules, all of which have linear residual connections around them, except for the first and last modules.</p></td><td align="left"><p>22.8 million</p></td><td align="left"><p>- Xception shows small gains in classification performance on the ImageNet dataset and large gains on the JFT dataset when compared to Inception-v3.</p></td></tr><tr><td align="left"><p>Inception-v4 (2016)</p></td><td align="left"><p>Inception-v4 consists of two main sections: a feature extractor and a fully connected layer. The feature extractor includes various convolutional blocks such as 1 stem block, 14 inception blocks, 2 reduction blocks and a pooling layer. The inception blocks are divided in three categories, namely, A, B, and C with 4, 7, and 3 blocks, respectively.</p></td><td align="left"><p>43 million</p></td><td align="left"><p>- Deep hierarchies of features, multilevel feature representation.↵ - Learning speed is slow.</p></td></tr><tr><td align="left"><p>Inception-ResNet-v2 (2016)</p></td><td align="left"><p>Inception-ResNet-v2 consists of 164 layers with several convolutional blocks which include 1 stem block, 20 residual inception blocks, 2 reduction blocks and a pooling layer. The residual inception blocks are divided in three categories, namely, A, B, and C with 5, 10, and 5 blocks, respectively.</p></td><td align="left"><p>56 million</p></td><td align="left"><p>- It improves training speed.↵ - Deep hierarchies of features, multilevel feature representation.</p></td></tr><tr><td align="left"><p>ResNeXt-50 (2016)</p></td><td align="left"><p>ResNeXt-50 has initial convolutional and max-pooling layers, and final average pooling and fully connected layers. In between, there are 3, 4, 6 and 3 residual blocks separated in 4 stages where each block contains 3 convolutional layers. In comparison to ResNet-50, it scales up the number of parallel towers (cardinality=32) within each residual block.</p></td><td align="left"><p>25 millions</p></td><td align="left"><p>- Has homogeneous topology. ↵ - Performs grouped convolution.</p></td></tr><tr><td align="left"><p>DenseNet-121 (2016)</p></td><td align="left"><p>DenseNet architecture includes 4 dense blocks. Each layer in a dense block is connected to every other layer. The dense blocks, consisting of convolution, pooling, batch normalization and activation, are separated by transition layers.</p></td><td align="left"><p> 8 millions</p></td><td align="left"><p>- Introduces depth or cross-layer dimension.↵ - Ensures maximum data flow between the layers in the network. ↵ - Avoids relearning of redundant feature maps.</p></td></tr></tbody></table></table-wrap></p><p id="Par22"><table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Deep learning applied to biological data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Type</p></th><th align="left"><p>Data [base/set]</p></th><th align="left"><p>DL architecture</p></th><th align="left"><p>Task</p></th></tr></thead><tbody><tr><td align="left"><p>Images</p></td><td align="left"><p>ABIDE</p></td><td align="left"><p>DNN [<xref ref-type="bibr" rid="CR95">95</xref>]</p></td><td align="left"><p>Autism disorder identification</p></td></tr><tr><td align="left"/><td align="left"><p>ADHD-200 dataset</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR96">96</xref>]</p></td><td align="left"><p>ADHD detection</p></td></tr><tr><td align="left"/><td align="left"><p>ADNI dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR97">97</xref>], DBM [<xref ref-type="bibr" rid="CR98">98</xref>], DBN [<xref ref-type="bibr" rid="CR99">99</xref>]</p></td><td align="left"><p>AD/MCI diagnosis</p></td></tr><tr><td align="left"/><td align="left"><p>BRATS Dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR100">100</xref>]</p></td><td align="left"><p>Brain pathology segmentation</p></td></tr><tr><td align="left"/><td align="left"><p>CT dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR101">101</xref>]</p></td><td align="left"><p>Fast segmentation of 3D medical images</p></td></tr><tr><td align="left"/><td align="left"><p>DRIVE, STARE datasets</p></td><td align="left"><p>GAN [<xref ref-type="bibr" rid="CR102">102</xref>]</p></td><td align="left"><p>Retinal blood vessel segmentation</p></td></tr><tr><td align="left"/><td align="left"><p>EM segmentation challenge dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR103">103</xref>]</p></td><td align="left"><p>Segment neuronal membranes</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>LSTM [<xref ref-type="bibr" rid="CR104">104</xref>]</p></td><td align="left"><p>Biomedical volumetric image segmentation</p></td></tr><tr><td align="left"/><td align="left"><p>IBSR, LPBA40, and OASIS dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR105">105</xref>]</p></td><td align="left"><p>Skull stripping</p></td></tr><tr><td align="left"/><td align="left"><p>LIDC-IDRI dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR106">106</xref>]</p></td><td align="left"><p>Lung nodule malignancy classification</p></td></tr><tr><td align="left"/><td align="left"><p>MICCAI 2009 LV dataset</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR107">107</xref>]</p></td><td align="left"><p>Heart LV segmentation</p></td></tr><tr><td align="left"/><td align="left"><p>MITOS dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR108">108</xref>]</p></td><td align="left"><p>Mitosis detection in breast cancer</p></td></tr><tr><td align="left"/><td align="left"><p>PACS dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR106">106</xref>]</p></td><td align="left"><p>Medical image classification</p></td></tr><tr><td align="left"/><td align="left"><p>TBI dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR109">109</xref>]</p></td><td align="left"><p>Brain lesion segmentation</p></td></tr><tr><td align="left"><p>Signals</p></td><td align="left"><p>BCI competition IV</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR110">110</xref>], CNN [<xref ref-type="bibr" rid="CR111">111</xref>–<xref ref-type="bibr" rid="CR113">113</xref>]</p></td><td align="left"><p>Motion action decoding</p></td></tr><tr><td align="left"/><td align="left"><p>DEAP dataset</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR114">114</xref>, <xref ref-type="bibr" rid="CR115">115</xref>]</p></td><td align="left"><p>Affective state recognition</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR116">116</xref>]</p></td><td align="left"><p>Emotion classification</p></td></tr><tr><td align="left"/><td align="left"><p>DECAF</p></td><td align="left"><p>GAN [<xref ref-type="bibr" rid="CR117">117</xref>]</p></td><td align="left"/></tr><tr><td align="left"/><td align="left"><p>Freiburg dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR118">118</xref>]</p></td><td align="left"><p>Seizure prediction</p></td></tr><tr><td align="left"/><td align="left"><p>MAHNOB-HCI</p></td><td align="left"><p>DA [<xref ref-type="bibr" rid="CR119">119</xref>]</p></td><td align="left"><p>Emotion recognition</p></td></tr><tr><td align="left"/><td align="left"><p>MIT-BIH arrhythmia database</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR120">120</xref>, <xref ref-type="bibr" rid="CR121">121</xref>]</p></td><td align="left"><p>ECG arrhythmia classification</p></td></tr><tr><td align="left"/><td align="left"><p>MIT-BIH, INCART, and SVDB</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR122">122</xref>]</p></td><td align="left"><p>Movement decoding</p></td></tr><tr><td align="left"/><td align="left"><p>NinaPro database</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR123">123</xref>], CNN [<xref ref-type="bibr" rid="CR122">122</xref>]</p></td><td align="left"><p>Motion action decoding</p></td></tr><tr><td align="left"><p>Sequences</p></td><td align="left"><p>CullPDB, CB513, CASP datasets, CAMEO</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR124">124</xref>]</p></td><td align="left"><p>2ps prediction</p></td></tr><tr><td align="left"/><td align="left"><p>DREAM</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR125">125</xref>]</p></td><td align="left"><p>DNA/RNA sequence prediction</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>DNN [<xref ref-type="bibr" rid="CR126">126</xref>]</p></td><td align="left"><p>Predict effective drug combination</p></td></tr><tr><td align="left"/><td align="left"><p>ENCODE database</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR127">127</xref>, <xref ref-type="bibr" rid="CR128">128</xref>]</p></td><td align="left"><p>Gene expression identification</p></td></tr><tr><td align="left"/><td align="left"><p>ENCODE DGF dataset</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR129">129</xref>]</p></td><td align="left"><p>Predict noncoding variant of gene</p></td></tr><tr><td align="left"/><td align="left"><p>GEO database</p></td><td align="left"><p>GAN [<xref ref-type="bibr" rid="CR130">130</xref>]</p></td><td align="left"><p>Gene expression data augmentation</p></td></tr><tr><td align="left"/><td align="left"><p>GWH and UCSC datasets</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR131">131</xref>]</p></td><td align="left"><p>Splice junctions prediction</p></td></tr><tr><td align="left"/><td align="left"><p>JASPAR database and ENCODE</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR132">132</xref>]</p></td><td align="left"><p>Predicting DNA-binding protein</p></td></tr><tr><td align="left"/><td align="left"><p>miRBoost</p></td><td align="left"><p>RNN [<xref ref-type="bibr" rid="CR133">133</xref>]</p></td><td align="left"><p>micro-RNA Prediction</p></td></tr><tr><td align="left"/><td align="left"><p>miRNA-mRNA pairing data repository</p></td><td align="left"><p>LSTM [<xref ref-type="bibr" rid="CR134">134</xref>]</p></td><td align="left"><p>micro-RNA target prediction</p></td></tr><tr><td align="left"/><td align="left"><p>Protein Data Bank (PDB)</p></td><td align="left"><p>DA [<xref ref-type="bibr" rid="CR135">135</xref>]</p></td><td align="left"><p>Protein structure reconstruction</p></td></tr><tr><td align="left"/><td align="left"><p>SRBCT, prostate tumour, and MLL GE</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR136">136</xref>]</p></td><td align="left"><p>Gene/MiRNA feature selection</p></td></tr><tr><td align="left"/><td align="left"><p>sbv IMPROVER</p></td><td align="left"><p>DBN [<xref ref-type="bibr" rid="CR137">137</xref>]</p></td><td align="left"><p>Human diseases and drug development</p></td></tr><tr><td align="left"/><td align="left"><p>TCGA database</p></td><td align="left"><p>DA [<xref ref-type="bibr" rid="CR138">138</xref>]</p></td><td align="left"><p>Cancer detection and gene identification</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>DBM [<xref ref-type="bibr" rid="CR139">139</xref>]</p></td><td align="left"/></tr><tr><td align="left"/><td align="left"/><td align="left"><p>DNN [<xref ref-type="bibr" rid="CR140">140</xref>]</p></td><td align="left"><p>Drug combination estimation</p></td></tr><tr><td align="left"/><td align="left"><p>UCSC, CGHV Data, SPIDEX database</p></td><td align="left"><p>CNN [<xref ref-type="bibr" rid="CR141">141</xref>]</p></td><td align="left"><p>Genetic variants identification</p></td></tr></tbody></table></table-wrap></p><p id="Par23"><fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>Trends in publication involving different DL architectures from 2015 to 2019 in three major types of data—images <bold>a</bold>, signals <bold>b</bold>, and sequences <bold>c</bold>. The numbers of papers have been normalized within each data type. However, it is noteworthy that the ratio of number of publications involving DL techniques applied to different data types (images, signals, and sequences) are approximately—1:<inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{4}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq4.gif"/></alternatives></inline-formula>:<inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{1}{10}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq5.gif"/></alternatives></inline-formula></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig3_HTML.png" id="MO10"/></fig></p></sec></sec><sec id="Sec11"><title>Deep Learning and Biological Data</title><p id="Par24">Many studies have been reported in the literature which employ diverse DL architectures with related and varied parameter sets (see section <xref rid="Sec2" ref-type="sec">2</xref>) to analyse patterns in biological data. For most of the DL architectures, as shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>, the number of publications is increasing steadily over the years. A set of randomly selected representative studies from the large amount of reported literature are described below and summarized in Table <xref rid="Tab3" ref-type="table">3</xref>. These studies belong to the three data types we have considered within the context of this paper, that is, images, signals, and sequences.</p><sec id="Sec12"><title>Images</title><p id="Par25">CNN was used by on histology images of the breast to find mitosis [<xref ref-type="bibr" rid="CR108">108</xref>, <xref ref-type="bibr" rid="CR142">142</xref>] and to segment neuronal structures in Electron Microscope Images (EMI) [<xref ref-type="bibr" rid="CR103">103</xref>]. Havaei et al. used CNN to segment brain tumour from Magnetic Resonance Imaging (MRI) [<xref ref-type="bibr" rid="CR100">100</xref>] and Hosseini et al. used it for the diagnosis of Alzheimer’s disease (AD) from MRI [<xref ref-type="bibr" rid="CR56">56</xref>, <xref ref-type="bibr" rid="CR97">97</xref>]. DBM [<xref ref-type="bibr" rid="CR98">98</xref>] and RBM [<xref ref-type="bibr" rid="CR99">99</xref>] were used in detecting AD and mild cognitive impairment (MCI) from MRI and Positron Emission Tomography (PET) scans. Again, CNN was used on MRI to detect neuroendocrine carcinoma [<xref ref-type="bibr" rid="CR55">55</xref>, <xref ref-type="bibr" rid="CR74">74</xref>, <xref ref-type="bibr" rid="CR105">105</xref>]. CNN’s dual pathway version was used by Kamnitsas et al. to segment lesions related to tumours, traumatic injuries, and ischemic strokes [<xref ref-type="bibr" rid="CR109">109</xref>]. CNN was also used by Fritscher et al. for volume segmentation [<xref ref-type="bibr" rid="CR101">101</xref>] and by Cho et al. to find anatomical structures (Lung nodule to classify malignancy) [<xref ref-type="bibr" rid="CR106">106</xref>] from Computed Tomography (CT) scans. DBN was applied on MRIs to detect Attention Deficit Hyperactivity Disorder [<xref ref-type="bibr" rid="CR96">96</xref>] and on cardiac MRIs to segment the heart’s left ventricle [<xref ref-type="bibr" rid="CR107">107</xref>]. GANs have gained popularity in image synthesis and data augmentation to reduce overfitting. GAN’s application in data augmentation and image translation has been reviewed in [<xref ref-type="bibr" rid="CR143">143</xref>] and data augmentation in the CT segmentation tasks was done using CycleGAN [<xref ref-type="bibr" rid="CR144">144</xref>]. GAN-based framework called MedGAN was proposed for medical image-to-image translation [<xref ref-type="bibr" rid="CR145">145</xref>]. GAN was used as survival prediction model for chest CT scan images of patients suffering from idiopathic pulmonary fibrosis [<xref ref-type="bibr" rid="CR146">146</xref>, <xref ref-type="bibr" rid="CR147">147</xref>]. GAN was also used by Halicek for synthesizing hyperspectral images from digitized histology of breast cancer cells [<xref ref-type="bibr" rid="CR148">148</xref>].</p></sec><sec id="Sec13"><title>Signals</title><p id="Par26">A stacked DA was employed to detect emotion from Electroencephalography (EEG) signals after extracting relevant features using Principal Component Analysis (PCA) and reducing non-stationary effect using covariate shift adaptation [<xref ref-type="bibr" rid="CR119">119</xref>]. DBN was applied to decode motor imagery through classifying EEG signal [<xref ref-type="bibr" rid="CR110">110</xref>]. For a similar purpose, CNN was used with augmented common spatial pattern features [<xref ref-type="bibr" rid="CR111">111</xref>]. EEG signals were also classified using DA after features such as location, time, and frequency were extracted using CNN [<xref ref-type="bibr" rid="CR112">112</xref>]. Li et al. used DBN to extract low-dimensional latent features, and select critical channels to classify affective state using EEG signals [<xref ref-type="bibr" rid="CR114">114</xref>]. Also, Jia et al. used an active learning to train DBN and generative RBMs for the classification [<xref ref-type="bibr" rid="CR115">115</xref>]. Tripathi et al. utilized DNN- and CNN-based model for emotion classification [<xref ref-type="bibr" rid="CR116">116</xref>]. CNN was employed to predict seizures through synchronization patterns classification [<xref ref-type="bibr" rid="CR118">118</xref>]. DBN [<xref ref-type="bibr" rid="CR123">123</xref>] and CNN [<xref ref-type="bibr" rid="CR122">122</xref>] were used to decode motion action from NinaPro database. The later approach was also used on MIT-BIH, INCART, and SVDB repositories [<xref ref-type="bibr" rid="CR122">122</xref>]. Moreover, the Electrocardiogram (ECG) Arrhythmias were classified using DBN [<xref ref-type="bibr" rid="CR120">120</xref>, <xref ref-type="bibr" rid="CR121">121</xref>] from the data supplied by MIT-BIH arrhythmia database. Zhu et al. used a GAN model with LSTM and CNN to generate ECG signals with high morphological similarity [<xref ref-type="bibr" rid="CR149">149</xref>]. Another GAN model, RPSeqGAN, trained with SeqGAN [<xref ref-type="bibr" rid="CR150">150</xref>] generated arrhythmic ECG data with five periods and showed high stability and data quality [<xref ref-type="bibr" rid="CR151">151</xref>]. GAN is also used by Luo and Lu for EEG data augmentation [<xref ref-type="bibr" rid="CR152">152</xref>]. You et al. [<xref ref-type="bibr" rid="CR153">153</xref>] and Jiao et al. [<xref ref-type="bibr" rid="CR154">154</xref>] utilized GAN-based model for detecting seizure using EEG signal and Driver sleepiness using EEG and Electrooculography (EOG) signals, respectively. Singh et al. proposed a new GAN framework for denoising ECG [<xref ref-type="bibr" rid="CR155">155</xref>].</p></sec><sec id="Sec14"><title>Sequences</title><p id="Par27">The stacked denoising DA has been used to extract features for cancer diagnosis and classification along with the identification of related genes from Gene Expression (GE) data [<xref ref-type="bibr" rid="CR138">138</xref>]. GAN was also used for identifying expression patterns from GE data [<xref ref-type="bibr" rid="CR156">156</xref>]. A template-based DA learning model was used in reconstructing the protein structures [<xref ref-type="bibr" rid="CR135">135</xref>]. Lee et al. applied a DBN-based unsupervised method to perform autoprediction of splicing junction at Deoxyribonucleic Acid (DNA) level [<xref ref-type="bibr" rid="CR131">131</xref>]. Combining DBN with active learning, Ibrahim et al. devised a method to select feature groups from genes or micro-Ribonucleic Acids (miRNAs) based on expression profiles [<xref ref-type="bibr" rid="CR136">136</xref>]. For translational research, bimodal DBNs were used by Chen et al. to predict responses of human cells using model organisms [<xref ref-type="bibr" rid="CR137">137</xref>]. Pan et al. applied a hybrid CNN-DBN model on RNAs for the prediction of RNA-binding protein (RBP) interaction sites and motifs [<xref ref-type="bibr" rid="CR157">157</xref>], and Alipanahi et al. used CNN to predict sequence specificities of [D/R]BPs [<xref ref-type="bibr" rid="CR125">125</xref>]. Denas and Taylor used CNN to preprocess data generated from Chromatin Immunoprecipitation followed by sequencing (ChIP-seq) and created gene transcription factor activity profiles [<xref ref-type="bibr" rid="CR127">127</xref>]. CNN was used by Kelley et al. to predict DNA sequence accessibility [<xref ref-type="bibr" rid="CR128">128</xref>], by Zeng et al. to predict the DBP [<xref ref-type="bibr" rid="CR132">132</xref>], by Zhou et al. [<xref ref-type="bibr" rid="CR129">129</xref>] and Huang et al.
[<xref ref-type="bibr" rid="CR141">141</xref>] to find non-coding gene variation, and by Wang et al. to predict secondary protein structure (2ps) [<xref ref-type="bibr" rid="CR124">124</xref>]. Park et al. used LSTM to predict miRNA precursor [<xref ref-type="bibr" rid="CR133">133</xref>] and Lee et al. [<xref ref-type="bibr" rid="CR134">134</xref>] used it to predict miRNA precursors’ targets. GAN was used by Marouf et al. for the realistic generation of single-cell RNA-seq data [<xref ref-type="bibr" rid="CR130">130</xref>], by Jiang et al. to predict disease gene from RNA-seq data [<xref ref-type="bibr" rid="CR158">158</xref>], by Zhao et al. as a semi-supervised procedure for predicting drug target binding [<xref ref-type="bibr" rid="CR159">159</xref>], and by Wang et al. for identifying expression patterns from GE data [<xref ref-type="bibr" rid="CR156">156</xref>].</p></sec></sec><sec id="Sec15"><title>Open Access Biological Data Sources</title><p id="Par28">Reproducing scientific results, reported as statistically processed quantitative data or carefully selected representative qualitative data, has been facilitated greatly by data sharing initiatives [<xref ref-type="bibr" rid="CR160">160</xref>]. In the last few decades, many open access data repositories have been made available for this purpose [<xref ref-type="bibr" rid="CR161">161</xref>]. Indeed, many research funders and journals now require data used for studies to be made openly available for verification. To facilitate method development, here we list the leading and popular open access data repositories pertaining to the Sequences, Images, and Signals data which are summarized in Tables <xref rid="Tab4" ref-type="table">4</xref>, <xref rid="Tab5" ref-type="table">5</xref>, and <xref rid="Tab6" ref-type="table">6</xref>, respectively.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Application-wise categorization of open access data repositories and datasets pertaining to [bio/medical/health/clinical] images</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Application</p></th><th align="left"><p>Name</p></th><th align="left"><p>Description</p></th><th align="left"><p>Ref.</p></th></tr></thead><tbody><tr><td align="left"><p>Bio/medical image processing and analysis</p></td><td align="left"><p>CCDB</p></td><td align="left"><p>High-resolution 2/3/4-D light and electron microscope images</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR162">162</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>CIL</p></td><td align="left"><p>Cell image datasets and cell library app.</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR163">163</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Euro Bioimaging</p></td><td align="left"><p>Biological and biomedical imaging data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR164">164</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>HAPS</p></td><td align="left"><p>Microscopic image of human cells and tissues</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR165">165</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>IDR</p></td><td align="left"><p>Viewing, analysis, and sharing of multi-D image data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR166">166</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SMIR</p></td><td align="left"><p>Post-mortem CT scans of the whole body</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR167">167</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>TCIA</p></td><td align="left"><p>CT, MRI, and PET images of cancer patients</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR168">168</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>TMA</p></td><td align="left"><p>Microscopic tissue images of human</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR169">169</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>UCSB BioSeg</p></td><td align="left"><p>2D/3D cellular, subcellular and tissue images</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR170">170</xref>]</p></td></tr><tr><td align="left"><p>Disease detection and diagnosis</p></td><td align="left"><p>ABIDE</p></td><td align="left"><p>Autism brain imaging datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR171">171</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>ADHD-200</p></td><td align="left"><p>fMRI/anatomical datasets fused over the 8 imaging sites</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR172">172</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>ADNI</p></td><td align="left"><p>MCI, early AD and elderly control subjects’ diagnosis data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR173">173</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>BCDR</p></td><td align="left"><p>Multimodal mammography and ultrasound scan data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR174">174</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Kaggle CXRayP</p></td><td align="left"><p>Chest X-ray scans for pneumonia</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR175">175</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>MITOS</p></td><td align="left"><p>Breast cancer histological images</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR176">176</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NAMIC</p></td><td align="left"><p>Lupus, brain, prostate MRI scans</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR177">177</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>nCOV-CXray</p></td><td align="left"><p>COVID-19 cases with chest X-ray/CT images</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR178">178</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Neurosynth</p></td><td align="left"><p>fMRI datasets and synthesis platform</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR179">179</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NIH</p></td><td align="left"><p>Labelled chest X-ray images with diagnoses</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR180">180</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>OASIS</p></td><td align="left"><p>MRI datasets and XNAT data management platform</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR181">181</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Open NI</p></td><td align="left"><p>Imaging modalities and brain diseases data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR182">182</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SMIR</p></td><td align="left"><p>CT of human temporal bones</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR183">183</xref>]</p></td></tr><tr><td align="left"><p>Neuroimage processing and analysis</p></td><td align="left"><p>IXI</p></td><td align="left"><p>It provides neuroimaging data and toolkit software</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR184">184</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>LPBA40</p></td><td align="left"><p>Maps of brain regions and a set of whole-head MRI</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR185">185</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NeuroVault.org</p></td><td align="left"><p>API for collecting and sharing statistical maps of brain</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR186">186</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NITRC</p></td><td align="left"><p>MRI, PET, SPECT, CT, MEG/EEG and optical imaging</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR187">187</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>OpenfMRI</p></td><td align="left"><p>Multimodal MRI and EEG datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR188">188</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>UK data service</p></td><td align="left"><p>fMRI dataset</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR189">189</xref>]</p></td></tr><tr><td align="left"><p>Segmentation</p></td><td align="left"><p>DRIVE</p></td><td align="left"><p>Digital Retinal Images diabetic patient</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR190">190</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>IBSR</p></td><td align="left"><p>Segmentation results of MRI data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR191">191</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>STARE</p></td><td align="left"><p>The dataset contains raw/labelled retinal images</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR192">192</xref>]</p></td></tr></tbody></table><table-wrap-foot><p>Legend: <italic>CXRayP </italic>Chest X-ray Pneumonia, <italic>JHDTI </italic>Johns Hopkins Diffusion Tensor Imaging</p></table-wrap-foot></table-wrap></p><sec id="Sec16"><title>Images</title><p id="Par29">Table <xref rid="Tab4" ref-type="table">4</xref> lists the leading open access data sources including databases and individual datasets that provide access to data pertaining to biological image research. For the sake of simplicity, these sources have been grouped to four broad application areas—[bio/medical] image processing and analysis, disease detection and diagnosis, neuroimage processing and analysis, and segmentation—and these are briefly described below.</p><sec id="Sec17"><title>Bio/Medical Image Processing and Analysis</title><p id="Par30">The Cell Centered Database (CCDB) [<xref ref-type="bibr" rid="CR162">162</xref>] collection provides high-resolution 3-D light and electron microscopic reconstructions of cells and subcellular structures. It also contains [2/3/4]-D protein distribution and structural information from a number of different microscopic image acquisition systems.</p><p id="Par31">Another image library, called the Cell Image Library (CIL) [<xref ref-type="bibr" rid="CR163">163</xref>], presents more than 10,000 unique datasets and 20 TB of images, videos, and animations data. These data belong to a wide diversity of organisms, cell types, and cellular processes.</p><p id="Par32">The Euro Bioimaging [<xref ref-type="bibr" rid="CR164">164</xref>] database provides biological and biomedical imaging data aiming to provide collaboration among different stakeholders including scientists, industry, national and European authorities. Its mission is to give access and services to state-of-the-art imaging techniques and bioimaging data for scientists in Europe and beyond. Euro Bioimaging also includes image analysis tools.</p><p id="Par33">The HAPS is a histology image database [<xref ref-type="bibr" rid="CR165">165</xref>] contains medium-/high-resolution photograph of microscopic image of human cells and tissues which are free of any copyright. Another image database, the Image Data Resource (IDR) [<xref ref-type="bibr" rid="CR166">166</xref>], contains individual datasets of cellular and tissue images. Various categories of images include time-lapse imaging, protein localization studies, digital pathology imaging, yeast study, human high-content screening, etc. It is also public API which facilitates viewing, analysis, and sharing of multi-D image data for cell biology.</p><p id="Par34">The SICAS Medical Image Repository (SMIR) is an image repository for medical research purpose. Two of their featured collections include post-mortem full-body CT [<xref ref-type="bibr" rid="CR167">167</xref>] scan of 50 anonymized subjects of different age groups and gender, and CT, micro-CT, segmentation, and shape models of the cochlea [<xref ref-type="bibr" rid="CR183">183</xref>].</p><p id="Par35">The Cancer Imaging Archive (TCIA) [<xref ref-type="bibr" rid="CR168">168</xref>] contains CT, MRI, and nuclear medicine (e.g. PET) images for clinical diagnostic, biomarker, and cross-disciplinary investigation. The Stanford Tissue Microarray Database (TMA) [<xref ref-type="bibr" rid="CR169">169</xref>] is a source for annotated microscopic tissue images and associated expression data. The data can be used for studying cell biology. The UCSB bio-segmentation benchmark dataset [<xref ref-type="bibr" rid="CR170">170</xref>] contains 2/3-D cellular, subcellular, and tissue images. These datasets can be used for segmentation and classification task.</p></sec><sec id="Sec18"><title>Disease Detection and Diagnosis</title><p id="Par36">A large amount of imaging data has been acquired from patients with neurological disorders. The Autism Brain Imaging Data Exchange (ABIDE) [<xref ref-type="bibr" rid="CR171">171</xref>] database includes autism brain imaging datasets for studying the autism spectrum disorder. The other dataset pertains to the Attention Deficit Hyperactivity Disorder (ADHD) [<xref ref-type="bibr" rid="CR172">172</xref>] and includes 776 resting-state fMRI and anatomical datasets which are fused over the 8 independent imaging sites. The phenotypic information includes age, sex, diagnostic status, measured ADHD symptom, intelligence quotient, and medication status. Imaging-based diagnostic classification is the main aim of the ADHD 200 dataset. The ADNI (Alzheimer’s Disease Neuroimaging Initiative [<xref ref-type="bibr" rid="CR173">173</xref>]) is a popular database and contains neuroimaging datasets from neurodegenerative diseases, in particular, AD, MCI, early and late AD and elderly control subjects. The datasets offered by this repository are mainly dedicated for development of novel methods for diseases related to AD. Another dataset focusing on AD is the Open Access Series of Imaging Studies (OASIS) [<xref ref-type="bibr" rid="CR181">181</xref>] dataset. This contains MRI datasets and open-source data management platform (XNAT) to study and analyse AD. Neurosynth [<xref ref-type="bibr" rid="CR179">179</xref>] is yet another database which includes fMRI literature (with some datasets) and synthesis platform to study brain structure, functions, and disease. On the other hand, the Open Neuroimaging (Open NI) [<xref ref-type="bibr" rid="CR182">182</xref>] dataset contains imaging modalities and brain diseases data which can be used to study decision support system for disease identification.</p><p id="Par37">The recent novel coronavirus disease or COVID-19 pandemic has attracted a number of researchers to focus their attention on the detection of the novel coronavirus disease. The NIH [<xref ref-type="bibr" rid="CR180">180</xref>]</p><p id="Par38">nCOV chest X-ray database [<xref ref-type="bibr" rid="CR178">178</xref>] contains COVID-19 cases with chest X-ray/CT images. The data can be used for identifying bacterial vs viral vs COVID-19 pneumonia. Similar chest X-ray datasets [<xref ref-type="bibr" rid="CR175">175</xref>] are hosted by Kaggle which include chest X-ray scans data for detecting traditional viral and bacterial pneumonia.</p><p id="Par39">Breast cancer is also another important disease which can be addressed through imaging and this has attracted a number of databased hosting breast cancer images.</p><p id="Par40">The Breast Cancer Digital Repository (BCDR) [<xref ref-type="bibr" rid="CR174">174</xref>] database contains multimodal mammography and ultrasound scan and patient history data collected from 1734 anonymized patients. The data can be used for disease detection and diagnosis methods. Another dataset, MITOS [<xref ref-type="bibr" rid="CR176">176</xref>], contains breast cancer histological images (haematoxylin and eosin stained slides). The detection of mitosis and evaluation of nuclear atypia are key uses.</p></sec><sec id="Sec19"><title>Neuroimage Processing and Analysis</title><p id="Par41">The Information eXtraction from Images (IXI) dataset [<xref ref-type="bibr" rid="CR184">184</xref>] provides 600 MRI images from healthy subjects to study brain functions. These images saved in NIFTI file format and were acquired using protocol—T1, T2, proton-density weighted images; magnetic resonance angiography images; and diffusion weighted images. These images have been collected from three different hospitals in London, UK. Another database, called the Loni Probabilistic Brain Atlas (LPBA40) [<xref ref-type="bibr" rid="CR185">185</xref>], contains maps of brain anatomic regions of 40 human volunteers. Each map generates a set of whole-head MRI, whereas each MRI describes to identify 56 structures of brain, most of them lies in the cortex. The study of skull-stripped MRI volumes, and classification of the native-space MRI, probabilistic maps are key uses of LPBA40. The NeuroVault.org [<xref ref-type="bibr" rid="CR186">186</xref>] is a web-based repository (API) for collecting and sharing statistical maps of the human brain to study human brain regions. The Neuroimaging Informatics Tools and Resources Clearing house (NITRC) [<xref ref-type="bibr" rid="CR187">187</xref>] provides range of imaging data from MRI to PET, SPECT, CT, MEG/EEG, and optical imaging for analysing functional and structural neuroimages. The Open fMRI [<xref ref-type="bibr" rid="CR188">188</xref>] dataset contains MRI images acquired using different modalities including diffusion-weighted, T1-weighted magnetization prepared rapid acquisition with gradient echo (MPRAGE) MRI, and multiecho fast low-angle shot (FLASH) MRI. It also contains biosignal datasets to study brain regions and its functions. These can be used as a benchmark dataset in order to differentiate outcome from various neuroimaging analysis tools. The UK data service [<xref ref-type="bibr" rid="CR189">189</xref>] contains T1/2, diffusion tensor imaging, and fMRI datasets from 22 patients suffering from brain tumours which can be useful for studying brain tumour surgical planning.</p></sec><sec id="Sec20"><title>Segmentation</title><p id="Par42">Segmentation is an important step in any image processing pipeline. Many datasets mentioned above can be used for segmentation purposes.</p><p id="Par43">Focusing on eye diseases, the Digital Retinal Images for Vessel Extraction (DRIVE) contains JPEG Compressed retinal images of 400 diabetic patients between 25-90 years old. This dataset can be used to understand segmentation of blood vessels in retinal images and identify diabetic retinopathy. Another dataset called STructured Analysis of the Retina (STARE) was initiated in 1975. The project contains datasets of 400 raw retinal images, 10 labelled images of artery/vein, and 80 images with ground truth. Each image is annotated and features are shown in image by the expert. The dataset can be used for blood vessel segmentation and optic nerve detection.</p><p id="Par44">The Internet Brain Segmentation Repository (IBSR) gives segmentation results of MRI data. Development of segmentation methods is the main application of this IBSR.<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>Application-wise categorization of open access data repositories and datasets pertaining to biological signals</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Application</p></th><th align="left"><p>Name</p></th><th align="left"><p>Description</p></th><th align="left"><p>Ref.</p></th></tr></thead><tbody><tr><td align="left"><p>Anomaly detection</p></td><td align="left"><p>SAD mc-EEG</p></td><td align="left"><p>Multichannel EEG data for sustained-attention driving task</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR193">193</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>TUH EEG Corpus</p></td><td align="left"><p>Repository for EEG datasets, tools and documents</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR194">194</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>MIT-BIH-ARH</p></td><td align="left"><p>ECG database containing 48 recordings</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR195">195</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>PTB D-ECG</p></td><td align="left"><p>ECG database containing 549 recordings</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR196">196</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>TELE ECG</p></td><td align="left"><p>250 ECG recordings with annotated QRS and artifact masks</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR197">197</xref>]</p></td></tr><tr><td align="left"><p>Human–Machine Interfacing</p></td><td align="left"><p>BNCI</p></td><td align="left"><p>Various BMI signal datasets</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR198">198</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>EMG DataRep</p></td><td align="left"><p>Various EMG datasets</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR199">199</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Facial sEMG</p></td><td align="left"><p>Contains EMG data from 15 participants</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR200">200</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NinaPro database</p></td><td align="left"><p>Kinematic as well as the sEMG data of 27 subjects</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR201">201</xref>]</p></td></tr><tr><td align="left"><p>Emotion/affective state detection</p></td><td align="left"><p>DEAP</p></td><td align="left"><p>Simultaneously recorded EMG/EEG data</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR202">202</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>DECAF</p></td><td align="left"><p>MEG, hEOG, ECG, trapezius muscle EMG, face video data</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR203">203</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Imagine</p></td><td align="left"><p>EEG datasets of 31 subjects while listening voice</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR204">204</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>MAHNOB-HCI</p></td><td align="left"><p>EMG, ECG, and respiration and skin temperature data</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR205">205</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SEED</p></td><td align="left"><p>EEG dataset for emotion and vigilance</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR206">206</xref>]</p></td></tr><tr><td align="left"><p>Motor imagery classification</p></td><td align="left"><p>EEG-BCI-MI</p></td><td align="left"><p>EEG signals from 13 subjects with 60,000 MI examples</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR207">207</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>EEG-MI-BCI</p></td><td align="left"><p>EEG data from BCI for MI tasks</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR208">208</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>EEG-MMI</p></td><td align="left"><p>EEG data from PhysioNet for MI task</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR209">209</xref>]</p></td></tr><tr><td align="left"><p>Neurological condition evaluation</p></td><td align="left"><p>V-P300 BCI</p></td><td align="left"><p>16-electrode dry EEG from 71 subjects (SP mode)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR210">210</xref>]</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>32-electrode wet EEG from 50 subjects (SP mode)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR211">211</xref>]</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>32-electrode wet EEG from 38 subjects (MPC mode)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR212">212</xref>]</p></td></tr><tr><td align="left"/><td align="left"/><td align="left"><p>32-electrode wet EEG from 44 subjects (MPCC mode)</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR213">213</xref>]</p></td></tr><tr><td align="left"><p>Signal processing and classification</p></td><td align="left"><p>BCI competition</p></td><td align="left"><p>EEG, ECoG, and MEG data from a range of BCI applications</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR214">214</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>BCI-NER challenge</p></td><td align="left"><p>56 channel EEG dataset decoded by a P300 speller</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR215">215</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>DRYAD</p></td><td align="left"><p>EEG datasets of 13 subjects recorded under various conditions</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR216">216</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>PhysioNet</p></td><td align="left"><p>Various EEG, ECG, EMG and sEMG datasets</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR217">217</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>UCI ML</p></td><td align="left"><p>Various ECG, EMG, sEMG datasets</p></td><td align="left"><p>[<xref ref-type="bibr" rid="CR218">218</xref>]</p></td></tr></tbody></table><table-wrap-foot><p>Legend:<italic> MI</italic> Motor Imagery, <italic>MMI</italic> Motor Movement/Imagery, <italic>ERP</italic> Event-Related Potentials, <italic>SADmc-EEG</italic> Sustained-Attention Driving multichannel EEG, <italic>V-P300</italic> Visual P300, <italic>SP</italic> Single Player, <italic>MP</italic> Multiplayer, <italic>BCI-SSVEP</italic> Steady State Visual Evoked Potentials, <italic>EMG DataRep</italic> EMG Dataset Repository, <italic>ARH</italic> Arrhythmia, <italic>D-ECG</italic> Diagnostic ECG</p></table-wrap-foot></table-wrap></p></sec></sec><sec id="Sec21"><title>Signals</title><p id="Par45">Table <xref rid="Tab5" ref-type="table">5</xref> lists leading open access data repositories and datasets (also referred as data sources) pertaining to biological signals. These sources are broadly mapped to six application areas—anomaly detection, human–machine interfacing which includes brain–machine interfacing as well as rehabilitation research, emotion/affective state detection, motor imagery classification, neurological condition evaluation, and signal processing and classification—which are described in the following subsections.</p><sec id="Sec22"><title>Anomaly Detection</title><p id="Par46">Anomaly detection is one of the major application areas in which scientists have devoted much efforts. In this process, a number of open access data sources, largely containing EEG and ECG data, have been frequently used.</p><p id="Par47">Starting with the EEG signals, the SAD mc-EEG [<xref ref-type="bibr" rid="CR193">193</xref>] dataset contains 32 channel EEG signals from 27 subjects recorded while they were test-driving. That is, signals were acquired when each subject attended two 90-minute virtual reality session for sustained-attention driving.</p><p id="Par48">The TUH EEG corpus [<xref ref-type="bibr" rid="CR194">194</xref>] is also an open-source clinical EEG data repository for clinical EEG data, tool and documentation. The major datasets include seizure detection, abnormal EEG, EEG with artifacts (introduced by eye movement, chewing, shivering, electrode pop, electrode static, and lead artifacts, and muscle artifacts), EEG for epilepsy, etc.</p><p id="Par49">Regarding the ECG signals, the MIT-BIH arrhythmia [<xref ref-type="bibr" rid="CR195">195</xref>] arrhythmia database includes 2-channel ambulatory ECG recording taken from 47 subjects for studying arrhythmia. There are 48 complete ECG recordings and about 24 recordings are freely available. The PTB diagnostic ECG database [<xref ref-type="bibr" rid="CR196">196</xref>] comprises 549 ECG recordings taken from 290 subjects of age ranged from 17 to 87 years using conventional 12 leads and 3 Frank lead ECG recorder. Each recording includes 15 signals coming from these leads and each subject was represented in 1 to 5 records. Both the datasets can be used for anomaly detection. Another ECG dataset, the TELE-ECG dataset [<xref ref-type="bibr" rid="CR197">197</xref>] includes 250 ECG records with annotated QRS and artifact masks. It also includes QRS and artifact detection algorithms to study QRS and detect artifacts from ECG signals.</p></sec><sec id="Sec23"><title>Human–Machine Interfacing</title><p id="Par50">The application area of Human–Machine Interfacing focuses on [body and brain]–machine interfacing and rehabilitation. This is done largely through Electromyography (EMG) and sometimes with EEG signals.</p><p id="Par51">The BNCI Horizon 2020 database contains more than 25 datasets such as stimulated EEG datasets, Electrocorticography (ECoG)-based BCI datasets, Event Related Potential (ERP)-based BCI datasets, mental arithmetic, motor imagery (extracted from EEG, EOG, fNIRS, EMG) datasets, EEG/EOG datasets of neuroprosthetic control, speller datasets. Modelling and designing of BMI devices are the key application of this database. While the BNCI contains a variety of signals, the EMG Datasets Repository [<xref ref-type="bibr" rid="CR199">199</xref>] includes single/multifinger movements datasets of 2 channels, 10 classes and 8 channels, 15 classes; single-/multifinger pressure on a steering wheel; EMG controlled multifunctional upper-limb prostheses and EMG pattern recognition datasets.</p><p id="Par52">For surface EMG (sEMG), the facial sEMG dataset contains facial sEMG signals from the muscles corrugator supercilii, zygomaticus major, orbicularis oris, orbicularis oculi, and masseter. Archived data are from 15 participants (8 females and 7 males) aged between 26 and 57 years (mean age <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mrow><mml:mn>40.7</mml:mn><mml:mo>±</mml:mo><mml:mn>9.6</mml:mn></mml:mrow></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$40.7 \pm 9.6$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq6.gif"/></alternatives></inline-formula> years). These data can be used for rehabilitation research. Also, the NinaPro database includes kinematic as well as sEMG data of 27 subjects, while these subjects were moving finger, hand, and wrist. These data can be employed to study biorobotics and activity detection.</p></sec><sec id="Sec24"><title>Emotion/Affective State Detection</title><p id="Par53">Emotion and affective state detection has been a very active research field over the years. A combination of different signals has been utilized in detecting emotion and affective states, and a number of data sources providing these signals are described below.</p><p id="Par54">A Database for Emotion Analysis using Physiological Signals (DEAP) provides various datasets for analysing the human affective states. It provides EEG and sEMG signals of 32 volunteers, while they were watching music videos to analyse the affective states. These volunteers also rated the video, and the front face was also recorded for 22 volunteers. DECAF is a multimodal dataset for decoding user physiological responses to affective multimedia content. It contains magnetoencephalogram (MEG), horizontal electrooculogram (hEOG), ECG, trapezius muscle EMG, and near-infrared face video data to study physiological and mental states. Another multimodal dataset is the MAHNOB-HCI [<xref ref-type="bibr" rid="CR205">205</xref>] dataset which includes ECG, respiration, and skin temperature data in addition to 32-channel EEG signals from 30 subjects, while they were watching movie clips and photos. The different sensors were synchronized to record a synchronized multimodal dataset. The subjects were asked to label their own emotion state.</p><p id="Par55">On the other hand, the Imagined Emotion [<xref ref-type="bibr" rid="CR204">204</xref>] dataset provides EEG signals recorded when subjects were listening to voice recording. The SJTU Emotion EEG Dataset [<xref ref-type="bibr" rid="CR206">206</xref>] contains three individual datasets (SEED, SEED-IV and SEED-VIG) of EEG signals. In the SEED dataset EEG signals were recorded, while the subjects were watching movie clips and annotated their emotional state as positive, negative and neural. In case of SEED-IV, four emotional states such as happy, sad, fear, and neutral were annotated, whereas the SEED-VIG dataset contains EEG signals related to vigilance when the subjects were driving.</p></sec><sec id="Sec25"><title>Motor Imagery Classification</title><p id="Par56">Motor imagery (MI) is yet another very active area of research. As an outcome of a large number of community contributors, many datasets have been developed from which the popular ones are described below.</p><p id="Par57">The electroencephalographic brain–computer interface mental imagery (EEG-BCI-MI) [<xref ref-type="bibr" rid="CR207">207</xref>] dataset contains 60 hours of EEG recording from 13 subjects and 75 experiments. This contains around 60,000 mental imagery examples which is approximately 4.8 hours of EEG recordings (with 4600 MI examples) per participant. The datasets can be used for the rehabilitation of patients having movement disorders. Another EEG dataset for MI brain–computer interface (EEG-MI-BCI) [<xref ref-type="bibr" rid="CR208">208</xref>] contains EEG signals with 3-D electrode location and EEG for non-task-related states as well. The dataset was recorded from 52 participants which also contains [physio/psyco]logical data and EMG signals in addition to the EEG. The dataset can be employed to find the human factors which influence MI BCI performances. Yet another EEG signal centric dataset is called EEG motor movement/imagery (EEG-MMI) dataset [<xref ref-type="bibr" rid="CR209">209</xref>] and incorporates 1500 (1–2 minutes) EEG recordings taken from 109 volunteers. The dataset can be used in designing BCI systems for rehabilitation purposes.</p></sec><sec id="Sec26"><title>Neurological Condition Evaluation</title><p id="Par58">A number of visual P300-based datasets are available with open-access attributes to perform a range of neurological condition evaluation. These datasets, V-P300 BCI, are composed of data recorded using dry or wet electrode with 16 or 32 channels while the subjects were playing the Brain Invaders game [<xref ref-type="bibr" rid="CR219">219</xref>]. These datasets were recorded using different playing modalities such as single player (16 dry electrodes [<xref ref-type="bibr" rid="CR210">210</xref>] from 71 subjects and 32 wet electrodes [<xref ref-type="bibr" rid="CR211">211</xref>] from 50 subjects), multiplayer in collaborative mode (32 wet electrodes from 38 subjects [<xref ref-type="bibr" rid="CR212">212</xref>]), and multiplayer cooperation and competition mode (32 wet electrodes from 44 subjects [<xref ref-type="bibr" rid="CR213">213</xref>]).</p></sec><sec id="Sec27"><title>Signal Processing and Classification</title><p id="Par59">To solve various signal processing and classification problems, a number of datasets have been made available under open-access. Most of these problems are released to the community in the form of challenges with relevant datasets to solve them. The competitions during the BCI meetings have served this purpose for several years and have released datasets (the BCI competition datasets [<xref ref-type="bibr" rid="CR214">214</xref>]) which are still available with relevant problem statements and sample codes for others to use. The challenge dataset provided by the IEEE Neural Engineering Conference (NER2015) is known as BCI-NER dataset [<xref ref-type="bibr" rid="CR215">215</xref>]. This dataset was mainly intended for methodological development of an error detection algorithm suitable for the P300-based BCI systems. The BCI competition datasets include EEG datasets (e.g., cortical negativity or positivity, feedback test trials, self-paced key typing, P300 speller paradigm, motor/mental imagery data, continuous EEG, EEG with eye movement), ECoG datasets (e.g., finger movement, motor/mental imagery signals in the form of EEG/ECoG), and MEG dataset (e.g., wrist movement). These datasets can be used for signal processing and classification methods for BMI. Similarly, the BCI-NER Challenge [<xref ref-type="bibr" rid="CR215">215</xref>] dataset provides 56-channel EEG signals from 26 subjects using a P300 speller.</p><p id="Par60">In addition to the datasets released for challenges and competitions, there are repositories which provide rich datasets for this application area. The DRYAD [<xref ref-type="bibr" rid="CR216">216</xref>] is a versatile repository which has been recently unveiled. It contains a range of EEG recorded datasets when 19 subjects listen to natural speech time-reversed speech, cocktail party attention, and noisy audiovisual speech. The PhysioNet repository [<xref ref-type="bibr" rid="CR217">217</xref>] contains a large number of neuroelectric and myoelectric datasets. As the name suggests, it is mainly for physiological data. These datasets mainly pertain to signals such as EEG, ECoG, EMG, and ECG and are acquired from many diverse experimental settings. The UCI ML repository [<xref ref-type="bibr" rid="CR218">218</xref>] contains a large number of diverse datasets with direct application to machine learning methods. Some relevant biosignal datasets include ECG, EEG, and (s)EMG signals from diverse experimental and physiological conditions.</p><p id="Par61"><table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>Application-wise categorization of open access data repositories and datasets pertaining to Omics data</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Application</p></th><th align="left"><p>Name</p></th><th align="left"><p>Description</p></th><th align="left"><p>Ref.</p></th></tr></thead><tbody><tr><td align="left"><p>Bioassay analysis and drug design</p></td><td align="left"><p>COVID-19</p></td><td align="left"><p>Gene sequence, pathway, and bioassay datasets of COVID-19</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR220">220</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>PubChem</p></td><td align="left"><p>Contains compound structures, molecular datasets, and tool</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR221">221</xref>]</p></td></tr><tr><td align="left"><p>Genetic disorder analysis</p></td><td align="left"><p>Cancer GeEx</p></td><td align="left"><p>Different cancer genome datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR222">222</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>IGDD</p></td><td align="left"><p>Mutation data on common genetic diseases</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR223">223</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>TCGA</p></td><td align="left"><p>Contains cancer genome data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR224">224</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>BDTNP</p></td><td align="left"><p>3D Gene expression, DNA-binding data and ChAcD</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR225">225</xref>]</p></td></tr><tr><td align="left"><p>Nucleic acid research</p></td><td align="left"><p>ENCODE</p></td><td align="left"><p>Human genome dataset</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR226">226</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>ESP</p></td><td align="left"><p>Contains sequencing data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR227">227</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>GEO</p></td><td align="left"><p>Contains high-throughput gene expression and functional genomics datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR228">228</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>gnomAD</p></td><td align="left"><p>Large-scale exomes and genomes sequencing data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR229">229</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>GTEx</p></td><td align="left"><p>Gene expression datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR230">230</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Harmonizome</p></td><td align="left"><p>Collection of genes and proteins datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR231">231</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>INSDC</p></td><td align="left"><p>Contains nucleotide sequence data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR232">232</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>IGSR</p></td><td align="left"><p>Genome data of various ethnicities, age and sex</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR233">233</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>JASPAR</p></td><td align="left"><p>Transcription factor DNA-binding preferences dataset</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR234">234</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NIHREM</p></td><td align="left"><p>Human genome datasets</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR235">235</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NSD</p></td><td align="left"><p>Includes omics and health science data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR236">236</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SysGenSim</p></td><td align="left"><p>Bioinformatics tools and gene sequence dataset</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR237">237</xref>]</p></td></tr><tr><td align="left"><p>Protein structure analysis</p></td><td align="left"><p>PDB</p></td><td align="left"><p>Proteins, nucleic acids, and complex assemblies data</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR238">238</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SCOP2</p></td><td align="left"><p>Contains structural classification of proteins</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR239">239</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SCOPe</p></td><td align="left"/><td align="left"><p>
[<xref ref-type="bibr" rid="CR240">240</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>UCI MB</p></td><td align="left"><p>2ps and splice–junction gene sequences</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR241">241</xref>]</p></td></tr><tr><td align="left"><p>Signal transduction pathway study</p></td><td align="left"><p>NCI Nature</p></td><td align="left"><p>Molecular interactions and reactions of cells</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR242">242</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>NetPath</p></td><td align="left"><p>Signal transduction pathways in humans</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR243">243</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>Reactome</p></td><td align="left"><p>Database for reactions, pathways and biological processes</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR244">244</xref>]</p></td></tr><tr><td align="left"><p>Single-cell omics</p></td><td align="left"><p>miRBoost</p></td><td align="left"><p>The genomes of eukaryotes containing at least 100 miRNAs</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR245">245</xref>]</p></td></tr><tr><td align="left"/><td align="left"><p>SGD</p></td><td align="left"><p>Provides biological data for budding yeast and analysis tool</p></td><td align="left"><p>
[<xref ref-type="bibr" rid="CR246">246</xref>]</p></td></tr></tbody></table></table-wrap></p></sec></sec><sec id="Sec28"><title>Sequences</title><p id="Par62">Table <xref rid="Tab6" ref-type="table">6</xref> lists the leading popular open access data sources pertaining to the various omics-related researches which include genomics, proteomics, and metabolomics. Grouped to six broad application areas, namely, bioassay analysis and drug design, genetic disorder analysis, nucleic acid research, protein structure analysis, signal transduction pathway study, and single-cell omics, the following subsections provide brief discussions about the leading open access omics data sources.</p><sec id="Sec29"><title>Bioassay Analysis and Drug Design</title><p id="Par63">Since December 2019, the world has experienced a pandemic caused by the SARS-CoV-2 (COVID-19) virus. Triggered by the necessity to facilitate the ongoing researches, the SARS-CoV-2 [<xref ref-type="bibr" rid="CR220">220</xref>] dataset provides gene sequence, proteins, pathway, and bioassay for SARS-CoV-2 along with compounds used in clinical trials. This dataset can be used for studying biological/chemical process and drug design.</p><p id="Par64">The PubChem database [<xref ref-type="bibr" rid="CR221">221</xref>] contains millions of compound structures and descriptive datasets of chemical molecules and their activities against biological assays. Maintained by the National Center for Biotechnology Information of the United States National Institutes of Health, it can be freely accessed through a web user interface and downloaded via FTP. It also contains software services (such as plotting and clustering). It can be used for [gen/prote]-omics study and drug design.</p></sec><sec id="Sec30"><title>Genetic Disorder Analysis</title><p id="Par65">The cancer gene expression (GE) [<xref ref-type="bibr" rid="CR222">222</xref>] serves as a small repository containing several cancer GE datasets which can be employed for designing tool/algorithm for cancer detection. The cancer genome atlas (TCGA) [<xref ref-type="bibr" rid="CR224">224</xref>] repository contains more than 2.5 petabytes of genomic, epigenomic, transcriptomic, and proteomic data. It contains data about 33 different cancer types and over 20,000 samples. These data are generated by the National Cancer Institute and the National Human Genome Research Institute. This repository is used in facilitating genomic study for improving the prevention, diagnosis, and treatment of cancer. To analyse region-specific diseases, the Indian Genetic Disease Database (IGDD) [<xref ref-type="bibr" rid="CR223">223</xref>] tracks mutations in the normal genes for genetic diseases reported in India.</p></sec><sec id="Sec31"><title>Nucleic Acid Research</title><p id="Par66">The Berkeley Drosophila Transcription Network Project (BDTNP) [<xref ref-type="bibr" rid="CR225">225</xref>] database contains datasets pertaining to 3D Gene expression data, in vivo and in vitro DNA-binding data as well as Chromatin Accessibility data (ChAcD). Research on GE and anomaly detection is the key application of the datasets provided by this database.</p><p id="Par67">The Encyclopedia of DNA Elements (ENCODE) [<xref ref-type="bibr" rid="CR226">226</xref>] is a whole-genome database curated by the ENCODE Consortium. It contains a large number of datasets pertaining to functional genomics and characterization data including meta-data of human, worm, mouse, and fly. Another database, called the Exome Sequencing Project (ESP) [<xref ref-type="bibr" rid="CR227">227</xref>], includes genome datasets which can be used to find lung and blood disorders and their management and treatment. The Gene Expression Omnibus (GEO) [<xref ref-type="bibr" rid="CR228">228</xref>] is an open-access functional genomics (microarray and sequence) data repository. This database can be used for functional genomic and epigenomic studies such as genome methylation, chromatin structure, and genome–protein interactions. It is supported by the National Center for Biotechnology Information at the National Library of Medicine of the USA [<xref ref-type="bibr" rid="CR228">228</xref>]. The Genome Aggregation Database (gnomAD) [<xref ref-type="bibr" rid="CR229">229</xref>] database contains large-scale exome and genome sequencing data from different sequencing projects. The dataset can be used for disease diagnosis and genetic studies. The Genotype-Tissue Expression (GTEx) [<xref ref-type="bibr" rid="CR230">230</xref>] database contains GE datasets of 54 healthy tissue sites collected from 1000 subjects and histology images. It also includes samples from GTEx biobank.</p><p id="Par68">The Harmonizome [<xref ref-type="bibr" rid="CR231">231</xref>] database provides details about genes and proteins from 114 datasets provided by 66 online resources with 71927784 associations between 295496 attributes and 56720 genes. The International Nucleotide Sequence Database [<xref ref-type="bibr" rid="CR232">232</xref>], popularly known as INSDC, corroborates biological data from three major sources: i) DNA Databank of Japan [<xref ref-type="bibr" rid="CR247">247</xref>], ii) European Nucleotide Archive [<xref ref-type="bibr" rid="CR248">248</xref>], and iii) GenBank [<xref ref-type="bibr" rid="CR249">249</xref>]. These sources provide the spectrum of data raw reads, though alignments, and assemblies to functional annotation, enriched with contextual information relating to samples and experimental configurations. Similar to this, the International Genome Sample Resource (IGSR) [<xref ref-type="bibr" rid="CR233">233</xref>] includes genome sequencing data from 1000 genomes project. The genome data was taken from people of various ethnicities, age, and sex with the final dataset contains gene sequencing data from 2,504 individuals from 26 populations. These data can be used for disease diagnosis and genetic studies. Also, the SysGenSim [<xref ref-type="bibr" rid="CR237">237</xref>] database includes bioinformatics tool, and Pula-Magdeburg single-gene knockout, StatSeq, and DREAM 5 benchmark datasets for studying Gene Sequence.</p><p id="Par69">JASPAR [<xref ref-type="bibr" rid="CR234">234</xref>] is a database for transcription factor DNA-binding profile. The data spans through six different taxonomic groups covering Vertebrata, Nematoda, Insecta, Plantae, Fungi, and Urochordata. The database can be used for translational genomics research.</p><p id="Par70">The NIH Roadmap Epigenomics Mapping repository (NIHREM) [<xref ref-type="bibr" rid="CR235">235</xref>] includes 2,804 datasets, i.e., 1,821 histone modification, 360 DNase, 277 DNA methylation, and 166 RNA-Seq datasets. The repository provides 3,174-fold 150.21 billion mapped sequencing the human and tools for analysing these datasets. It can be used for stem cell mapping and selection of tissues that are responsible for human disease. Also, the database known as Nature scientific data (NSD) [<xref ref-type="bibr" rid="CR236">236</xref>] includes datasets pertaining to omics, taxonomy and species diversity, mathematical and modelling resources, cytometry, organism-focused resources, and health science data. This can be used for studying and modelling different aspects of genomics.</p></sec><sec id="Sec32"><title>Protein Structure Analysis</title><p id="Par71">The Protein Data Bank (PDB) [<xref ref-type="bibr" rid="CR238">238</xref>] contains 3D structural data proteins and nucleic acids. These data are obtained tools such as X-ray crystallography, NMR spectroscopy, and cryo-electron microscopy. It includes more than 135 thousand data of proteins, nucleic acids, and complex assemblies. These can be used to understand all aspects of biomedicine and agriculture.</p><p id="Par72">Structural classification of proteins (SCOP) is a repository which hosts manually classified protein structure datasets. The classification was done based on amino acid sequences and their structural similarity. The main objective is to find the evolutionary relationship between the proteins. Currently two versions of SCOP are maintained. The SCOP Version 2 (SCOP2) [<xref ref-type="bibr" rid="CR239">239</xref>] is the up-to-date SCOP database released at the first quarter of 2020. In contrast, the SCOP-extended (SCOPe) [<xref ref-type="bibr" rid="CR240">240</xref>] is an extended version of the original SCOP maintained by UC Berkeley. SCOPe includes many new classified protein structures via a fusion of manual and automation curation.</p><p id="Par73">Molecular Biology Databases at the UCI (UCI MB) contain three individual databases: i) Secondary Protein Structure [<xref ref-type="bibr" rid="CR241">241</xref>], which is a bench repository that classifies secondary structure of certain globular proteins; ii) Splice–Junction Gene Sequences [<xref ref-type="bibr" rid="CR250">250</xref>], which contain primate splice–junction gene sequences (DNA) with associated imperfect domain theory; and iii) Promoter Gene Sequences [<xref ref-type="bibr" rid="CR251">251</xref>], which contain E. coli promoter gene sequences (DNA) with partial domain theory. Objectives include i) sequencing and predicting the secondary structure of certain proteins; ii) studying primate splice–junction gene sequences (DNA) with associated imperfect domain theory; iii) studying E. Coli promoter gene sequences (DNA) with partial domain theory.</p></sec><sec id="Sec33"><title>Signal Transduction Pathway Study</title><p id="Par74">The NCI–Nature Pathway Interaction Database [<xref ref-type="bibr" rid="CR242">242</xref>] hosts cellular signalling (molecular interactions/reactions) pathways in humans. The database can be employed for cancer research. The database was created by the U.S. National Cancer Institute, NIH, with the collaboration of Nature Publishing Group and published in the last quarter of 2006. Another database, NetPath [<xref ref-type="bibr" rid="CR243">243</xref>], also contains signal transduction pathways in humans. Created jointly by Johns Hopkins University and the Institute of Bioinformatics (IOB) in India; it includes 45 signalling pathway ranging from protein–protein interactions to enzyme–protein substrate reactions including 10 major pathway of immune system and 10 pathway relevant to cancer regulation. The other one, Reactome [<xref ref-type="bibr" rid="CR244">244</xref>], is an open access database hosting biological pathways of metabolic processes to hormonal signalling in humans. Created through a collaboration between North America and Europe, it can be used for cancer research and treatment.</p></sec><sec id="Sec34"><title>Single-cell Omics</title><p id="Par75">The miRBoost dataset [<xref ref-type="bibr" rid="CR245">245</xref>] contains the genomes of eukaryotes containing at least 100 miRNAs. This dataset is used for studying post-transcriptional gene regulation (PTGeR) and miRNA-related pathology. Saccharomyces Genome Database (SGD) [<xref ref-type="bibr" rid="CR246">246</xref>] also provides complete biological information for the budding yeast <italic>Saccharomyces cerevisiae</italic>. They also give an open-source tool for searching and analysing these data and thereby enable the discovery of functional relationships between sequence and gene products in fungi and higher organisms. The study of genome expression, transcriptome, and computational biology is the main function of the SGD.</p></sec></sec></sec><sec id="Sec35"><title>Open-Source Deep Learning Tools</title><p id="Par76">Due to surging interest and concurrent multidisciplinary efforts towards DL in the recent years, several open-source libraries, frameworks, and platforms have been made available to the community. However, for a new user of these tools to mine biological data, it is not always straightforward to know their characteristics, advantages, and disadvantages. In this process, one of the main hurdles for a new analyst is to select the appropriate DL architecture/model and relevant library providing suitable implementations of the selected architecture. Towards introducing a beginner to the field of biological data analysis using these open-source tools, this section describes the tools in a tutorial style indicating their characteristics, pros, and cons. The focus of the section has been to review and summarize the most popular open-source tools, which aim to facilitate the technological developments for the community. This comprehensive collection contains tools (also developed by individuals) which are well maintained with a reasonable amount of implemented algorithms (i.e., deep learning architectures). For the sake of brevity, the individual publication references of the tools are omitted and interested readers may consult them at their respective websites from the provided URLs.</p><p id="Par77">Table <xref rid="Tab7" ref-type="table">7</xref> summarizes the main features and differences of the various tools. To measure the impact and acceptability of a tool in the community, we provide GitHub-based measures such as numbers of Stars, Forks, and Contributors. These numbers are indicative of the popularity, maturity, and diffusion of a tool in the community.<table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>Summary of Open-Source Deep Learning Tools (* as of July 2020)</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Tool</p></th><th align="left"><p>Platform</p></th><th align="left"><p>Language(s)</p></th><th align="left"><p>Stars*</p></th><th align="left"><p>Forks*</p></th><th align="left"><p>Contrib.*</p></th><th align="left"><p>Supported DL Architecture</p></th></tr></thead><tbody><tr><td align="left"><p>Caffe<sup>b</sup></p></td><td align="left"><p>L, M, W, A</p></td><td align="left"><p>Py, C++, Ma</p></td><td align="left"><p>30100</p></td><td align="left"><p>18200</p></td><td align="left"><p>266</p></td><td align="left"><p>CNN, RNN, GAN</p></td></tr><tr><td align="left"><p>Chainer<sup>c</sup></p></td><td align="left"><p>L</p></td><td align="left"><p>Py</p></td><td align="left"><p>5300</p></td><td align="left"><p>1400</p></td><td align="left"><p>251</p></td><td align="left"><p>DA, CNN, RNN, GAN</p></td></tr><tr><td align="left"><p>DL4j<sup>a</sup></p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Ja</p></td><td align="left"><p>11500</p></td><td align="left"><p>4800</p></td><td align="left"><p>32</p></td><td align="left"><p>DA, CNN, RNN, RBM, LSTM, GAN</p></td></tr><tr><td align="left"><p>DyNet<sup>a</sup></p></td><td align="left"><p>L</p></td><td align="left"><p>C++</p></td><td align="left"><p>3000</p></td><td align="left"><p>687</p></td><td align="left"><p>117</p></td><td align="left"><p>CNN, RNN, LSTM</p></td></tr><tr><td align="left"><p>H<sub>2</sub>O<sup>a</sup></p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Ja, Py, R</p></td><td align="left"><p>4700</p></td><td align="left"><p>1700</p></td><td align="left"><p>132</p></td><td align="left"><p>CNN, RNN</p></td></tr><tr><td align="left"><p>Keras<sup>c</sup></p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Py</p></td><td align="left"><p>47500</p></td><td align="left"><p>18000</p></td><td align="left"><p>816</p></td><td align="left"><p>CNN, RNN, DBN, GAN</p></td></tr><tr><td align="left"><p>Lasagne<sup>a</sup></p></td><td align="left"><p>L, M</p></td><td align="left"><p>Py</p></td><td align="left"><p>3700</p></td><td align="left"><p>980</p></td><td align="left"><p>68</p></td><td align="left"><p>CNN, RNN, LSTM, GAN</p></td></tr><tr><td align="left"><p>MCT<sup>c</sup>  </p></td><td align="left"><p>W</p></td><td align="left"><p>C++</p></td><td align="left"><p>16720</p></td><td align="left"><p>4400</p></td><td align="left"><p>197</p></td><td align="left"><p>CNN, DBN, RNN, LSTM</p></td></tr><tr><td align="left"><p>MXNet<sup>a</sup></p></td><td align="left"><p>L, M, W, A, I</p></td><td align="left"><p>C++</p></td><td align="left"><p>18500</p></td><td align="left"><p>6600</p></td><td align="left"><p>780</p></td><td align="left"><p>DA, CNN, RNN, LSTM, GAN</p></td></tr><tr><td align="left"><p>Neon<sup>a</sup>  </p></td><td align="left"><p>L, M</p></td><td align="left"><p>Py</p></td><td align="left"><p>3800</p></td><td align="left"><p>846</p></td><td align="left"><p>78</p></td><td align="left"><p>DA, CNN, RNN, LSTM, GAN</p></td></tr><tr><td align="left"><p>PyTorch<sup>b</sup></p></td><td align="left"><p>L, M</p></td><td align="left"><p>Py</p></td><td align="left"><p>37400</p></td><td align="left"><p>9500</p></td><td align="left"><p>1345</p></td><td align="left"><p>CNN, RNN, LSTM, GAN</p></td></tr><tr><td align="left"><p>Singha<sup>a</sup> </p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Py, C++, Ja</p></td><td align="left"><p>2000</p></td><td align="left"><p>499</p></td><td align="left"><p>46</p></td><td align="left"><p>CNN, RNN, RBM, DBM</p></td></tr><tr><td align="left"><p>TensorFlow<sup>a</sup></p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Py, C++</p></td><td align="left"><p>14300</p></td><td align="left"><p>80600</p></td><td align="left"><p>2450</p></td><td align="left"><p>CNN, RNN, RBM, LSTM, GAN</p></td></tr><tr><td align="left"><p>TF.Learn<sup>c</sup></p></td><td align="left"><p>L, M</p></td><td align="left"><p>Py, C++</p></td><td align="left"><p>9400</p></td><td align="left"><p>2400</p></td><td align="left"><p>120</p></td><td align="left"><p>CNN, BRNN, RNN, LSTM, GAN</p></td></tr><tr><td align="left"><p>Theano<sup>b</sup></p></td><td align="left"><p>L, M, W</p></td><td align="left"><p>Py</p></td><td align="left"><p>9103</p></td><td align="left"><p>2500</p></td><td align="left"><p>332</p></td><td align="left"><p>CNN, RNN, RBM, LSTM, GAN</p></td></tr><tr><td align="left"><p>Torch<sup>b</sup> </p></td><td align="left"><p>L, M, W, A, I</p></td><td align="left"><p>Lu, C, C++</p></td><td align="left"><p>8495</p></td><td align="left"><p>2400</p></td><td align="left"><p>130</p></td><td align="left"><p>CNN, RNN, RBM, LSTM, GAN</p></td></tr><tr><td align="left"><p>Veles<sup>a</sup></p></td><td align="left"><p>L, M, W, A</p></td><td align="left"><p>Py</p></td><td align="left"><p>891</p></td><td align="left"><p>185</p></td><td align="left"><p>10</p></td><td align="left"><p>DA, CNN, RNN, LSTM, RBM</p></td></tr></tbody></table><table-wrap-foot><p><italic>L</italic> Linux/Unix, <italic>M</italic> MacOSX, <italic>W</italic> Windows, <italic>A</italic> Android, <italic>I</italic> iOS, <italic>CP</italic> Cross-platform, <italic>Py</italic> Python,<italic> Ja</italic> Java, <italic>Lu</italic> Lua, <italic>Ma</italic> Matlab</p><p>*GitHub parameters (as of 1 April. 2020)</p><p><sup>a</sup>Apache2 License</p><p><sup>b</sup>BSD License</p><p><sup>c</sup>MIT License</p></table-wrap-foot></table-wrap></p><sec id="Sec36"><title>Caffe</title><p id="Par78">Caffe (<ext-link xlink:href="http://caffe.berkeleyvision.org/" ext-link-type="uri">http://caffe.berkeleyvision.org/</ext-link>) is scalable, written in C++ and provides bindings for Python as well as MATLAB. Dedicated for experiment, training, and deploying general purpose DL models, this framework allows switching between development and deployment platforms. Targeting computer vision applications, it is considered as the fastest implementation of the CNN.</p><p id="Par79"><bold>Pros.</bold><list list-type="bullet"><list-item><p id="Par80">Easy to deploy;</p></list-item><list-item><p id="Par81">Pretrained models are available;</p></list-item><list-item><p id="Par82">Faster training speed;</p></list-item><list-item><p id="Par83">Used for feedforward networks.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par84">Requires writing code for generating new layers;</p></list-item><list-item><p id="Par85">Less support for recurrent networks;</p></list-item><list-item><p id="Par86">No support for distributed training.</p></list-item></list></p></sec><sec id="Sec37"><title>Chainer</title><p id="Par87">Chainer (<ext-link xlink:href="http://chainer.org/" ext-link-type="uri">http://chainer.org/</ext-link>) is a DL framework provided as Python library. Besides the availability of popular optimization techniques and NN related computations (e.g., convolution, loss, and activation functions), dynamic creation of graphs makes Chainer powerful. It supports a wide range of DL architectures including CNN, GAN, RNN, and DA.</p><p id="Par88"><bold>Pros.</bold><list list-type="bullet"><list-item><p id="Par89">One of the tools for leading dynamic computation graphs/networks;</p></list-item><list-item><p id="Par90">Notably faster than other Python-oriented frameworks.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par91">Open Computing Language framework/Open Multi-Processing API is not supported.</p></list-item></list></p></sec><sec id="Sec38"><title>DeepLearning4j</title><p id="Par92">DeepLearning4j (DL4J, <ext-link xlink:href="https://deeplearning4j.org/" ext-link-type="uri">https://deeplearning4j.org/</ext-link>), written in Java with core libraries in C/C++, is a distributed framework for quick prototyping that targets mainly non-researchers. Compatible with JVM supported languages (e.g., Scala/Clojure), it works on distributed processing frameworks (e.g., Hadoop and Spark). Through Keras (see section <xref rid="Sec41" ref-type="sec">5.6</xref>) as a Python API, it allows importing existing DL models from other frameworks. It allows creation of NN architectures by combining available shallow NN architectures.</p><p id="Par93"><bold>Pros.</bold></p><p id="Par94"><list list-type="bullet"><list-item><p id="Par95">Supports integration with Big Data frameworks Apache Spark and Hadoop;</p></list-item><list-item><p id="Par96">Supports distributed GPU and CPU platforms and capable to work with tensor.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par97">Open Computing Language framework is not supported;</p></list-item><list-item><p id="Par98">GUI is supported for workflow and visualization.</p></list-item></list></p></sec><sec id="Sec39"><title>DyNet</title><p id="Par99">The DyNet library (<ext-link xlink:href="https://dynet.readthedocs.io/" ext-link-type="uri">https://dynet.readthedocs.io/</ext-link>), written in C++ with Python bindings, is the successor of the ‘C++ neural network library’. In DyNet, computational graphs are dynamically created for each training example; thus, it is computationally efficient and flexible. Targeting NLP applications, its specialty is in CNN, RNN, and LSTM.</p><p id="Par100"><bold>Pros.</bold></p><p id="Par101"><list list-type="bullet"><list-item><p id="Par102">Designed to be efficient for running on CPU or GPU.</p></list-item><list-item><p id="Par103">Dynamic computation graph like PyTorch and Chainer.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par104">In terms of TensorFlow, limited functions are available.</p></list-item></list></p></sec><sec id="Sec40"><title>H<inline-formula id="IEq25"><alternatives><mml:math id="IEq25_Math"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math><tex-math id="IEq25_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_2$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq25.gif"/></alternatives></inline-formula>O</title><p id="Par105">H<inline-formula id="IEq26"><alternatives><mml:math id="IEq26_Math"><mml:msub><mml:mrow/><mml:mn>2</mml:mn></mml:msub></mml:math><tex-math id="IEq26_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$_2$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq26.gif"/></alternatives></inline-formula>O (<ext-link xlink:href="http://www.h2o.ai" ext-link-type="uri">http://www.h2o.ai</ext-link>) is an ML software that includes DL and data analysis. It provides a unified interface to other DL frameworks like TensorFlow, MXNet, and Caffe. It also supports training of DL models (CNN and RNN) designed in R, Python, Java, and Scala.</p><p id="Par106"><bold>Pros.</bold></p><p id="Par107"><list list-type="bullet"><list-item><p id="Par108">Due to its in-memory distributed parallel processing capacities, it can be used for real-time data;</p></list-item><list-item><p id="Par109">GUI is supported (called Flow) for workflow and visualization;</p></list-item><list-item><p id="Par110">GPU support for Deep Water and NVIDIA;</p></list-item><list-item><p id="Par111">Fast training, memory-efficient DataFrame manipulation;</p></list-item><list-item><p id="Par112">Easy-to-use algorithms and well documented;</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par113">Lacks the data manipulation capabilities of R and Pandas DataFrames;</p></list-item><list-item><p id="Par114">Slow in learning and supports limited model running at a time.</p></list-item></list></p></sec><sec id="Sec41"><title>Keras</title><p id="Par115">The Python-based Keras (<ext-link xlink:href="https://keras.io/" ext-link-type="uri">https://keras.io/</ext-link>) library is used on top of Theano or TensorFlow. Its models can be imported to DL4J (see section <xref rid="Sec38" ref-type="sec">5.3</xref>). It was developed as a user friendly tool enabling fast experimentation, and easy and fast prototyping. Keras supports CNN, GAN, RNN, and DBN [<xref ref-type="bibr" rid="CR252">252</xref>].</p><p id="Par116"><bold>Pros.</bold></p><p id="Par117"><list list-type="bullet"><list-item><p id="Par118">Rich documentation;</p></list-item><list-item><p id="Par119">A high-level API for neural networks;</p></list-item><list-item><p id="Par120">Ability to run on top of state-of-the-art deep learning libraries/frameworks such as TensorFlow, CNTK, or Theano.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par121">Cannot utilize multi-GPU directly;</p></list-item><list-item><p id="Par122">Requires Theano as backend for OpenMP support and Theano/TensorFlow/PlaidML as backend for OpenCL.</p></list-item></list></p></sec><sec id="Sec42"><title>Lasagne</title><p id="Par123">Lasagne (<ext-link xlink:href="http://lasagne.readthedocs.io" ext-link-type="uri">http://lasagne.readthedocs.io</ext-link>) DL library is built on top of Theano. It allows multiple input, output, and auxiliary classifiers. It supports user-defined cost functions and provides many optimization functions. Lasagne supports CNN, GAN, RNN, and LSTM.</p><p id="Par124"><bold>Pros.</bold></p><p id="Par125"><list list-type="bullet"><list-item><p id="Par126">Lasagne is a lightweight library to build and train DL algorithms in Theano;</p></list-item><list-item><p id="Par127">Layers, regularizers, and optimizers can be used independently;</p></list-item><list-item><p id="Par128">Clear documentation is available;</p></list-item><list-item><p id="Par129">Supports training the network on a GPU.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par130">Small community than TensorFlow.</p></list-item></list></p></sec><sec id="Sec43"><title>Microsoft Cognitive Toolkit</title><p id="Par131">Replacing CNTK, the Microsoft Cognitive Toolkit (MCT, <ext-link xlink:href="https://cntk.ai/" ext-link-type="uri">https://cntk.ai/</ext-link>) is mainly coded in C++. It provides implementations of various learning rules and supports different DL architectures including DNN, CNN, RNN, and LSTM.</p><p id="Par132"><bold>Pros.</bold></p><p id="Par133"><list list-type="bullet"><list-item><p id="Par134">It is a framework for feedforward DNNs, CNN and RNN;</p></list-item><list-item><p id="Par135">Can train production systems very fast;</p></list-item><list-item><p id="Par136">Can achieve state-of-the-art performance on benchmark tasks;</p></list-item><list-item><p id="Par137">Allow directed graph visualization.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par138">Less community support;</p></list-item><list-item><p id="Par139">Difficult to install;</p></list-item><list-item><p id="Par140">Draw lass interest among the research community.</p></list-item></list></p></sec><sec id="Sec44"><title>MXNet</title><p id="Par141">MXNet (<ext-link xlink:href="https://mxnet.io/" ext-link-type="uri">https://mxnet.io/</ext-link>) framework allows defining, training, and deploying deep NN (DA, CNN, GAN, RNN and LSTM) on a wide range of devices—from cloud infrastructure to mobile or even embedded devices (e.g. Raspberry Pi). Written in C++, it is memory efficient and supports Go, JavaScript, Julia, MATLAB, Perl, Python, R, and Scala.</p><p id="Par142"><bold>Pros.</bold></p><p id="Par143"><list list-type="bullet"><list-item><p id="Par144">A DL framework which has a high-performance imperative API;</p></list-item><list-item><p id="Par145">Rich Language support;</p></list-item><list-item><p id="Par146">MXNet features advanced GPU support;</p></list-item><list-item><p id="Par147">Highly scalable.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par148">Small community than TensorFlow;</p></list-item><list-item><p id="Par149">Poor API documentation;</p></list-item><list-item><p id="Par150">Less popular with the research community.</p></list-item></list></p></sec><sec id="Sec45"><title>Neon</title><p id="Par151">Neon (<ext-link xlink:href="http://www.nervanasys.com/technology/neon/" ext-link-type="uri">www.nervanasys.com/technology/neon/</ext-link>) is a DL framework written in Python. It provides implementations of various learning rules, along with functions for optimization and activation. Its support for DL architecture includes CNN, GAN, RNN, LSTM, and DA.</p><p id="Par152"><bold>Pros.</bold></p><p id="Par153"><list list-type="bullet"><list-item><p id="Par154">Better visualization properties than other frameworks;</p></list-item><list-item><p id="Par155">Apply optimization at data loading level,</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par156">Small community than TensorFlow;</p></list-item><list-item><p id="Par157">Less popular with the research community.</p></list-item></list></p></sec><sec id="Sec46"><title>PyTorch</title><p id="Par158">PyTorch (<ext-link xlink:href="http://pytorch.org/" ext-link-type="uri">http://pytorch.org/</ext-link>) provides Torch modules in Python. More than a wrapper, its deep integration allows exploiting the powerful features of Python. Inspired by Chainer, it allows dynamic network creation for variable workload and supports CNN, GAN, RNN and LSTM.</p><p id="Par159"><bold>Pros.</bold></p><p id="Par160"><list list-type="bullet"><list-item><p id="Par161">Pretrained models are available;</p></list-item><list-item><p id="Par162">OpenCL support via separately maintained package.</p></list-item><list-item><p id="Par163">Easily combine modular pieces;</p></list-item><list-item><p id="Par164">Easy to create a layer and run on GPU.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par165">Requires writing training code;</p></list-item><list-item><p id="Par166">Limited documentation.</p></list-item></list></p></sec><sec id="Sec47"><title>Singa</title><p id="Par167">Singa (<ext-link xlink:href="https://singa.incubator.apache.org/" ext-link-type="uri">https://singa.incubator.apache.org/</ext-link>), it is a distributed DL platform written in C++, Java, and Python.</p><p id="Par168">Its flexible architecture allows synchronous, asynchronous, and hybrid training frameworks to run. It supports a wide range of DL architectures including CNN, RNN, RBM, and DBM.</p><p id="Par169"><bold>Pros. </bold></p><p id="Par170"><list list-type="bullet"><list-item><p id="Par171">Pretrained models are available;</p></list-item><list-item><p id="Par172">Supports model/data or hybrid partitioning, and synchronous/asynchronous/hybrid training;</p></list-item><list-item><p id="Par173">Distributed deep learning system and handle Big data.</p></list-item><list-item><p id="Par174">Widely used for healthcare data analytics.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par175">No Open Multi-Processing support.</p></list-item></list></p></sec><sec id="Sec48"><title>TensorFlow</title><p id="Par176">TensorFlow (<ext-link xlink:href="http://www.tensorflow.org" ext-link-type="uri">www.tensorflow.org</ext-link>), written in C++ and Python, was developed by Google and supports very large-scale deep NN. Amended recently as ‘TensorFlow Fold’, its capability to dynamically create graphs made the architecture flexible, allowing deployment to a wide range of devices (e.g., multi-CPU/GPU desktop, server, mobile devices, etc.) without code rewriting [<xref ref-type="bibr" rid="CR253">253</xref>, <xref ref-type="bibr" rid="CR254">254</xref>]. Also it contains a data visualization tool named TensorBoard and supports many DL architectures including CNN, GAN, RNN, LSTM, and RBMs [<xref ref-type="bibr" rid="CR255">255</xref>].</p><p id="Par177"><bold>Pros.</bold></p><p id="Par178"><list list-type="bullet"><list-item><p id="Par179">Handles large-scale data and operate in heterogeneous environments;</p></list-item><list-item><p id="Par180">Faster compile time than Theano;</p></list-item><list-item><p id="Par181">Computational graph abstraction;</p></list-item><list-item><p id="Par182">Supports parallelism.</p></list-item><list-item><p id="Par183">TensorBoard is used for workflow and visualization.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par184">Large memory footprint;</p></list-item><list-item><p id="Par185">Less number of pretrained models are available;</p></list-item><list-item><p id="Par186">Computational graph can be slow;</p></list-item><list-item><p id="Par187">No support for matrix operations;</p></list-item><list-item><p id="Par188">Difficulties in debugging.</p></list-item></list></p></sec><sec id="Sec49"><title>TF.Learn</title><p id="Par189">TF.Learn (<ext-link xlink:href="http://www.tflearn.org" ext-link-type="uri">www.tflearn.org</ext-link>) is a TensorFlow (see section <xref rid="Sec48" ref-type="sec">5.13</xref>)-based high-level Python API. It supports fast prototyping with modular NN layers and multiple optimizers, inputs, and outputs. Supported DL architectures include CNN, GAN, BRNN, and LSTM.</p><p id="Par190"><bold>Pros.</bold></p><p id="Par191"><list list-type="bullet"><list-item><p id="Par192">Modular and transparent DL library built on the top of TensorFlow;</p></list-item><list-item><p id="Par193">Provides a higher-level API to TensorFlow.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par194">Slower compared to its competitors.</p></list-item></list></p></sec><sec id="Sec50"><title>Theano</title><p id="Par195">Theano (<ext-link xlink:href="http://www.deeplearning.net/software/theano/" ext-link-type="uri">www.deeplearning.net/software/theano/</ext-link>) is a Python library that builds on core packages like NumPy and SymPy. It defines, optimizes, and evaluates mathematical expressions with tensors and served as foundation for many DL libraries.</p><p id="Par196"><bold>Pros.</bold></p><p id="Par197"><list list-type="bullet"><list-item><p id="Par198">High flexibility;</p></list-item><list-item><p id="Par199">High computational stability;</p></list-item><list-item><p id="Par200">Well suited for tensor-based mathematical expressions;</p></list-item><list-item><p id="Par201">Open-source libraries such as Keras, Lasagne and Blocks built on the top of Theano;</p></list-item><list-item><p id="Par202">Able to visualize convolutional filters, images, and graphs;</p></list-item><list-item><p id="Par203">High-level wrappers like Keras and Lasagne increases usability.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par204">Difficult to learn;</p></list-item><list-item><p id="Par205">Difficult to deploy;</p></list-item><list-item><p id="Par206">Deployed on single GPU;</p></list-item><list-item><p id="Par207">Slower compilation time than TensorFlow.</p></list-item></list></p></sec><sec id="Sec51"><title>Torch</title><p id="Par208">Started in 2000, Torch (<ext-link xlink:href="http://torch.ch/" ext-link-type="uri">http://torch.ch/</ext-link>), a ML library and scientific computing framework, has evolved as a powerful DL library. Core functions are implemented in C and the rest via LuaJIT scripting language made Torch superfast. Software giants like Facebook and Google use Torch extensively. Recently, Facebook’s DL modules (fbcunn) focusing on CNN have been open-sourced as a plug-in to Torch.</p><p id="Par209"><bold>Pros.</bold></p><p id="Par210"><list list-type="bullet"><list-item><p id="Par211">User friendly;</p></list-item><list-item><p id="Par212">Convenient for employ with GPUs;</p></list-item><list-item><p id="Par213">Pretrained models are available;</p></list-item><list-item><p id="Par214">Highly modular;</p></list-item><list-item><p id="Par215">Easy to create a layer and run on GPU.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par216">Special data format and requires conversion;</p></list-item><list-item><p id="Par217">Require to write training code;</p></list-item><list-item><p id="Par218">Less documentation available.</p></list-item></list><fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Relative comparison of DL tools. <bold>a</bold> Popularity trend of individual DL tools as per mention in google search generated globally (data courtesy: Google Trend). <bold>b</bold> Mention in articles submitted to arXiv preprint server during the first quarter of 2020. <bold>c</bold> The effect of community’s participation on individual tools is shown by the bubble size, which is product of normalized number of GitHub forks and contributors. <bold>d</bold> As for the interoperability among the DL tools, Keras allows model importing from Caffe, MCT (CNTK), Theano, and TensorFlow and lets DL4j to import. <bold>e</bold> Regarding hardware-based scalability of the DL tools, most of the tools provide CPU and GPU support, whereas FPGA and ASIC can mainly execute pretrained models</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig4_HTML.png" id="MO11"/></fig></p></sec><sec id="Sec52"><title>Veles</title><p id="Par219">Veles (<ext-link xlink:href="https://github.com/Samsung/veles" ext-link-type="uri">https://github.com/Samsung/veles</ext-link>) is a Python-based distributed platform for rapid DL application development. It provides machine learning and data processing services and supports IPython notebooks. Developed by Samsung, one of its advantages is that it supports OpenCL for cross-platform parallel programming, and allows execution across heterogenous platforms (e.g. servers, PC, mobile, and embedded devices). The supported DL architectures include DA, CNN, RNN, LSTM, and RBM.</p><p id="Par220"><bold>Pros.</bold></p><p id="Par221"><list list-type="bullet"><list-item><p id="Par222">Distributed platform support;</p></list-item><list-item><p id="Par223">Supports Jupyter Notebook;</p></list-item><list-item><p id="Par224">Supports OpenCL for cross-platform parallel programming.</p></list-item></list><bold>Cons.</bold><list list-type="bullet"><list-item><p id="Par225">Less community support;</p></list-item><list-item><p id="Par226">Draws lass interest from the research community.</p></list-item></list></p></sec></sec><sec id="Sec53"><title>Relative Comparison of DL Tools</title><p id="Par227">To perform relative comparison among the available open-source DL tools, we selected four metrics which are detailed below: trend in their usage, community participation in their development, interoperability among themselves, and their scalability (Fig. <xref rid="Fig4" ref-type="fig">4</xref>).</p><sec id="Sec54"><title>Trend</title><p id="Par228">To assess the popularity and trend of the various DL tools among the DL consumers, we looked into two different sources to assess the utilization of the tools. Firstly, we extracted globally generated search data from Google Trends<xref ref-type="fn" rid="Fn1">1</xref> for five years (January 2015 to December 2019) related to search terms consisting of <inline-formula id="IEq27"><alternatives><mml:math id="IEq27_Math"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>D</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>L</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">⟩</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math><tex-math id="IEq27_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\langle [tool name] + Deep Learning\rangle .$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq27.gif"/></alternatives></inline-formula> The data showed a progressive increase of search about TensorFlow since its release followed by Keras (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a). Secondly, mining the content of around 2,000 papers submitted to arXiv’s cs.[CV | CL | LG | AI | NE], and stat.ML categories, during the first quarter of 2020 (i.e. January to March), for the presence of the tool names [<xref ref-type="bibr" rid="CR256">256</xref>]. As seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b which shows the percentage of each individual tool’s mention in the papers, the top six tools were identified as: PyTorch, TensorFlow, Keras, Caffe, MXNet, and Theano.</p></sec><sec id="Sec55"><title>Community</title><p id="Par229">The community-based development score for each tool discussed in Section <xref rid="Sec35" ref-type="sec">5</xref> was calculated from repository popularity parameters of GitHub (<ext-link xlink:href="https://github.com/" ext-link-type="uri">https://github.com/</ext-link>) (i.e., star, fork, and contributors). The bubble plot shown in Fig. <xref rid="Fig4" ref-type="fig">4</xref>c depicts community involvement in the development of the tools indicating the year of initial stable release. Each bubble size in the figure, pertaining to a tool, represents the normalized combined effect of fork and contributors of that tool. It is clearly seen that a very large part of the community effort is concentrated on TensorFlow, followed by Keras and Caffe.</p></sec><sec id="Sec56"><title>Interoperability</title><p id="Par230">In today’s cross-platform development environments, an important measure to judge a tool’s flexibility is its interoperability with other tools. In this respect, Keras is the most flexible one whose high-level neural networks are capable of running on top of either Tensor or Theano. Alternatively, DL4j model imports neural network models originally configured and trained using Keras that provides abstraction layers on top of TensorFlow, Theano, Caffe, and CNTK backends (Fig. <xref rid="Fig4" ref-type="fig">4</xref>d).</p></sec><sec id="Sec57"><title>Scalability</title><p id="Par231">Hardware-based scalability is an important feature of the individual tools (Fig. <xref rid="Fig4" ref-type="fig">4</xref>e). Today’s hardware for computing devices are dominated by graphics processing units (GPUs) and central processing units (CPUs). But considering increased computing capacity and energy efficiency, the coming years are expected to witness expanded role for other chipset types including application-specific integrated circuits (ASICs) and field-programmable gate arrays (FPGAs). So far DL has been predominantly used through software. The requirement for hardware acceleration, energy efficiency, and higher performance has driven the development of chipset-based DL systems.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Benchmarking stacked autoencoder or DA <bold>(a, b)</bold> and LSTM <bold>(c-f)</bold> in CPU and GPU platforms. The numbers in <bold>(a, c)</bold> denote the number of CPU threads employed in the benchmarking process, and in <bold>(d-f)</bold> denote the batch size. In case of DA the batch size was 64</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig5_HTML.png" id="MO12"/></fig></p></sec></sec><sec id="Sec58"><title>Performance of Tools and Benchmark</title><p id="Par232">The power of DL methods lies in their capability to recognize patterns for which they are trained. Despite the availability of several accelerating hardware (e.g., multicore [C/G]PUs/FPGAs), this training phase is very time-consuming, cumbersome, and computationally challenging. Moreover, as each tool provides implementations of several DL architectures and often emphasizing separate components of them on different hardware platforms, selecting an appropriate tool suitable for an application is getting increasingly difficult. Besides, different DL tools have different targets, e.g., Caffe targets applications, whereas Torch and Theano are more for DL research. To facilitate scientists in picking the right tool for their application, scientists benchmarked the performances of the popular tools concerning their training times [<xref ref-type="bibr" rid="CR257">257</xref>, <xref ref-type="bibr" rid="CR258">258</xref>]. Moreover, to the best of our knowledge, there exist two main efforts that provide the benchmarking details of the various DL tools and frameworks publicly [<xref ref-type="bibr" rid="CR259">259</xref>, <xref ref-type="bibr" rid="CR260">260</xref>]. Summarizing those seminal works, below we provide the time required to complete the training process as a performance measure of four different DL architectures (e.g., FCN, CNN, RNN, and DA) among the popular tools (e.g., Caffe, CNTK, MXNET, Theano, TensorFlow, and Torch) on multicore [C/G]PU platforms.<table-wrap id="Tab8"><label>Table 8</label><caption xml:lang="en"><p>Hardware configuration of the evaluating setup</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>ESN</p></th><th align="left"><p>Processor</p></th><th align="left"><p>Memory</p></th></tr></thead><tbody><tr><td align="left"><p>1</p></td><td align="left"><p><bold>CPU:</bold> E5-1650<sup>a</sup> @ 3.50 GHz</p></td><td align="left"><p>32 GB</p></td></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Nvidia GeForce GTX Titan X<sup>b</sup>  </p></td><td align="left"/></tr><tr><td align="left"><p>2</p></td><td align="left"><p><bold>CPU:</bold> E5-2630<sup>c</sup> @ 2.20 GHz</p></td><td align="left"><p>128 GB</p></td></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Nvidia GeForce GTX 980<sup>d</sup>  </p></td><td align="left"/></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Nvidia GeForce GTX 1080<sup>e</sup>  </p></td><td align="left"/></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Tesla K80 accelerator with GK210 GPUs<sup>f</sup></p></td><td align="left"/></tr><tr><td align="left"><p>3</p></td><td align="left"><p><bold>CPU:</bold> E5-2690<sup>c</sup> @ 2.60 GHz</p></td><td align="left"><p>256 GB</p></td></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Tesla P100 accelerator<sup>g</sup>  </p></td><td align="left"/></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Tesla M40 accelerator<sup>h</sup>  </p></td><td align="left"/></tr><tr><td align="left"/><td align="left"><p><bold>GPU:</bold> Tesla K80 accelerator with GK210 GPUs<sup>f</sup>  </p></td><td align="left"/></tr></tbody></table><table-wrap-foot><p><italic>ESN</italic> Experimental Setup Numbers</p><p><sup>a</sup>Intel Xeon CPU v2</p><p><sup>b</sup>3072 cores, 1000 MHz base clock, 12 GB memory</p><p><sup>c</sup>Intel Xeon CPU v4</p><p><sup>d</sup>2048 cores, 1126 MHz base clock, 4 GB memory</p><p><sup>e</sup>2560 cores, 1607 MHz base clock, 8 GB memory</p><p><sup>f</sup>Tesla K80 accelerator has two Tesla GK210 GPUs with 2496 cores, 560 MHz base clock, 12 GB memory</p><p><sup>g</sup>3584 cores, 1189 MHz base clock, 16 GB memory</p><p><sup>h</sup>3072 cores, 948 MHz base clock, 12 GB memory</p></table-wrap-foot></table-wrap></p><p id="Par233">Table <xref rid="Tab8" ref-type="table">8</xref> lists the experimental setups used in benchmarking the specified tools. Mainly three different setups, each with Intel Xeon E5 CPU, were utilized during the process. Though the CPU was similar, the GPU hardware was different: GeForce GTX Titan X, GTX 980, GTX 1080, Tesla K80, M40, and P100.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p>The speedup of CNN training in different DL tools across various GPUs in comparison to CPU. The reported values were calculated for a batch size of 128, except for VGG for which the batch size was 64</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/12559_2020_9773_Fig6_HTML.png" id="MO13"/></fig></p><p id="Par234">Stacked autoencoders or DA were benchmarked using the experimental setup number 1 in Table <xref rid="Tab8" ref-type="table">8</xref>. To estimate the performance of the various tools on implementing DA, three autoencoders (number of hidden layers: 400, 200, and 100, respectively) were stacked with tied weights and sigmoid activation functions. A two-step network training was performed on the MNIST dataset [<xref ref-type="bibr" rid="CR261">261</xref>]. As reported in Fig. <xref rid="Fig5" ref-type="fig">5</xref> (a, b), the performances of various DL tools are evaluated using forward runtime and training time. The forward runtime refers to the required time for evaluating the information flow through the full network to produce the intended output for an input batch, dataset, and network. In contrast, the gradient computation time measures the time that required to train DL tools. The results suggest that, regardless of the number of CPU threads used or GPU, Theano and Torch outperform TensorFlow in both gradient and forward times (Fig. <xref rid="Fig5" ref-type="fig">5</xref> a, b).</p><p id="Par235">Experimental setup number 2 (Table <xref rid="Tab8" ref-type="table">8</xref>) was used in benchmarking RNN. The adapted LSTM network [<xref ref-type="bibr" rid="CR262">262</xref>] was designed with 10000 input and output units with two layers and <inline-formula id="IEq46"><alternatives><mml:math id="IEq46_Math"><mml:mo>∼</mml:mo></mml:math><tex-math id="IEq46_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sim$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="12559_2020_9773_Article_IEq46.gif"/></alternatives></inline-formula>13 millions parameters. As the performance of RNN depends on the input length, an input length of 32 was used for the experiment. As the results indicate (Fig. <xref rid="Fig5" ref-type="fig">5</xref> c-f), MCT outperforms other tools on both CPU and all three GPU platforms. On CPUs, TensorFlow performs little better than Torch (Fig. <xref rid="Fig5" ref-type="fig">5</xref> c). On GPUs, Torch is the slowest with TensorFlow and MXNet performing similarly (Fig. <xref rid="Fig5" ref-type="fig">5</xref> d-f).</p><p id="Par236">Still a large portion of the pattern analysis is done using CNN; therefore, we further focused on CNN and investigated how the leading tools performed and scaled in training different CNN networks in different GPU platforms. Time speedup of GPU over CPU is considered as a metric for this purpose. The individual values are calculated using the benchmark scripts of DeepMark [<xref ref-type="bibr" rid="CR259">259</xref>] on experimental setup number 3 (Table <xref rid="Tab8" ref-type="table">8</xref>) for one training iteration per batch. The time needed to execute a training iteration per batch equals the time taken to complete a forward propagation operation followed by a backpropagation operation. Figure <xref rid="Fig6" ref-type="fig">6</xref> summarizes the training time per iteration per batch for both CPU and GPUs (left y-axis) and the corresponding GPU speedup over CPU (right y-axis).</p><p id="Par237">These findings for four different CNN network models (i.e. Alexnet [<xref ref-type="bibr" rid="CR92">92</xref>], GoogLeNet [<xref ref-type="bibr" rid="CR94">94</xref>], Overfeat [<xref ref-type="bibr" rid="CR263">263</xref>], and VGG [<xref ref-type="bibr" rid="CR93">93</xref>]) available in four tools (i.e. Caffe, TensorFlow, Theano, and Torch) [<xref ref-type="bibr" rid="CR264">264</xref>] clearly suggest that network training process is much accelerated in GPUs in comparison to CPUs. Moreover, another important message is that, all GPUs are not the same and all tools don’t scale up at the same rate. The time required to train a neural network strongly depends on which DL framework is being used. As for the hardware platform, the Tesla P100 accelerator provides the best speedup with Tesla M40 being the second and Tesla K80 being the last among the three. In CPUs, TensorFlow achieves the least training time indicating a quicker training of the network. In GPUs, Caffe usually provides the best speedup over CPU but TensorFlow and Torch perform faster training than Caffe. Though TensorFlow and Torch have similar performances (indicated by the height of the lines), Torch slightly outperforming TensorFlow in most of the networks. Finally, most of the tools outperform Theano.</p></sec><sec id="Sec59"><title>Open Issues and Future Perspectives</title><p id="Par238">The brain has the capability to recognize and understand patterns almost instantaneously. Over several decades, scientists have been trying decode the biological mechanism of natural pattern recognition that takes place in the brain and translate those principles into AI systems. The increasing knowledge about the brain’s information processing policies enabled this analogy to be adopted and implemented in computing systems. Recent technological breakthroughs, seamless integration of diverse techniques, better understanding of the learning systems, declination of computing costs, and expansion of computational power empowered computing systems to reach human-level computation in certain scenarios [<xref ref-type="bibr" rid="CR265">265</xref>]. Nonetheless, many of these methods require improvements. Though admittedly, there are distinctions on how a DL-based method can be used and applied on biological data, however, the common open issues and challenges are equally applicable and important for biological data. We identify below shortcomings and bottlenecks of the popular methods, open research questions, and challenges and outline possible directions which requires attention in the near future.</p><p id="Par239">First of all, DL methods usually require large datasets. Though the computing cost is declining with increasing computational power and speed, it is not worthwhile to apply DL methods in cases of small to moderate sized datasets. This is particularly so as considering that many of the DL methods perform continuous geometric transformations of one data manifold to another with an assumption that there exist learnable transfer functions which can perform the mapping [<xref ref-type="bibr" rid="CR266">266</xref>]. However, in cases when the relationships among the data are causal or very complex to be learned by the geometric transformations, the DL methods fail regardless the size of the dataset [<xref ref-type="bibr" rid="CR267">267</xref>]. Also, interpreting high-level outcomes of DL methods is difficult due to inadequate in-depth understanding of the DL theories which causes many of such models to be considered as ‘Black box’ [<xref ref-type="bibr" rid="CR268">268</xref>]. Moreover, like many other ML techniques, DL is also susceptible to misclassification [<xref ref-type="bibr" rid="CR269">269</xref>] and overclassification [<xref ref-type="bibr" rid="CR270">270</xref>].</p><p id="Par240">Additionally, the ability to exploit the full benefits offered by open access data repositories, in terms of data sharing and reuse, is often hampered by the lack of unified reporting data standards and non-uniformity of reported information [<xref ref-type="bibr" rid="CR271">271</xref>]. Data provenance, curation, and annotation of these biological big data are huge challenges too [<xref ref-type="bibr" rid="CR272">272</xref>].</p><p id="Par241">Furthermore, except for very few large enterprises, the power of distributed and parallel computation through cloud computing remains largely unexplored for the DL techniques. Due to the fact that the DL techniques require retraining for different datasets, repeated training becomes a bottleneck for cloud computing environments. Also, in such distributed environments, data privacy and security concerns are still prevailing [<xref ref-type="bibr" rid="CR273">273</xref>], and real-time processing capability of experimental data is underdeveloped [<xref ref-type="bibr" rid="CR274">274</xref>].</p><p id="Par242">To mitigate the shortcomings and address the open issues, the existing theoretical foundations of the DL methods need to be improved. The DL models are required not only to be able to describe specific data but also generalize them on the basis of experimental data which is crucial to quantify the performances of individual NN models [<xref ref-type="bibr" rid="CR275">275</xref>]. These improvements should take place in several directions and address issues like quantitative assessment of individual model’s learning efficiency and associated computational complexity in relation to well-defined parameter tuning strategies, the ability to generalize and topologically self-organize based on data-driven properties. Also, to facilitate intuitive and less cumbersome interpretation of the analysis results, novel tools for data visualization should be incorporated in the DL frameworks.</p><p id="Par243">Recent developments in combined methods pertaining to deep reinforcement learning (deep RL) have been popularly applied to many application domains (for a review on deep RL, see [<xref ref-type="bibr" rid="CR276">276</xref>]). However, deep RL methods have not yet been applied to biological pattern recognition problems. For example, analysing and aggregating dynamically changing patterns in biological data coming from multiple levels could help to remove data redundancy and discover novel biomarkers for disease detection and prevention. Also, novel deep RL methods are needed to reduce the currently required large set of labelled training data.</p><p id="Par244">Renewing efforts are required for standardization, annotation, curation, and provenance of data and their sources along with ensuring uniformity of information among the different repositories. Additionally, to keep up with the rapidly growing big data, powerful and secure computational infrastructures in terms of distributed, cloud, and parallel computing tailored to such well-understood learning mechanisms are badly needed. Lastly, there are many other popular DL tools (e.g., Keras, Chainer, Lasagne) and architectures (e.g., DBN) which need to be benchmarked providing the users with a more comprehensive list to choose. Also, the currently available benchmarks are mostly performed on non-biological data, and their scalability to biological data is poor; thus, specialized benchmarking on biological data are needed.</p><p id="Par245">In order to derive insights from an image, a sequence or a signal analysis problem, a selected DL algorithm using a library or a tool (e.g., TensorFlow, Keras, PyTorch, etc.) may need to integrate with a big data framework (e.g., Hadoop, Spark, etc.). In such cases, troubleshooting in the model and debugging the code may be very challenging for the system designer due to the parallel execution of multiple threads which may not always execute in an orderly fashion. The lack of documentation and model transparency of these libraries may make it impossible for the project manager to estimate efforts required in successful completion of a project.</p></sec><sec id="Sec60" sec-type="conclusions"><title>Conclusion</title><p id="Par246">The diverse biological data coming from different application domains are multimodal, multidimensional, and complex in nature. At present, a huge amount of such data is publicly available. The affordable access to these data came with a huge challenge to analyse and recognize patterns in them which require sophisticated ML tools to do the job. As a result, many ML-based analytical tools have been developed and reported over the last decades and this process has been facilitated greatly by the decrease of computational costs, increase of computing power, and availability of cheap storage. With the help of these learning techniques, machines have been trained to understand and decipher complex patterns and interactions of variables in biological data. To facilitate a wider dissemination of DL techniques applied to biological data and serve as a reference point, this article provides a comprehensive survey of the literature on those techniques’ application on biological data and the relevant open-access data repositories. It also lists existing open-source tools and frameworks implementing various DL methods and compares these tools for their popularity and performance. Finally, it concludes by pointing out some open issues and proposing some future perspectives.</p></sec></body><back><ack><title>Acknowledgements</title><p>The authors would like to thank the members of the acslab (<ext-link xlink:href="http://www.acslab.info/" ext-link-type="uri">http://www.acslab.info/</ext-link>) for valuable discussions.</p></ack><sec sec-type="author-contribution"><title>Author Contributions</title><p>This work was carried out in close collaboration among all authors. M.M. and M.S.K. conceived the idea, developed the method and experiments, analysed the obtained data, and wrote the manuscript. T.M.M and A.H. edited the manuscript. All authors have contributed to, seen, and approved the paper.</p></sec><sec sec-type="ethics-statement"><title>Compliance with Ethical Standards</title><sec id="FPar1" sec-type="COI-statement"><title>Conflict of Interest</title><p id="Par247">The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec id="FPar2"><title>Ethical Approval</title><p id="Par248">This article does not contain any studies with human participants or animals.</p></sec><sec id="FPar3"><title>Informed Consent</title><p id="Par249">As this article does not contain any studies with human participants or animals, the informed consent is not applicable</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coleman</surname><given-names>W</given-names></name></person-group><source>Biology in the nineteenth century : problems of form, function, and transformation</source><year>1977</year><publisher-loc>Cambridge; New York</publisher-loc><publisher-name>Cambridge University Press</publisher-name></mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Magner</surname><given-names>LN</given-names></name></person-group><source>A history of the life sciences</source><year>2002</year><edition>3</edition><publisher-loc>New York</publisher-loc><publisher-name>M. Dekker</publisher-name></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Brenner S. History of science. The revolution in the life sciences. Science. 2012;338(6113):1427–8.</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shendure</surname><given-names>J</given-names></name><name><surname>Ji</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Next-generation DNA sequencing</article-title><source>Nat Biotechnol.</source><year>2008</year><volume>26</volume><issue>10</issue><fpage>1135</fpage><lpage>45</lpage></mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metzker</surname><given-names>ML</given-names></name></person-group><article-title xml:lang="en">Sequencing technologies the next generation</article-title><source>Nat Rev Genet.</source><year>2010</year><volume>11</volume><issue>1</issue><fpage>31</fpage><lpage>46</lpage></mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vadivambal</surname><given-names>R</given-names></name><name><surname>Jayas</surname><given-names>DS</given-names></name></person-group><source>Bio-imaging : principles, techniques, and applications</source><year>2016</year><publisher-loc>Boca Raton, FL</publisher-loc><publisher-name>CRC Press, Taylor &amp; Francis Group</publisher-name></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Farah</surname><given-names>MJ</given-names></name></person-group><article-title xml:lang="en">Progress and challenges in probing the human brain</article-title><source>Nature.</source><year>2015</year><volume>526</volume><issue>7573</issue><fpage>371</fpage><lpage>379</lpage></mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname><given-names>MA</given-names></name><name><surname>Nicolelis</surname><given-names>MAL</given-names></name></person-group><article-title xml:lang="en">Brain-machine interfaces: from basic science to neuroprostheses and neurorehabilitation</article-title><source>Phys Rev.</source><year>2017</year><volume>97</volume><issue>2</issue><fpage>767</fpage><lpage>837</lpage></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quackenbush</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Extracting biology from high-dimensional biological data</article-title><source>J Exp Biol.</source><year>2007</year><volume>210</volume><fpage>1507</fpage><lpage>17</lpage></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mattmann</surname><given-names>CA</given-names></name></person-group><article-title xml:lang="en">Computing: A vision for data science</article-title><source>Nature.</source><year>2013</year><volume>493</volume><issue>7433</issue><fpage>473</fpage><lpage>475</lpage></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name></person-group><article-title xml:lang="en">Big biological data: challenges and opportunities</article-title><source>Genomics Proteomics Bioinformatics.</source><year>2014</year><volume>12</volume><issue>5</issue><fpage>187</fpage><lpage>189</lpage></mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marx</surname><given-names>V</given-names></name></person-group><article-title xml:lang="en">Biology: The big challenges of big data</article-title><source>Nature.</source><year>2013</year><volume>498</volume><issue>7453</issue><fpage>255</fpage><lpage>260</lpage></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Tarca AL, Carey VJ, Chen Xw, Romero R, Draghici S. Machine learning and its applications to biology. PLoS Comput Biol. 2007;3(6):e116.</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title xml:lang="en">Artificial neural networks</article-title><source>IEEE Circuits Devices Mag.</source><year>1988</year><volume>4</volume><issue>5</issue><fpage>3</fpage><lpage>10</lpage></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Hecht-Nielsen R. Theory of the backpropagation neural network. In: Proc. IJCNN 1989; 1989. p. 593–605.</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><article-title xml:lang="en">Neurons with graded response have collective computational properties like those of two-state neurons</article-title><source>PNAS.</source><year>1984</year><volume>81</volume><issue>10</issue><fpage>3088</fpage><lpage>3092</lpage><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1371.92015</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ackley</surname><given-names>DH</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name></person-group><article-title xml:lang="en">A learning algorithm for Boltzmann machines</article-title><source>Cogn Sci.</source><year>1985</year><volume>9</volume><issue>1</issue><fpage>147</fpage><lpage>169</lpage></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Salakhutdinov R, Mnih A, Hinton G. Restricted Boltzmann machines for collaborative filtering. In: Proc. ICML; 2007. p. 791–798.</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname><given-names>W</given-names></name></person-group><article-title xml:lang="en">Networks of spiking neurons: The third generation of neural network models</article-title><source>Neural Netw.</source><year>1997</year><volume>10</volume><issue>9</issue><fpage>1659</fpage><lpage>1671</lpage></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Heckerman D. A Tutorial on learning with Bayesian networks. In: Jordan MI, editor. Learning in Graphical Models. 89. Springer Netherlands; 1998. p. 301–354.</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Vapnik</surname><given-names>V</given-names></name></person-group><article-title xml:lang="en">Support-vector networks</article-title><source>Mach Learn.</source><year>1995</year><volume>20</volume><issue>3</issue><fpage>273</fpage><lpage>297</lpage><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0831.68098</pub-id></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>GX</given-names></name><name><surname>Ho</surname><given-names>CH</given-names></name><name><surname>Lin</surname><given-names>CJ</given-names></name></person-group><article-title xml:lang="en">Recent advances of large-scale linear classification</article-title><source>Proc IEEE.</source><year>2012</year><volume>100</volume><issue>9</issue><fpage>2584</fpage><lpage>2603</lpage></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>RA</given-names></name></person-group><article-title xml:lang="en">The use of multiple measurements in taxonomic problems</article-title><source>Ann Eugenics.</source><year>1936</year><volume>7</volume><issue>2</issue><fpage>179</fpage><lpage>188</lpage></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Uysal</surname><given-names>I</given-names></name><name><surname>Güvenir</surname><given-names>HA</given-names></name></person-group><article-title xml:lang="en">An overview of regression techniques for knowledge discovery</article-title><source>Knowl Eng Rev.</source><year>1999</year><volume>14</volume><issue>4</issue><fpage>319</fpage><lpage>340</lpage></mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Rish I. An empirical study of the naive Bayes classifier. In: Proc. 2001 IJCAI. vol. 3; 2001. p. 41–46.</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cover</surname><given-names>T</given-names></name><name><surname>Hart</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Nearest neighbor pattern classification</article-title><source>IEEE Trans Inf Theory.</source><year>1967</year><volume>13</volume><issue>1</issue><fpage>21</fpage><lpage>27</lpage><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0154.44505</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>L</given-names></name><name><surname>Juang</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">An introduction to hidden Markov models</article-title><source>IEEE ASSP Mag.</source><year>1986</year><volume>3</volume><issue>1</issue><fpage>4</fpage><lpage>16</lpage></mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kohavi</surname><given-names>R</given-names></name><name><surname>Quinlan</surname><given-names>JR</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Klösgen</surname><given-names>W</given-names></name><name><surname>Zytkow</surname><given-names>JM</given-names></name></person-group><article-title xml:lang="en">Data mining tasks and methods: classification: decision-tree discovery</article-title><source>Handbook of data mining and knowledge discovery</source><year>2002</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>Oxford University Press, Inc.</publisher-name><fpage>267</fpage><lpage>276</lpage></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><article-title xml:lang="en">Connectionist learning procedures</article-title><source>Artif Intell.</source><year>1989</year><volume>40</volume><issue>1–3</issue><fpage>185</fpage><lpage>234</lpage></mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dempster</surname><given-names>AP</given-names></name><name><surname>Laird</surname><given-names>NM</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><article-title xml:lang="en">Maximum likelihood from incomplete data via the EM algorithm</article-title><source>J R Stat Soc Series B Methodol.</source><year>1977</year><volume>39</volume><issue>1</issue><fpage>1</fpage><lpage>38</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">501537</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0364.62022</pub-id></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Tishby N, Pereira FC, Bialek W. The information bottleneck method. In: Proc. 37th ACCCC. 1999. p. 368-377.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Kohonen T. Self-organized formation of topologically correct feature maps. Biol Cybernet. 1982;43(1):59–69.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Agrawal R, Imieliński T, Swami A. Mining association rules between sets of items in large databases. In: Proc. ACM SIGMOD ’93. 1993. p. 207–216.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>AD</given-names></name></person-group><article-title xml:lang="en">A review of hierarchical classification</article-title><source>J R Stat Soc Series A General.</source><year>1987</year><volume>150</volume><issue>2</issue><fpage>119</fpage><lpage>137</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">896652</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0616.62086</pub-id></mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>G</given-names></name><name><surname>Hall</surname><given-names>D</given-names></name></person-group><source>ISODATA, a novel method of data anlysis and pattern classification</source><year>1965</year><publisher-loc>Stanford, CA</publisher-loc><publisher-name>Stanford Research Institute</publisher-name></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunn</surname><given-names>JC</given-names></name></person-group><article-title xml:lang="en">A fuzzy relative of the ISODATA process and its use in detecting compact well-separated clusters</article-title><source>J Cybernet.</source><year>1973</year><volume>3</volume><issue>3</issue><fpage>32</fpage><lpage>57</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">375857</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0291.68033</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>John</surname><given-names>AH</given-names></name></person-group><source>Clustering algorithms</source><year>1975</year><publisher-loc>New York, NY, USA</publisher-loc><publisher-name>John Wiley &amp; Sons, Inc.</publisher-name><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0372.62040</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegel</surname><given-names>HP</given-names></name><name><surname>Kroger</surname><given-names>P</given-names></name><name><surname>Sander</surname><given-names>J</given-names></name><name><surname>Zimek</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Density-based clustering</article-title><source>WIRES Data Min Knowl.</source><year>2011</year><volume>1</volume><issue>3</issue><fpage>231</fpage><lpage>240</lpage></mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Ankerst M, Breunig MM, Kriegel HP, Sander J. OPTICS: ordering points to identify the clustering structure. In: Proc. ACM SIGMOD’99. 1999. p. 49–60.</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horgan</surname><given-names>RP</given-names></name><name><surname>Kenny</surname><given-names>LC</given-names></name></person-group><article-title xml:lang="en">Omic technologies: genomics, transcriptomics, proteomics and metabolomics</article-title><source>Obstet Gynecol.</source><year>2011</year><volume>13</volume><issue>3</issue><fpage>189</fpage><lpage>195</lpage></mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Libbrecht</surname><given-names>MW</given-names></name><name><surname>Noble</surname><given-names>WS</given-names></name></person-group><article-title xml:lang="en">Machine learning applications in genetics and genomics</article-title><source>Nat Rev Genet.</source><year>2015</year><volume>16</volume><issue>6</issue><fpage>321</fpage><lpage>332</lpage></mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lemm</surname><given-names>S</given-names></name><name><surname>Blankertz</surname><given-names>B</given-names></name><name><surname>Dickhaus</surname><given-names>T</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name></person-group><article-title xml:lang="en">Introduction to machine learning for brain imaging</article-title><source>NeuroImage.</source><year>2011</year><volume>56</volume><issue>2</issue><fpage>387</fpage><lpage>399</lpage></mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erickson</surname><given-names>BJ</given-names></name><name><surname>Korfiatis</surname><given-names>P</given-names></name><name><surname>Akkus</surname><given-names>Z</given-names></name><name><surname>Kline</surname><given-names>TL</given-names></name></person-group><article-title xml:lang="en">Machine learning for medical imaging</article-title><source>RadioGraphics.</source><year>2017</year><volume>37</volume><issue>2</issue><fpage>505</fpage><lpage>515</lpage></mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kan</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Machine learning applications in cell image analysis</article-title><source>Immunol Cell Biol.</source><year>2017</year><volume>95</volume><issue>6</issue><fpage>525</fpage><lpage>30</lpage></mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>C</given-names></name><name><surname>Sannelli</surname><given-names>C</given-names></name><name><surname>Müller</surname><given-names>KR</given-names></name><name><surname>Blankertz</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Machine-learning-based coadaptive calibration for brain-computer interfaces</article-title><source>Neural Computat.</source><year>2010</year><volume>23</volume><issue>3</issue><fpage>791</fpage><lpage>816</lpage><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1208.92016</pub-id></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Mala S, Latha K. Feature selection in classification of eye movements using electrooculography for activity recognition. Com Math Met Med. 2014 Dec;2014.</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Mahmud M. Vassanelli S. Processing and analysis of multichannel extracellular neuronal signals: state-of-the-art and challenges. Front Neurosci. 2016. p. 10.</mixed-citation></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Lecun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015 5;521(7553):436–444.</mixed-citation></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yahaya</surname><given-names>SW</given-names></name><name><surname>Lotfi</surname><given-names>A</given-names></name><name><surname>Mahmud</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">A consensus novelty detection ensemble approach for anomaly detection in activities of daily living</article-title><source>Applied Soft Computing.</source><year>2019</year><volume>83</volume><fpage>105613</fpage></mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Fabietti M, Mahmud M, Lotfi A, Averna A, Guggenmo D, Nudo R, et al. Neural network-based artifact detection in local field potentials recorded from chronically implanted neural probes. In: Proc. IJCNN; 2020. p. 1–8.</mixed-citation></ref><ref id="CR51"><label>51.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>M</given-names></name><name><surname>Kaiser</surname><given-names>MS</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name><name><surname>Vassanelli</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Applications of deep learning and reinforcement learning to biological data</article-title><source>IEEE Trans Neural Netw Learn Syst</source><year>2018</year><volume>29</volume><issue>6</issue><fpage>2063</fpage><lpage>2079</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3811399</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Mahmud M, Kaiser MS, Hussain A. Deep learning in mining biological data. <ext-link xlink:href="http://arxiv.org/abs/200300108" ext-link-type="uri">arXiv:200300108</ext-link> [cs, q-bio, stat]. 2020 Feb;p. 1–36. <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">ArXiv: 2003.00108</ext-link>. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>.</mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Dey N, Rajinikanth V, Fong SJ, Kaiser MS, Mahmud M. Social-group-optimization assisted Kapur’s entropy and morphological segmentation for automated detection of COVID-19 infection from computed tomography images. Cogn Comput. 2020;12(5):1011–1023.</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Aradhya MVN, Mahmud M, Guru D, S Agrawal B, Kaiser MS. One shot cluster based approach for the detection of COVID-19 from chest X-ray images. Cogn Comput. 2020;p. 1–8.</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Noor</surname><given-names>MBT</given-names></name><name><surname>Zenia</surname><given-names>NZ</given-names></name><name><surname>Kaiser</surname><given-names>MS</given-names></name><name><surname>Mahmud</surname><given-names>M</given-names></name><name><surname>Al</surname><given-names>Mamun S</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Liang</surname><given-names>P</given-names></name><name><surname>Goel</surname><given-names>V</given-names></name><name><surname>Shan</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Detecting neurodegenerative disease from MRI: a brief review on a deep learning perspective</article-title><source>Brain Inform</source><year>2019</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>115</fpage><lpage>125</lpage></mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ali</surname><given-names>HM</given-names></name><name><surname>Kaiser</surname><given-names>MS</given-names></name><name><surname>Mahmud</surname><given-names>M</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Liang</surname><given-names>P</given-names></name><name><surname>Goel</surname><given-names>V</given-names></name><name><surname>Shan</surname><given-names>C</given-names></name></person-group><article-title xml:lang="en">Application of convolutional neural network in segmenting brain regions from MRI data</article-title><source>Brain Inform</source><year>2019</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>136</fpage><lpage>146</lpage></mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Miah Y, Prima CNE, Seema SJ, Mahmud M, Kaiser MS. Performance comparison of machine learning techniques in identifying dementia from open access clinical datasets. In: Proc. ICACIn. Springer, Singapore; 2020. p. 79–89.</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Watkins J, Fabietti M, Mahmud M. SENSE: a student performance quantifier using sentiment analysis. In: Proc. IJCNN; 2020. p. 1–6.</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabby</surname><given-names>G</given-names></name><name><surname>Azad</surname><given-names>S</given-names></name><name><surname>Mahmud</surname><given-names>M</given-names></name><name><surname>Zamli</surname><given-names>KZ</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name></person-group><article-title xml:lang="en">TeKET: a tree-based unsupervised keyphrase extraction technique</article-title><source>Cogn Comput.</source><year>2020</year><volume>12</volume><issue>5</issue><fpage>811</fpage><lpage>833</lpage></mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Orojo O, Tepper J, McGinnity TM, Mahmud M. A multi-recurrent network for crude oil price prediction. In: Proc. SSCI; 2019. p. 2940–2945.</mixed-citation></ref><ref id="CR61"><label>61.</label><mixed-citation publication-type="other">Ching T, et al . Opportunities and obstacles for deep learning in biology and medicine. bioRxiv. 2017;p. 142760.</mixed-citation></ref><ref id="CR62"><label>62.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Learning deep architectures for AI</article-title><source>Found Trends Mach Learn.</source><year>2009</year><volume>2</volume><issue>1</issue><fpage>1</fpage><lpage>127</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">2480723</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1192.68503</pub-id></mixed-citation></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><source>Deep learning</source><year>2016</year><publisher-loc>Cambridge, USA</publisher-loc><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1373.68009</pub-id></mixed-citation></ref><ref id="CR64"><label>64.</label><mixed-citation publication-type="other">Saxe AM, McClelland JL, Ganguli S. Exact solutions to the nonlinear dynamics of learning in deep linear neural nets. In: Proc. ICLR; 2014. p. 1–22.</mixed-citation></ref><ref id="CR65"><label>65.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Deep learning in neural networks: an overview</article-title><source>Neural Netw.</source><year>2015</year><volume>61</volume><fpage>85</fpage><lpage>117</lpage></mixed-citation></ref><ref id="CR66"><label>66.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>D</given-names></name><name><surname>Zhao</surname><given-names>F</given-names></name><name><surname>Shen</surname><given-names>W</given-names></name><name><surname>Ge</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Compressing and accelerating neural network for facial point localization</article-title><source>Cogn Comput.</source><year>2018</year><volume>10</volume><issue>2</issue><fpage>359</fpage><lpage>367</lpage></mixed-citation></ref><ref id="CR67"><label>67.</label><mixed-citation publication-type="other">Salakhutdinov R, Hinton GE. Deep Boltzmann machines. In: Proc. AISTATS2009; 2009. p. 448–455.</mixed-citation></ref><ref id="CR68"><label>68.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geman</surname><given-names>S</given-names></name><name><surname>Geman</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images</article-title><source>IEEE Trans Pattern Anal Mach Intell.</source><year>1984</year><volume>6</volume><issue>6</issue><fpage>721</fpage><lpage>741</lpage><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0573.62030</pub-id></mixed-citation></ref><ref id="CR69"><label>69.</label><mixed-citation publication-type="other">Fischer A, Igel C. An introduction to restricted Boltzmann machines. In: Proc. CIARP 2012; 2012. p. 14–36.</mixed-citation></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">Desjardins G, Courville AC, Bengio Y. On training deep Boltzmann machines. CoRR. 2012;abs/1203.4416.</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Tieleman T. Training restricted Boltzmann machines using approximations to the likelihood gradient. In: Proc. ICML; 2008. p. 1064–1071.</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Oerlemans</surname><given-names>A</given-names></name><name><surname>Lao</surname><given-names>S</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Lew</surname><given-names>MS</given-names></name></person-group><article-title xml:lang="en">Deep learning for visual understanding: A review</article-title><source>Neurocomputing.</source><year>2016</year><volume>187</volume><fpage>27</fpage><lpage>48</lpage></mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Osindero</surname><given-names>S</given-names></name><name><surname>Teh</surname><given-names>YW</given-names></name></person-group><article-title xml:lang="en">A fast learning algorithm for deep belief nets</article-title><source>Neural Comput.</source><year>2006</year><volume>18</volume><issue>7</issue><fpage>1527</fpage><lpage>1554</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">2224485</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1106.68094</pub-id></mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>X</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Chen</surname><given-names>D</given-names></name><name><surname>Ma</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Functional brain network classification for Alzheimer’s disease detection with deep features and extreme learning machine</article-title><source>Cogn Comput.</source><year>2020</year><volume>12</volume><fpage>513</fpage><lpage>527</lpage></mixed-citation></ref><ref id="CR75"><label>75.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravi</surname><given-names>D</given-names></name><name><surname>Wong</surname><given-names>C</given-names></name><name><surname>Deligianni</surname><given-names>F</given-names></name><name><surname>Berthelot</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Deep learning for health informatics</article-title><source>IEEE J Biomed Health Inform.</source><year>2017</year><volume>21</volume><issue>1</issue><fpage>4</fpage><lpage>21</lpage></mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincent</surname><given-names>P</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Lajoie</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Manzagol</surname><given-names>PA</given-names></name></person-group><article-title xml:lang="en">Stacked denoising autoencoders: learning useful representations in a deep network with a local denoising criterion</article-title><source>J Mach Learn Res.</source><year>2010</year><volume>11</volume><fpage>3371</fpage><lpage>3408</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">2756188</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1242.68256</pub-id></mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="other">Baldi P. Autoencoders, unsupervised learning and deep architectures. In: Proc. ICUTLW; 2012. p. 37–50.</mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Ranzato M, Poultney C, Chopra S, Cun YL. Efficient learning of sparse representations with an energy-based model. In: Proc. NIPS; 2006. p. 1137–1144.</mixed-citation></ref><ref id="CR79"><label>79.</label><mixed-citation publication-type="other">Kingma DP, Welling M. Auto-encoding variational bayes. CoRR. 2014;abs/1312.6114.</mixed-citation></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Rifai S, Vincent P, Muller X, Glorot X, Bengio Y. Contractive auto-encoders: explicit invariance during feature extraction. In: Proc. ICML; 2011. p. 833–840.</mixed-citation></ref><ref id="CR81"><label>81.</label><mixed-citation publication-type="other">Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, et al. Generative adversarial nets. In: Advances in neural information processing systems; 2014. p. 2672–2680.</mixed-citation></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="other">Isola P, Zhu JY, Zhou T, Efros AA. Image-to-image translation with conditional adversarial networks. In: Proc.IEEE CVPR; 2017. p. 1125–1134.</mixed-citation></ref><ref id="CR83"><label>83.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Healy</surname><given-names>G</given-names></name><name><surname>Smeaton</surname><given-names>AF</given-names></name><name><surname>Ward</surname><given-names>TE</given-names></name></person-group><article-title xml:lang="en">Use of neural signals to evaluate the quality of generative adversarial network performance in facial image generation</article-title><source>Cogn Comput.</source><year>2020</year><volume>12</volume><issue>1</issue><fpage>13</fpage><lpage>24</lpage></mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Pascanu R, Gulcehre C, Cho K, Bengio Y. How to construct deep recurrent neural networks. In: Proc. ICLR; 2014. p. 1–13.</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elman</surname><given-names>JL</given-names></name></person-group><article-title xml:lang="en">Finding structure in time</article-title><source>Cognitive Sci.</source><year>1990</year><volume>14</volume><issue>2</issue><fpage>179</fpage><lpage>211</lpage></mixed-citation></ref><ref id="CR86"><label>86.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Paliwal</surname><given-names>KK</given-names></name></person-group><article-title xml:lang="en">Bidirectional recurrent neural networks</article-title><source>IEEE Tran Signal Proces.</source><year>1997</year><volume>45</volume><issue>11</issue><fpage>2673</fpage><lpage>2681</lpage></mixed-citation></ref><ref id="CR87"><label>87.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">Long short-term memory</article-title><source>Neural Comput.</source><year>1997</year><volume>9</volume><issue>8</issue><fpage>1735</fpage><lpage>1780</lpage></mixed-citation></ref><ref id="CR88"><label>88.</label><mixed-citation publication-type="other">Lipton ZC, Berkowitz J, Elkan C. A critical review of recurrent neural networks for sequence learning. CoRR. 2015 May;CoRR: 1506.00019.</mixed-citation></ref><ref id="CR89"><label>89.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>Y</given-names></name><name><surname>Peng</surname><given-names>H</given-names></name><name><surname>Khan</surname><given-names>T</given-names></name><name><surname>Cambria</surname><given-names>E</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Sentic LSTM: a hybrid network for targeted aspect-based sentiment analysis</article-title><source>Cogn Comput.</source><year>2018</year><volume>10</volume><issue>4</issue><fpage>639</fpage><lpage>650</lpage></mixed-citation></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiatowski</surname><given-names>T</given-names></name><name><surname>Bölcskei</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">A mathematical theory of deep convolutional neural networks for feature extraction</article-title><source>IEEE Trans Inf Theory.</source><year>2017</year><volume>64</volume><issue>3</issue><fpage>1845</fpage><lpage>1866</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">3766318</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1390.94053</pub-id></mixed-citation></ref><ref id="CR91"><label>91.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Arbib</surname><given-names>MA</given-names></name></person-group><article-title xml:lang="en">Convolutional networks for images, speech, and time series</article-title><source>The handbook of brain theory and neural networks</source><year>1998</year><publisher-loc>Cambridge, MA, USA</publisher-loc><publisher-name>MIT Press</publisher-name><fpage>255</fpage><lpage>258</lpage></mixed-citation></ref><ref id="CR92"><label>92.</label><mixed-citation publication-type="other">Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. In: Proc. NIPS; 2012. p. 1097–1105.</mixed-citation></ref><ref id="CR93"><label>93.</label><mixed-citation publication-type="other">Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition. CoRR. 2014;abs/1409.1556.</mixed-citation></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="other">Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al. Going deeper with convolutions. In: Proc. CVPR2015; 2015. p. 1–9.</mixed-citation></ref><ref id="CR95"><label>95.</label><mixed-citation publication-type="other">Heinsfeld AS, Franco AR, Craddock RC, Buchweitz A, Meneguzzi F. Identification of autism spectrum disorder using deep learning and the ABIDE dataset. NeuroImage: Clin. 2018;17:16 – 23.</mixed-citation></ref><ref id="CR96"><label>96.</label><mixed-citation publication-type="other">Kuang D, He L. Classification on ADHD with deep learning. In: Proc. CCBD; 2014. p. 27–32.</mixed-citation></ref><ref id="CR97"><label>97.</label><mixed-citation publication-type="other">HosseiniAsl E, Gimelfarb GL, El-Baz A. Alzheimer’s disease diagnostics by a deeply supervised adaptable3D convolutional network. CoRR. 2016;abs/1607.00556.</mixed-citation></ref><ref id="CR98"><label>98.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suk</surname><given-names>HI</given-names></name><name><surname>Lee</surname><given-names>SW</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Hierarchical feature representation and multimodal fusion with deep learning for AD/MCI diagnosis</article-title><source>NeuroImage.</source><year>2014</year><volume>101</volume><fpage>569</fpage><lpage>582</lpage></mixed-citation></ref><ref id="CR99"><label>99.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Tran</surname><given-names>L</given-names></name><name><surname>Thung</surname><given-names>KH</given-names></name><name><surname>Ji</surname><given-names>S</given-names></name><name><surname>Shen</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">A robust deep model for improved classification of AD/MCI patients</article-title><source>IEEE J Biomed Health Inform.</source><year>2015</year><volume>19</volume><issue>5</issue><fpage>1610</fpage><lpage>1616</lpage></mixed-citation></ref><ref id="CR100"><label>100.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Havaei</surname><given-names>M</given-names></name><name><surname>Guizard</surname><given-names>N</given-names></name><name><surname>Larochelle</surname><given-names>H</given-names></name><name><surname>Jodoin</surname><given-names>PM</given-names></name></person-group><person-group person-group-type="editor"><name><surname>Holzinger</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Deep learning trends for focal brain pathology segmentation in MRI</article-title><source>Machine learning for health informatics: state-of-the-art and future challenges</source><year>2016</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><fpage>125</fpage><lpage>148</lpage></mixed-citation></ref><ref id="CR101"><label>101.</label><mixed-citation publication-type="other">Fritscher K, Raudaschl P, Zaffino P, Spadea MF, Sharp GC, et al. Deep neural networks for fast segmentation of 3D medical images. In: Proc. MICCAI; 2016. p. 158–165.</mixed-citation></ref><ref id="CR102"><label>102.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iqbal</surname><given-names>T</given-names></name><name><surname>Ali</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Generative adversarial network for medical images (MI-GAN)</article-title><source>J Med Syst.</source><year>2018</year><volume>42</volume><issue>11</issue><fpage>231</fpage></mixed-citation></ref><ref id="CR103"><label>103.</label><mixed-citation publication-type="other">Ciresan D, Giusti A, Gambardella L, Schmidhuber J. Deep neural nets segment neuronal membrane in electron microscopy images. In: Proc. NIPS; 2012. p. 2843–2851.</mixed-citation></ref><ref id="CR104"><label>104.</label><mixed-citation publication-type="other">Stollenga MF, Byeon W, Liwicki M, Schmidhuber J. Parallel multi-dimensional LSTM, with application to fast biomedical volumetric image segmentation. In: Proc. NIPS; 2015. p. 2980–88.</mixed-citation></ref><ref id="CR105"><label>105.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleesiek</surname><given-names>J</given-names></name><name><surname>Urban</surname><given-names>G</given-names></name><name><surname>Hubert</surname><given-names>A</given-names></name><name><surname>Schwarz</surname><given-names>D</given-names></name><name><surname>Maier-Hein</surname><given-names>K</given-names></name><name><surname>Bendszus</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Deep MRI brain extraction: A 3D convolutional neural network for skull stripping</article-title><source>NeuroImage.</source><year>2016</year><volume>129</volume><fpage>460</fpage><lpage>469</lpage></mixed-citation></ref><ref id="CR106"><label>106.</label><mixed-citation publication-type="other">Cho J, Lee K, Shin E, Choy G, Do S. Medical image deep learning with hospital PACS dataset. CoRR. 2015;abs/1511.06348.</mixed-citation></ref><ref id="CR107"><label>107.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ngo</surname><given-names>T</given-names></name><etal/></person-group><article-title xml:lang="en">Combining deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine mr</article-title><source>Med Image Anal.</source><year>2017</year><volume>35</volume><fpage>159</fpage><lpage>171</lpage></mixed-citation></ref><ref id="CR108"><label>108.</label><mixed-citation publication-type="other">Ciresan D, Giusti A, Gambardella L, Schmidhuber J. Mitosis detection in breast cancer histology images with deep neural networks. In: Proc. MICCAI; 2013. p. 411–4188.</mixed-citation></ref><ref id="CR109"><label>109.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kamnitsas</surname><given-names>K</given-names></name><name><surname>Ledig</surname><given-names>C</given-names></name><name><surname>Newcombe</surname><given-names>VFJ</given-names></name><name><surname>Simpson</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Efficient multi-scale 3D CNN with fully connected CRF for accurate brain lesion segmentation</article-title><source>Med Image Anal.</source><year>2017</year><volume>36</volume><fpage>61</fpage><lpage>78</lpage></mixed-citation></ref><ref id="CR110"><label>110.</label><mixed-citation publication-type="other">Lu N, Li T, Ren X, Miao H. A deep learning scheme for motor imagery classification based on restricted Boltzmann machines. IEEE Trans Neural Syst Rehabil Eng. 2016; 99</mixed-citation></ref><ref id="CR111"><label>111.</label><mixed-citation publication-type="other">Yang H, Sakhavi S, Ang KK, Guan C. On the use of convolutional neural networks and augmented CSP features for multi-class motor imagery of EEG signals classification. In: Proc. 37th IEEE EMBC; 2015. p. 2620–2623.</mixed-citation></ref><ref id="CR112"><label>112.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabar</surname><given-names>YR</given-names></name><name><surname>Halici</surname><given-names>U</given-names></name></person-group><article-title xml:lang="en">A novel deep learning approach for classification of EEG motor imagery signals</article-title><source>J Neural Eng.</source><year>2017</year><volume>14</volume><issue>1</issue><fpage>016003</fpage></mixed-citation></ref><ref id="CR113"><label>113.</label><mixed-citation publication-type="other">Sakhavi S, Guan C, Yan S. Parallel convolutional-linear neural network for motor imagery classification. In: Proc. EUSIPCO; 2015. p. 2786–2790.</mixed-citation></ref><ref id="CR114"><label>114.</label><mixed-citation publication-type="other">Li K, Li X, Zhang Y, Zhang A. Affective state recognition from EEG with deep belief networks. In: Proc. BIBM; 2013. p. 305–310.</mixed-citation></ref><ref id="CR115"><label>115.</label><mixed-citation publication-type="other">Jia X, Li K, Li X, Zhang A. A novel semi-supervised deep learning framework for affective state recognition on EEG signals. In: Proc. IEEE BIBE; 2014. p. 30–37.</mixed-citation></ref><ref id="CR116"><label>116.</label><mixed-citation publication-type="other">Tripathi S, Acharya S, Sharma R, Mittal S, et al. Using deep and convolutional neural networks for accurate emotion classification on DEAP dataset. In: Proc. 29th IAAI; 2017. p. 4746–4752.</mixed-citation></ref><ref id="CR117"><label>117.</label><mixed-citation publication-type="other">Chen G, Zhu Y, Hong Z, Yang Z. EmotionalGAN: generating ECG to enhance emotion state classification. In: Proc. AICS 2019. 2019. p. 309-313.</mixed-citation></ref><ref id="CR118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mirowski</surname><given-names>P</given-names></name><name><surname>Madhavan</surname><given-names>D</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Kuzniecky</surname><given-names>R</given-names></name></person-group><article-title xml:lang="en">Classification of patterns of EEG synchronization for seizure prediction</article-title><source>Clin Neurophysiol.</source><year>2009</year><volume>120</volume><issue>11</issue><fpage>1927</fpage><lpage>1940</lpage></mixed-citation></ref><ref id="CR119"><label>119.</label><mixed-citation publication-type="other">Jirayucharoensak S, Pan-Ngum S, Israsena P. EEG-based emotion recognition using deep learning network with principal component based covariate shift adaptation. Scientific World J. 2014;p. 1–10.</mixed-citation></ref><ref id="CR120"><label>120.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>Z</given-names></name><name><surname>Ding</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">A novel method for classification of ECG arrhythmias using deep belief networks [journal article]</article-title><source>J Comp Intel Appl.</source><year>2016</year><volume>15</volume><fpage>1650021</fpage></mixed-citation></ref><ref id="CR121"><label>121.</label><mixed-citation publication-type="other">Yan Y, Qin X, Wu Y, Zhang N, Fan J, et al. A restricted Boltzmann machine based two-lead electrocardiography classification. In: Proc. BSN; 2015. p. 1–9.</mixed-citation></ref><ref id="CR122"><label>122.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atzori</surname><given-names>M</given-names></name><name><surname>Cognolato</surname><given-names>M</given-names></name><name><surname>Müller</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Deep learning with convolutional neural networks applied to electromyography data: a resource for the classification of movements for prosthetic hands</article-title><source>Front Neurorobot.</source><year>2016</year><volume>10</volume><fpage>9</fpage></mixed-citation></ref><ref id="CR123"><label>123.</label><mixed-citation publication-type="other">Huanhuan M, Yue Z. Classification of electrocardiogram signals with DBN. In: Proc. IEEE CSE; 2014. p. 7–12.</mixed-citation></ref><ref id="CR124"><label>124.</label><mixed-citation publication-type="other">Wang S, Peng J, Ma J, Xu J. Protein secondary structure prediction using deep convolutional neural fields. Scientific Reports. 2016 Nov;6(1).</mixed-citation></ref><ref id="CR125"><label>125.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Delong</surname><given-names>A</given-names></name><name><surname>Weirauch</surname><given-names>MT</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><article-title xml:lang="en">Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title><source>Nature Biotechnol.</source><year>2015</year><volume>33</volume><issue>8</issue><fpage>831</fpage><lpage>838</lpage></mixed-citation></ref><ref id="CR126"><label>126.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Tsoi</surname><given-names>A</given-names></name><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Zheng</surname><given-names>WJ</given-names></name></person-group><article-title xml:lang="en">Predict effective drug combination by deep belief network and ontology fingerprints</article-title><source>J Biomed Inform.</source><year>2018</year><volume>85</volume><fpage>149</fpage><lpage>154</lpage></mixed-citation></ref><ref id="CR127"><label>127.</label><mixed-citation publication-type="other">Denas O, Taylor J. Deep modeling of gene expression regulation in an Erythropoiesis model. In: Proc. ICMLRL; 2013. p. 1–5.</mixed-citation></ref><ref id="CR128"><label>128.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name><name><surname>Rinn</surname><given-names>JL</given-names></name></person-group><article-title xml:lang="en">Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title><source>Genome Res.</source><year>2016</year><volume>26</volume><issue>7</issue><fpage>990</fpage><lpage>9</lpage></mixed-citation></ref><ref id="CR129"><label>129.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><article-title xml:lang="en">Predicting effects of noncoding variants with deep learning-based sequence model</article-title><source>Nature Methods.</source><year>2015</year><volume>12</volume><issue>10</issue><fpage>931</fpage><lpage>934</lpage></mixed-citation></ref><ref id="CR130"><label>130.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marouf</surname><given-names>M</given-names></name><etal/></person-group><article-title xml:lang="en">Realistic in silico generation and augmentation of single-cell RNA-seq data using generative adversarial networks</article-title><source>Nat Commun.</source><year>2020</year><volume>11</volume><fpage>166</fpage></mixed-citation></ref><ref id="CR131"><label>131.</label><mixed-citation publication-type="other">Lee T, Yoon S. Boosted categorical restricted boltzmann machine for computational prediction of splice junctions. In: Proc. ICML; 2015. p. 2483–2492.</mixed-citation></ref><ref id="CR132"><label>132.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeng</surname><given-names>H</given-names></name><name><surname>Edwards</surname><given-names>MD</given-names></name><name><surname>Liu</surname><given-names>G</given-names></name><name><surname>Gifford</surname><given-names>DK</given-names></name></person-group><article-title xml:lang="en">Convolutional neural network architectures for predicting DNA-protein binding</article-title><source>Bioinformatics.</source><year>2016</year><volume>32</volume><issue>12</issue><fpage>121</fpage><lpage>127</lpage></mixed-citation></ref><ref id="CR133"><label>133.</label><mixed-citation publication-type="other">Park S, Min S, Choi H, Yoon S. deepMiRGene: Deep neural network based precursor microRNA prediction. CoRR. 2016;abs/1605.00017.</mixed-citation></ref><ref id="CR134"><label>134.</label><mixed-citation publication-type="other">Lee B, Baek J, Park S, Yoon S. deepTarget: end-to-end learning framework for miRNA target prediction using deep recurrent neural networks. CoRR. 2016;abs/1603.09123.</mixed-citation></ref><ref id="CR135"><label>135.</label><mixed-citation publication-type="other">Li H. A template-based protein structure reconstruction method using DA learning. J Proteomics Bioinform. 2016;9(12).</mixed-citation></ref><ref id="CR136"><label>136.</label><mixed-citation publication-type="other">Ibrahim R, Yousri NA, Ismail MA, El-Makky NM. Multi-level gene/MiRNA feature selection using deep belief nets and active learning. In: Proc. IEEE EMBC; 2014. p. 3957–3960.</mixed-citation></ref><ref id="CR137"><label>137.</label><mixed-citation publication-type="other">Chen L, Cai C, Chen V, Lu X. Trans-species learning of cellular signaling systems with bimodal deep belief networks. Bioinformatics. 2015 sep;31(18):3008–3015.</mixed-citation></ref><ref id="CR138"><label>138.</label><mixed-citation publication-type="other">Danaee P, Ghaeini R, Hendrix DA. A deep learning approach for cancer detection and relevant gene identification. In: Proc. Pac. Symp. Biocomput.. vol. 22; 2016. p. 219–229.</mixed-citation></ref><ref id="CR139"><label>139.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Fauteux</surname><given-names>F</given-names></name><name><surname>Zou</surname><given-names>J</given-names></name><name><surname>Nantel</surname><given-names>A</given-names></name><name><surname>Pan</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">Personalized prediction of genes with tumor-causing somatic mutations based on multi-modal deep Boltzmann machine</article-title><source>Neurocomputing.</source><year>2019</year><volume>324</volume><fpage>51</fpage><lpage>62</lpage></mixed-citation></ref><ref id="CR140"><label>140.</label><mixed-citation publication-type="other">Zhang T, Zhang L, Payne PRO, Li F. Synergistic drug combination prediction by integrating multi-omics data in deep learning models. <ext-link xlink:href="http://arxiv.org/abs/181107054" ext-link-type="uri">arXiv:181107054</ext-link> [cs, q-bio, stat]. 2018 Nov;ArXiv: 1811.07054. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>.</mixed-citation></ref><ref id="CR141"><label>141.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Gulko</surname><given-names>B</given-names></name><name><surname>Siepel</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Fast, scalable prediction of deleterious noncoding variants from functional and population genomic data</article-title><source>Nature Genet.</source><year>2017</year><volume>49</volume><fpage>618</fpage><lpage>624</lpage></mixed-citation></ref><ref id="CR142"><label>142.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le</surname><given-names>EPV</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Hickman</surname><given-names>S</given-names></name><name><surname>Gilbert</surname><given-names>FJ</given-names></name></person-group><article-title xml:lang="en">Artificial intelligence in breast imaging</article-title><source>Clin Radiol.</source><year>2019</year><volume>74</volume><issue>5</issue><fpage>357</fpage><lpage>366</lpage></mixed-citation></ref><ref id="CR143"><label>143.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yi</surname><given-names>X</given-names></name><name><surname>Walia</surname><given-names>E</given-names></name><name><surname>Babyn</surname><given-names>P</given-names></name></person-group><article-title xml:lang="en">Generative adversarial network in medical imaging: A review</article-title><source>Med Image Anal.</source><year>2019</year><volume>58</volume><fpage>101552</fpage></mixed-citation></ref><ref id="CR144"><label>144.</label><mixed-citation publication-type="other">Sandfort V, Yan K, Pickhardt PJ, Summers RM. Data augmentation using generative adversarial networks (CycleGAN) to improve generalizability in CT segmentation tasks. Sci Rep. 2019;9(1):1–9.</mixed-citation></ref><ref id="CR145"><label>145.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armanious</surname><given-names>K</given-names></name><etal/></person-group><article-title xml:lang="en">MedGAN: Medical image translation using GANs</article-title><source>Comput Med Imaging Graph.</source><year>2020</year><volume>79</volume><fpage>101684</fpage></mixed-citation></ref><ref id="CR146"><label>146.</label><mixed-citation publication-type="other">Uemura T, et al. GAN-based survival prediction model from CT images of patients with idiopathic pulmonary fibrosis. In: Chen PH, Deserno TM, editors. Medical Imaging 2020: Imaging Informatics for Healthcare, Research, and Applications. vol. 11318. SPIE; 2020. p. 354 – 359. </mixed-citation></ref><ref id="CR147"><label>147.</label><mixed-citation publication-type="other">Thambawita V, Hammer HL, Riegler M, Halvorsen P. GANEx: A complete pipeline of training, inference and benchmarking GAN experiments. In: Proc. 2019 CBMI. 2019. p. 1–4.</mixed-citation></ref><ref id="CR148"><label>148.</label><mixed-citation publication-type="other">Halicek M, et al. Conditional generative adversarial network for synthesizing hyperspectral images of breast cancer cells from digitized histology. In: Tomaszewski JE, Ward AD, editors. Medical Imaging 2020: Digital Pathology, vol. 11320. SPIE; 2020. p. 198–205.</mixed-citation></ref><ref id="CR149"><label>149.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>F</given-names></name><name><surname>Ye</surname><given-names>F</given-names></name><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Shen</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">Electrocardiogram generation with a bidirectional LSTM-CNN generative adversarial network</article-title><source>Sci Rep.</source><year>2019</year><volume>9</volume><issue>1</issue><fpage>1</fpage><lpage>11</lpage></mixed-citation></ref><ref id="CR150"><label>150.</label><mixed-citation publication-type="other">Yu L, Zhang W, Wang J, Yu Y. Seqgan: Sequence generative adversarial nets with policy gradient. In: Proc. 31st AAAI Conf. AI; 2017. p. 2852–2858.</mixed-citation></ref><ref id="CR151"><label>151.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ye</surname><given-names>F</given-names></name><name><surname>Zhu</surname><given-names>F</given-names></name><name><surname>Fu</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>B</given-names></name></person-group><article-title xml:lang="en">ECG generation with sequence generative adversarial nets optimized by policy gradient</article-title><source>IEEE Access.</source><year>2019</year><volume>7</volume><fpage>159369</fpage><lpage>159378</lpage></mixed-citation></ref><ref id="CR152"><label>152.</label><mixed-citation publication-type="other">Luo Y, Lu BL. EEG data augmentation for emotion recognition using a conditional Wasserstein GAN. In: 2018 IEEE EMBC; 2018. p. 2535–2538. ISSN: 1558-4615.</mixed-citation></ref><ref id="CR153"><label>153.</label><mixed-citation publication-type="other">You S, et al. Unsupervised automatic seizure detection for focal-onset seizures recorded with behind-the-ear EEG using an anomaly-detecting generative adversarial network. Comput Methods Programs Biomed. 2020;p. 105472.</mixed-citation></ref><ref id="CR154"><label>154.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiao</surname><given-names>Y</given-names></name><name><surname>Deng</surname><given-names>Y</given-names></name><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>Lu</surname><given-names>BL</given-names></name></person-group><article-title xml:lang="en">Driver sleepiness detection from EEG and EOG signals using GAN and LSTM networks</article-title><source>Neurocomputing.</source><year>2020</year><volume>408</volume><fpage>100</fpage><lpage>111</lpage></mixed-citation></ref><ref id="CR155"><label>155.</label><mixed-citation publication-type="other">Singh P, Pradhan G. A new ECG denoising framework using generative adversarial network. IEEE/ACM Trans Comput Biol Bioinform. 2020;p. 3114–3128.</mixed-citation></ref><ref id="CR156"><label>156.</label><mixed-citation publication-type="other">Wang X, Ghasedi Dizaji K, Huang H. Conditional generative adversarial network for gene expression inference. Bioinformatics. 2018 09;34(17):i603–i611.</mixed-citation></ref><ref id="CR157"><label>157.</label><mixed-citation publication-type="other">Pan X, Shen HB. RNA-protein binding motifs mining with a new hybrid deep learning based cross-domain knowledge integration approach. BMC Bioinform. 2017;18(1).</mixed-citation></ref><ref id="CR158"><label>158.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>X</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Qian</surname><given-names>W</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>GN</given-names></name></person-group><article-title xml:lang="en">A generative adversarial network model for disease gene prediction with RNA-seq data</article-title><source>IEEE Access.</source><year>2020</year><volume>8</volume><fpage>37352</fpage><lpage>37360</lpage></mixed-citation></ref><ref id="CR159"><label>159.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Pang</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name></person-group><article-title xml:lang="en">GANsDTA: predicting drug-target binding affinity using GANs</article-title><source>Front Genet.</source><year>2020</year><volume>10</volume><fpage>1243</fpage></mixed-citation></ref><ref id="CR160"><label>160.</label><mixed-citation publication-type="other">Editorial. Sharing data. Nat Cell Biol. 2009 11;11(11):1273.</mixed-citation></ref><ref id="CR161"><label>161.</label><mixed-citation publication-type="other">Lord PW, et al . Large-scale data sharing in the life sciences: data standards, incentives, barriers and funding models (the ’Joint Data Standards Study’). 2005. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>.</mixed-citation></ref><ref id="CR162"><label>162.</label><mixed-citation publication-type="other">Martone ME, Ellisman MH, Sosinsky GE, Gupta A, Tran J, Wong W, et al. Cell Centered Database. UC San Diego Library Digital Collections. 2017.<ext-link xlink:href="10.6075/J0S180PX" ext-link-type="doi"> https://doi.org/10.6075/J0S180PX</ext-link>.</mixed-citation></ref><ref id="CR163"><label>163.</label><mixed-citation publication-type="other">Ellisman M, et al. Cell Image Library. 2016. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR164"><label>164.</label><mixed-citation publication-type="other">ERIC. EuroBioimaging. 2016. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR165"><label>165.</label><mixed-citation publication-type="other">Karkow W. HAPS Histology Image Database. 2008. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 23 Jan 2017.</mixed-citation></ref><ref id="CR166"><label>166.</label><mixed-citation publication-type="other">Dundee U. IDR: Image Data Resource. 2016. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>.</mixed-citation></ref><ref id="CR167"><label>167.</label><mixed-citation publication-type="other">Kistler M. SMIR Full Body CT. SMIR. 2017. Available from: <ext-link xlink:href="10.1007/BF00337288" ext-link-type="doi">https://doi.org/10.1007/BF00337288</ext-link>.</mixed-citation></ref><ref id="CR168"><label>168.</label><mixed-citation publication-type="other">Arkansas for Medical Sciences U. The Cancer Imaging Archive. 2015. Available from: <ext-link xlink:href="https://www.cancerimagingarchive.net/" ext-link-type="uri">https://www.cancerimagingarchive.net/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR169"><label>169.</label><mixed-citation publication-type="other">Marinelli RJ, et al. The Stanford Tissue Microarray Database. 2007. Available from: <ext-link xlink:href="https://tma.stanford.edu" ext-link-type="uri">https://tma.stanford.edu</ext-link>. Accessed on 23 Jan 2017.</mixed-citation></ref><ref id="CR170"><label>170.</label><mixed-citation publication-type="other">University of California SB. UCSB Bio-Segmentation Benchmark dataset. 2008. Available from: <ext-link xlink:href="https://bioimage.ucsb.edu/research/bio-segmentation" ext-link-type="uri">https://bioimage.ucsb.edu/research/bio-segmentation</ext-link>. Accessed on 23 Jan 2017.</mixed-citation></ref><ref id="CR171"><label>171.</label><mixed-citation publication-type="other">ABIDE. Autism Brain Imaging Data Exchange. 2012. Available from: <ext-link xlink:href="https://goo.gl/n694sN" ext-link-type="uri">https://goo.gl/n694sN</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR172"><label>172.</label><mixed-citation publication-type="other">Milham MP. ADHD200. 2011. Available from: <ext-link xlink:href="https://doi.org/10.1007/BF00337288" ext-link-type="uri">https://doi.org/10.1007/BF00337288</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR173"><label>173.</label><mixed-citation publication-type="other">ANDI. Alzheimer’s Disease Neuroimaging Initiative (ADNI) datasets. 2009. Available from: <ext-link xlink:href="https://adni.loni.usc.edu/" ext-link-type="uri">https://adni.loni.usc.edu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR174"><label>174.</label><mixed-citation publication-type="other">Lopez M. Breast Cancer Digital Repository. 2008. Available from:<ext-link xlink:href="https://bcdr.eu/" ext-link-type="uri">https://bcdr.eu/</ext-link>. Accessed on 8 April 2020.</mixed-citation></ref><ref id="CR175"><label>175.</label><mixed-citation publication-type="other">Mooney P. Chest X-Ray Images (Pneumonia) | Kaggle. 2018. Available from: <ext-link xlink:href="10.1007/BF00337288" ext-link-type="doi">https://doi.org/10.1007/BF00337288</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR176"><label>176.</label><mixed-citation publication-type="other">MITOS-ATYPIA. MITOS-ATYPIA-14 - Dataset. 2012. Available from: <ext-link xlink:href="10.1007/BF00337288" ext-link-type="doi">https://doi.org/10.1007/BF00337288</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR177"><label>177.</label><mixed-citation publication-type="other">NAMIC. MIDAS - Community National Alliance for Medical Image Computing (NAMIC). 2010. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>.</mixed-citation></ref><ref id="CR178"><label>178.</label><mixed-citation publication-type="other">Cohen JP, Morrison P, Dao L. COVID-19 image data collection. 2020. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR179"><label>179.</label><mixed-citation publication-type="other">Yarkoni T. Neurosynth. 2012. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR180"><label>180.</label><mixed-citation publication-type="other">NIH. NIH chest x-ray datasets. 2017. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR181"><label>181.</label><mixed-citation publication-type="other">LaMontagne PJ, et al. Open Access Series of Imaging Studies (OASIS). 2019. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link></mixed-citation></ref><ref id="CR182"><label>182.</label><mixed-citation publication-type="other">Muschelli J. Open Neuroimaging Datasets. 2015. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR183"><label>183.</label><mixed-citation publication-type="other">Reyes M. The HEAR-EU multiscale imaging and modelling dataset of the human inner ear. SMIR. 2017. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.001086" ext-link-type="uri">http://arxiv.org/abs/2003.001086</ext-link>.</mixed-citation></ref><ref id="CR184"><label>184.</label><mixed-citation publication-type="other">Dataset I. Brain development datasets. 2014. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR185"><label>185.</label><mixed-citation publication-type="other">Shattuck DW, Mirza M, Adisetiyo V, Hojatkashani C, Salamon G, et al. Construction of a 3D probabilistic atlas of human cortical structures. NeuroImage. 2008 Feb;39(3):1064–1080.</mixed-citation></ref><ref id="CR186"><label>186.</label><mixed-citation publication-type="other">Gorgolewski KJ, et al. NeuroVault. 2015. Available from: <ext-link xlink:href="http://arxiv.org/abs/2003.00108" ext-link-type="uri">http://arxiv.org/abs/2003.00108</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR187"><label>187.</label><mixed-citation publication-type="other">Boekel W. Neuroimaging informatics tools and resources clearinghouse dataset. 2015. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR188"><label>188.</label><mixed-citation publication-type="other">Poldrack, et al. OPEN fMRI: A multi-subject, multi-modal human neuroimaging dataset. 2015. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR189"><label>189.</label><mixed-citation publication-type="other">Pernet C, Gorgolewski K, Ian W. Neuroimaging dataset of brain tumour patients. 2016. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR190"><label>190.</label><mixed-citation publication-type="other">van Ginneken B, Kerkstra S, Meakin J. DRIVE - Grand Challenge. 2004. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>.</mixed-citation></ref><ref id="CR191"><label>191.</label><mixed-citation publication-type="other">Repository IBS. NITRC: IBSR: Tool/Resource Info. 2007. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR192"><label>192.</label><mixed-citation publication-type="other">Goldbaum M. The STARE Project. 1975. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>.</mixed-citation></ref><ref id="CR193"><label>193.</label><mixed-citation publication-type="other">Cao Z, Chuang M, King JT, Lin CT. Multi-channel EEG recordings during a sustained-attention driving task. Figshare. 2019;Collection. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>.</mixed-citation></ref><ref id="CR194"><label>194.</label><mixed-citation publication-type="other">Picone J. Temple University EEG Corpus. 2011. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 7 April 2020.</mixed-citation></ref><ref id="CR195"><label>195.</label><mixed-citation publication-type="other">GB M, RG M. MIT-BIH Arrhythmia Database. 1999. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR196"><label>196.</label><mixed-citation publication-type="other">Goldberger A, et al. PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. 2003. Circulation. 101(23):e215-e220. Available from: <ext-link xlink:href="10.1007/s12559-020-09751-3" ext-link-type="doi">https://doi.org/10.1007/s12559-020-09751-3</ext-link>.</mixed-citation></ref><ref id="CR197"><label>197.</label><mixed-citation publication-type="other">Khamis H, Weiss R, Xie Y, Chang CW, Lovell NH, Redmond SJ. TELE ECG Database: 250 telehealth ECG records (collected using dry metal electrodes) with annotated QRS and artifact masks, and MATLAB code for the UNSW artifact detection and UNSW QRS detection algorithms. 2016. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>.</mixed-citation></ref><ref id="CR198"><label>198.</label><mixed-citation publication-type="other">2020 BH. BNCIHorizon2020. 2015. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR199"><label>199.</label><mixed-citation publication-type="other">Khushaba RM. Electromyogram (EMG) Repository. 2012. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR200"><label>200.</label><mixed-citation publication-type="other">Rantanen V, et al. Mimetic Interfaces: Facial Surface EMG Dataset 2015. 2015. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR201"><label>201.</label><mixed-citation publication-type="other">Atzori M. NinaPro database non-invasive adaptive hand prosthetics. 2012. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR202"><label>202.</label><mixed-citation publication-type="other">Koelstra S, et al. Database for emotion analysis using physiological signals. 2011. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR203"><label>203.</label><mixed-citation publication-type="other">Abadi MK, et al. MEG-based multimodal database for decoding affective physiological responses. 2007. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR204"><label>204.</label><mixed-citation publication-type="other">HeadIT of University of California SD. Imagined emotion. 2009. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR205"><label>205.</label><mixed-citation publication-type="other">Soleymani M, Lichtenauer J, Pun T, M P. HCI tagging database. 2012. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR206"><label>206.</label><mixed-citation publication-type="other">Lu PBL. SEED Datasets. 2013. Available from: <ext-link xlink:href="http://arxiv.org/abs/1811.07054" ext-link-type="uri">http://arxiv.org/abs/1811.07054</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR207"><label>207.</label><mixed-citation publication-type="other">Kaya M, Binli MK, Ozbay E, Yanar H, Mishchenko Y. A large electroencephalographic motor imagery dataset for electroencephalographic brain computer interfaces. figshare. 2018;Collection. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>.</mixed-citation></ref><ref id="CR208"><label>208.</label><mixed-citation publication-type="other">Cho M H amd Ahn, Ahn S, Kwon M, C JS. Supporting data for EEG datasets for motor imagery brain computer interface. GigaScience Database. 2017. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>.</mixed-citation></ref><ref id="CR209"><label>209.</label><mixed-citation publication-type="other">Schalk G, McFarland DJ, Hinterberger T, Birbaumer N, Wolpaw JR. EEG Motor Movement/Imagery Dataset. 2009. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR210"><label>210.</label><mixed-citation publication-type="other">Korczowski L, Ostaschenko E, Andreev A, Cattan G, Rodrigues PC, Gautheret V, et al. Brain Invaders calibration-less P300-based BCI using dry EEG electrodes Dataset (bi2014a). 2019. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR211"><label>211.</label><mixed-citation publication-type="other">Korczowski L, Cederhout M, Andreev A, Cattan G, Rodrigues PL, Gautheret V, et al. Brain Invaders calibration-less P300-based BCI with modulation of flash duration Dataset (bi2015a). 2019. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR212"><label>212.</label><mixed-citation publication-type="other">Korczowski L, Ostaschenko E, Andreev A, Cattan G, Rodrigues PC, Gautheret V, et al. Brain Invaders Solo versus Collaboration: Multi-User P300-based Brain-Computer Interface Dataset (bi2014b). 2019. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR213"><label>213.</label><mixed-citation publication-type="other">Korczowski L, Cederhout M, Andreev A, Cattan G, Rodrigues PL, Gautheret V, et al. Brain Invaders Cooperative versus Competitive: Multi-User P300-based Brain-Computer Interface Dataset (bi2015b). 2019. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR214"><label>214.</label><mixed-citation publication-type="other">BCI Competitions. BCI Competition datasets. 2008. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR215"><label>215.</label><mixed-citation publication-type="other">BCI Challenge NER2015. BCI Challenge @ NER 2015. 2015. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR216"><label>216.</label><mixed-citation publication-type="other">Broderick MPea. Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech, v3, Dryad; 2020. Available from: <ext-link xlink:href="http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf" ext-link-type="uri">http://www.nesc.ac.uk/technical_papers/UKeS-2006-02.pdf</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR217"><label>217.</label><mixed-citation publication-type="other">for Complex Physiologic Signals RR. Physionet. 1999. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR218"><label>218.</label><mixed-citation publication-type="other">Aha D. UCI ML repository. 1987. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR219"><label>219.</label><mixed-citation publication-type="other">Congedo M, et al. ”Brain Invaders”: a prototype of an open-source P300-based video game working with the OpenViBE platform. In: Proc. BCI 2011; 2011. p. 280–283.</mixed-citation></ref><ref id="CR220"><label>220.</label><mixed-citation publication-type="other">PubChem. PubChem Data Sources. 2020. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR221"><label>221.</label><mixed-citation publication-type="other">PubChem. PubChem Data Sources. 2005. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR222"><label>222.</label><mixed-citation publication-type="other">Biolab. Bioinformatics Laboratory. 1999. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR223"><label>223.</label><mixed-citation publication-type="other">Pradhan S, et al. Indian Genetic Disease Database. 2011. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR224"><label>224.</label><mixed-citation publication-type="other">Atlas TCG. The Cancer Genome Atlas Home Page [nciHome]. 2005. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR225"><label>225.</label><mixed-citation publication-type="other">Network BDT. Berkeley Drosophila Transcription Network Project. 2001. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>.</mixed-citation></ref><ref id="CR226"><label>226.</label><mixed-citation publication-type="other">ENCODE. Encyclopedia of DNA Elements. 2003. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR227"><label>227.</label><mixed-citation publication-type="other">NHLBI GO ESP. Exome Variant Server. 2011. Available from: <ext-link xlink:href="http://www.cellimagelibrary.org/" ext-link-type="uri">http://www.cellimagelibrary.org/</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR228"><label>228.</label><mixed-citation publication-type="other">GEO. Gene Expression Omnibus. 2000. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 April 2020.</mixed-citation></ref><ref id="CR229"><label>229.</label><mixed-citation publication-type="other">Abreu M, et al. gnomAD. 2016. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR230"><label>230.</label><mixed-citation publication-type="other">of MIT TBI, Harvar. GTEx Portal. 2012. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR231"><label>231.</label><mixed-citation publication-type="other">Rouillard AD, Gundersen GW, Fernandez NF, Wang Z, Monteiro CD, McDermott MG, et al. The harmonizome: a collection of processed datasets gathered to serve and mine knowledge about genes and proteins. Database 2016. 2016;07:baw100.</mixed-citation></ref><ref id="CR232"><label>232.</label><mixed-citation publication-type="other">INSDC. The International Nucleotide Sequence Database Collaboration. 2016. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR233"><label>233.</label><mixed-citation publication-type="other">Resource TIGS. 1000 Genomes Project. 2008. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR234"><label>234.</label><mixed-citation publication-type="other">JASPAR. JASPAR 2018: An open-access database of transcription factor binding profiles. 2008. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR235"><label>235.</label><mixed-citation publication-type="other">Consortium NREM. Roadmap Epigenomics Project - Data. 2007. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 6 April 2020.</mixed-citation></ref><ref id="CR236"><label>236.</label><mixed-citation publication-type="other">NSD. Nature Scientific data. 2014. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR237"><label>237.</label><mixed-citation publication-type="other">SysGENSIM. SysGenSIM - Benchmark datasets. 2013. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR238"><label>238.</label><mixed-citation publication-type="other">in Research BMSEB, Education. RCSB Protein Data Bank - RCSB PDB. 2015. Available from: <ext-link xlink:href="http://www.eurobioimaging.eu/" ext-link-type="uri">http://www.eurobioimaging.eu/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR239"><label>239.</label><mixed-citation publication-type="other">Murzin AG, Brenner SE, Hubbard TJP, Chothia C. Structural classification of proteins database 2. 2020. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 10 April 2020.</mixed-citation></ref><ref id="CR240"><label>240.</label><mixed-citation publication-type="other">Fox NK, Brenner SE, Chandonia JM. Structural classification of proteins database - extended. 2018. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 10 April 2020.</mixed-citation></ref><ref id="CR241"><label>241.</label><mixed-citation publication-type="other">Qian N, Sejnowski TJ. UCI Molecular Biology (UCI MB) protein secondary structure data set. 1988. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR242"><label>242.</label><mixed-citation publication-type="other">Schaefer CFAKKSBJDMHTBKH. NCI-Nature pathway interaction database. 2009. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 10 April 2020.</mixed-citation></ref><ref id="CR243"><label>243.</label><mixed-citation publication-type="other">Kandasamy K, et al. NetPath. 2010. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 10 April 2020.</mixed-citation></ref><ref id="CR244"><label>244.</label><mixed-citation publication-type="other">Stein L, D’Eustachio P, Hermjakob H, Wu G. Reactome. 2010. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 10 April 2020.</mixed-citation></ref><ref id="CR245"><label>245.</label><mixed-citation publication-type="other">Tran VD. miRBoost. 2015. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR246"><label>246.</label><mixed-citation publication-type="other">SGD. Saccharomyces Genome Database. 2012. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR247"><label>247.</label><mixed-citation publication-type="other">DNAD-J. DNA Databank of Japan. 1980. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 4 Jan 2020</mixed-citation></ref><ref id="CR248"><label>248.</label><mixed-citation publication-type="other">ENA. European Nucleotide Archive. 1990. Available from: <ext-link xlink:href="http://hapshistology.wikifoundry.com/" ext-link-type="uri">http://hapshistology.wikifoundry.com/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR249"><label>249.</label><mixed-citation publication-type="other">GenBank. GenBank; 2013. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR250"><label>250.</label><mixed-citation publication-type="other">Noordewier MO, Towell GG, Shavlik JW. UCI Molecular Biology (UCI MB) splice-junction gene sequences data set. 1981. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR251"><label>251.</label><mixed-citation publication-type="other">UCI-MB. UCI Molecular Biology (UCI MB) promoter gene sequences data set. 1985. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR252"><label>252.</label><mixed-citation publication-type="other">Manaswi NK. Understanding and working with Keras. In: Deep learning with applications using Python. Springer; 2018. p. 31–43.</mixed-citation></ref><ref id="CR253"><label>253.</label><mixed-citation publication-type="other">Kunkel R, et al. TensorSCONE: a secure TensorFlow framework using Intel SGX. CoRR. 2019. p. 1–12.</mixed-citation></ref><ref id="CR254"><label>254.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Peng</surname><given-names>X</given-names></name><name><surname>Ding</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">Emotional human-machine conversation generation based on long short-term memory</article-title><source>Cogn Comput.</source><year>2018</year><volume>10</volume><issue>3</issue><fpage>389</fpage><lpage>397</lpage></mixed-citation></ref><ref id="CR255"><label>255.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hao</surname><given-names>L</given-names></name><name><surname>Liang</surname><given-names>S</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>Z</given-names></name></person-group><article-title xml:lang="en">TensorD: A tensor decomposition library in TensorFlow</article-title><source>Neurocomputing.</source><year>2018</year><volume>318</volume><fpage>196</fpage><lpage>200</lpage></mixed-citation></ref><ref id="CR256"><label>256.</label><mixed-citation publication-type="other">Karpathy A. A peek at trends in machine learning. 2017. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>.</mixed-citation></ref><ref id="CR257"><label>257.</label><mixed-citation publication-type="other">Bahrampour S, Ramakrishnan N, Schott L, Shah M. Comparative study of deep learning software frameworks. CoRR. 2016;abs/1511.06435. <ext-link xlink:href="http://arxiv.org/abs/1511.06435" ext-link-type="uri">ArXiv: 1511.06435</ext-link>.</mixed-citation></ref><ref id="CR258"><label>258.</label><mixed-citation publication-type="other">Shi S, et al . Benchmarking state-of-the-art deep learning software tools. CoRR. 2016;abs/1608.07249.</mixed-citation></ref><ref id="CR259"><label>259.</label><mixed-citation publication-type="other">Deepmark. The deep learning benchmarks. 2017. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 17 Dec 2017.</mixed-citation></ref><ref id="CR260"><label>260.</label><mixed-citation publication-type="other">Narang S. The source code and experimental data of benchmarking state-of-the-art deep learning software tools. 2017. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 17 Dec 2017.</mixed-citation></ref><ref id="CR261"><label>261.</label><mixed-citation publication-type="other">LeCun Y, Cortes C, Burges CJC. The MNIST database of handwritten digits. 1998. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR262"><label>262.</label><mixed-citation publication-type="other">Zaremba W, Sutskever I, Vinyals O. Recurrent neural network regularization. CoRR. 2014;abs/1409.2329.</mixed-citation></ref><ref id="CR263"><label>263.</label><mixed-citation publication-type="other">Sermanet P, Eigen D, Zhang X, Mathieu M, Fergus R, LeCun Y. OverFeat: integrated recognition, localization and detection using convolutional networks. CoRR. 2013;abs/1312.6229.</mixed-citation></ref><ref id="CR264"><label>264.</label><mixed-citation publication-type="other">Murphy J. Deep learning benchmarks of NVIDIA Tesla P100 PCIe, Tesla K80, and Tesla M40 GPUs; 201. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 4 Jan 2020.</mixed-citation></ref><ref id="CR265"><label>265.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><etal/></person-group><article-title xml:lang="en">Human-level control through deep reinforcement learning</article-title><source>Nature.</source><year>2015</year><volume>518</volume><issue>7540</issue><fpage>529</fpage><lpage>533</lpage></mixed-citation></ref><ref id="CR266"><label>266.</label><mixed-citation publication-type="other">Chollet F. The limitations of deep learning. 2017. Available from: <ext-link xlink:href="https://idr.openmicroscopy.org/" ext-link-type="uri">https://idr.openmicroscopy.org/</ext-link>. Accessed on 12 Dec 2017.</mixed-citation></ref><ref id="CR267"><label>267.</label><mixed-citation publication-type="other">Zenil H, et al . An algorithmic information calculus for causal discovery and reprogramming systems. bioRxiv. 2017;p. 185637.</mixed-citation></ref><ref id="CR268"><label>268.</label><mixed-citation publication-type="other">Shwartz-Ziv R, Tishby N. Opening the black box of deep neural networks via information. CoRR. 2017 Mar;abs/1703.00810.</mixed-citation></ref><ref id="CR269"><label>269.</label><mixed-citation publication-type="other">Nguyen AM, Yosinski J, Clune J. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In: Proc. CVPR; 2015. p. 427–436.</mixed-citation></ref><ref id="CR270"><label>270.</label><mixed-citation publication-type="other">Szegedy C, Zaremba W, Sutskever I, Bruna J, Erhan D, Goodfellow IJ, et al. Intriguing properties of neural networks. In: CoRR. vol. abs/1312.6199; 2013. p. 1–10.</mixed-citation></ref><ref id="CR271"><label>271.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>NA</given-names></name><name><surname>Klemm</surname><given-names>JD</given-names></name><name><surname>Harper</surname><given-names>SL</given-names></name><name><surname>Gaheen</surname><given-names>S</given-names></name><name><surname>Heiskanen</surname><given-names>M</given-names></name><name><surname>Rocca-Serra</surname><given-names>P</given-names></name><etal/></person-group><article-title xml:lang="en">Standardizing data</article-title><source>Nat Nanotechnol.</source><year>2013</year><volume>8</volume><issue>2</issue><fpage>73</fpage></mixed-citation></ref><ref id="CR272"><label>272.</label><mixed-citation publication-type="other">Wittig U, Rey M, Weidemann A, Muller W. Data management and data enrichment for systems biology projects. J Biotechnol. 2017;261:229–237.</mixed-citation></ref><ref id="CR273"><label>273.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmud</surname><given-names>M</given-names></name><name><surname>Rahman</surname><given-names>MM</given-names></name><name><surname>Travalin</surname><given-names>D</given-names></name><name><surname>Raif</surname><given-names>P</given-names></name><name><surname>Hussain</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Service oriented architecture based web application model for collaborative biomedical signal analysis</article-title><source>Biomed Tech (Berl).</source><year>2012</year><volume>57</volume><fpage>780</fpage><lpage>783</lpage></mixed-citation></ref><ref id="CR274"><label>274.</label><mixed-citation publication-type="other">Mahmud M, Pulizzi R, Vasilaki E, Giugliano M. A web-based framework for semi-online parallel processing of extracellular neuronal signals recorded by microelectrode arrays. In: Proc. MEAMEETING; 2014. p. 202–203.</mixed-citation></ref><ref id="CR275"><label>275.</label><mixed-citation publication-type="other">Angelov P, Sperduti A. Challenges in deep learning. In: Proc. ESANN; 2016. p. 489–495.</mixed-citation></ref><ref id="CR276"><label>276.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arulkumaran</surname><given-names>K</given-names></name><name><surname>Deisenroth</surname><given-names>MP</given-names></name><name><surname>Brundage</surname><given-names>M</given-names></name><name><surname>Bharath</surname><given-names>AA</given-names></name></person-group><article-title xml:lang="en">Deep reinforcement learning: a brief survey</article-title><source>IEEE Signal Process Mag.</source><year>2017</year><volume>34</volume><issue>6</issue><fpage>26</fpage><lpage>38</lpage></mixed-citation></ref><ref id="CR277"><label>277.</label><mixed-citation publication-type="other">Noor MB, Zenia NZ, Kaiser MS, Al Mamun S, Mahmud M. Application of deep learning in detecting neurological disorders from magnetic resonance images: a survey on the detection of Alzheimer’s disease, Parkinson’s disease and schizophrenia. Brain Inform. 2020;7(1):1-21.</mixed-citation></ref><ref id="CR278"><label>278.</label><mixed-citation publication-type="other">Al Banna MH, Taher KA, Kaiser MS, Mahmud M, Rahman MS, Hosen AS, Cho GH. Application of artificial intelligence in predicting earthquakes: state-of-the-art and future challenges. IEEE Access. 2020;8:192880–192923.</mixed-citation></ref><ref id="CR279"><label>279.</label><mixed-citation publication-type="other">Jesmin S, Kaiser MS, Mahmud M. Artificial and internet of healthcare things based Alzheimer care during COVID 19. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 263–274.</mixed-citation></ref><ref id="CR280"><label>280.</label><mixed-citation publication-type="other">Ruiz J, Mahmud M, Modasshir M, Shamim Kaiser M, Alzheimer’s disease neuroimaging initiative for the 3D DenseNet ensemble in 4-way classification of Alzheimer’s disease. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 85–96.</mixed-citation></ref><ref id="CR281"><label>281.</label><mixed-citation publication-type="other">Rahman S, Sharma T, Mahmud M. Improving alcoholism diagnosis: comparing instance-based classifiers against neural networks for classifying EEG signal. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 239–50. </mixed-citation></ref><ref id="CR282"><label>282.</label><mixed-citation publication-type="other">Nahiduzzaman Md, Tasnim M, Newaz NT, Kaiser MS, Mahmud M. Machine learning based early fall detection for elderly people with neurological disorder using multimodal data fusion. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 204–14.</mixed-citation></ref><ref id="CR283"><label>283.</label><mixed-citation publication-type="other">Farah L, Hussain A, Kerrouche A, Ieracitano C, Ahmad J, Mahmud M. A highly-efficient fuzzy-based controller with high reduction inputs and membership functions for a grid-connected photovoltaic system. IEEE Access. 2020;8:163225–37. </mixed-citation></ref><ref id="CR284"><label>284.</label><mixed-citation publication-type="other">Fabietti M, Mahmud M, Lotfi A. Machine learning in analysing invasively recorded neuronal signals: available open access data sources. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 151–62.</mixed-citation></ref><ref id="CR285"><label>285.</label><mixed-citation publication-type="other">Chen L, Yan J, Chen J, Sheng Y, Xu Z, Mahmud M. An event based topic learning pipeline for neuroimaging literature mining. 2020 Oct 23 [cited 2020 Nov 22]; Available from: <ext-link xlink:href="https://www.researchsquare.com/article/rs-95392/v1" ext-link-type="uri">https://www.researchsquare.com/article/rs-95392/v1</ext-link></mixed-citation></ref><ref id="CR286"><label>286.</label><mixed-citation publication-type="other">Nahian MJA, Ghosh T, Uddin MN, Islam MdM, Mahmud M, Kaiser MS. Towards artificial intelligence driven emotion aware fall monitoring framework suitable for elderly people with neurological disorder. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 275–86.</mixed-citation></ref><ref id="CR287"><label>287.</label><mixed-citation publication-type="other">Al Banna MdH, Ghosh T, Taher KA, Kaiser MS, Mahmud M. A monitoring system for patients of autism spectrum disorder using artificial intelligence. In: Mahmud M, Vassanelli S, Kaiser MS, Zhong N, editors. Brain Inform. Cham: Springer International Publishing; 2020. p. 251–62. </mixed-citation></ref><ref id="CR288"><label>288.</label><mixed-citation publication-type="other">Adiba FI, Islam T, Kaiser MS, Mahmud M, Rahman MA. Effect of corpora on classification of fake news using naive Bayes classifier. International Journal of Automation, Artificial Intelligence and Machine Learning. 2020;1(1):80–92.</mixed-citation></ref><ref id="CR289"><label>289.</label><mixed-citation publication-type="other">Sumi AI, Zohora MostF, Mahjabeen M, Faria TJ, Mahmud M, Kaiser MS. fASSERT: A fuzzy assistive system for children with autism using internet of things. In: Wang S, Yamamoto V, Su J, Yang Y, Jones E, Iasemidis L, et al., editors. Brain Inform. Cham: Springer International Publishing; 2018. p. 403–12. </mixed-citation></ref><ref id="CR290"><label>290.</label><mixed-citation publication-type="other">Mahmud M, Kaiser MS, Rahman MM, Rahman MA, Shabut A, Al-Mamun S, et al. A brain-inspired trust management model to assure security in a cloud based IoT framework for neuroscience applications. Cogn Comput. 2018 Oct 1;10(5):864–73.</mixed-citation></ref><ref id="CR291"><label>291.</label><mixed-citation publication-type="other">Kaiser MS, Chowdhury ZI, Mamun SA, Hussain A, Mahmud M. A neuro-fuzzy control system based on feature extraction of surface electromyogram signal for solar-powered wheelchair. Cogn Comput. 2016 Oct 1;8(5):946–54.</mixed-citation></ref></ref-list></ref-list><fn-group><fn id="Fn1"><label>1</label><p id="Par250"><ext-link xlink:href="https://trends.google.com/" ext-link-type="uri">https://trends.google.com</ext-link></p></fn></fn-group></back></article></records><facets><facet name="subject"><facet-value count="1">Artificial Intelligence</facet-value><facet-value count="1">Computation by Abstract Devices</facet-value><facet-value count="1">Computational Biology/Bioinformatics</facet-value><facet-value count="1">Computer Science</facet-value></facet><facet name="keyword"><facet-value count="1">Bioimaging</facet-value><facet-value count="1">Brain–Machine Interfaces</facet-value><facet-value count="1">Deep learning performance comparison</facet-value><facet-value count="1">Medical imaging</facet-value><facet-value count="1">Omics</facet-value><facet-value count="1">Open access data sources</facet-value><facet-value count="1">Open-source tools</facet-value></facet><facet name="pub"><facet-value count="1">Cognitive Computation</facet-value></facet><facet name="year"><facet-value count="1">2021</facet-value></facet><facet name="country"><facet-value count="1">Bangladesh</facet-value><facet-value count="1">United Kingdom</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
