<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1007/978-3-030-60910-8_3</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><book-part-wrapper dtd-version="2.0" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><collection-meta collection-type="series"><collection-id collection-id-type="publisher-id">8563</collection-id><title-group><title>Embedded Systems</title><alt-title>Embedded Systems</alt-title></title-group><contrib-group content-type="collaborators"><contrib contrib-type="Series Editor"><name><surname>Dutt</surname><given-names>Nikil D.</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="Series Editor"><name><surname>Martin</surname><given-names>Grant</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="Series Editor"><name><surname>Marwedel</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="Aff3">3</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap/><addr-line content-type="city">Irvine</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff2"><label>2</label><institution-wrap/><addr-line content-type="city">Santa Clara</addr-line><addr-line content-type="state">CA</addr-line><country country="US">USA</country></aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="GRID">grid.5675.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0416 9637</institution-id><institution content-type="org-division">Informatik 12</institution><institution content-type="org-name">TU Dortmund</institution></institution-wrap><addr-line content-type="city">Dortmund</addr-line><country country="DE">Germany</country></aff></contrib-group><issn publication-format="print">2193-0155</issn><issn publication-format="electronic">2193-0163</issn></collection-meta><book-meta><book-id book-id-type="doi">10.1007/978-3-030-60910-8</book-id><book-id book-id-type="publisher-id">978-3-030-60910-8</book-id><book-id book-id-type="book-title-id">106758</book-id><book-title-group><book-title>Embedded System Design</book-title><subtitle>Embedded Systems Foundations of Cyber-Physical Systems, and the Internet of Things</subtitle></book-title-group><contrib-group><contrib contrib-type="author"><name><surname>Marwedel</surname><given-names>Peter</given-names><prefix>Prof. Dr.</prefix></name><address><email>peter.marwedel@tu-dortmund.de</email></address><xref ref-type="aff" rid="Aff4">4</xref></contrib><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="GRID">grid.5675.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0416 9637</institution-id><institution content-type="org-name">TU Dortmund</institution></institution-wrap><addr-line content-type="city">Dortmund</addr-line><country country="DE">Germany</country></aff></contrib-group><pub-date date-type="pub"><string-date>2021-01-01</string-date></pub-date><isbn content-type="ppub">978-3-030-60909-2</isbn><isbn content-type="epub">978-3-030-60910-8</isbn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher><edition>4th ed. 2021</edition><permissions><copyright-statement content-type="compact">© The Editor(s) (if applicable) and The Author(s) 2021</copyright-statement><copyright-statement content-type="comment">This book is an open access publication.</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Editor(s) (if applicable) and The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This book is licensed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</license-p><license-p>The images or other third party material in this book are included in the book's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the book's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</license-p></license></permissions><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>book-chapter-count</meta-name><meta-value>9</meta-value></custom-meta><custom-meta><meta-name>media-type</meta-name><meta-value>eBook</meta-value></custom-meta><custom-meta><meta-name>contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>book-product-type</meta-name><meta-value>Undergraduate textbook</meta-value></custom-meta><custom-meta><meta-name>book-subject-primary</meta-name><meta-value>Engineering</meta-value></custom-meta><custom-meta><meta-name>book-subject-secondary</meta-name><meta-value>Circuits and Systems</meta-value></custom-meta><custom-meta><meta-name>book-subject-secondary</meta-name><meta-value>Cyber-physical systems, IoT</meta-value></custom-meta><custom-meta><meta-name>book-subject-secondary</meta-name><meta-value>Professional Computing</meta-value></custom-meta><custom-meta><meta-name>book-subject-secondary</meta-name><meta-value>Processor Architectures</meta-value></custom-meta><custom-meta><meta-name>book-subject-collection</meta-name><meta-value>Engineering</meta-value></custom-meta></custom-meta-group></book-meta><book-part book-part-type="chapter" id="b978-3-030-60910-8_3" seq="3"><book-part-meta><book-part-id book-part-id-type="doi">10.1007/978-3-030-60910-8_3</book-part-id><book-part-id book-part-id-type="chapter">3</book-part-id><title-group><label>Chapter 3</label><title>Embedded System Hardware</title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Marwedel</surname><given-names>Peter</given-names></name><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="GRID">grid.5675.1</institution-id><institution-id institution-id-type="ISNI">0000 0001 0416 9637</institution-id><institution content-type="org-name">TU Dortmund</institution></institution-wrap><addr-line content-type="city">Dortmund</addr-line><country country="DE">Germany</country></aff></contrib-group><pub-date date-type="pub" publication-format="electronic"><day>26</day><month>01</month><year>2021</year></pub-date><fpage>127</fpage><lpage>201</lpage><pub-history><date date-type="registration"><day>17</day><month>09</month><year>2020</year></date><date date-type="online"><day>26</day><month>01</month><year>2021</year></date></pub-history><permissions><copyright-statement content-type="compact">© The Author(s) 2021</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This chapter is licensed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license and indicate if changes were made.</license-p><license-p>The images or other third party material in this chapter are included in the chapter's Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the chapter's Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder.</license-p></license></permissions><abstract xml:lang="en" id="Abs1" specific-use="web-only"><title>Abstract</title><p id="Par1">In this chapter, we will present the interface between the physical environment and information processing (the <bold>cyphy-interface</bold>) together with the hardware required for processing, storing, and communicating information. Due to considering CPS, covering the cyphy-interface is indispensable. The need to cover other hardware components as well is a consequence of their impact on the performance, timing characteristics, power consumption, safety, and security.</p></abstract><custom-meta-group><custom-meta><meta-name>chapter-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>chapter-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/978-3-030-60910-8_Chapter_3.pdf</meta-value></custom-meta><custom-meta><meta-name>output-medium</meta-name><meta-value>Online</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta></custom-meta-group></book-part-meta><body><p id="Par2"><index-term id="ITerm1"><term>embedded system</term><index-term><term>hardware</term></index-term></index-term></p><p id="Par3">In this chapter, we will present the interface between the physical environment and information processing (the <bold>cyphy-interface</bold>) together with the hardware required for processing, storing, and communicating information. Due to considering CPS, covering the cyphy-interface is indispensable. The need to cover other hardware components as well is a consequence of their impact on the performance, timing characteristics, power consumption, safety, and security.</p><p id="Par4">Regarding the cyphy-interface, we will present circuits for sampling and digitization of physical quantities as well as for the reverse process. We will present the sampling theorem and its impact. Regarding information processing, we will provide details of efficient hardware, in particular of digital signal processors, general-purpose computing on graphics processors, multi-core systems, and field programmable gate arrays (FPGAs). With respect to information storage, we will explain the memory hierarchy as it is used in embedded systems. We will also explain if and how existing communication technologies can be used.</p><p id="Par5">Electronic information processing requires electrical energy. Accordingly, this chapter includes a section on the generation (e.g., harvesting), storage, and efficient use of electrical energy in embedded systems, including battery and energy consumption models. This chapter closes with a survey on the challenges of supporting security in hardware.</p><sec id="Sec1"><title>Introduction</title><p id="Par6"><index-term id="ITerm2"><term>platform-based design</term></index-term></p><p id="Par7">Frequently, hardware designs are reused, either in the form of real hardware components or in the form of intellectual property (IP). The reuse of available hard- and software components is at the heart of the <bold>platform-based design methodology</bold> (see also p. 296).<index-term id="ITerm3"><term>platform-based design</term></index-term> This methodology is seen as a key method for mastering the growing complexity of embedded systems. Consistent with the need to consider available hardware components and with the design information flow shown in Fig. <xref rid="Fig1" ref-type="fig">3.1</xref>, we are now going to describe some of the essentials of embedded system hardware.<fig id="Fig1"><label>Fig. 3.1</label><caption xml:lang="en"><p>Simplified design information flow</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig1_HTML.png" id="MO1"/></fig></p><p id="Par8">Hardware for embedded systems is much less standardized than hardware for personal computers. Due to the huge variety of embedded system hardware, it is impossible to provide a comprehensive overview of all types of hardware components. Nevertheless, we will try to provide a survey of some of the essential components which can be found in most systems. In many cyber-physical systems, especially in control systems, hardware is used in a loop (see Fig. <xref rid="Fig2" ref-type="fig">3.2</xref>).<index-term id="ITerm4"><term>hardware in the loop</term></index-term><index-term id="ITerm5"><term>cyber-physical system (CPS)</term></index-term> We will use this loop to structure the presentation of components in this chapter. In this (control) loop, information about the physical environment is made available through <bold>sensors</bold>.<index-term id="ITerm6"><term>sensor</term></index-term><index-term id="ITerm7"><term>control loop</term></index-term> Typically, sensors generate continuous sequences of analog values. In this book, we will restrict ourselves to information processing where digital computers process discrete sequences of values. Appropriate conversions are performed by two kinds of circuits: sample-and-hold circuits and analog-to-digital converters (ADCs). <index-term id="ITerm8"><term>analog-to-digital converter (ADC)</term></index-term> After such conversion, information can be processed digitally. Generated results can be displayed and also be used to control the physical environment through actuators. Since many actuators are analog actuators, conversion from digital to analog signals may also be needed. We will see how this conversion can be achieved either by digital-to-analog converters (DACs) or indirectly by pulse-width modulation (PWM).<index-term id="ITerm9"><term>digital-to-analog converter (DAC)</term></index-term><fig id="Fig2"><label>Fig. 3.2</label><caption xml:lang="en"><p>Hardware in the loop</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig2_HTML.png" id="MO2"/></fig></p><p id="Par9">Due to the prevailing <italic>electronic</italic> information processing, we assume that we require electrical energy. Some source of this energy must be available. If our energy source does not provide energy permanently, we may need to store energy, e.g., in rechargeable batteries or capacitors. During system operation, much of the electrical energy will be converted into thermal energy (heat). It may be necessary to remove thermal energy from the system.</p><p id="Par10">This model is obviously appropriate for control applications. For other applications, it can be employed as a first-order approximation. In the following, we will describe essential hardware components of embedded and cyber-physical systems following the structure of Fig. <xref rid="Fig2" ref-type="fig">3.2</xref>.</p></sec><sec id="Sec2"><title>Input: Interface Between Physical and Cyber-World</title><sec id="Sec3"><title>Sensors</title><sec><p id="Par11"><index-term id="ITerm10"><term>sensor</term></index-term></p></sec><sec><p id="Par12">Sensors are key components of the cyphy-interface. <index-term id="ITerm11"><term>cyphy-interface</term></index-term> Sensors can be designed for virtually every physical quantity. There are sensors for weight, velocity, acceleration, electrical current, voltage, temperature, etc. A wide variety of physical effects can be exploited in the construction of sensors [<xref ref-type="bibr" rid="CR151">151</xref>]. Examples include the law of induction (generation of voltages in an electric field) and photoelectric effects. There are also sensors for chemical substances [<xref ref-type="bibr" rid="CR152">152</xref>].</p></sec><sec><p id="Par13">Recent years have seen the design of a huge range of sensors, and much of the progress in designing smart systems can be attributed to modern sensor technology. The availability of sensors has enabled the design of sensor networks (see, e.g., Tiwari et al. [<xref ref-type="bibr" rid="CR543">543</xref>]), a key element of the Internet of Things. <index-term id="ITerm12"><term>Internet of Things (IoT)</term></index-term><index-term id="ITerm13"><term>sensor network</term></index-term> It is impossible to cover this subset of cyber-physical hardware technology comprehensively, and we can only give characteristic examples: <list list-type="bullet"><list-item><p id="Par14"><bold>Acceleration sensors:</bold> Figure <xref rid="Fig3" ref-type="fig">3.3</xref> shows a small sensor manufactured using microsystem technology. The sensor contains a small mass in its center. When accelerated, the mass will be displaced from its standard position, thereby changing the resistance of the tiny wires connected to the mass. <index-term id="ITerm14"><term>sensor</term><index-term><term>acceleration ˜</term></index-term></index-term><fig id="Fig3"><label>Fig. 3.3</label><caption xml:lang="en"><p>Acceleration sensor (courtesy S. Büttgenbach, IMT, TU Braunschweig), ⒸTU Braunschweig, Germany</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/106758_4_En_3_Fig3_HTML.jpg" id="MO3"/></fig></p><p id="Par15">Acceleration sensors are included in the powerful inertial measurement units (IMUs) (see, e.g., Siciliano et al. [<xref ref-type="bibr" rid="CR487">487</xref>], Section 20.4).<index-term id="ITerm15"><term>Inertial Measurement Unit (IMU)</term></index-term> They contain gyros and accelerometers, and they capture up to six degrees of freedom, comprising position (<italic>x</italic>, <italic>y</italic>, and <italic>z</italic>) and orientation (roll, pitch, and yaw) [<xref ref-type="bibr" rid="CR575">575</xref>]. They are contained in airplanes, cars, robots, and other products in order to provide inertial navigation.</p></list-item><list-item><p id="Par16"><bold>Image sensors:</bold><index-term id="ITerm16"><term>sensor</term><index-term><term>image ˜</term></index-term></index-term> There are essentially two kinds of image sensors: charge-coupled devices (CCDs)<index-term id="ITerm17"><term>charge-coupled devices (CCD)</term></index-term> and CMOS sensors. In both cases, arrays of light sensors are used. The architecture of CMOS sensor arrays is similar to that of standard memories: individual pixels can be randomly addressed and read out. CMOS sensors use standard CMOS technology for integrated circuits. Due to this, sensors and logic circuits can be integrated on the same chip. This allows some preprocessing to be done already on the sensor chip, leading to so-called smart sensors. CMOS sensors require only a single standard supply voltage and interfacing in general is easy. Therefore, CMOS-based sensors can be cheap.</p><p id="Par17">In contrast, CCD technology is optimized for optical applications. In CCD technology, charges must be transferred from one pixel to the next until they can finally be read out at an array boundary. This sequential charge transfer also gave CCDs their name. For CCD sensors, interfacing is more complex.</p><p id="Par18">Selecting the most appropriate image sensor depends on several constraints, which change as technology evolves. The image quality of CMOS sensors has been improved over the recent years, and the initial image superiority of CCDs became questionable. Therefore, achieving a good image quality is feasible with CCD and with CMOS sensors. Due to their faster readout speed, CMOS sensors are preferred for cameras with live view modes or video recording functionality [<xref ref-type="bibr" rid="CR404">404</xref>]. Also, CMOS sensors are preferred for low-cost devices and if smart sensors are to be designed. Several application areas for CCDs have disappeared, but they are still used in areas such as scientific image acquisition.</p></list-item><list-item><p id="Par19"><bold>Biometric sensors:</bold><index-term id="ITerm18"><term>sensor</term><index-term><term>bio-metrical ˜</term></index-term></index-term> Demands for higher security standards as well as the need to protect mobile and removable equipment have led to an increased interest in authentication. Due to the limitations of password-based security (e.g., stolen and lost passwords), biometric sensors and biomedical authentication receive attention. Biometric authentication tries to identify whether or not a certain person is actually the person she or he claims to be. Methods for biometric authentication include iris scans, fingerprint sensors, and face recognition. False accepts as well as false rejects are an inherent problem of biometric authentication (see definitions on p. 257). In contrast to password-based authentication, exact matches are not possible.</p></list-item><list-item><p id="Par20"><bold>Artificial eyes</bold>:<index-term id="ITerm19"><term>artificial eye</term></index-term> Artificial eye projects have received significant attention. Some projects have an impact on the eye, but others provide vision in an indirect way. For example, the Dobelle Institute experimented with a camera attached to a computer sending electrical pulses to a direct brain contact [<xref ref-type="bibr" rid="CR532">532</xref>]. More recently, the less invasive translation of images into audio has been preferred.</p></list-item><list-item><p id="Par21"><bold>Radio frequency identification (RFID)</bold>: RFID <index-term id="ITerm20"><term>radio frequency identification (RFID)</term></index-term> technology is based on the response of a <bold>tag</bold> to radio frequency signals [<xref ref-type="bibr" rid="CR226">226</xref>]. The tag consists of an integrated circuit and an antenna, and it provides its identification to <bold>RFID readers</bold>. The maximum distance between tags and readers depends on the type of the tag. The technology is used to identify objects, animals, or people and is a key enabler for the Internet of Things. <index-term id="ITerm21"><term>Internet of Things (IoT)</term></index-term></p></list-item><list-item><p id="Par22"><bold>Automotive sensors:</bold> Today’s cars contain a large number of sensors. This includes rain sensors, tire pressure sensors, collision sensors, etc. The overall goal is to provide comfort and safety to the passengers and the environment.</p></list-item><list-item><p id="Par23"><bold>Other sensors:</bold> Other common sensors include thermal sensors, engine control sensors, Hall effect sensors, and many more.</p></list-item></list></p></sec><sec><p id="Par24">Machine learning algorithms [<xref ref-type="bibr" rid="CR188">188</xref>, <xref ref-type="bibr" rid="CR204">204</xref>, <xref ref-type="bibr" rid="CR453">453</xref>, <xref ref-type="bibr" rid="CR560">560</xref>] may need to be used to obtain meaningful information from noisy sensor readouts.</p></sec><sec><p id="Par25">Sensors are generating <bold>signals</bold>.<index-term id="ITerm22"><term>signal</term></index-term> Mathematically, the following definition applies:</p></sec><sec id="FPar1"><title>Definition 3.1</title><p id="Par26">A <bold>signal</bold><italic>σ</italic> is a mapping from a time domain <italic>D</italic><sub><italic>T</italic></sub> to a value domain <italic>D</italic><sub><italic>V</italic></sub>: <disp-formula id="Equa"><alternatives><mml:math id="Equa_Math"><mml:mtable columnalign="right left"><mml:mtr><mml:mtd class="align-1"><mml:mi>σ</mml:mi><mml:mo>:</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equa_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \sigma: D_T \rightarrow D_V {} \end{aligned}$$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equa.gif"/></alternatives></disp-formula></p></sec><sec><p id="Par27">Signals may be defined over a continuous or a discrete time domain as well as over a continuous or a discrete value domain.</p></sec></sec><sec id="Sec4"><title>Discretization of Time: Sample-and-Hold Circuits</title><p id="Par28"><index-term id="ITerm23"><term>sample-and-hold circuit</term></index-term></p><p id="Par29">All known digital computers work in a <bold>discrete</bold> time domain <italic>D</italic><sub><italic>T</italic></sub>. This means that they can process discrete sequences or <bold>streams</bold> of values. Hence, incoming signals over the continuous time domain must be converted to signals over the discrete time domain. This is the purpose of <bold>sample-and-hold circuits</bold>. These are included in the cyphy-interface. <index-term id="ITerm24"><term>cyphy-interface</term></index-term> Figure <xref rid="Fig4" ref-type="fig">3.4</xref> (left) shows a simple sample-and-hold circuit. In essence, the circuit consists of a clocked transistor and a capacitor. <index-term id="ITerm25"><term>capacitor</term></index-term> The transistor operates like a switch. Each time the switch is closed by the clock signal, the capacitor is charged so that its voltage <italic>h</italic>(<italic>t</italic>) is practically the same as the incoming voltage <italic>e</italic>(<italic>t</italic>). After opening the switch again, this voltage will remain essentially unchanged until the switch is closed again. Each of the values stored on the capacitor can be considered as an element of a discrete sequence of values <italic>h</italic>(<italic>t</italic>), generated from a continuous function <italic>e</italic>(<italic>t</italic>) (see Fig. <xref rid="Fig4" ref-type="fig">3.4</xref> (right)). If we sample <italic>e</italic>(<italic>t</italic>) at times {<italic>t</italic><sub><italic>s</italic></sub>}, then <italic>h</italic>(<italic>t</italic>) will be defined only at those times.<fig id="Fig4"><label>Fig. 3.4</label><caption xml:lang="en"><p>Sample-and-hold phase: <bold>left</bold>, circuit; <bold>right</bold>, signals</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig4_HTML.png" id="MO4"/></fig></p><p id="Par30">An ideal sample-and-hold circuit would be able to change the voltage at the capacitor in an arbitrarily short amount of time. This way, the input voltage at a particular instance in time could be transferred to the capacitor, and each element in the discrete sequence would correspond to the input voltage at a particular point in time. In practice, however, the transistor has to be kept closed for a short time window in order to really charge or discharge the capacitor. The voltage stored on the capacitor will then correspond to a voltage reflecting that short time window.</p></sec><sec id="Sec5"><title>Fourier Approximation of Signals</title><sec><p id="Par31">Would we be able to reconstruct the original signal <italic>e</italic>(<italic>t</italic>) from the sampled signal <italic>h</italic>(<italic>t</italic>)? In order to answer this question, we revert to the fact that arbitrary signals can be approximated by summing (possibly phase-shifted) sine functions of different frequencies (Fourier approximation).<xref ref-type="fn" rid="Fn1">1</xref></p></sec><sec id="FPar2"><title>Example 3.1</title><p id="Par33">A square wave can be approximated by Eq. (<xref rid="Equ1" ref-type="disp-formula">3.1</xref>) [<xref ref-type="bibr" rid="CR440">440</xref>]: <disp-formula id="Equ1"><label>3.1</label><alternatives><mml:math id="Equ1_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msubsup><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:munderover accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo>,</mml:mo><mml:mn>9</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mi>k</mml:mi></mml:mrow></mml:mfrac><mml:mo>sin</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} e^{\prime}_K(t) &amp;\displaystyle =&amp;\displaystyle \sum_{k=1,3,5,7,9,\ldots}^K \left( \frac{4}{\pi k} \sin{}(\frac{2\pi kt}{T}) \right) {} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ1.gif"/></alternatives></disp-formula> In this equation, <italic>T</italic> is the period and approximation is improved for increasing <italic>K</italic>. Figures <xref rid="Fig5" ref-type="fig">3.5</xref> and <xref rid="Fig6" ref-type="fig">3.6</xref> visualize Eq. (<xref rid="Equ1" ref-type="disp-formula">3.1</xref>).<fig id="Fig5"><label>Fig. 3.5</label><caption xml:lang="en"><p>Approximation of a square wave by sine waves for <italic>K</italic> = 1 (left) and <italic>K</italic> = 3 (right)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig5_HTML.png" id="MO5"/></fig><fig id="Fig6"><label>Fig. 3.6</label><caption xml:lang="en"><p>Approximation of a square wave by sine waves for <italic>K</italic> = 7 (left) and <italic>K</italic> = 11 (right)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig6_HTML.png" id="MO6"/></fig></p><p id="Par34"><index-term id="ITerm27"><term>sine wave</term></index-term></p><p id="Par35">The larger difference between the square wave and its approximation at the jump discontinuities of the square wave (best visible for <italic>K</italic>=11) is called <bold>Gibbs phenomenon</bold> [<xref ref-type="bibr" rid="CR440">440</xref>]. <index-term id="ITerm28"><term>Gibbs Phenomenon</term></index-term> ∇</p></sec><sec id="FPar3"><title>Definition 3.2</title><p id="Par36"><index-term id="ITerm29"><term>linear transformation</term></index-term> A signal transformation <italic>Tr</italic> is <bold>linear</bold> if for all signals <italic>e</italic><sub>1</sub>(<italic>t</italic>) and <italic>e</italic><sub>2</sub>(<italic>t</italic>) we have <disp-formula id="Equ2"><label>3.2</label><alternatives><mml:math id="Equ2_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} Tr(e_1+e_2) = Tr(e_1) + Tr(e_2) {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ2.gif"/></alternatives></disp-formula></p></sec><sec><p id="Par37">Next, we restrict ourselves to linear systems. Then, in order to answer the question raised above, we study sampling each of the sine waves independently.</p></sec><sec id="FPar4"><title>Example 3.2</title><p id="Par38">Consider signals described by either of the two functions <italic>e</italic><sub>3</sub> or <italic>e</italic><sub>4</sub>: <disp-formula id="Equ3"><label>3.3</label><alternatives><mml:math id="Equ3_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mo>sin</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>sin</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} e_3(t) &amp;\displaystyle =&amp;\displaystyle \sin{}(\frac{2\pi t}{8})+ 0.5 \sin\left(\frac{2\pi t}{4}\right) {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ3.gif"/></alternatives></disp-formula><disp-formula id="Equ4"><label>3.4</label><alternatives><mml:math id="Equ4_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mo>sin</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>sin</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn><mml:mo>sin</mml:mo><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} e_4(t) &amp;\displaystyle =&amp;\displaystyle \sin\left(\frac{2\pi t}{8}\right)+ 0.5 \sin\left(\frac{2\pi t}{4}\right)+ 0.5 \sin\left(\frac{2\pi t}{1}\right) \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ4.gif"/></alternatives></disp-formula> The sine waves used in these functions have periods of <italic>T</italic> = 8, 4, and 1, respectively (this can be seen by comparing these sine waves with those of Eq. (<xref rid="Equ1" ref-type="disp-formula">3.1</xref>)). A graphical representation of these functions is shown in Fig. <xref rid="Fig7" ref-type="fig">3.7</xref>. Suppose that we will be sampling these signals at integer times.<index-term id="ITerm30"><term>sampling</term></index-term> It then so happens that both signals have the same value whenever they are sampled. Obviously, it is not possible to distinguish between <italic>e</italic><sub>3</sub>(<italic>t</italic>) and <italic>e</italic><sub>4</sub>(<italic>t</italic>) if we sample at these instances in time and if only the sampled signal is available. ∇<fig id="Fig7"><label>Fig. 3.7</label><caption xml:lang="en"><p>Visualization of functions <italic>e</italic><sub>3</sub>(<italic>t</italic>) (blue) and <italic>e</italic><sub>4</sub>(<italic>t</italic>) (red)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig7_HTML.png" id="MO7"/></fig></p></sec><sec><p id="Par39">In general, sampled signals will not allow us to distinguish between some slow signal <italic>e</italic><sub>3</sub>(<italic>t</italic>) and some other faster varying signal <italic>e</italic><sub>4</sub>(<italic>t</italic>) if <italic>e</italic><sub>3</sub>(<italic>t</italic>) and <italic>e</italic><sub>4</sub>(<italic>t</italic>) are identical each time we are sampling the signals. The fact that two or more unsampled signals can have the same sampled representation is called <bold>aliasing</bold>.<index-term id="ITerm31"><term>aliasing</term></index-term> We are not sampling <italic>e</italic><sub>4</sub>(<italic>t</italic>) frequently enough to notice, for example, that it has slope changes between integer times. So, from this counterexample we can conclude that <bold>reconstruction of the original unsampled signal is not feasible unless we have additional knowledge about the frequencies or the waveforms present in the input signal</bold>.</p></sec><sec><p id="Par40">How frequently do we have to sample signals to be able to distinguish between different sine waves? Let us assume that we are sampling the input signal at constant time intervals, such that <italic>T</italic><sub><italic>s</italic></sub> is the <bold>sampling period</bold>: <index-term id="ITerm32"><term>sampling period</term></index-term><disp-formula id="Equ5"><label>3.5</label><alternatives><mml:math id="Equ5_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mo>∀</mml:mo><mml:mi>s</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>:</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} \forall s&amp;\displaystyle :&amp;\displaystyle T_s = t_{s+1} -t_s \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ5.gif"/></alternatives></disp-formula> Let <disp-formula id="Equ6"><label>3.6</label><alternatives><mml:math id="Equ6_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} f_s&amp;\displaystyle =&amp;\displaystyle \frac{1}{T_s} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ6.gif"/></alternatives></disp-formula> be the <bold>sampling rate</bold> or <bold>sampling frequency</bold>. <index-term id="ITerm33"><term>sampling rate</term></index-term> Then, sampling theory provides us with the following theorem (see, e.g., [<xref ref-type="bibr" rid="CR440">440</xref>]):</p></sec><sec id="FPar5"><title>Theorem 3.1 (Sampling Theorem)</title><p id="Par41"><italic>Given the above definitions of variables,</italic><bold><italic>aliasing is avoided if we restrict the frequencies of the incoming signal to less than half of the sampling frequency</italic></bold><italic>f</italic><sub><italic>s</italic></sub>: <index-term id="ITerm34"><term>sampling</term></index-term><index-term id="ITerm35"><term>aliasing</term></index-term><disp-formula id="Equ7"><label>3.7</label><alternatives><mml:math id="Equ7_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>&lt;</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mfrac><mml:mspace width="0.3em"/><mml:mtext mathvariant="italic">where</mml:mtext><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mtext mathvariant="italic">is the period of the “fastest” sine wave, or</mml:mtext></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} T_s &amp;\displaystyle &lt; &amp;\displaystyle \frac{T_{N}}{2} \mathit{\mbox{ where }} T_{N} \mathit{\mbox{ is the period of the ``fastest'' sine wave, or}} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ7.gif"/></alternatives></disp-formula></p><p id="Par42"><disp-formula id="Equ8"><label>3.8</label><alternatives><mml:math id="Equ8_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>&gt;</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>2</mml:mn><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mtext mathvariant="italic">where</mml:mtext><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:mtext mathvariant="italic">is the frequency of the “fastest” sine wave</mml:mtext></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} f_s &amp;\displaystyle &gt; &amp;\displaystyle 2 f_N \mathit{\mbox{ where }} f_N \mathit{\mbox{ is the frequency of the ``fastest'' sine wave}} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ8.gif"/></alternatives></disp-formula></p></sec><sec id="FPar6"><title>Definition 3.3</title><p id="Par43"><italic>f</italic><sub><italic>N</italic></sub> is called the Nyquist frequency; <italic>f</italic><sub><italic>s</italic></sub> is the sampling rate. <index-term id="ITerm36"><term>Nyquist frequency</term></index-term><index-term id="ITerm37"><term>sampling rate</term></index-term></p></sec><sec><p id="Par44">The condition in Eq. (<xref rid="Equ8" ref-type="disp-formula">3.8</xref>) is called <bold>sampling criterion</bold>, and sometimes the <bold>Nyquist sampling criterion</bold>.<index-term id="ITerm38"><term>sampling criterion</term></index-term></p></sec><sec><p id="Par45">Therefore, reconstruction of input signals <italic>e</italic>(<italic>t</italic>) from discrete samples <italic>h</italic>(<italic>t</italic>) can be successful only if we make sure that higher-frequency components such as the one in <italic>e</italic><sub>4</sub>(<italic>t</italic>) are removed. This is the purpose of anti-aliasing filters. Anti-aliasing filters are placed in front of the sample-and-hold circuit (see Fig. <xref rid="Fig8" ref-type="fig">3.8</xref>).<index-term id="ITerm39"><term>anti-aliasing</term></index-term><fig id="Fig8"><label>Fig. 3.8</label><caption xml:lang="en"><p>Anti-aliasing placed in front of the sample-and-hold circuit</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig8_HTML.png" id="MO8"/></fig></p></sec><sec><p id="Par46">Figure <xref rid="Fig9" ref-type="fig">3.9</xref> demonstrates the ratio between the amplitudes of the output and the input waves as a function of the frequency for this filter. Ideally, such a filter would remove all frequencies at and above half the sampling frequency and keep all other components unchanged. This way, it would convert signal <italic>e</italic><sub>4</sub>(<italic>t</italic>) into signal <italic>e</italic><sub>3</sub>(<italic>t</italic>).<fig id="Fig9"><label>Fig. 3.9</label><caption xml:lang="en"><p>Ideal and realizable anti-aliasing filters (low-pass filters)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig9_HTML.png" id="MO9"/></fig></p></sec><sec><p id="Par47"><index-term id="ITerm40"><term>brick-wall filter</term></index-term></p></sec><sec><p id="Par48">In practice, such ideal filters (so-called brick-wall filters) do not exist.<xref ref-type="fn" rid="Fn2">2</xref> Realizable filters will already start attenuating frequencies smaller than <italic>f</italic><sub><italic>s</italic></sub>∕2 and will still not eliminate all frequencies larger than <italic>f</italic><sub><italic>s</italic></sub>∕2 (see Fig. <xref rid="Fig9" ref-type="fig">3.9</xref>). Attenuated high-frequency components will exist even after filtering. For frequencies smaller than <italic>f</italic><sub><italic>s</italic></sub>∕2, there may also be some “overshooting,” i.e., frequencies for which there is some amplification of the input signal.</p></sec><sec><p id="Par50">The design of good anti-aliasing filters is an art by itself. This art has been studied, for example, in great detail for high-quality audio equipment, involving detailed hearing tests. Many of the perceived differences between high-quality equipment have been attributed to the design of such filters.</p></sec></sec><sec id="Sec6"><title>Discretization of Values: Analog-to-Digital Converters</title><p id="Par51"><index-term id="ITerm41"><term>analog-to-digital converter (ADC)</term></index-term></p><p id="Par52">Since we are restricting ourselves to digital computers, we must also replace signals that map time to a continuous value domain <italic>D</italic><sub><italic>V</italic></sub> by signals that map time to a discrete value domain <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:msubsup><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$D^{\prime }_V$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq1.gif"/></alternatives></inline-formula>. This conversion from analog-to-digital values is done by analog-to-digital converters (ADCs). There is a large range of ADCs with varying speed/precision characteristics. Typically, fast ADCs have a low precision and high-precision converters are slow.</p><p id="Par53">We will present several converters in the next subsections.</p><sec id="Sec7"><title>Flash ADC</title><sec><p id="Par54"><index-term id="ITerm42"><term>ADC</term><index-term><term>flash ˜</term></index-term></index-term> This type of ADCs uses<index-term id="ITerm43"><term>flash ADC</term></index-term> a large number of comparators. Each comparator has two inputs, denoted as + and -. If the voltage at input + exceeds that at input -, the output corresponds to a logical ’1’, and it corresponds to a logical ’0’ otherwise.<xref ref-type="fn" rid="Fn3">3</xref></p></sec><sec><p id="Par56">In the ADC, all - inputs are connected to a voltage divider. If input voltage <italic>h</italic>(<italic>t</italic>) exceeds <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\frac {3}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq2.gif"/></alternatives></inline-formula>, the comparator at the top of Fig. <xref rid="Fig10" ref-type="fig">3.10</xref> (left) will generate a ’1’. The encoder at the output of the comparators will try to identify the most significant ’1’ and will encode this case as the largest output value. The case <italic>h</italic>(<italic>t</italic>) &gt; <italic>V</italic><sub><italic>ref</italic></sub> should normally be avoided since <italic>V</italic><sub><italic>ref</italic></sub> is typically close to the supply voltage of the circuit and input voltages exceeding the supply voltage can lead to electrical problems. In our case, input voltages larger than <italic>V</italic><sub><italic>ref</italic></sub> generate the largest digital value as long as the converter does not fail due to the high input voltage.<fig id="Fig10"><label>Fig. 3.10</label><caption xml:lang="en"><p>Flash ADC: <bold>left</bold>, schematic; <bold>right</bold>, <italic>w</italic> as a function of <italic>h</italic></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig10_HTML.png" id="MO10"/></fig></p></sec><sec><p id="Par57">Now, if input voltage <italic>h</italic>(<italic>t</italic>) is less than <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mfrac><mml:mrow><mml:mn>3</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\frac {3}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq3.gif"/></alternatives></inline-formula>, but still larger than <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\frac {2}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq4.gif"/></alternatives></inline-formula>, the comparator at the top of Fig. <xref rid="Fig10" ref-type="fig">3.10</xref> will generate a ’0’, while the next comparator will still signal a ’1’. The encoder will encode this as the second largest value.</p></sec><sec><p id="Par58">Similar arguments hold for cases <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\frac {1}{4} V_{ref} &lt; h(t) &lt; \frac {2}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq5.gif"/></alternatives></inline-formula> and <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$0 &lt; h(t) &lt; \frac {1}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq6.gif"/></alternatives></inline-formula>, which will be encoded as the third largest and the smallest value, respectively. Figure <xref rid="Fig10" ref-type="fig">3.10</xref> (right) shows the relation between input voltages and generated digital values.</p></sec><sec><p id="Par59">The outputs of the comparators encode numbers in a special way: if a certain comparator output is equal to ’1’, then all the less significant outputs are all equal to ’1’. The encoder transforms this representation of numbers into the usual representation of natural numbers. The encoder is actually a so-called priority encoder, <index-term id="ITerm44"><term>priority encoder</term></index-term> encoding the most significant input number carrying a ’1’ in binary.<xref ref-type="fn" rid="Fn4">4</xref></p></sec><sec><p id="Par61">The circuit can convert positive analog input voltages into digital values. Converting both positive and negative voltages and generating two’s complement numbers requires some extensions. One nice property of the flash ADC is the fact that it is automatically <bold>monotonic</bold>: For any increase in the analog voltage from 0 to the maximum, the corresponding digital value increases as well. This property is maintained even if the actual value of the resistors would deviate from the nominal value. However, such a deviation would have an impact on the precision of the linear relation expected between analog and digital values.</p></sec><sec><p id="Par62">Unfortunately, the chain of resistors forms a conducting path, which exists even if the converter is not used. This could make it impossible to use this converter for low-power equipment.</p></sec><sec><p id="Par63">In general, ADCs are also characterized by their <bold>resolution</bold>. <index-term id="ITerm45"><term>resolution</term></index-term> This term has several different but related meanings [<xref ref-type="bibr" rid="CR15">15</xref>]. The resolution (measured in bits) is the number of bits produced by an ADC. For example, ADCs with a resolution of 16 bits are needed for many audio applications. However, the resolution is also measured in volts, and in this case it denotes the difference between two input voltages causing the output to be incremented by 1: <disp-formula id="Equ9"><label>3.9</label><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Equ9_HTML.png" id="MO11"/></disp-formula></p></sec><sec id="FPar7"><title>Example 3.3</title><p id="Par64">For the ADC of Fig. <xref rid="Fig10" ref-type="fig">3.10</xref>, the resolution is 2 bits or <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:mfrac><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\frac {1}{4} V_{ref}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq7.gif"/></alternatives></inline-formula> volts, if we assume <italic>V</italic><sub><italic>ref</italic></sub> as the largest voltage. ∇</p></sec><sec><p id="Par65">The key advantage of the flash ADC is its speed. It does not need any clock. The delay between the input and the output is very small, and the circuit can be used easily, for example, for high-speed video applications. The disadvantage is its hardware complexity: we need <italic>n</italic> − 1 comparators in order to distinguish between <italic>n</italic> values. Imagine using this circuit in generating digital audio signals for CD recorders. We would need 2<sup>16</sup> − 1 comparators! High-resolution ADCs must be built differently.</p></sec></sec><sec id="Sec8"><title>Successive Approximation</title><p id="Par66"><index-term id="ITerm46"><term>successive approximation</term></index-term><index-term id="ITerm47"><term>ADC</term><index-term><term>successive approximation ˜</term></index-term></index-term> Distinguishing between a large number of digital values is possible with ADCs using successive approximation. The circuit is shown in Fig. <xref rid="Fig11" ref-type="fig">3.11</xref>.<fig id="Fig11"><label>Fig. 3.11</label><caption xml:lang="en"><p>Circuit using successive approximation</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig11_HTML.png" id="MO12"/></fig></p><p id="Par67">The key idea of this circuit is to use binary search. Initially, the most significant output bit of the successive approximation register is set to ’1’; all other bits are set to ’0’. This digital value is then converted to an analog value, corresponding to 0.5 ∗ the maximum input voltage.<xref ref-type="fn" rid="Fn5">5</xref> If <italic>h</italic>(<italic>t</italic>) exceeds the generated analog value, the most significant bit is kept at ’1’; otherwise it is reset to ’0’.</p><p id="Par69">This process is repeated with the next bit. It will remain set to ’1’ if the input value is either within the second or the fourth quarter of the input value range. The same procedure is repeated for all the other bits.</p><p id="Par70">Figure <xref rid="Fig12" ref-type="fig">3.12</xref> shows an example. Initially the most significant bit is set to ’1’. This value is kept, since the resulting <italic>V</italic><sub>−</sub> is less than <italic>h</italic>(<italic>t</italic>). Then, the second most significant bit is set to ’1’. It is reset to ’0’, since the resulting <italic>V</italic><sub>−</sub> is exceeding <italic>h</italic>(<italic>t</italic>). Next, the third most significant bit is tried. It is set to ’1’, and this value is kept. Finally, the least significant bit is also set, and it remains set after the comparison has been completed. Obviously, <italic>h</italic>(<italic>t</italic>) must be constant during the conversion, otherwise the whole procedure would be jeopardized. This requirement is met if we employ a sample-and-hold circuit as shown above. The resulting digital signal is called <italic>w</italic>(<italic>t</italic>).<fig id="Fig12"><label>Fig. 3.12</label><caption xml:lang="en"><p>Successive approximation</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig12_HTML.png" id="MO13"/></fig></p><p id="Par71">The key advantage of the successive approximation technique is its hardware efficiency. In order to distinguish between <italic>n</italic> digital values, we need <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mfenced close="⌉" open="⌈" separators=""><mml:mrow><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfenced></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\left \lceil log_2(n)\right \rceil $$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq8.gif"/></alternatives></inline-formula> bits in the successive approximation register and the D/A converter. The disadvantage is its speed, since it needs <inline-formula id="IEq9"><alternatives><mml:math id="IEq9_Math"><mml:mi mathvariant="script">O</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:math><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\mathcal {O}(log_2(n))$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq9.gif"/></alternatives></inline-formula> steps. These converters can therefore be used for high-resolution applications, where moderate speeds are required. Examples include audio applications.</p></sec><sec id="Sec9"><title>Pipelined Converters</title><p id="Par72"><index-term id="ITerm49"><term>analog-to-digital converter (ADC)</term></index-term> These converters consist of a chain of converters, where each stage in the chain is in charge of converting a few bits (see Fig. <xref rid="Fig13" ref-type="fig">3.13</xref>). <index-term id="ITerm50"><term>ADC</term><index-term><term>pipelined ˜</term></index-term></index-term> Each stage passes the remaining residue of the voltage to the next stage (if any). For example, each stage could convert a single bit and subtract the corresponding voltage. The resulting residue would typically be scaled up by a factor of two (in order to avoid too small voltages) and be passed on to the next stage. Typically, each stage would include a flash ADC of a few bits and a D/A converter to compute the voltage to be subtracted. Resulting digital values must be aligned in time. Required hardware resources increase linearly with the number of bits. With this structure, a good throughput can be achieved, but the latency is larger than for flash converters.<fig id="Fig13"><label>Fig. 3.13</label><caption xml:lang="en"><p>Pipelined ADC [<xref ref-type="bibr" rid="CR291">291</xref>]</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig13_HTML.png" id="MO14"/></fig></p></sec><sec id="Sec10"><title>Other Converters</title><p id="Par73"><index-term id="ITerm51"><term>analog-to-digital converter (ADC)</term></index-term><index-term id="ITerm52"><term>ADC</term><index-term><term>integrating ˜</term></index-term></index-term><bold>Integrating converters</bold> use (at least) two phases for the measurement. During the first phase of length <italic>t</italic><sub>1</sub>, the integral of the input voltage over time is computed.<xref ref-type="fn" rid="Fn6">6</xref> For constant inputs, the resulting value <italic>V</italic><sub><italic>out</italic></sub> is proportional to the input voltage (<italic>V</italic><sub><italic>out</italic></sub> ∼ <italic>V</italic><sub><italic>in</italic></sub> ∗ <italic>t</italic><sub>1</sub>). During the second phase, this value is decreased at a constant rate, and the time to reach a value of zero is counted. The final count is proportional to the input voltage. Hence, using proper scaling, the final count represents the input voltage. If the input voltage contains some noise, its impact is likely to be averaged out during the first integration phase. Hence, these converters are capable of compensating noise. They are typically found in slow, high-resolution multimeters.</p><p id="Par75">For <bold>folding ADCs</bold>, the input voltage range is divided into 2<sup><italic>m</italic></sup> segments [<xref ref-type="bibr" rid="CR100">100</xref>, <xref ref-type="bibr" rid="CR321">321</xref>]. A coarse-grained converter detects the segment of the current input voltage, yielding the <italic>m</italic> most significant output bits. A fine-grained converter computes the value within a segment, yielding the less significant output bits. <index-term id="ITerm54"><term>ADC</term><index-term><term>folding ˜</term></index-term></index-term></p><p id="Par76">For <bold>delta-sigma ADCs</bold> (Δ Σ ADCs), the name indicates that signal differences (Δs) are encoded and that they are summed up (Σ). A description of these converters is beyond the scope of this book. For details refer to Khorramabadi [<xref ref-type="bibr" rid="CR292">292</xref>]. <index-term id="ITerm55"><term>ADC</term><index-term><term>Delta-Sigma ˜</term></index-term></index-term></p></sec><sec id="Sec11"><title>Comparison of ADCs</title><p id="Par77"><index-term id="ITerm56"><term>analog-to-digital converter (ADC)</term></index-term></p><p id="Par78">Figure <xref rid="Fig14" ref-type="fig">3.14</xref> provides an overview of the speed/resolution trade-offs of ADCs, using a trade-off analysis of Vogels et al. [<xref ref-type="bibr" rid="CR558">558</xref>]. Flash ADCs are clearly the fastest but provide only a small resolution. Pipelining is frequently superior to successive approximation. Another overview of ADCs is provided by IEEE TV [<xref ref-type="bibr" rid="CR437">437</xref>].<fig id="Fig14"><label>Fig. 3.14</label><caption xml:lang="en"><p>Comparison of the speed/resolution characteristics of various ADCs [<xref ref-type="bibr" rid="CR558">558</xref>]</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig14_HTML.png" id="MO15"/></fig></p></sec><sec id="Sec12"><title>Quantization Noise</title><sec><p id="Par79">Figure <xref rid="Fig15" ref-type="fig">3.15</xref> shows the behavior of a flash ADC when the input signal is that of Eq. (<xref rid="Equ3" ref-type="disp-formula">3.3</xref>). Only the behavior for a positive input signal is shown. The figure includes the voltage corresponding to the digital value, the original voltage, and the difference between the two. Obviously, the converter is “truncating” the digital representation of the analog signal to the number of available bits (i.e., the digital value is always less than or equal to the analog value). This is a consequence of the way in which the flash converter is doing comparisons. “Rounding” converters would need an internal correction by “half a bit.” Effectively, the digital signal encodes values corresponding to the sum of the original analog values and the difference <italic>w</italic>(<italic>t</italic>) − <italic>h</italic>(<italic>t</italic>). This means, it appears <bold>as if the difference between the two signals had been added to the original signal</bold>. This difference is a signal called <bold>quantization noise</bold>:<index-term id="ITerm57"><term>quantization noise</term></index-term><fig id="Fig15"><label>Fig. 3.15</label><caption xml:lang="en"><p><italic>h</italic>(<italic>t</italic>) (blue), <italic>w</italic>(<italic>t</italic>) (red), <italic>w</italic>(<italic>t</italic>) − <italic>h</italic>(<italic>t</italic>) (black)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig15_HTML.png" id="MO16"/></fig></p></sec><sec><p id="Par80"><index-term id="ITerm58"><term>ADC</term></index-term></p></sec><sec id="FPar8"><title>Definition 3.4</title><p id="Par81">Let <italic>h</italic>(<italic>t</italic>) be some analog signal. Let <italic>w</italic>(<italic>t</italic>) be derived from <italic>h</italic>(<italic>t</italic>) by quantization. The difference between the two is called <bold>quantization noise</bold>: <index-term id="ITerm59"><term>quantization noise</term></index-term><disp-formula id="Equ10"><label>3.10</label><alternatives><mml:math id="Equ10_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext>quantization noise</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>w</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>&lt;</mml:mo><mml:mi>Q</mml:mi></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} \mbox{quantization noise}(t) &amp;\displaystyle = &amp;\displaystyle w(t) - h(t) &lt; Q \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ10.gif"/></alternatives></disp-formula></p></sec><sec><p id="Par82">Increasing the resolution of the ADC decreases quantization noise. The impact of quantization noise is captured in the definition of the <bold>signal-to-noise ratio</bold> (SNR)<index-term id="ITerm60"><term>signal-to-noise ratio (SNR)</term></index-term>, measured in decibels (tenth of a bel, named after Alexander G. Bell).</p></sec><sec id="FPar9"><title>Definition 3.5</title><p id="Par83">The SNR is defined as follows: <disp-formula id="Equ11"><label>3.11</label><alternatives><mml:math id="Equ11_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext>SNR (in dB = decibels)</mml:mtext></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>10</mml:mn><mml:mo>∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.3em"/><mml:mfrac><mml:mrow><mml:mtext>power of the “useful” signal</mml:mtext></mml:mrow><mml:mrow><mml:mtext>power of the noise signal</mml:mtext></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} \mbox{SNR (in dB = decibels)} &amp;\displaystyle = &amp;\displaystyle 10 * log {~ \frac{\mbox{power of the ``useful'' signal}}{\mbox{power of the noise signal}}} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ11.gif"/></alternatives></disp-formula><disp-formula id="Equ12"><label>3.12</label><alternatives><mml:math id="Equ12_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>20</mml:mn><mml:mo>∗</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mspace width="0.3em"/><mml:mfrac><mml:mrow><mml:mtext>voltage of the “useful” signal</mml:mtext></mml:mrow><mml:mrow><mml:mtext>voltage of the noise signal</mml:mtext></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} &amp;\displaystyle = &amp;\displaystyle 20 * log {~ \frac{\mbox{voltage of the ``useful'' signal}}{\mbox{voltage of the noise signal}}} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ12.gif"/></alternatives></disp-formula></p></sec><sec><p id="Par84"><index-term id="ITerm61"><term>ADC</term></index-term> We have used that, for any given impedance <italic>R</italic>, the power of a signal is proportional to the square of the voltage. Decibels are no physical units, since the SNR is dimensionless.</p></sec><sec><p id="Par85">For any signal <italic>h</italic>(<italic>t</italic>), the power<index-term id="ITerm62"><term>power</term></index-term> of the quantization noise is equal to <italic>α</italic> ∗ <italic>Q</italic>, where <italic>α</italic> ≤ 1 depends on the waveform of <italic>h</italic>(<italic>t</italic>). If <italic>h</italic>(<italic>t</italic>) can always be represented exactly by a digital value, then <italic>α</italic> = 0. If <italic>h</italic>(<italic>t</italic>) is always “just a little” below the next value that can be represented, <italic>α</italic> may be close to 1.</p></sec><sec id="FPar10"><title>Example 3.4</title><p id="Par86">The SNR of 16 bit CD audio is (for <italic>α</italic> ≈ 1) about 20 ∗ <italic>log</italic>(2<sup>16</sup>) = 96 dB. <index-term id="ITerm63"><term>decibels</term></index-term> Values of <italic>α</italic> &lt; 1 and imperfect ADCs change this number. ∇</p></sec></sec></sec></sec><sec id="Sec13"><title>Processing Units</title><p id="Par87"><index-term id="ITerm64"><term>processing units</term></index-term></p><p id="Par88">Let us now discuss the next hardware element in the loop of Fig. <xref rid="Fig2" ref-type="fig">3.2</xref>, processing units. For information processing in embedded systems, we will consider ASICs (application-specific integrated circuits) using hardwired multiplexed designs, <index-term id="ITerm65"><term>application-specific integrated circuit (ASIC)</term></index-term> reconfigurable logic, and programmable processors. We will consider ASICs first.</p><sec id="Sec14"><title>Application-Specific Integrated Circuits (ASICs)</title><p id="Par89"><index-term id="ITerm66"><term>application-specific integrated circuit (ASIC)</term></index-term></p><p id="Par90">For high-performance applications and for large markets, application-specific integrated circuits (ASICs) can be designed. In general, ASICs are very energy-efficient (see Sect. <xref rid="Sec44" ref-type="sec">3.7.3</xref> on p. 67). However, the cost of designing and<index-term id="ITerm67"><term>cost</term><index-term><term>of ASICs</term></index-term></index-term> manufacturing such chips is quite high. The cost of the mask set (which is used for transferring geometrical patterns onto the chip) has grown.<xref ref-type="fn" rid="Fn7">7</xref></p><p id="Par92">It is feasible to decrease this cost by using less advanced semiconductor fabrication technologies and by using multi-project wafers (MPW) containing several designs. <index-term id="ITerm68"><term>multi-project wafer (MPW)</term></index-term> But there is a lack of flexibility: correcting design errors typically requires a new mask set and a new fabrication run (unless the ASIC contains processors with writable memories). This approach also has to cope with potentially large design efforts requiring dedicated skills and expensive tools. Therefore, ASICs are appropriate only under special circumstances, like large market volumes, ultimate energy efficiency demands, special voltage or temperature ranges, mixed analog/digital signals, or security-driven designs. Hence, the design of ASICs is not covered in this book.</p></sec><sec id="Sec15"><title>Processors</title><p id="Par93"><index-term id="ITerm69"><term>processor</term></index-term></p><p id="Par94">The key advantage of processors is their flexibility. With processors, the behavior of embedded systems can be changed by changing the software running on those processors. Changes of the behavior may be required in order to correct design errors, to update the system to a new standard, or to add features. Because of this, processors have found widespread use in embedded systems. In particular, processors which are available commercially “off-the-shelf” (COTS) have become very popular. <index-term id="ITerm70"><term>commercial off-the-shelf (COTS)</term></index-term></p><p id="Par95">Embedded processors must be used in a resource-aware manner, i.e., we need to care about resources required for running applications on them. Furthermore, they do not need to be instruction set compatible with commonly used personal computers (PCs) or servers. Therefore, their architectures may be different from those processors. Efficiency has different aspects (see p. 13), some of which are discussed next.</p><sec id="Sec16"><title>Energy Efficiency</title><p id="Par96"><index-term id="ITerm71"><term>energy efficiency</term></index-term></p><p id="Par97">The energy <italic>E</italic> for an application is related to the power <italic>P</italic> as a function of time, since<index-term id="ITerm72"><term>energy</term></index-term><index-term id="ITerm73"><term>power</term></index-term><disp-formula id="Equ13"><label>3.13</label><alternatives><mml:math id="Equ13_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo mathsize="big">∫</mml:mo><mml:mi>P</mml:mi><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} E = \int P dt {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ13.gif"/></alternatives></disp-formula> Let us assume that we start with some design having a power <index-term id="ITerm74"><term>power</term></index-term> consumption of <italic>P</italic><sub>0</sub>(<italic>t</italic>), leading to an energy consumption of <disp-formula id="Equb"><alternatives><mml:math id="Equb_Math"><mml:mtable columnalign="right left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∫</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equb_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned}E_0=\int_0^{t_0} P_0(t) dt \end{aligned}$$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equb.gif"/></alternatives></disp-formula> after <italic>t</italic><sub>0</sub> units of execution time. Suppose that a modified design finishing computations already at time <italic>t</italic><sub>1</sub> comes with a power consumption of <italic>P</italic><sub>1</sub>(<italic>t</italic>) and an energy consumption of <disp-formula id="Equc"><alternatives><mml:math id="Equc_Math"><mml:mtable columnalign="right left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo mathsize="big">∫</mml:mo></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equc_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned}E_1=\int_0^{t_1} P_1(t) dt\end{aligned}$$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equc.gif"/></alternatives></disp-formula> If <italic>P</italic><sub>1</sub>(<italic>t</italic>) is not too much larger than <italic>P</italic><sub>0</sub>(<italic>t</italic>), then a reduction of the execution time also reduces the energy consumption. However, in general this is not necessarily always true. The situation is also shown in Fig. <xref rid="Fig16" ref-type="fig">3.16</xref>: <italic>E</italic><sub>1</sub> may be smaller than <italic>E</italic><sub>0</sub>, but <italic>E</italic><sub>1</sub> can also be larger than <italic>E</italic><sub>0</sub>. So, if the energy consumption is to be minimized, it should be used as a cost function. Just minimizing the execution time can be misleading.<fig id="Fig16"><label>Fig. 3.16</label><caption xml:lang="en"><p>Comparison of energies <italic>E</italic><sub>0</sub> and <italic>E</italic><sub>1</sub></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig16_HTML.png" id="MO17"/></fig></p><p id="Par98">Minimization of power<index-term id="ITerm75"><term>power</term></index-term> and energy consumption are both important. Power consumption has an effect on the size of the power supply, the design of the voltage regulators, the dimensioning of the interconnect, and short-term cooling. Minimizing the energy<index-term id="ITerm76"><term>energy</term></index-term> consumption is required especially for mobile applications, since battery technology is only slowly improving and since the cost<index-term id="ITerm77"><term>cost</term><index-term><term>of energy</term></index-term></index-term> of energy may be quite high. Also, a reduced energy consumption decreases cooling requirements and improves the reliability (since the lifetime of electronic circuits decreases for high temperatures).</p><p id="Par99">Next, we would like to demonstrate that for CMOS technology, it is preferable to replace high-speed sequential computations by reduced speed parallel computations. This is shown by—first of all—considering the power<index-term id="ITerm78"><term>power</term></index-term> consumption of CMOS devices. The <bold>dynamic power consumption</bold> is the power consumption caused by switching <index-term id="ITerm79"><term>power consumption</term><index-term><term>dynamic ˜</term></index-term></index-term> (in contrast to the <bold>static power consumption</bold> which exists even if no switching takes place). The average dynamic power consumption <italic>P</italic><sub><italic>dyn</italic></sub> of CMOS circuits is given by Chandrakasan et al. [<xref ref-type="bibr" rid="CR90">90</xref>] <disp-formula id="Equ14"><label>3.14</label><alternatives><mml:math id="Equ14_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>α</mml:mi><mml:mspace width="0.3em"/><mml:mspace width="0.3em"/><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mspace width="0.3em"/><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mspace width="0.3em"/><mml:mi>f</mml:mi></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} P_{dyn} &amp;\displaystyle = &amp;\displaystyle \alpha \mbox{ ~} C_L  V_{dd}^2  f {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ14.gif"/></alternatives></disp-formula> where <italic>α</italic> is the switching activity, <italic>C</italic><sub><italic>L</italic></sub> is the load capacitance, <index-term id="ITerm80"><term>switching activity</term></index-term><italic>V</italic><sub><italic>dd</italic></sub> is the supply voltage, and <italic>f</italic> is the clock frequency. This means that the power consumption of CMOS processors increases (at least)<xref ref-type="fn" rid="Fn8">8</xref> quadratically with the supply voltage <italic>V</italic><sub><italic>dd</italic></sub>.</p><p id="Par101">The delay of CMOS circuits can be approximated as [<xref ref-type="bibr" rid="CR90">90</xref>] <disp-formula id="Equ15"><label>3.15</label><alternatives><mml:math id="Equ15_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi mathvariant="normal">Δ</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>k</mml:mi><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} \Delta &amp;\displaystyle = &amp;\displaystyle k C_L \frac{V_{dd}}{(V_{dd}-V_t)^2} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ15.gif"/></alternatives></disp-formula> where <italic>k</italic> is a constant and <italic>V</italic><sub><italic>t</italic></sub> is the threshold voltage. <italic>V</italic><sub><italic>t</italic></sub> has an impact on the transistor input voltage required to switch the transistor on. For example, for a maximum supply voltage of <italic>V</italic><sub><italic>dd</italic>,<italic>max</italic></sub> = 3.3 V, <italic>V</italic><sub><italic>t</italic></sub> may be in the order of 0.8 V. Consequently, the maximum clock frequency is a function of the supply voltage. However, decreasing the supply voltage reduces the power quadratically, while the run-time of algorithms is only linearly increased (ignoring the effects of the memory system).</p><p id="Par102">We can use this to reduce the amount of energy required for a certain amount of computations. Let us assume that we are initially performing computations sequentially at voltage <italic>V</italic><sub><italic>dd</italic></sub>, constant power <italic>P</italic>, clock frequency <italic>f</italic>, run-time of <italic>t</italic>, and energy consumption <italic>E</italic> = <italic>P</italic> ∗ <italic>t</italic>.</p><p id="Par103">Now let us assume that we are moving toward executing <italic>β</italic> operations in parallel. Due to parallel execution, we can extend the time for each operation by a factor of <italic>β</italic>. In turn, we can also reduce frequency <italic>f</italic> by a factor of <italic>β</italic> and use a new frequency <disp-formula id="Equ16"><label>3.16</label><alternatives><mml:math id="Equ16_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>f</mml:mi><mml:mi>′</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} f'&amp;\displaystyle = &amp;\displaystyle \frac{f}{\beta} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ16.gif"/></alternatives></disp-formula> This allows us to also reduce the voltage to a new voltage <disp-formula id="Equ17"><label>3.17</label><alternatives><mml:math id="Equ17_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msubsup><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>′</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd class="eqnarray-2"/><mml:mtd class="eqnarray-3"/><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ17_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} V_{dd}^{\prime}=\frac{V_{dd}}{\beta} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ17.gif"/></alternatives></disp-formula> This reduces the power<index-term id="ITerm81"><term>power</term></index-term><italic>P</italic><sup>0</sup> per operation quadratically: <disp-formula id="Equ18"><label>3.18</label><alternatives><mml:math id="Equ18_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ18_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} P^0 &amp;\displaystyle =&amp;\displaystyle \frac{P}{\beta^2} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ18.gif"/></alternatives></disp-formula> Due to executing <italic>β</italic> operations in parallel, the overall power <italic>P′</italic> can be computed as <disp-formula id="Equ19"><label>3.19</label><alternatives><mml:math id="Equ19_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>P</mml:mi><mml:mi>′</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>β</mml:mi><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>P</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ19_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} P' &amp;\displaystyle =&amp;\displaystyle \beta * P^0 = \frac{P}{\beta} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ19.gif"/></alternatives></disp-formula> The time <italic>t′</italic> to execute operations in parallel is the same as the time to compute them sequentially (<italic>t′</italic> = <italic>t</italic>). Hence, the energy to execute the operations in parallel is <disp-formula id="Equ20"><label>3.20</label><alternatives><mml:math id="Equ20_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>E</mml:mi><mml:mi>′</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>P</mml:mi><mml:mi>′</mml:mi><mml:mo>∗</mml:mo><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ20_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} E' &amp;\displaystyle =&amp;\displaystyle P' * t= \frac{E}{\beta} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ20.gif"/></alternatives></disp-formula></p><p id="Par104">We conclude that it is more energy-efficient to execute <italic>β</italic> operations in parallel instead of computing them sequentially. However, our derivation contains a number of approximations. On the one hand, power may be depending even cubically on the voltage, and we have ignored the fact that memory speed is frequently a limiting constraint. Faster processor clock speeds might just lead to more waiting for memory accesses (but there may be also conflicts for memory access from multiple cores). The energy would decrease quadratically if we would be able to keep the power consumption independent of the level of parallelism. On the other hand, we need to be able to find <italic>β</italic> operations which can be executed in parallel. Overall, we keep in mind that parallel execution is a means for deriving energy-efficient implementations, regardless of which hardware technology we are using.</p><p id="Par105">Architectures must be optimized for their energy efficiency, and we must make sure that we are not losing efficiency in the software generation process. For example, compilers generating 50% overhead in terms of the number of cycles will take us further away from the efficiency of ASICs,<index-term id="ITerm82"><term>application-specific integrated circuit (ASIC)</term></index-term> possibly by even more than 50%, if the supply voltage and the clock frequency must be increased in order to meet timing deadlines.<index-term id="ITerm83"><term>efficiency</term><index-term><term>energy ˜</term></index-term></index-term><index-term id="ITerm84"><term>energy</term></index-term></p><p id="Par106">There is a large amount of techniques available that can make processors energy-efficient, and energy efficiency should be considered at various levels of abstraction, from the design of the instruction set down to the design of the chip manufacturing process [<xref ref-type="bibr" rid="CR77">77</xref>]. Gated clocking and power gating are examples of such techniques. With<index-term id="ITerm85"><term>gated clocking</term></index-term> gated clocking, parts of the processor are disconnected from the clock during idle periods. In a similar way, the power can be disconnected for some components. For example, direct memory access (DMA) hardware or bus bridges can be disconnected if they are not needed. Also, there are attempts, to get rid of the clock for major parts of the processor altogether. There are two contrasting approaches: globally synchronous locally asynchronous (GSLA) processors [<xref ref-type="bibr" rid="CR436">436</xref>] and globally asynchronous locally synchronous (GALS) processors [<xref ref-type="bibr" rid="CR262">262</xref>]. Further information about low-power <index-term id="ITerm86"><term>power</term></index-term> design techniques is available in a book by E. Macii [<xref ref-type="bibr" rid="CR359">359</xref>] and in the PATMOS proceedings (see <ext-link xlink:href="http://www.patmos-conf.org/" ext-link-type="uri">http://www.patmos-conf.org/</ext-link>).</p><p id="Par107">At least three techniques can be applied at a rather high level of abstraction: <list list-type="bullet"><list-item><p id="Par108"><bold>Parallel execution:</bold> According to Eq. (<xref rid="Equ20" ref-type="disp-formula">3.20</xref>), parallel execution is an effective means of improving the overall energy efficiency.</p></list-item><list-item><p id="Par109"><bold>Dynamic power<index-term id="ITerm87"><term>power</term></index-term> management (DPM):</bold> With this approach, processors have several power-saving states in addition to the standard operating state. Each power-saving state has a different power consumption and a different time for transitions into the operating state. Figure <xref rid="Fig17" ref-type="fig">3.17</xref> shows the three states for the StrongARM SA-1100 processor.<fig id="Fig17"><label>Fig. 3.17</label><caption xml:lang="en"><p>Dynamic power<index-term id="ITerm88"><term>power</term></index-term> management states of the StrongARM SA-1100 processor [<xref ref-type="bibr" rid="CR47">47</xref>]</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig17_HTML.png" id="MO18"/></fig></p><p id="Par110">The processor is fully operational in the <italic>run</italic> state. In the <italic>idle</italic> state, it is just monitoring the interrupt inputs. <index-term id="ITerm89"><term>interrupt</term></index-term> In the <italic>sleep</italic> state, on-chip activity is shut down, the processor is reset, and the chip’s power <index-term id="ITerm90"><term>power</term></index-term> supply is shut off [<xref ref-type="bibr" rid="CR593">593</xref>]. A separate I/O power supply provides power to power manager hardware. The processor can be restarted by the power manager hardware by a preprogrammed wake-up event. Note the large difference in the power consumption between the <italic>sleep</italic> state and the other states, and note also the large delay for transitions from the <italic>sleep</italic> to the <italic>run</italic> state.</p></list-item><list-item><p id="Par111"><bold>Dynamic voltage and frequency scaling (DVFS):</bold><index-term id="ITerm91"><term>dynamic voltage and frequency scaling (DVFS)</term></index-term> Equation (<xref rid="Equ14" ref-type="disp-formula">3.14</xref>) can be exploited in a technique called <bold>dynamic voltage and frequency scaling (DVFS)</bold>. For example, the Crusoe<sup>™</sup> processor by Transmeta [<xref ref-type="bibr" rid="CR295">295</xref>] provided 32 voltage levels between 1.1 and 1.6 V, and the clock could be varied between 200 MHz and 700 MHz in increments of 33 MHz. Transitions from one voltage/frequency pair to the next took about 20 ms. Design issues for DVFS-capable processors are described in a paper by Burd and Brodersen [<xref ref-type="bibr" rid="CR76">76</xref>]. In 2004, Intel <italic>SpeedStep</italic><sup>®</sup> Technology provided six different voltage/frequency combinations for Pentium<sup>™</sup> M processors [<xref ref-type="bibr" rid="CR246">246</xref>]. More recent processors include more comprehensive mechanisms for power management.</p></list-item></list></p></sec><sec id="Sec17"><title>Code Size Efficiency</title><p id="Par112"><index-term id="ITerm92"><term>efficiency</term><index-term><term>code size ˜</term></index-term></index-term></p><p id="Par113">Minimizing the code size is very important for embedded systems, since large hard disk drives (HDDs) or solid-state disks (SSDs) are typically not available and since the capacity of memory is typically also very limited.<xref ref-type="fn" rid="Fn9">9</xref> This is even more pronounced for <bold>systems on a chip</bold> (SoCs). For SoCs, the memory and processors are implemented on the same chip. In this particular case, memory is called <bold>embedded memory</bold>. Embedded memory may be more expensive to fabricate than separate memory chips, since the fabrication processes for memories and processors must be compatible. Nevertheless, a large percentage of the total chip area may be consumed by the memory.<index-term id="ITerm93"><term>system on a chip (SoC)</term></index-term> There are several techniques for improving the code size efficiency: <index-term id="ITerm94"><term>complex instruction set computers (CISC)</term></index-term><index-term id="ITerm95"><term>ColdFire</term></index-term><list list-type="bullet"><list-item><p id="Par115"><bold>CISC machines</bold>: Standard RISC processors have been designed for speed, not for code size efficiency. Earlier complex instruction set processors (CISC machines) were actually designed for code size efficiency, since they had to be connected to slow memories. Caches were not frequently used. Therefore, “old-fashioned” CISC processors are finding applications in embedded systems. ColdFire processors [<xref ref-type="bibr" rid="CR170">170</xref>], which are based on the Motorola 68000 family of CISC processors, are an example.<index-term id="ITerm96"><term>cache</term></index-term></p><p id="Par116"><index-term id="ITerm97"><term>compression</term></index-term></p></list-item><list-item><p id="Par117"><bold>Compression techniques</bold>: In order to reduce the amount of silicon needed for storing instructions as well as in order to reduce the energy<index-term id="ITerm98"><term>energy</term></index-term> needed for fetching these instructions, instructions are stored in memory in compressed form. This reduces both the area and the energy necessary for fetching instructions. Due to the reduced bandwidth requirements, fetching can also be faster. A (hopefully small and fast) decoder is placed between the processor and the (instruction) memory in order to generate the original instructions on the fly (see Fig. <xref rid="Fig18" ref-type="fig">3.18</xref> (right)).<xref ref-type="fn" rid="Fn10">10</xref> Instead of using a potentially large memory of uncompressed instructions, we are storing the instructions in a compressed format.<fig id="Fig18"><label>Fig. 3.18</label><caption xml:lang="en"><p>Schemes for instruction fetch: <bold>left</bold>, uncompressed; <bold>right</bold>, compressed</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig18_HTML.png" id="MO19"/></fig></p><p id="Par119">The goals of compression can be summarized as follows: <list list-type="bullet"><list-item><p id="Par120">We would like to save ROM and RAM areas, since these may be more expensive than the processors themselves.</p></list-item><list-item><p id="Par121">We would like to use some encoding technique for instructions and possibly also for data with the following properties: <list list-type="bullet"><list-item><p id="Par122">⋅ There should be little or no run-time penalty for these techniques.</p></list-item><list-item><p id="Par123">⋅ Decoding should work from a limited context (it is, e.g., impossible to read the entire program to find the destination of a branch instruction).</p></list-item><list-item><p id="Par124">⋅ Word sizes of the memory, of instructions, and of addresses must be taken into account.</p></list-item><list-item><p id="Par125">⋅ Branch instructions branching to arbitrary addresses must be supported.</p></list-item><list-item><p id="Par126">⋅ Fast encoding is only required if writable data is encoded. Otherwise, fast decoding is sufficient.</p></list-item></list></p></list-item></list> There are several variations of this scheme: <list list-type="bullet"><list-item><p id="Par127">For some processors, there is a <bold>second instruction set</bold>. This second instruction set has a narrower instruction format. An example of this is the ARM<sup>®</sup><index-term id="ITerm100"><term>ARM<sup>®</sup></term></index-term> processor family. The original ARM instruction set is a 32 bit instruction set. Most ARM processors also provide a second instruction set, with 16 bit wide instructions, called THUMB instructions.<index-term id="ITerm101"><term>THUMB</term></index-term> THUMB instructions are shorter, since they do not support predication,<sup>11</sup> use shorter and less register fields, and use shorter immediate fields (see Fig. <xref rid="Fig19" ref-type="fig">3.19</xref>).<fig id="Fig19"><label>Fig. 3.19</label><caption xml:lang="en"><p>Re-encoding THUMB into ARM instructions</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig19_HTML.png" id="MO20"/></fig></p><p id="Par128"><index-term id="ITerm102"><term>predicated execution</term></index-term><xref ref-type="fn" rid="Fn11">11</xref></p><p id="Par130">THUMB instructions are dynamically converted into ARM instructions while programs are decoded. THUMB instructions can use only half the registers in arithmetic instructions. Therefore, register fields of THUMB instructions are concatenated with a ’0’ bit.<xref ref-type="fn" rid="Fn12">12</xref> In the THUMB instruction set, source and destination registers are identical, and the length of constants that can be used is reduced by 4 bits. During decoding, pipelining is used to keep the run-time penalty low.</p><p id="Par132">Similar techniques also exist for other processors. The disadvantage of this approach is that the tools (compilers, assemblers, debuggers, etc.) must be extended to support a second instruction set. Therefore, this approach can be quite expensive in terms of software development cost.<index-term id="ITerm103"><term>cost</term><index-term><term>of second instruction set</term></index-term></index-term></p></list-item><list-item><p id="Par133">A second approach is the use of <bold>dictionaries</bold>.<index-term id="ITerm104"><term>dictionary</term></index-term> With this approach, each instruction pattern is stored only once. For each value of the program counter, a look-up table provides a pointer to the corresponding instruction in the instruction table, the dictionary (see Fig. <xref rid="Fig20" ref-type="fig">3.20</xref>).<fig id="Fig20"><label>Fig. 3.20</label><caption xml:lang="en"><p>Dictionary approach for instruction compression</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig20_HTML.png" id="MO21"/></fig></p><p id="Par134">This approach relies on using only very few different instruction patterns. Therefore, only few entries are required for the instruction table. Hence, the bit width of the pointers can be quite small. Many variations of this scheme exist. Some are called <italic>two-level control store</italic> [<xref ref-type="bibr" rid="CR118">118</xref>], <italic>nanoprogramming</italic> [<xref ref-type="bibr" rid="CR514">514</xref>], or <italic>procedure ex-lining</italic> [<xref ref-type="bibr" rid="CR551">551</xref>]. <index-term id="ITerm105"><term>two-level control store</term></index-term><index-term id="ITerm106"><term>nanoprogramming</term></index-term><index-term id="ITerm107"><term>procedure ex-lining</term></index-term></p></list-item></list> Beszedes [<xref ref-type="bibr" rid="CR52">52</xref>] and Latendresse [<xref ref-type="bibr" rid="CR324">324</xref>] provide overviews of a large number of known compression techniques. In addition, Bonny et al. [<xref ref-type="bibr" rid="CR58">58</xref>] published a Huffman-based technique.</p></list-item></list></p></sec><sec id="Sec18"><title>Execution Time Efficiency Using Digital Signal Processing as an Example</title><sec><p id="Par135"><index-term id="ITerm108"><term>processor</term><index-term><term>digital signal ˜</term></index-term></index-term><index-term id="ITerm109"><term>digital signal processing (DSP)</term></index-term><index-term id="ITerm110"><term>efficiency</term><index-term><term>run-time ˜</term></index-term></index-term></p></sec><sec><p id="Par136">In order to meet time constraints without having to use high clock frequencies, architectures can be customized to certain application domains, such as digital signal processing (DSP).<index-term id="ITerm111"><term>digital signal processing (DSP)</term></index-term> Let us have a closer look at DSP now! In digital signal processing, digital filtering is a very frequent operation. Let us assume that we are extending the pipeline of Fig. <xref rid="Fig8" ref-type="fig">3.8</xref> on p. 9. We add a processing component, supposed to perform filtering. Names of signals are shown in Fig. <xref rid="Fig21" ref-type="fig">3.21</xref>.<fig id="Fig21"><label>Fig. 3.21</label><caption xml:lang="en"><p>Naming conventions for signals</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig21_HTML.png" id="MO22"/></fig></p></sec><sec><p id="Par137">Equation (<xref rid="Equ21" ref-type="disp-formula">3.21</xref>) describes a digital filter generating an output signal <italic>x</italic>(<italic>t</italic>) from an input signal <italic>w</italic>(<italic>t</italic>). Both signals are defined over the (usually unbounded) domain {<italic>t</italic><sub><italic>s</italic></sub>} of sampling instances. We write <italic>x</italic><sub><italic>s</italic></sub> instead of <italic>x</italic>(<italic>t</italic><sub><italic>s</italic></sub>) and <italic>w</italic><sub><italic>s</italic>−<italic>n</italic>+<italic>k</italic>+1</sub> instead of <italic>w</italic>(<italic>t</italic><sub><italic>s</italic>−<italic>n</italic>+<italic>k</italic>+1</sub>):<xref ref-type="fn" rid="Fn13">13</xref><disp-formula id="Equ21"><label>3.21</label><alternatives><mml:math id="Equ21_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:munderover accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>−</mml:mo><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ21_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} {x_s} &amp;\displaystyle {=}&amp;\displaystyle {\sum_{k=0}^{n-1} w_{s-n+k+1} * a_k} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ21.gif"/></alternatives></disp-formula> Output element <italic>x</italic><sub><italic>s</italic></sub> corresponds to a weighted average over the last <italic>n</italic> signal elements of <italic>w</italic> and can be computed iteratively, adding one product at a time. Processors for DSP are designed such that each iteration can be encoded as a single instruction.</p></sec><sec id="FPar11"><title>Example 3.5</title><p id="Par139">This is feasible with DSP processors from the ADSP 2100 family, whose architecture is shown in Fig. <xref rid="Fig22" ref-type="fig">3.22</xref>.<fig id="Fig22"><label>Fig. 3.22</label><caption xml:lang="en"><p>Internal architecture of the ADSP 2100 processor family (simplified)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig22_HTML.png" id="MO23"/></fig></p><p id="Par140">The processor has two memories, called DM and PM. A special address generating unit (AGU) can be used to provide the pointers for accessing these memories in index registers I0-I7. There are separate units for additions and multiplications, each with their own argument registers AX0, AY0, AF, MX0, MY0, and MF. The multiplier is connected to a second adder in order to compute the combination of a multiplication and an addition (so-called MAC operation) quickly. For this processor, one iteration is performed in a single cycle. For this purpose, the two memories are allocated to hold the two arrays <italic>w</italic> and <italic>a</italic>.</p><p id="Par141">Pointers to array elements can be kept in index registers. At each iteration, the value contained in one of the modify registers M0-M7 is added to the used index register. This is typically encoded as a side effect of accessing an array element.</p><p id="Par142">Partial sums are stored in MR.</p><p id="Par143">We would need unlimited memory space if, at each time instance <italic>t</italic><sub><italic>s</italic></sub>, we would be storing a new value in the next unused memory element. However, a bounded memory is sufficient, since we only need to access the most recent <italic>n</italic> values. This is feasible with a ring buffer, implemented with modulo operations for index values. The size of this buffer can be stored in length registers L0 to L7.<index-term id="ITerm112"><term>heterogeneous registers</term></index-term></p><p id="Par144">Obviously, mentioned registers serve different purposes. Therefore, they are called <bold>heterogeneous registers</bold>. Heterogeneous registers are frequently found in DSP processors.</p><p id="Par145">In order to avoid extra cycles for testing for the end of the loop, <bold>zero-overhead loop instructions</bold> are frequently provided in DSP processors. <index-term id="ITerm113"><term>zero-overhead loop instruction</term></index-term> With such instructions, a single or a small number of instructions can be executed a fixed number of times.</p><p id="Par146">Next, we are able to present the pipelined computation of Eq. (<xref rid="Equ21" ref-type="disp-formula">3.21</xref>), using processors from the ADSP 2100 family (adopted from [<xref ref-type="bibr" rid="CR14">14</xref>]):<graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Figa_HTML.png" id="MO24"/></p><p id="Par147">The outer loop corresponds to the progressing time. For each iteration of the outer loop, we initialize some registers. For the inner loop, a single instruction encodes the inner loop body, comprising the following operations: <list list-type="bullet"><list-item><p id="Par148">reading of two arguments from argument registers MX0 and MY0, multiplying them, and adding the product to register MR storing partial sums (so-called MAC operation),</p></list-item><list-item><p id="Par149">fetching the next elements of arrays <italic>a</italic> and <italic>w</italic> from memories PM and DM and storing them in argument registers MX0 and MY0,</p></list-item><list-item><p id="Par150">updating pointers to the next arguments, stored in address registers I0 and I4, by adding values stored in M1 and M5 and considering lengths in L0 and L4,</p></list-item><list-item><p id="Par151">testing for the end of the loop.</p></list-item></list> For given computational requirements, this (limited) form of parallelism leads to relatively low clock frequencies. Processors not optimized for DSP would probably need several instructions per iteration and would therefore require a higher clock frequency if available. ∇</p></sec><sec><p id="Par152">In addition to allowing single instruction realizations of loop bodies for filtering, DSP processors provide a number of other application domain-oriented features: <list list-type="bullet"><list-item><p id="Par153"><bold>Saturating arithmetic</bold><index-term id="ITerm114"><term>arithmetic</term><index-term><term>saturating ˜</term></index-term></index-term> changes overflow and underflow handling. In standard binary arithmetic, wrap-around is used for the values returned after an overflow or underflow. Table <xref rid="Tab1" ref-type="table">3.1</xref> shows an example in which two unsigned 4 bit numbers are added. A carry is generated which cannot be returned in any of the standard registers. The result register will contain a pattern of all zeros. No result could be further away from the true result than this one. <table-wrap id="Tab1"><label>Table 3.1</label><caption xml:lang="en"><p>Wrap-around vs. saturating arithmetic for unsigned integers</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"/><th align="left"><p>0</p></th><th align="left"><p>1</p></th><th align="left"><p>1</p></th><th align="left"><p>1</p></th></tr><tr><th align="left"><p>+</p></th><th align="left"/><th align="left"><p>1</p></th><th align="left"><p>0</p></th><th align="left"><p>0</p></th><th align="left"><p>1</p></th></tr></thead><tbody><tr><td align="left"><p>Standard <italic>wrap-around</italic> arithmetic</p></td><td align="left"><p>1</p></td><td align="left"><p>0</p></td><td align="left"><p>0</p></td><td align="left"><p>0</p></td><td align="left"><p>0</p></td></tr><tr><td align="left"><p><italic>Saturating</italic> arithmetic</p></td><td align="left"/><td align="left"><p>1</p></td><td align="left"><p>1</p></td><td align="left"><p>1</p></td><td align="left"><p>1</p></td></tr></tbody></table></table-wrap></p><p id="Par154">In saturating arithmetic, the result is as close as possible to the true result. For saturating arithmetic, the largest value is returned in the case of an overflow, and the smallest value is returned in the case of an underflow. This approach makes sense especially for video and audio applications: the user will hardly recognize the difference between the true result value and the largest value that can be represented. Also, it would be useless to raise exceptions if overflows occur, since it is difficult to handle exceptions in real time. Returning the right value is feasible only if we know whether we are dealing with signed or unsigned numbers.</p></list-item><list-item><p id="Par155"><bold>Fixed-point arithmetic:</bold> Sometimes, properties of floating-point computations [<xref ref-type="bibr" rid="CR186">186</xref>] are not welcome, and floating-point hardware increases the cost and<index-term id="ITerm115"><term>cost</term><index-term><term>of floating point arithmetic</term></index-term></index-term> power consumption of processors. Hence, it has been estimated that 80% of the DSP processors do not include floating-point hardware [<xref ref-type="bibr" rid="CR1">1</xref>]. However, in addition to supporting integers, many processors support fixed-point numbers. Fixed-point data types can be specified by a 3-tuple (<italic>wl</italic>, <italic>iwl</italic>, <italic>sign</italic>), where <italic>wl</italic> is the total word length, <italic>iwl</italic> is the integer word length (the number of bits left of the binary point), and sign <italic>s</italic> ∈{<italic>s</italic>, <italic>u</italic>} denotes whether numbers are unsigned or signed. See also Fig. <xref rid="Fig23" ref-type="fig">3.23</xref>. Furthermore, there may be different rounding modes (e.g., truncation) and overflow modes (e.g., saturating and wrap-around arithmetic). <index-term id="ITerm116"><term>arithmetic</term><index-term><term>saturating ˜</term></index-term></index-term><index-term id="ITerm117"><term>arithmetic</term><index-term><term>fixed-point ˜</term></index-term></index-term><fig id="Fig23"><label>Fig. 3.23</label><caption xml:lang="en"><p>Parameters of a fixed-point number system</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig23_HTML.png" id="MO25"/></fig></p><p id="Par156">For fixed-point numbers, the position of the binary point is maintained after multiplications (some low-order bits are truncated or rounded). For fixed-point processors, this operation is supported by hardware.</p></list-item><list-item><p id="Par157"><bold>Real-time capability:</bold><index-term id="ITerm118"><term>real-time</term><index-term><term>capability</term></index-term></index-term> Some of the features of modern processors used in PCs are designed to improve the average execution time of programs. In many cases, it is difficult if not impossible to formally verify that they improve the worst case execution time. In such cases, it may be better not to implement these features. For example, it is difficult (though not impossible [<xref ref-type="bibr" rid="CR4">4</xref>]) to guarantee a certain speed-up resulting from the use of caches.<index-term id="ITerm119"><term>cache</term></index-term> Therefore, caches are sometimes not used for embedded applications. Also, virtual addressing and demand paging<xref ref-type="fn" rid="Fn14">14</xref> are frequently not found in embedded systems. Techniques for computing worst case execution times will be presented in subsection <ext-link xlink:href="10.1007/978-3-030-60910-8_5#Sec7" ext-link-type="doi">5.2.2</ext-link>.</p></list-item></list></p></sec><sec><p id="Par159">Due to the importance of signal processing, instructions for DSP have been added to many instruction sets.</p></sec></sec><sec id="Sec19"><title>Multimedia and Short Vector Instruction Sets</title><p id="Par160"><index-term id="ITerm120"><term>processor</term><index-term><term>multimedia ˜</term></index-term></index-term></p><p id="Par161">Registers and arithmetic units of many modern architectures are at least 64 bit wide. Two 32 bit data types, four 16 bit data types, or eight 8 bit data types (“bytes”) can be packed into a single 64 bit register (see Fig. <xref rid="Fig24" ref-type="fig">3.24</xref>).<fig id="Fig24"><label>Fig. 3.24</label><caption xml:lang="en"><p>Using 64 bit registers for 16 bit data types</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig24_HTML.png" id="MO26"/></fig></p><p id="Par162">Arithmetic units can be designed such that they suppress carry bits at 32 bit, 16 bit, or byte boundaries. Multimedia instruction sets exploit this fact by supporting operations on packed data types. Such instructions are sometimes called single-instruction, multiple-data (SIMD) instructions,<index-term id="ITerm121"><term>SIMD-instructions</term></index-term> since a single instruction encodes operations on several data elements. With bytes packed into 64 bit registers, speed-ups of up to about eight over non-packed data types are possible. Data types are typically stored in packed form in memory. Unpacking and packing are avoided if arithmetic operations on packed data types are used. Furthermore, multimedia instructions can usually be combined with saturating arithmetic and therefore provide a more efficient form of overflow handling than standard instructions. Hence, the overall speed-up achieved with multimedia instructions can be significantly larger than the factor of eight enabled by operations on packed 64 bit data types. Due to the advantages of operations on packed data types, new instructions have been added to several processors. For example, so-called streaming SIMD extensions (SSE) have been added to Intel’s family of Pentium<sup>®</sup>-compatible processors [<xref ref-type="bibr" rid="CR247">247</xref>]. New instructions have also been called <bold>short vector instructions</bold> and introduced by Intel<sup>®</sup> as Advanced Vector Extensions (AVX) [<xref ref-type="bibr" rid="CR248">248</xref>]. <index-term id="ITerm122"><term>short vector instructions</term></index-term><index-term id="ITerm123"><term>SSE</term></index-term></p></sec><sec id="Sec20"><title>Very Long Instruction Word (VLIW) Processors</title><sec><p id="Par163"><index-term id="ITerm124"><term>processor</term><index-term><term>very long instruction word ˜</term></index-term></index-term><index-term id="ITerm125"><term>very long instruction word (VLIW)</term></index-term></p></sec><sec><p id="Par164">Computational demands for embedded systems are increasing, especially when multimedia applications, advanced coding techniques, or cryptography are involved. Performance improvement techniques used in high-performance microprocessors are not appropriate for embedded systems: driven by the need for instruction set compatibility, processors found, for example, in PCs spend a huge amount of resources and energy on<index-term id="ITerm126"><term>energy</term></index-term> automatically finding parallelism in application programs. Still, their performance is frequently not sufficient. For embedded systems, we can exploit the fact that instruction set compatibility with PCs is not required. Therefore, we can use instructions which explicitly identify operations to be performed in parallel. This is possible with <bold>explicit parallelism instruction set computers</bold> (EPICs). <index-term id="ITerm127"><term>explicit parallelism instruction set computers (EPIC)</term></index-term> With EPICs, detection of parallelism is moved from the processor to the compiler. This avoids spending silicon and energy on the detection of parallelism at run-time. As a special case, we consider very long instruction word (VLIW) processors. For VLIW processors, several operations or instructions are encoded in a long instruction word (sometimes called <bold>instruction packet</bold>) and are assumed to be executed in parallel. Each operation/instruction is encoded in a separate field of the instruction packet. Each field controls certain hardware units. Four such fields are used in Fig. <xref rid="Fig25" ref-type="fig">3.25</xref>, each one controlling one of the hardware units. <index-term id="ITerm128"><term>very long instruction word (VLIW)</term></index-term><fig id="Fig25"><label>Fig. 3.25</label><caption xml:lang="en"><p>VLIW architecture (example)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig25_HTML.png" id="MO27"/></fig></p></sec><sec><p id="Par165">For VLIW architectures, the compiler has to generate instruction packets. This requires that the compiler is aware of the available hardware units and schedules their use.</p></sec><sec><p id="Par166">Instruction fields must be present, regardless of whether or not the corresponding functional unit is actually used in a certain instruction cycle. As a result, the code density of VLIW architectures may be low if insufficient parallelism is detected to keep all functional units busy. The problem can be avoided if more flexibility is added.</p></sec><sec><p id="Par167">For example, the Texas Instruments TMS 320C6xx family of processors implements a variable instruction packet size of up to 256 bits. In each instruction field, 1 bit is reserved to indicate whether or not the operation encoded in the next field is still assumed to be executed in parallel. No instruction bits are wasted for unused functional units. Due to its variable length instruction packets, TMS 320C6xx processors do not quite correspond to the classical model of VLIW processors. Due to their explicit description of parallelism, they are EPIC processors, though. <index-term id="ITerm129"><term>very long instruction word (VLIW)</term></index-term></p></sec><sec><p id="Par168">Implementing register files for VLIW and EPIC processors is far from trivial. Due to the large number of operations that can be performed in parallel, a large number of register accesses has to be provided in parallel. Therefore, a large number of ports is required. However, the delay, size, and energy consumption of register files<index-term id="ITerm130"><term>energy</term></index-term> increase with their number of ports. Hence, register files with very large numbers of ports are inefficient. As a consequence, many VLIW/EPIC architectures use partitioned register files. Functional units are then only connected to a subset of the registers.</p></sec><sec id="FPar12"><title><bold>VLIW Pipelines</bold></title><p id="Par169">A potential problem of VLIW and EPIC architectures is their possibly large <bold>delay penalty</bold>: this delay penalty might originate from branch instructions found in some instruction packets. <index-term id="ITerm131"><term>very long instruction word (VLIW)</term></index-term> Instruction packets normally must pass through pipelines. Each stage of these pipelines implements only part of the operations to be performed by the instructions executed. Branch instructions cannot be detected in the first stage of the pipeline. When the execution of the branch instruction is finally completed, additional instructions have already entered the pipeline (see Fig. <xref rid="Fig26" ref-type="fig">3.26</xref>).<fig id="Fig26"><label>Fig. 3.26</label><caption xml:lang="en"><p>Branch instruction and delay slots</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig26_HTML.png" id="MO28"/></fig></p><p id="Par170">There are essentially two ways to deal with these additional instructions: <list list-type="order"><list-item><p id="Par171">They are executed as if no branch had been present. This case is called <bold>delayed branch</bold>. Instruction packet slots that are still executed after a branch are called <bold>branch delay slots</bold>. These branch delay slots can be filled with instructions which would be executed before the branch if there were no delay slots. However, it is normally difficult to fill all delay slots with useful instructions, and some must be filled with no-operation instructions (NOPs). The term <bold>branch delay penalty</bold> denotes the loss of performance resulting from these NOPs.<index-term id="ITerm132"><term>branch delay penalty</term></index-term></p></list-item><list-item><p id="Par172">The pipeline is stalled until instructions from the branch target address have been fetched. There are no branch delay slots in this case. In this organization, the branch delay penalty is caused by the stall.</p></list-item></list></p><p id="Par173">Branch delay penalties<index-term id="ITerm133"><term>branch delay penalty</term></index-term> can be significant, and efficiency can be improved by avoiding branches if possible. In order to avoid branches originating from <bold>if</bold> statements, <bold>predicated instructions</bold> have been introduced (see p. 23).</p><p id="Par174">The Crusoe<sup>™</sup> processor is a (commercially finally unsuccessful) example of an EPIC processor designed for PCs [<xref ref-type="bibr" rid="CR295">295</xref>]. Its instruction set includes 64 bit and 128 bit VLIW instructions. <index-term id="ITerm134"><term>very long instruction word (VLIW)</term></index-term> Efforts for making EPIC instruction sets available in the PC sector resulted in Intel’s IA-64 instruction set [<xref ref-type="bibr" rid="CR249">249</xref>] and its implementation in the Itanium<sup>®</sup> processor. Due to legacy problems, it has been used mainly in the server market. Many MPSoCs (see p. 36) are based on VLIW and EPIC processors.</p></sec></sec><sec id="Sec21"><title>Multi-core Processors</title><p id="Par175"><index-term id="ITerm135"><term>processor</term><index-term><term>multi-core ˜</term></index-term></index-term><index-term id="ITerm136"><term>core</term><index-term><term>multi ˜</term></index-term></index-term></p><p id="Par176">Processor features for single processors described above have helped to design high-performance processors in a resource-aware manner. However, it turned out that a further performance increase for single processors hits the <bold>power wall</bold>: <index-term id="ITerm137"><term>power wall</term></index-term> a further increase in clock speeds would result in a too large power consumption and in too hot circuits. Further increase in the level of VLIW parallelism was not feasible either. Due to advances in fabrication technology, it is now feasible to manufacture multiple processors on the same semiconductor die. Multiple processors integrated on the same chip are called <bold>multicores</bold>. This is in contrast to multiprocessor systems which have been used in computing centers for decades. The integration of multiple cores on the same die enables a much faster communication, compared to multiprocessor systems. Also, this approach facilitates the sharing of resources (such as caches) among the cores. As an example, Fig. <xref rid="Fig27" ref-type="fig">3.27</xref> demonstrates the architecture of the Intel<sup>®</sup> Core<sup>™</sup> Duo [<xref ref-type="bibr" rid="CR540">540</xref>].<fig id="Fig27"><label>Fig. 3.27</label><caption xml:lang="en"><p>Intel<sup>®</sup> Core<sup>™</sup> Duo Processor</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig27_HTML.png" id="MO29"/></fig></p><p id="Par177">In this case, L1 caches are private, whereas L2 caches are shared. Implementing efficient accesses to caches needs some consideration [<xref ref-type="bibr" rid="CR540">540</xref>]. With such architectures, cache coherence is becoming an issue also within one die. This means, we have to know whether updates of data and possibly also instructions by one core are seen by the others. Protocols for automatic cache coherence (like the MESI protocol) are known for many years in computer architecture [<xref ref-type="bibr" rid="CR211">211</xref>]. Now, they have to be implemented on the chip. Scalability is an issue: for how many cores can we reasonably provide enough bandwidth in the communication architecture to always keep caches coherent? Also, the system memory bandwidth may be insufficient for a growing number of cores. Architectures other than the above Intel architecture exist.</p><p id="Par178">In the architecture of Fig. <xref rid="Fig27" ref-type="fig">3.27</xref>, all processors are of the same type. Such an architecture is called a <bold>homogeneous multi-core</bold> architecture. <index-term id="ITerm138"><term>homogeneous multi-core</term></index-term> Advantages of homogeneous multi-core architectures include the fact that the design effort is limited (processors will be replicated) and that software can easily be migrated from one processor to another one. This is very useful in case one of the cores fails.</p><p id="Par179">In contrast to homogeneous multi-core architectures, there are also <bold>heterogeneous multi-core architectures</bold> incorporating processors of different types. Processors which are best suited for certain applications can be selected. Typically, heterogeneous architectures achieve the best energy efficiency that is feasible.</p><p id="Par180">In order to find a good compromise between homogeneous and (totally) heterogeneous architectures, architectures with a single instruction set but different internal architectures, so-called single-ISA heterogeneous multi-cores [<xref ref-type="bibr" rid="CR316">316</xref>], have been proposed. The ARM<sup>®</sup> big.LITTLE architecture is a very prominent example of this.</p><p id="Par181">Figure <xref rid="Fig28" ref-type="fig">3.28</xref> contains the pipeline architecture of the Cortex<sup>®</sup> -A15 processor [<xref ref-type="bibr" rid="CR165">165</xref>]. <index-term id="ITerm139"><term>ARM<sup>®</sup></term></index-term><index-term id="ITerm140"><term>Cortex<sup>®</sup></term></index-term><fig id="Fig28"><label>Fig. 3.28</label><caption xml:lang="en"><p>ARM<sup>®</sup> Cortex<sup>®</sup> -A15 pipeline</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig28_HTML.png" id="MO30"/></fig></p><p id="Par182">It is a complex pipeline, containing multiple pipeline stages for instruction fetch, instruction decoding, instruction issue, execution, and write-back. Instructions have to pass through at least 15 pipeline stages before their result is stored. Dynamic scheduling of instructions allows executing instructions in a sequence different from the one in which they are fetched from memory (so-called out-of-order execution). Several instructions can be issued in one clock cycle (so-called multi-issue). The architecture offers a high performance but requires much power. <index-term id="ITerm141"><term>power</term></index-term></p><p id="Par183">In contrast, Fig. <xref rid="Fig29" ref-type="fig">3.29</xref> shows the pipeline of the Cortex<sup>®</sup> -A7 architecture [<xref ref-type="bibr" rid="CR165">165</xref>].<fig id="Fig29"><label>Fig. 3.29</label><caption xml:lang="en"><p>ARM<sup>®</sup> Cortex<sup>®</sup> -A7 pipeline</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig29_HTML.png" id="MO31"/></fig></p><p id="Par184">It is a simple pipeline. Instructions pass through 8 to 11 stages; they are always processed in the order in which they are fetched from memory (so-called in-order execution). There are few situations in which two instructions are issued concurrently. Hence, the architecture is power-efficient but has a limited performance. <index-term id="ITerm142"><term>power</term></index-term></p><p id="Par185">Figure <xref rid="Fig30" ref-type="fig">3.30</xref> [<xref ref-type="bibr" rid="CR165">165</xref>] demonstrates trade-offs between power consumption and performance. For each of the two architectures shown, there is flexibility for these two objectives, depending upon the supply voltage and the clock frequency.<fig id="Fig30"><label>Fig. 3.30</label><caption xml:lang="en"><p>DVFS curves for a large, representative workload on a single A7 or A15</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig30_HTML.png" id="MO32"/></fig></p><p id="Par186">Obviously, the Cortex<sup>®</sup> -A15 is more appropriate for more demanding high-performance applications, e.g., in video processing. The Cortex<sup>®</sup> -A7 is more appropriate for “always-on applications” like low-volume message processing. It would be a waste of energy if mobile phones would only contain Cortex<sup>®</sup> -A15 cores.</p><p id="Par187">Therefore, today’s multi-core chips typically are heterogeneous in that they contain a mixture of high-performance and energy-efficient processors, as in Fig. <xref rid="Fig31" ref-type="fig">3.31</xref>.<fig id="Fig31"><label>Fig. 3.31</label><caption xml:lang="en"><p>ARM<sup>®</sup> big. LITTLE architecture comprising Cortex<sup>®</sup> -A7 and Cortex<sup>®</sup> -A15 cores</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig31_HTML.png" id="MO33"/></fig></p></sec><sec id="Sec22"><title>Graphics Processing Units (GPUs)</title><sec><p id="Par188"><index-term id="ITerm143"><term>graphics processing unit (GPU)</term></index-term></p></sec><sec><p id="Par189">In the last century, many computers used specialized graphics processing units (GPUs) in order to generate an appealing graphical representation of computer output. This hardwired solution suffered from being unable to support non-standard computer graphics algorithms. Therefore, these highly specialized GPUs have been replaced by programmable solutions. Current GPUs try to run a large number of computations concurrently in order to achieve the desired performance. The standard approach to concurrency is to run many fine-grained threads at the same time. The goal is to keep many processing units busy and to hide memory latencies by fast switching between threads.</p></sec><sec id="FPar13"><title>Example 3.6</title><p id="Par190">Let us consider the multiplication of two large matrices on a GPU. Figure <xref rid="Fig32" ref-type="fig">3.32</xref> [<xref ref-type="bibr" rid="CR211">211</xref>] shows how the computations can be mapped to a GPU.<fig id="Fig32"><label>Fig. 3.32</label><caption xml:lang="en"><p>Partitioning of matrix multiplication for execution of a GPU</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig32_HTML.png" id="MO34"/></fig></p><p id="Par191">The matrix is partitioned into so-called thread blocks. <index-term id="ITerm144"><term>thread block</term></index-term> Each thread block can be allocated to one of the cores contained in a GPU. Each thread block, in turn, contains a number of threads, and each thread includes a number of instructions. In Fig. <xref rid="Fig32" ref-type="fig">3.32</xref>, the overall set of computations is called a <bold>grid</bold>. ∇</p></sec><sec><p id="Par192">Each core will try to achieve progress by executing threads. If some thread gets blocked, e.g., due to waiting for memory, the core will execute some other thread. The instructions contained in a thread can also be executed concurrently, e.g., by using multiple pipelines. The thread blocks can be executed concurrently on contemporary GPUs. Fast switching between the execution of threads and in this way hiding memory latencies is an essential feature for GPUs. <index-term id="ITerm145"><term>graphics processing unit (GPU)</term></index-term></p></sec><sec id="FPar14"><title>Example 3.7</title><p id="Par193">Figure <xref rid="Fig33" ref-type="fig">3.33</xref> shows the architecture of the ARM<sup>®</sup> Mali<sup>™</sup> -T880 GPU [<xref ref-type="bibr" rid="CR23">23</xref>]. <index-term id="ITerm146"><term>Mali<sup>™</sup></term></index-term><fig id="Fig33"><label>Fig. 3.33</label><caption xml:lang="en"><p>ARM<sup>®</sup> Mali<sup>™</sup> -T880 GPU</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig33_HTML.png" id="MO35"/></fig></p><p id="Par194">The architecture is defined as intellectual property (IP), <index-term id="ITerm147"><term>intellectual property (IP)</term></index-term> comprising a synthesizable model. In this model, the number of SC cores is configurable between 1 and 16. Each core includes several pipelines for the execution of arithmetic, load/store, or texture-related instructions. In the thread issue hardware, as many threads as possible are issued each clock phase. The GPU also contains additional components like a memory management unit (see Appendix C), up to two caches and an AMBA<sup>®</sup> bus interface. <index-term id="ITerm148"><term>memory management unit (MMU)</term></index-term> Programming support includes an interface to the OpenGL library [<xref ref-type="bibr" rid="CR484">484</xref>] and to OpenCL (see <ext-link xlink:href="https://www.khronos.org/opencl/" ext-link-type="uri">https://www.khronos.org/opencl/</ext-link>). ∇</p></sec><sec><p id="Par195">In general, GPU computing achieves high performances in an energy-efficient way (see also Sect. <xref rid="Sec44" ref-type="sec">3.7.3</xref> on p. 67).</p></sec></sec><sec id="Sec23"><title>Multiprocessor Systems on a Chip (MPSoCs)</title><sec><p id="Par196"><index-term id="ITerm149"><term>multiprocessor system-on-a-chip (MPSoC)</term></index-term></p></sec><sec><p id="Par197">Going one step further, heterogeneous multi-core systems have also been merged with GPUs.</p></sec><sec id="FPar15"><title>Example 3.8</title><p id="Par198">Figure <xref rid="Fig34" ref-type="fig">3.34</xref> shows a contemporary heterogeneous multi-core system, also comprising a Mali GPU [<xref ref-type="bibr" rid="CR22">22</xref>]. <index-term id="ITerm150"><term>graphics processing unit (GPU)</term></index-term><fig id="Fig34"><label>Fig. 3.34</label><caption xml:lang="en"><p>ARM<sup>®</sup> big.LITTLE system on a chip (SoC)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig34_HTML.png" id="MO36"/></fig></p><p id="Par199">The architecture shown in Fig. <xref rid="Fig34" ref-type="fig">3.34</xref> does not only contain processor cores. Rather, it comprises a number of additional system components, such as memory management units (see Appendix C) and interfaces for peripheral devices. Overall, the idea behind this integration is to avoid extra chips for such functionality. <index-term id="ITerm151"><term>memory management unit (MMU)</term></index-term> As a result, a whole system is integrated on one chip. Therefore, we are calling such an architecture a system-on-a-chip (SoC) or even a multiprocessor system-on-a-chip (MPSoC) architecture. ∇</p></sec><sec><p id="Par200">Mapping techniques for such processors are important, since examples demonstrate that a power<index-term id="ITerm152"><term>power</term></index-term> efficiency close to that of ASICs can be achieved. For example, for IMEC’s ADRES processor, an efficiency of 55 ∗ 10<sup>9</sup> operations per watt (about 50% of the power <index-term id="ITerm153"><term>power</term></index-term> efficiency of ASICs) has been predicted [<xref ref-type="bibr" rid="CR363">363</xref>, <xref ref-type="bibr" rid="CR481">481</xref>]. However, the design effort for such architectures is larger than in the homogeneous case.</p></sec><sec id="FPar16"><title>Example 3.9</title><p id="Par201">There are MPSoCs comprising processors which we introduced earlier: 66AK2x MPSoCs from Texas Instruments contain ARM<sup>®</sup> and C66xxx processors [<xref ref-type="bibr" rid="CR530">530</xref>] (see Fig. <xref rid="Fig35" ref-type="fig">3.35</xref>), demonstrating relevance of the presented processors. ∇<fig id="Fig35"><label>Fig. 3.35</label><caption xml:lang="en"><p>MPSoC 66AK from Texas Instruments<sup>®</sup> containing ARM<sup>®</sup> and C6xxx processors</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig35_HTML.png" id="MO37"/></fig></p></sec><sec><p id="Par202">The number and the diversity of components can be even larger. For example, there may be specialized processors for mobile communication or image processing.</p></sec><sec id="FPar17"><title>Example 3.10</title><p id="Par203">Figure <xref rid="Fig36" ref-type="fig">3.36</xref> contains a simplified floor-plan of the SH-MobileG1 chip [<xref ref-type="bibr" rid="CR205">205</xref>]. The chip demonstrates that highly specialized processors are being used. There are special processors for image processing (red), for GSM and 3G mobile communication (green), etc. In order to save energy, power is shut down for <index-term id="ITerm154"><term>energy</term></index-term> unused areas, causing these areas to be a special case of <bold>dark silicon</bold> (c.f. p. 14). <index-term id="ITerm155"><term>dark silicon</term></index-term> ∇<fig id="Fig36"><label>Fig. 3.36</label><caption xml:lang="en"><p>Floor-plan of the SH-MobileG1 chip</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig36_HTML.png" id="MO38"/></fig></p></sec><sec><p id="Par204">Specialized processors are used since progress in semiconductor manufacturing and the design of new architectures is slowing down. Hence, specialized processors are needed to meet performance targets. This view is supported by the architecture which we will present next.</p></sec><sec id="FPar18"><title>Example 3.11</title><p id="Par205">Around 2013, Google predicted that it would soon become very expensive to provide the expected pattern recognition performance in their data centers with conventional CPUs or GPUs. As a result, the design of specialized machine learning processors for fast classification with deep neural networks (DNNs) was started with a high priority. The resulting so-called Tensor Processing Unit (TPU) architecture is shown in Fig. <xref rid="Fig37" ref-type="fig">3.37</xref>. <index-term id="ITerm156"><term>Tensor Processing Unit (TPU)</term></index-term><fig id="Fig37"><label>Fig. 3.37</label><caption xml:lang="en"><p>Tensor processing unit (TPU), v1, for fast classification [<xref ref-type="bibr" rid="CR277">277</xref>, <xref ref-type="bibr" rid="CR448">448</xref>]</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig37_HTML.png" id="MO39"/></fig></p><p id="Par206"><index-term id="ITerm157"><term>Tensor Processing Unit (TPU)</term></index-term><index-term id="ITerm158"><term>Deep Neural Network (DNN)</term></index-term></p><p id="Par207">At the core of the architecture, there is a 256 by 256 array of MAC units. 64k 8 bit MAC operations can be performed in a single cycle; 16 bit operations require more cycles. DNNs consist of layers of computations, where at each layer MAC operations involving weight factors are required. These are performed by “pumping” input data or data from intermediate layers through the MAC matrix. Each cycle, 256 result values become available. TPU version 1 outperforms commonly used CPUs and GPUs by a factor of 29.2 and 13.3, respectively. The performance/power ratio is improved by factors of 34 and 16, respectively. More recently, Google announced second- and third-generation TPUs [<xref ref-type="bibr" rid="CR93">93</xref>]. They do also support training DNNs. ∇</p></sec></sec></sec><sec id="Sec24"><title>Reconfigurable Logic</title><sec><p id="Par208"><index-term id="ITerm159"><term>logic</term><index-term><term>reconfigurable ˜</term></index-term></index-term><index-term id="ITerm160"><term>FPGA</term></index-term></p></sec><sec><p id="Par209">In many cases, full-custom hardware chips (ASICs)<index-term id="ITerm161"><term>application-specific integrated circuit (ASIC)</term></index-term> are too expensive, and software-based solutions are too slow or too energy-consuming.<index-term id="ITerm162"><term>energy</term></index-term> Reconfigurable logic provides a solution if algorithms can be efficiently implemented in custom hardware. It can be almost as fast as special-purpose hardware, but in contrast to special-purpose hardware, the performed function can be changed by using configuration data. Due to these properties, reconfigurable logic finds applications in the following areas: <list list-type="bullet"><list-item><p id="Par210"><bold>Fast prototyping:</bold> Modern ASICs can be very complex and the design effort can be large and take a long time. It is therefore frequently desirable to generate a prototype, which can be used for experimenting with a system which behaves “almost” like the final system. The prototype can be more costly and larger than the final system. Also, its power<index-term id="ITerm163"><term>power</term></index-term> consumption can be larger than the final system, some timing constraints can be relaxed, and only the essential functions need to be available. Such a system can then be used for checking the fundamental behavior of the future system.</p></list-item><list-item><p id="Par211"><bold>Low-volume applications:</bold> If the expected market volume is too small to justify the development of special-purpose ASICs, reconfigurable logic can be the right hardware technology for applications, for which software would be too slow or too inefficient.</p></list-item><list-item><p id="Par212"><bold>Real-time systems</bold>: The timing of reconfigurable logic-based designs is typically known very precisely. Therefore, they can be used to implement timing-predictable systems.</p></list-item><list-item><p id="Par213">Applications benefiting from a very <bold>high level of parallel processing</bold>: For example, parallel searches for certain patterns can be implemented as parallel hardware. Therefore, reconfigurable logic is employed in searches for genetic information, for patterns in Internet messages, in stock data, in seismic analysis, and more.</p></list-item></list></p></sec><sec><p id="Par214">Reconfigurable hardware frequently includes random access memory (RAM) to store configurations. We distinguish between <bold>persistent</bold> and <bold>volatile</bold> configuration memory. <index-term id="ITerm164"><term>persistent storage</term></index-term><index-term id="ITerm165"><term>volatile storage</term></index-term> For persistent memory, information is retained when power is shut off. For volatile memory, the information is lost once power is shut down. If the configuration memory is volatile, its content must be loaded from some persistent storage technology such as read-only memories (ROMs) or flash memories at startup.</p></sec><sec><p id="Par215"><bold>Field programmable gate arrays</bold> (FPGAs)<index-term id="ITerm166"><term>field programmable gate array (FPGA)</term></index-term> are the most common form of reconfigurable hardware. As the name indicates, such devices are programmable “in the field” (after fabrication). Furthermore, they consist of arrays of processing elements. As an example, Fig. <xref rid="Fig38" ref-type="fig">3.38</xref> shows the column-based structure of the Xilinx<sup>®</sup> UltraScale architecture [<xref ref-type="bibr" rid="CR602">602</xref>].<xref ref-type="fn" rid="Fn15">15</xref> Some columns contain I/O interfaces, clock devices, and/or RAM. Other columns comprise <bold>configurable logic blocks</bold> (CLBs), special hardware for digital signal processing, and some RAM. CLBs are the key components. They provide configurable functions. The architecture of Xilinx<sup>®</sup> UltraScale CLBs is shown in Fig. <xref rid="Fig39" ref-type="fig">3.39</xref> [<xref ref-type="bibr" rid="CR599">599</xref>]. <index-term id="ITerm167"><term>configurable logic block (CLB)</term></index-term><fig id="Fig38"><label>Fig. 3.38</label><caption xml:lang="en"><p>Floor-plan of column-based Xilinx<sup>®</sup> UltraScale FPGAs</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig38_HTML.png" id="MO40"/></fig><fig id="Fig39"><label>Fig. 3.39</label><caption xml:lang="en"><p>Xilinx<sup>®</sup> UltraScale CLB (one of eight blocks shown)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig39_HTML.png" id="MO41"/></fig></p></sec><sec><p id="Par217">In this architecture, each CLB contains eight blocks. Each block comprises a RAM which is used to implement logic functions by a look-up table (LUT, shown in red), two registers, multiplexers, and some additional logic. Each LUT has six address inputs and two outputs. It can implement any single Boolean function of six variables or two functions of five variables (provided that the two functions share input variables). This means that all 2<sup>64</sup> functions of 6 variables or all 2<sup>32</sup> functions of 5 inputs can be implemented! This is the key means for achieving <bold>configurability</bold>. <index-term id="ITerm168"><term>configurability</term></index-term> In addition, the logic contained in such a block can also be configured. This includes the control of the two registers, which can be programmed to store results of the LUT or some direct input values. Blocks in a CLB can be combined to form adders, multiplexers, shift registers, or memories. Configuration data determines the setting of multiplexers in the CLBs, the clocking of registers and RAM, the content of RAM components, and the connections between CLBs. Some of the LUTs can also be used as RAM. A single CLB can store up to 512 bits.</p></sec><sec><p id="Par218">Several CLBs can be combined to create, for example, adders having a larger bit width, memories having a larger capacity, or complex logic functions.</p></sec><sec><p id="Par219">Currently available FPGAs comprise a large number of specialized blocks, like hardware for digital signal processing (DSP), some memory, high-speed I/O devices for various I/O standards, a decryption facility for FPGA configuration data, debugging support, ADCs, high-speed clocking, etc.</p></sec><sec id="FPar19"><title>Example 3.12</title><p id="Par220">Virtex<sup>®</sup> UltraScale<sup>™</sup> VU13P devices include 1728 k LUTs, 48 Mbit distributed RAM, 94.5 Mbit “Block RAM,” 360 Mbit “UltraRAM,” about 12 k specialized DSP devices, 4 PCIe<sup>®</sup> devices, Ethernet interfaces, and up to 832 I/O pins [<xref ref-type="bibr" rid="CR601">601</xref>]. ∇</p></sec><sec><p id="Par221">Integration of reconfigurable computing with processors and software is simplified if processors are available in the FPGAs. <index-term id="ITerm169"><term>field programmable gate array (FPGA)</term></index-term> There may be either <bold>hard cores</bold> or <bold>soft cores</bold>. <index-term id="ITerm170"><term>core</term><index-term><term>soft ˜</term></index-term></index-term><index-term id="ITerm171"><term>hard core</term></index-term> For hard cores, the layout contains a special area implementing a core in a dense way. This area cannot be used for anything but the hard core. Soft cores are available as synthesizable models which are mapped to standard CLBs. Soft cores are more flexible but less efficient than hard cores. Soft cores can be implemented on any FPGA chip.</p></sec><sec id="FPar20"><title>Example 3.13</title><p id="Par222">The MicroBlaze processor [<xref ref-type="bibr" rid="CR598">598</xref>] is an example of a soft core. ∇</p></sec><sec id="FPar21"><title>Example 3.14</title><p id="Par223">At the time of writing this book, hard cores are available, for example, on Zynq UltraScale+ MPSoCs. They contain up to four ARM<sup>®</sup> Cortex-A53 cores, <index-term id="ITerm172"><term>ARM<sup>®</sup></term></index-term> two ARM Cortex-R5 cores, and a Mali-400MP2 GPU processor [<xref ref-type="bibr" rid="CR602">602</xref>]. ∇</p></sec><sec><p id="Par224"><index-term id="ITerm173"><term>graphics processing unit (GPU)</term></index-term></p></sec><sec><p id="Par225">Typically, configuration data is generated from a high-level description of the functionality of the hardware, for example, in VHDL. FPGA vendors provide the necessary design kits. Ideally, the same description could also be used for generating ASICs automatically. In practice, some interaction is required. Exploitation of the available parallelism typically requires manually parallelized applications, since automatic parallelization is frequently very limited. The parallelism offered by FPGAs is typically not fully exploited if all computations are mapped to processor cores. Overall, FPGAs allow implementing a huge variety of hardware devices without any need to create hardware other than FPGA boards.</p></sec><sec id="FPar22"><title>Example 3.15</title><p id="Par226">Currently (in 2020), alternate providers of FPGAs include Altera<sup>®</sup> (see <ext-link xlink:href="http://www.altera.com" ext-link-type="uri">http://www.altera.com</ext-link>, acquired by Intel<sup>®</sup>), Lattice Semiconductor (see <ext-link xlink:href="http://www.latticesemi.com" ext-link-type="uri">http://www.latticesemi.com</ext-link>), QuickLogic (see <ext-link xlink:href="http://www.quicklogic.com" ext-link-type="uri">http://www.quicklogic.com</ext-link>), Microsemi (formerly Actel; see <ext-link xlink:href="http://www.microsemi.com" ext-link-type="uri">http://www.microsemi.com</ext-link>), and Chinese vendors. ∇</p></sec></sec></sec><sec id="Sec25"><title>Memories</title><p id="Par227"><index-term id="ITerm174"><term>memory</term></index-term></p><sec id="Sec26"><title>Conflicting Goals</title><sec><p id="Par228">Data, programs, and FPGA configurations must be stored in some kind of memory. Memories must have a capacity as large as required by the applications, provide the expected performance, and still be efficient in terms of cost, size, and energy consumption. Requirements for memories also include the expected reliability and access granularity (e.g., bytes, words, pages). Furthermore, we distinguish between <bold>persistent</bold> and <bold>volatile</bold> memory (see p. 39). <index-term id="ITerm175"><term>volatile storage</term></index-term><index-term id="ITerm176"><term>persistent storage</term></index-term> The mentioned requirements are conflicting, as has already been observed by Burks, Goldstine, and von Neumann in 1946 [<xref ref-type="bibr" rid="CR78">78</xref>]:</p></sec><sec><p id="Par229">“<italic>Ideally one would desire an indefinitely large memory capacity such that any particular …word …would be immediately available — i.e. in a time which is …shorter than the operation time of a fast electronic multiplier. …It does not seem possible physically to achieve such a capacity.</italic>”</p></sec><sec><p id="Par230">Access times of some currently available memories can be estimated with CACTI. <index-term id="ITerm177"><term>CACTI</term></index-term> These estimates are based on the tentative generation of a memory layout and the extraction of capacitances [<xref ref-type="bibr" rid="CR589">589</xref>]. Many different parameters enable the selection of an appropriate fabrication technology.<xref ref-type="fn" rid="Fn16">16</xref></p></sec><sec id="FPar23"><title>Example 3.16</title><p id="Par232">Figure <xref rid="Fig40" ref-type="fig">3.40</xref> shows the results for a range of exponentially increasing sizes [<xref ref-type="bibr" rid="CR36">36</xref>]. Obviously, the access time increases as a function of the capacity of memories: the larger the memory, the longer it takes to access information. In addition, Fig. <xref rid="Fig40" ref-type="fig">3.40</xref> also includes the energy consumption. Large memories also tend to be energy-inefficient. The impact of the capacity of the memory on the energy consumption is even larger than the impact on the access time. ∇<fig id="Fig40"><label>Fig. 3.40</label><caption xml:lang="en"><p>Delay and access time of random access memory as predicted by CACTI</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig40_HTML.png" id="MO42"/></fig></p></sec><sec><p id="Par233">For a number of years, the difference in speeds between processors and memories increased (see Fig. <xref rid="Fig41" ref-type="fig">3.41</xref>) until processor clock rates saturated (around 2003). While the speed of memories increased by only a factor of about 1.07 per year, overall processor performance increased by a factor of 1.5–2 per year [<xref ref-type="bibr" rid="CR358">358</xref>]. Overall, the gap between processor performance and memory speeds has become large. Accordingly, a further increase of the overall performance is made at least very difficult due to memory access times. This fact has also been called the <bold>memory wall</bold> [<xref ref-type="bibr" rid="CR358">358</xref>]. <index-term id="ITerm178"><term>memory wall</term></index-term> Further increase of clock rates of single processors has come to a standstill, but the large gap remains which existed when clock speeds became essentially saturated and multi-cores require additional memory bandwidth. As a result, we have to find compromises between the different requirements for the memory architecture.<fig id="Fig41"><label>Fig. 3.41</label><caption xml:lang="en"><p>Historical speed gap increase (until about 2003)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig41_HTML.png" id="MO43"/></fig></p></sec></sec><sec id="Sec27"><title>Memory Hierarchies</title><p id="Par234">Due to the observed conflicts, Burks, Goldstine, and von Neumann wrote already in 1946 [<xref ref-type="bibr" rid="CR78">78</xref>]: “<italic>We are therefore forced to recognize the possibility of constructing a hierarchy of memories, each of which has greater capacity than the preceding but which is less quickly accessible.</italic>”</p><p id="Par235">The exact structure of the hierarchy depends on technological parameters and also on the application area. Typically, we can identify at least the following levels in the memory hierarchy: <list list-type="bullet"><list-item><p id="Par236"><bold>Processor registers</bold> can be seen as the fastest level in the memory hierarchy, with only a limited capacity of at most a few hundred words.</p></list-item><list-item><p id="Par237">The <bold>working memory</bold> (or <bold>main memory</bold>) of computer systems implements the storage implied by processor memory addresses. Usually it has a capacity between a few megabytes and some gigabytes and is volatile.</p></list-item><list-item><p id="Par238">Typically there is a large access speed difference between the main memory and registers. Hence, many systems include some type of buffer memory. Frequently used buffer memories include <bold>caches</bold>, <bold>translation look-aside buffers</bold> (TLBs; see Appendix C), and <bold>scratchpad memory</bold> (SPM). <index-term id="ITerm179"><term>cache</term></index-term> In contrast to PC-like systems and compute servers, the architecture of these small memories should guarantee a predictable real-time performance. A combination of small memories containing frequently used data and instructions and a larger memory containing the remaining data and instructions is generally also more energy efficient than a single, large memory.</p></list-item><list-item><p id="Par239">Memories introduced so far are normally implemented in volatile memory technologies. In order to provide persistent storage, some different memory technology must be used. For embedded systems, flash memory is frequently the best solution. In other cases, hard disks or Internet-based storage solutions (like the “cloud”) may be used. <index-term id="ITerm180"><term>cloud storage</term></index-term></p></list-item></list></p><p id="Par240">Memory hierarchies can be exploited in order to achieve a compromise between the design goals for the memory. Memory<index-term id="ITerm181"><term>energy</term></index-term> partitioning has been considered, for example, by A. Macii [<xref ref-type="bibr" rid="CR360">360</xref>]. New memory technologies (including persistent memories) have the potential to change currently dominating hierarchies [<xref ref-type="bibr" rid="CR388">388</xref>].</p></sec><sec id="Sec28"><title>Register Files</title><p id="Par241">The mentioned impact of the storage capacity on access times and energy consumption applies even to small memories such as register files. Figure <xref rid="Fig42" ref-type="fig">3.42</xref> shows the cycle time and the power<index-term id="ITerm182"><term>power</term></index-term> as a function of the size of memories used as register files [<xref ref-type="bibr" rid="CR471">471</xref>]. The power needs to be considered due to frequent accesses to registers, as a result of which they can get very hot.<fig id="Fig42"><label>Fig. 3.42</label><caption xml:lang="en"><p>Cycle time and power as a function of the register file size</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig42_HTML.png" id="MO44"/></fig></p></sec><sec id="Sec29"><title>Caches</title><p id="Par242"><index-term id="ITerm183"><term>efficiency</term><index-term><term>energy ˜</term></index-term></index-term></p><p id="Par243">For caches it is required that the hardware checks whether or not the cache has a valid copy of the information associated with a certain address. This check involves comparing the tag fields of caches, containing a subset of the relevant address bits [<xref ref-type="bibr" rid="CR211">211</xref>]. If the cache has no valid copy, the information in the cache is automatically updated.</p><p id="Par244">Caches<index-term id="ITerm184"><term>cache</term></index-term> were initially introduced in order to provide good run-time efficiency. The name is derived from the French word <italic>cacher</italic> (to hide), indicating that programmers do not need to see or to be aware of caches, since updating information in caches is automatic. However, when large amounts of information need to be accessed, caches are not so invisible anymore. This has been demonstrated very nicely by Drepper [<xref ref-type="bibr" rid="CR139">139</xref>]. Drepper analyzed execution times of a program traversing a linear list of entries. Each entry contained one 64 bit pointer to the next entry plus NPAD 64 bit words. Execution times were measured for a Pentium P4 processor comprising a 16 kB level 1 cache requiring 4 processor cycles per access, a 1 MB level 2 cache requiring 14 processor cycles per access, and a main memory requiring 200 cycles per access. Figure <xref rid="Fig43" ref-type="fig">3.43</xref> shows the average number of cycles per access to one list element as a function of the total size of the list for the case NPAD=0. For small sizes of the list, four cycles are required per list element. This means that we are almost always accessing the level 1 cache, since it is large enough for this size of the list. If we increase the size of the list, we need eight cycles per access on average. In this case, we are accessing the level 2 cache. However, since the cache block size is large enough to hold two list elements, only every second access is actually an access to the level 2 cache. For even larger lists, the access time increases to nine cycles. In these cases, the list is larger than the level 2 cache, but automatic prefetching of level 2 cache entries hides some of the access latency of the main memory.<fig id="Fig43"><label>Fig. 3.43</label><caption xml:lang="en"><p>Average number of cycles per access for NPAD=0</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig43_HTML.png" id="MO45"/></fig></p><p id="Par245">Figure <xref rid="Fig44" ref-type="fig">3.44</xref> shows the average number of cycles per access to one list element as a function of the total size of the list for cases NPAD=0, 7, 15, and 31. For NPAD=7, 15, and 31, prefetching fails due to the larger size of list items. Obviously, we see a dramatic increase of access times. This means that <bold>the cache architecture has a strong impact on the execution times of applications</bold>. Increasing cache size will only change the size of the application at which this increase in execution times happens. Clever exploitation of hierarchies can have a large impact on execution times.<fig id="Fig44"><label>Fig. 3.44</label><caption xml:lang="en"><p>Average number of cycles per access for NPAD=0, 7, 15, 31</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig44_HTML.png" id="MO46"/></fig></p><p id="Par246">So far, we have just looked at the impact of capacity on access times. In the context of Fig. <xref rid="Fig40" ref-type="fig">3.40</xref> however, it is obvious that caches potentially also improve the energy efficiency of a memory system. Accesses to caches are accesses to small memories and therefore require less energy per access than large memories.</p><p id="Par247">Predicting cache misses and hits at design time is difficult and is a burden for the accurate prediction of real-time performance (see p. 246).</p></sec><sec id="Sec30"><title>Scratchpad Memories</title><p id="Par248">Alternatively, small memories can be mapped into the address space (see Fig. <xref rid="Fig45" ref-type="fig">3.45</xref>).<fig id="Fig45"><label>Fig. 3.45</label><caption xml:lang="en"><p>Memory map with scratchpad included</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig45_HTML.png" id="MO47"/></fig></p><p id="Par249">Such memories are called <bold>scratchpad memories</bold> (SPMs) or <bold>tightly coupled memories</bold> (TCM).<index-term id="ITerm185"><term>scratchpad memory (SPM)</term></index-term><index-term id="ITerm186"><term>tightly coupled memory (TCM)</term></index-term> SPMs are accessed by a proper selection of memory addresses. There is no need for checking tags, as for caches. Instead, the SPM is accessed whenever some simple address decoder is signaling an address to be in the address range of the SPM. SPMs are typically integrated together with processors on the same die. Hence, they are a special case of <bold>on-chip memories</bold>. For <italic>n</italic>-way set associative caches, reads are usually reading <italic>n</italic> entries in parallel and select the right entry only afterward. These energy-hungry parallel reads are avoided for SPMs. As a result, SPMs are very energy-efficient.</p><p id="Par250">Figure <xref rid="Fig46" ref-type="fig">3.46</xref> shows a comparison between the energy required per access to the scratchpad (SPM) and the energy required per access to the cache.<fig id="Fig46"><label>Fig. 3.46</label><caption xml:lang="en"><p>Energy consumption per scratchpad and cache access</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig46_HTML.png" id="MO48"/></fig></p><p id="Par251">For a two-way set associative cache, the two values differ by a factor of about three. The values in this example were computed using the energy<index-term id="ITerm187"><term>energy</term></index-term> consumption for RAM arrays as estimated by CACTI [<xref ref-type="bibr" rid="CR589">589</xref>]. A detailed comparison between figures of merit for caches and scratchpads was published by Banakar et al. [<xref ref-type="bibr" rid="CR36">36</xref>]. <index-term id="ITerm188"><term>CACTI</term></index-term><index-term id="ITerm189"><term>cache</term></index-term></p><p id="Par252">Frequently used variables and instructions should be allocated to the address space of SPMs. SPMs can improve the memory access times very predictably if the compiler is in charge of keeping frequently used variables in the SPM (see p. 363).</p></sec></sec><sec id="Sec31"><title>Communication</title><p id="Par253"><index-term id="ITerm190"><term>communication</term></index-term></p><p id="Par254">Information must be communicated before it can be processed in an embedded system. Communication is particularly important for the Internet of Things. <index-term id="ITerm191"><term>Internet of Things (IoT)</term></index-term> Information can be communicated through various <bold>channels</bold>. Channels are abstract entities characterized by the essential properties of communication, like maximum information transfer capacity and noise parameters. The probability of communication errors can be computed using communication theory techniques. The physical entities enabling communication are called communication <bold>media</bold>. Important media classes include wireless media (radio frequency media, infrared), optical media (fibers), and wires.</p><p id="Par255">There is a huge variety of communication requirements between the various classes of embedded systems. In general, connecting the different embedded hardware components is far from trivial. Some common requirements can be identified.</p><sec id="Sec32"><title>Requirements</title><p id="Par256">The following list contains some of the requirements that must be met: <list list-type="bullet"><list-item><p id="Par257"><bold>Real-time behavior</bold>:<index-term id="ITerm192"><term>real-time</term><index-term><term>behavior</term></index-term></index-term> This requirement has far-reaching consequences on the design of the communication system. Several low-cost solutions such as standard Ethernet fail to meet this requirement.<index-term id="ITerm193"><term>cost</term><index-term><term>of communication</term></index-term></index-term></p></list-item><list-item><p id="Par258"><bold>Efficiency</bold>:<index-term id="ITerm194"><term>efficiency</term></index-term> Connecting different hardware components can be expensive. For example, point-to-point connections in large buildings are almost impossible. Also, it has been found that separate wires between control units and external devices in cars significantly add to the cost and the weight of the car. With separate wires, it is also difficult to add new components. The need for cost efficiency also affects the way in which power <index-term id="ITerm195"><term>power</term></index-term> is made available to external devices. There is frequently the need to use a central power supply to reduce the cost.</p></list-item><list-item><p id="Par259"><bold>Appropriate bandwidth and communication delay</bold>: Bandwidth requirements of embedded systems may vary. It is important to provide sufficient bandwidth without making the communication system too expensive.</p></list-item><list-item><p id="Par260"><bold>Support for event-driven communication</bold>: Polling-based systems provide a very predictable real-time behavior. However, their communication delay may be too large, and there should be mechanisms for fast, event-oriented communication. For example, emergency situations should be communicated immediately and should not remain unnoticed until some central controller polls for messages.<index-term id="ITerm196"><term>cyber-physical system (CPS)</term></index-term></p></list-item><list-item><p id="Par261"><bold>Security/privacy:</bold><index-term id="ITerm197"><term>privacy</term></index-term> Ensuring security/privacy of confidential information (confidentiality) may require the use of encryption.<index-term id="ITerm198"><term>confidentiality</term></index-term></p></list-item><list-item><p id="Par262"><bold>Safety/robustness</bold>:<index-term id="ITerm199"><term>robustness</term></index-term> For safety-critical systems, the required level of safety must be achieved. This includes robustness: cyber-physical systems may be used at extreme temperatures, close to major sources of electromagnetic radiation, etc. Car engines, for example, can be exposed to temperatures of, e.g., less than − 20 and up to + 180<sup>∘</sup>C (− 4–356 <sup>∘</sup>F). Voltage levels and clock frequencies could be affected due to this large variation in temperatures. Still, reliable communication must be maintained.</p></list-item><list-item><p id="Par263"><bold>Fault tolerance</bold>:<index-term id="ITerm200"><term>fault</term><index-term><term>tolerance</term></index-term></index-term> Despite all the efforts for robustness, faults may occur. Cyber-physical systems should be operational even after faults, if at all feasible.<index-term id="ITerm201"><term>cyber-physical system (CPS)</term></index-term> Restarts, like the ones found in PCs, cannot be accepted. This means that retries may be required after attempts to communicate failed. A conflict exists with the first requirement: if we allow retries, then it is difficult to meet real-time requirements.</p></list-item><list-item><p id="Par264"><bold>Maintainability, diagnosability</bold>: Obviously, it should be possible to repair embedded systems within reasonable time frames.<index-term id="ITerm202"><term>maintainability</term></index-term><index-term id="ITerm203"><term>diagnosability</term></index-term></p></list-item></list></p><p id="Par265">These communication requirements are a direct consequence of the general characteristics of embedded/cyber-physical systems mentioned in Chap. <ext-link xlink:href="10.1007/978-3-030-60910-8_1" ext-link-type="doi">1</ext-link>. <index-term id="ITerm204"><term>cyber-physical system (CPS)</term></index-term> Due to the conflicts between some of the requirements, compromises must be made. For example, there may be different communication modes: one high-bandwidth mode guaranteeing real-time behavior but no fault tolerance (this mode is appropriate for multimedia streams) and a second fault-tolerant, low-bandwidth mode for short messages that must not be dropped.</p></sec><sec id="Sec33"><title>Electrical Robustness</title><p id="Par266"><index-term id="ITerm205"><term>robustness</term></index-term></p><p id="Par267">There are some basic techniques for electrical robustness. Digital communication within chips is normally using so-called single-ended signaling. For single-ended signaling, signals are propagated on a single wire (see Fig. <xref rid="Fig47" ref-type="fig">3.47</xref>).<index-term id="ITerm206"><term>differential signaling</term></index-term><fig id="Fig47"><label>Fig. 3.47</label><caption xml:lang="en"><p>Single-ended signaling</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig47_HTML.png" id="MO49"/></fig></p><p id="Par268">Such signals are represented by voltages with respect to a common ground (less frequently by currents). A single ground wire is sufficient for a number of single-ended signals. Single-ended signaling is very much susceptible to external noise. If external noise (originating from, e.g., motors being switched on) affects the voltage, messages can easily be corrupted. Also, it is difficult to establish high-quality common ground signals between a large number of communicating systems, due to the resistance (and self-inductance) on the ground wires. This is different for differential signaling. For differential signaling, each signal needs two wires (see Fig. <xref rid="Fig48" ref-type="fig">3.48</xref>).<fig id="Fig48"><label>Fig. 3.48</label><caption xml:lang="en"><p>Differential signaling</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig48_HTML.png" id="MO50"/></fig></p><p id="Par269">Using differential signaling, binary values are encoded as follows: if the voltage on the first wire with respect to the second is positive, then this is decoded as ’1’; otherwise values are decoded as ’0’. The two wires will typically be twisted to form so-called twisted pairs. There will be local ground signals, but a non-zero voltage between the local ground signals does not hurt. Advantages of differential signaling include the following: <list list-type="bullet"><list-item><p id="Par270">Noise is added to the two wires in essentially the same way. The comparator therefore removes almost all the noise.</p></list-item><list-item><p id="Par271">The logic value depends just on the polarity of the voltage between the two wires. The magnitude of the voltage can be affected by reflections or because of the resistance of the wires; this has no effect on the decoded value.</p></list-item><list-item><p id="Par272">Signals do not generate any currents on the ground wires. Hence, the quality of the ground wires becomes less important.</p></list-item><list-item><p id="Par273">No common ground wire is required. Hence, there is no need to establish a high-quality ground wiring between a large number of communicating partners.</p></list-item><list-item><p id="Par274">As a consequence of the properties mentioned so far, differential signaling allows a larger throughput than single-ended signaling.</p></list-item></list></p><p id="Par275">However, differential signaling requires two wires for every signal, and it also requires negative voltages (unless it is based on complementary logic signals using voltages for single-ended signals). Differential signaling is used, for example, in standard Ethernet-based networks and the universal serial bus (USB).</p></sec><sec id="Sec34"><title>Guaranteeing Real-Time Behavior</title><p id="Par276"><index-term id="ITerm207"><term>behavior</term><index-term><term>real-time ˜</term></index-term></index-term></p><p id="Par277">For internal communication, computers may be using dedicated point-to-point communication or shared buses. Point-to-point communication can have a good real-time behavior but requires many connections, and there may be congestion at the receivers. Wiring is easier with common, shared buses. Typically, such buses use priority-based arbitration if several access requests to the communication media exist (see, e.g., [<xref ref-type="bibr" rid="CR211">211</xref>]). Priority-based arbitration comes with poor timing predictability, since conflicts are difficult to anticipate at design time. Priority-based schemes can even lead to “starvation” (low-priority communication can be completely blocked by higher-priority communication). In order to get around this problem, <italic>time division multiple access</italic><index-term id="ITerm208"><term>time division multiple access (TDMA)</term></index-term> (TDMA) can be used. In a TDMA scheme, each partner is assigned a fixed time slot. The partner is only allowed to transmit during that particular time slot. Typically, communication time is divided into frames. Each frame starts with some time slot for frame synchronization and possibly some gap to allow the sender to turn off (see Fig. <xref rid="Fig49" ref-type="fig">3.49</xref>, [<xref ref-type="bibr" rid="CR302">302</xref>]).<fig id="Fig49"><label>Fig. 3.49</label><caption xml:lang="en"><p>TDMA-based communication</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig49_HTML.png" id="MO51"/></fig></p><p id="Par278"><index-term id="ITerm209"><term>TDMA</term></index-term></p><p id="Par279">This gap is followed by a number of slices, each of which serves for communicating messages. Each slice also contains some gap and guard time to take clock speed variations of the partners into account. Slices are assigned to communication partners. Variations of this scheme exist. For example, truncation of unused slices or the assignment of partners to several slices are feasible. TDMA reduces the maximum amount of data available per frame and partner but guarantees a certain bandwidth for all partners. Starvation can be avoided. The ARM AMBA bus [<xref ref-type="bibr" rid="CR21">21</xref>] includes TDMA-based bus allocation.<index-term id="ITerm210"><term>AMBA-bus</term></index-term><index-term id="ITerm211"><term>ARM<sup>®</sup></term></index-term></p><p id="Par280">Communication between computers is frequently based on Ethernet standards. For 10 and 100 Mbit/s versions of Ethernet, there can be collisions between various communication partners. This means several partners are trying to communicate at about the same time and the signals on the wires are corrupted. Whenever this occurs, the partners must stop communications, wait for some time, and then retry. The waiting time is chosen at random, so that it is not very likely that the next attempt to communicate results in another collision. This method is called <bold>carrier-sense multiple access with collision detection</bold> (CSMA/CD).<index-term id="ITerm212"><term>CSMA/CD</term></index-term> For CSMA/CD, communication time can become huge, since conflicts can repeat a large number of times, even though this is not very likely. Hence, CSMA/CD cannot be used when real-time constraints must be met.</p><p id="Par281">This problem can be solved with CSMA/CA (<bold>carrier-sense multiple access with collision avoidance</bold>).<index-term id="ITerm213"><term>CSMA/CA</term></index-term> As the name indicates, collisions are completely avoided, rather than just detected. For CSMA/CA, priorities are assigned to all partners. Communication media are allocated to communication partners during <bold>arbitration phases</bold>, which follow communication phases. During arbitration phases, partners wanting to communicate indicate this on the media. Partners finding such indications of higher priority must immediately remove their indication.</p><p id="Par282">Provided that there is an upper bound on the time between arbitration phases, CSMA/CA<index-term id="ITerm214"><term>CSMA/CA</term></index-term> guarantees a predictable real-time behavior for the partner having the highest priority. For other partners, real-time behavior can be guaranteed if the higher priority partners do not continuously request access to the media.</p><p id="Par283">Note that high-speed versions of Ethernet (≥1 Gbit/s) also avoid collisions. TDMA schemes are also used for wireless communication. For example, mobile phone standards like GSM use TDMA for accesses to the communication medium.<index-term id="ITerm215"><term>Global System for Mobile Communication (GSM)</term></index-term></p></sec><sec id="Sec35"><title>Examples</title><p id="Par284"><index-term id="ITerm216"><term>actuator</term></index-term><list list-type="bullet"><list-item><p id="Par285"><bold>Sensor/actuator buses:</bold> Sensor/actuator buses provide communication between simple devices such as switches or lamps and the processing equipment. There may be many such devices and the cost of the wiring needs special attention for such buses.<index-term id="ITerm217"><term>cost</term><index-term><term>of wiring</term></index-term></index-term></p></list-item><list-item><p id="Par286"><bold>Field buses:</bold> Field buses are similar to sensor/actuator buses. In general, they are supposed to support larger data rates than sensor/actuator buses. Examples of field buses include the following: <index-term id="ITerm218"><term>controller area network (CAN)</term></index-term><list list-type="bullet"><list-item><p id="Par287"><bold>Controller Area Network (CAN):</bold> This bus was developed in 1981 by Bosch and Intel for connecting controllers and peripherals. It is popular in the automotive industry, since it allows the replacement of a large amount of wires by a single bus. Due to the size of the automotive market, CAN components are relatively cheap and are therefore also used in other areas such as smart homes and fabrication equipment. CAN is based on differential signaling and arbitration using CSMA/CA. The encoding of signals is similar to that of serial (RS-232) lines of early PCs, with modifications for differential signaling. CSMA/CA-based arbitration does not prevent starvation. This is an inherent problem of the CAN protocol. Extensions exist.</p><p id="Par288"><index-term id="ITerm219"><term>time-triggered-protocol (TTP)</term></index-term></p></list-item><list-item><p id="Par289">The <bold>Time-Triggered Protocol (TTP)</bold> [<xref ref-type="bibr" rid="CR304">304</xref>]: This is a protocol for fault-tolerant safety systems like airbags in cars.</p></list-item><list-item><p id="Par290"><bold>FlexRay</bold>™ [<xref ref-type="bibr" rid="CR253">253</xref>]<index-term id="ITerm220"><term>FlexRay<sup>™</sup></term></index-term>: This is a TDMA protocol which has been developed by the FlexRay consortium (BMW, Daimler AG, General Motors, Ford, Bosch, Motorola, and Philips Semiconductors).</p><p id="Par291">FlexRay includes a static as well as a dynamic arbitration phase. The static phase uses a TDMA-like arbitration scheme. It can be used for real-time communication and starvation can be avoided. The dynamic phase provides a good bandwidth for non-real-time communication. Communicating partners can be connected to up to two buses for fault-tolerance reasons. <bold>Bus guardians</bold> may protect partners against partners flooding the bus with redundant messages, so-called babbling idiots.<index-term id="ITerm221"><term>bus guardian</term></index-term><index-term id="ITerm222"><term>babbling idiot</term></index-term> Partners may use their own local clock periods. Periods common to all partners are defined as multiples of such local clock periods. Time slots allocated to partners for communication are based on these common periods.</p><p id="Par292">The levi simulation allows simulating the protocol in a lab environment<index-term id="ITerm223"><term>levi simulation</term></index-term> [<xref ref-type="bibr" rid="CR495">495</xref>].</p></list-item><list-item><p id="Par293"><bold>LIN</bold> (Local Interconnect Network): This is a low-cost communication standard for connecting sensors and actuators in the automotive domain [<xref ref-type="bibr" rid="CR346">346</xref>]. <index-term id="ITerm224"><term>Local Interconnect Network (LIN)</term></index-term><index-term id="ITerm225"><term>actuator</term></index-term></p></list-item><list-item><p id="Par294"><bold>MAP:</bold> MAP<index-term id="ITerm226"><term>MAP-bus</term></index-term> is a bus designed for car factories.</p></list-item><list-item><p id="Par295"><bold>EIB:</bold><index-term id="ITerm227"><term>European Installation Bus (EIB)</term></index-term> The European Installation Bus (EIB) is a bus designed for smart homes.</p></list-item></list></p></list-item><list-item><p id="Par296">The <bold>Inter-Integrated Circuit</bold> (I<sup>2</sup>C) Bus <index-term id="ITerm228"><term>I<sup>2</sup>C</term></index-term>: This is a simple low-cost bus designed to communicate at short distances (meter range) with relatively low data rates. The bus needs only four wires: ground, SCL (clock), SDA (data), and a voltage supply line. Data and clock lines are open collector lines (see pp. 61–63). This means that connected devices pull these lines only toward ground. Separate resistors are needed to pull these lines up. The standard speed of I<sup>2</sup>C is 100 kb/s, but versions for 10 kb/s and up to 3.4 Mb/s do also exist. The voltage on the supply voltage line may vary between interfaces. Only the standards for detecting high and low logic levels are defined relative to the supply voltage. The bus is supported on some micro-controller boards.</p></list-item><list-item><p id="Par297"><bold>Wired multimedia communication</bold>: For wired multimedia communication, larger data rates are required. For example, <bold>MOST</bold> (Media Oriented Systems Transport) is a communication standard for multimedia and infotainment equipment in the automotive domain [<xref ref-type="bibr" rid="CR402">402</xref>]. <index-term id="ITerm229"><term>Media Oriented Systems Transport (MOST)</term></index-term> Standards like IEEE 1394 (FireWire) may be used for the same purpose.<index-term id="ITerm230"><term>IEEE 1394</term></index-term></p></list-item><list-item><p id="Par298"><bold>Wireless communication:</bold> This kind of communication is becoming more popular. Standards for wireless communication include the following: <list list-type="bullet"><list-item><p id="Par299"><bold>Mobile communication</bold> is becoming available at increased data rates. 7 Mbit/s are obtained with HSPA (High Speed Packet Access). <index-term id="ITerm231"><term>HSPA</term></index-term> About ten times higher rates are available with <bold>long-term evolution (LTE)</bold>. <index-term id="ITerm232"><term>long-term evolution (LTE)</term></index-term> 5G networks are expected to provide data rates between 50 Mbit/s and more than a gigabit/s, with latencies less than those of earlier networks.<index-term id="ITerm233"><term>5G</term></index-term></p></list-item><list-item><p id="Par300"><bold>Bluetooth</bold><index-term id="ITerm234"><term>Bluetooth</term></index-term> is a standard for connecting devices such as mobile phones and their headsets over short distances.</p></list-item><list-item><p id="Par301"><bold>Wireless local area networks</bold> (WLANs) are standardized as IEEE standard 802.11, with several supplementary standards. <index-term id="ITerm235"><term>IEEE 802.11</term></index-term><index-term id="ITerm236"><term>wireless local area network (WLAN)</term></index-term></p></list-item><list-item><p id="Par302"><bold>ZigBee</bold> (see <ext-link xlink:href="http://www.zigbee.org" ext-link-type="uri">http://www.zigbee.org</ext-link>) is a communication protocol designed to create personal area networks using low-power radios. Applications include home automation and the Internet of Things. <index-term id="ITerm237"><term>Internet of Things (IoT)</term></index-term></p></list-item><list-item><p id="Par303"><bold>Digital European cordless telecommunications (DECT)</bold> is a standard used for wireless phones. It is being used throughout the world, except for different frequencies used in North America (see <ext-link xlink:href="https://en.wikipedia.org/wiki/Digital_Enhanced_Cordless_Telecommunications" ext-link-type="uri">https://en.wikipedia.org/wiki/Digital_Enhanced_Cordless_Telecommunications</ext-link>). <index-term id="ITerm238"><term>Digital European Cordless Telecommunications (DECT)</term></index-term></p></list-item></list></p></list-item></list></p></sec></sec><sec id="Sec36"><title>Output: Interface Between Cyber and Physical World</title><p id="Par304">Output devices are key components of the <bold>cyphy-interface</bold>. Examples include:<index-term id="ITerm239"><term>cyber-physical system (CPS)</term></index-term><list list-type="bullet"><list-item><p id="Par305"><bold>Displays:</bold><index-term id="ITerm240"><term>display</term></index-term> Display technology is an area which is extremely important. Accordingly, a large amount of information [<xref ref-type="bibr" rid="CR503">503</xref>] exists on this technology. Major research and development efforts lead to new display technology such as organic displays [<xref ref-type="bibr" rid="CR342">342</xref>]. Organic displays are emitting light and can be fabricated with very high densities. In contrast to LCDs, they do not need backlight and polarizing filters. Major changes are therefore expected in these markets.</p></list-item><list-item><p id="Par306"><bold>Electro-mechanical devices:</bold> These influence the environment through motors and other electro-mechanical equipment.</p></list-item></list></p><p id="Par307">Analog as well as digital output devices are used. In the case of analog output devices, the digital information must first be converted by digital-to-analog converters (DACs). These converters can be found on the path from analog inputs of embedded systems to their outputs. Figure <xref rid="Fig50" ref-type="fig">3.50</xref> shows the naming convention of signals along the path which we use.<index-term id="ITerm241"><term>digital-to-analog converter (DAC)</term></index-term> Purpose and function of the boxes will be explained in this section.<fig id="Fig50"><label>Fig. 3.50</label><caption xml:lang="en"><p>Naming convention for signals between analog inputs and outputs</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig50_HTML.png" id="MO52"/></fig></p><sec id="Sec37"><title>Digital-to-Analog Converters</title><p id="Par308"><index-term id="ITerm242"><term>digital-to-analog converter (DAC)</term></index-term></p><p id="Par309">Digital-to-analog converters (DACs) are also included in the <bold>cyphy-interface</bold>. <index-term id="ITerm243"><term>cyphy-interface</term></index-term> They are not very complex. Figure <xref rid="Fig51" ref-type="fig">3.51</xref> shows the schematic of a simple so-called weighted-resistor DAC.<fig id="Fig51"><label>Fig. 3.51</label><caption xml:lang="en"><p>DAC</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig51_HTML.png" id="MO53"/></fig></p><p id="Par310">The key idea of the converter is to first generate a current which is proportional to the value represented by a digital signal <italic>x</italic>. Such a current can hardly be used by a following system. Therefore, this current is converted into a proportional voltage <italic>y</italic>. This conversion is done with an operational amplifier (depicted by a triangle in Fig. <xref rid="Fig51" ref-type="fig">3.51</xref>). Essential characteristics of operational amplifiers are described in Appendix B of this book. <index-term id="ITerm244"><term>digital-to-analog converter (DAC)</term></index-term></p><p id="Par311">How do we compute the output voltage <italic>y</italic>? Consider the four resistors on the left in Fig. <xref rid="Fig51" ref-type="fig">3.51</xref>. The current through any resistor is zero if the corresponding element of digital signal <italic>x</italic> is ’0’. If it is ’1’, the current corresponds to the weight of that bit, since resistor values are chosen accordingly. Now, consider the loop indicated by the red dashed line in Fig. <xref rid="Fig51" ref-type="fig">3.51</xref>. We can apply Kirchhoff’s loop rule (see Appendix B) to the loop turned on by the least significant bit <italic>x</italic><sub>0</sub> of <italic>x</italic>. Let us start the loop traversal at the corresponding resistor and continue in a clockwise fashion. The second term is the voltage <italic>V</italic><sub>−</sub> between the inputs of the operational amplifier, counted as positive, since we proceed in the direction of the arrow. The third term is contributed by the constant voltage source, counted as negative, since we proceed against the direction of the arrow. Overall, we have<index-term id="ITerm245"><term>Kirchhoff’s laws</term></index-term><index-term id="ITerm246"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ22"><label>3.22</label><alternatives><mml:math id="Equ22_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mn>8</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>0</mml:mn></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ22_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} x_0 * I_0 * 8 * R + V_- - V_{ref} &amp;\displaystyle =&amp;\displaystyle 0 \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ22.gif"/></alternatives></disp-formula></p><p id="Par312"><italic>V</italic><sub>−</sub> is approximately 0 (see Appendix B, Eq. (B.14)). Therefore, we have <disp-formula id="Equ23"><label>3.23</label><alternatives><mml:math id="Equ23_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ23_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} I_0 &amp;\displaystyle =&amp;\displaystyle x_0*\frac{V_{ref}}{8*R} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ23.gif"/></alternatives></disp-formula></p><p id="Par313">Corresponding equations hold for the currents <italic>I</italic><sub>1</sub> to <italic>I</italic><sub>3</sub> through the other resistors. We can now apply Kirchhoff’s node rule to the circuit node connecting all resistors. At this node, the outgoing current must be equal to the sum of the incoming currents. Therefore, we have<index-term id="ITerm247"><term>Kirchhoff’s laws</term></index-term><disp-formula id="Equ24"><label>3.24</label><alternatives><mml:math id="Equ24_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>I</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ24_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} I &amp;\displaystyle = &amp;\displaystyle I_3 + I_2 + I_1 + I_0 \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ24.gif"/></alternatives></disp-formula></p><p id="Par314"><disp-formula id="Equ25"><label>3.25</label><alternatives><mml:math id="Equ25_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>I</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr><mml:mtr><mml:mtd class="eqnarray-1"/><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:munderover accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ25_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} I &amp;\displaystyle =&amp;\displaystyle x_3*\frac{V_{ref}}{R} + x_2*\frac{V_{ref}}{2*R} + x_1*\frac{V_{ref}}{4*R} + x_0*\frac{V_{ref}}{8*R} \\ &amp;\displaystyle =&amp;\displaystyle \frac{V_{ref}}{R} * \sum_{i=0}^{3} x_i * 2^{i-3} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ25.gif"/></alternatives></disp-formula></p><p id="Par315">Now, we can apply Kirchhoff’s loop rule to the loop comprising <italic>R</italic><sub>1</sub>, <italic>y</italic>, and <italic>V</italic><sub>−</sub>. Since <italic>V</italic><sub>−</sub> is approximately 0, we have<index-term id="ITerm248"><term>Kirchhoff’s laws</term></index-term><index-term id="ITerm249"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ26"><label>3.26</label><alternatives><mml:math id="Equ26_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>I</mml:mi><mml:mi>′</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>0.</mml:mn></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ26_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} y + R_1 * I' &amp;\displaystyle =&amp;\displaystyle 0.\end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ26.gif"/></alternatives></disp-formula></p><p id="Par316">Next, we can apply Kirchhoff’s node rule to the node connecting <italic>I</italic>, <italic>I′</italic>, and the inverting signal input of the operational amplifier.<index-term id="ITerm250"><term>operational amplifier (op-amp)</term></index-term> The current into this input is practically zero, and currents <italic>I</italic> and <italic>I′</italic> are equal: <italic>I</italic> = <italic>I′</italic>. Hence, we have<index-term id="ITerm251"><term>Kirchhoff’s laws</term></index-term><index-term id="ITerm252"><term>operational amplifier (op-amp)</term></index-term><index-term id="ITerm253"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ27"><label>3.27</label><alternatives><mml:math id="Equ27_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mi>I</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mn>0</mml:mn></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ27_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} y + R_1 * I&amp;\displaystyle =&amp;\displaystyle 0 {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ27.gif"/></alternatives></disp-formula></p><p id="Par317">From Eqs. (<xref rid="Equ25" ref-type="disp-formula">3.25</xref>) and (<xref rid="Equ27" ref-type="disp-formula">3.27</xref>), we obtain <index-term id="ITerm254"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ28"><label>3.28</label><alternatives><mml:math id="Equ28_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>y</mml:mi></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:munderover accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msup><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>8</mml:mn><mml:mo>∗</mml:mo><mml:mi>R</mml:mi></mml:mrow></mml:mfrac><mml:mo>∗</mml:mo><mml:mi>n</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ28_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} y &amp;\displaystyle =&amp;\displaystyle - V_{ref} * \frac{R_1}{R} * \sum_{i=0}^{3} x_i * 2^{i-3} = - V_{ref} * \frac{R_1}{8*R} * nat(x) \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ28.gif"/></alternatives></disp-formula></p><p id="Par318"><italic>nat</italic> denotes the natural number represented by digital signal <italic>x</italic>. Obviously, <italic>y</italic> is proportional to the value represented by <italic>x</italic>. Positive output voltages and bit vectors representing two’s complement numbers require minor extensions.</p><p id="Par319">From a DSP point of view, <italic>y</italic>(<italic>t</italic>) is a function over a discrete time domain: it provides us with a <bold>sequence</bold> of voltage levels. In our running example, it is defined only over integer times. From a practical point of view, this is inconvenient, since we would typically observe the output of the circuit of Fig. <xref rid="Fig51" ref-type="fig">3.51</xref> continuously. Therefore, DACs are frequently extended by a <bold>“zero-order hold” functionality</bold>. This means that the converter will keep the previous value until the next value is converted. Actually, the DAC of Fig. <xref rid="Fig51" ref-type="fig">3.51</xref> will do exactly this if we do not change the settings of the switches until the next discrete time instant. Hence, the output of the converter is a step function <italic>y′</italic>(<italic>t</italic>) corresponding to the sequence <italic>y</italic>(<italic>t</italic>).<xref ref-type="fn" rid="Fn17">17</xref><italic>y′</italic>(<italic>t</italic>) is a function over the continuous time domain.</p><p id="Par321">As an example, let us consider the output resulting from the conversion of the signal of Eq. (<xref rid="Equ3" ref-type="disp-formula">3.3</xref>), assuming a resolution of 0.125. For this case, Fig. <xref rid="Fig52" ref-type="fig">3.52</xref> shows <italic>y′</italic>(<italic>t</italic>) instead of <italic>y</italic>(<italic>t</italic>), since <italic>y′</italic>(<italic>t</italic>) is a bit easier to visualize.<fig id="Fig52"><label>Fig. 3.52</label><caption xml:lang="en"><p><italic>y′</italic>(<italic>t</italic>) (red) generated from signal <italic>e</italic><sub>3</sub>(<italic>t</italic>) (blue) (Eq. (<xref rid="Equ3" ref-type="disp-formula">3.3</xref>)) sampled at integer times</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig52_HTML.png" id="MO54"/></fig></p><p id="Par322">DACs enable a conversion from time- and value-discrete signals to signals in the continuous time and value domain. However, neither <italic>y</italic>(<italic>t</italic>) nor <italic>y′</italic>(<italic>t</italic>) reflects the values of the input signal in between the sampling instances.</p></sec><sec id="Sec38"><title>Sampling Theorem</title><p id="Par323"><index-term id="ITerm255"><term>sampling theorem</term></index-term></p><p id="Par324">Suppose that the processors used in the hardware loop forward values from ADCs unchanged to the DACs. We could also think of storing values <italic>x</italic>(<italic>t</italic>) on a CD and aiming at generating an excellent analog audio signal. Would it be possible to reconstruct the original analog voltage <italic>e</italic>(<italic>t</italic>) (see Figs. <xref rid="Fig8" ref-type="fig">3.8</xref>, <xref rid="Fig21" ref-type="fig">3.21</xref>, and <xref rid="Fig50" ref-type="fig">3.50</xref>) at the outputs of the DACs?</p><p id="Par325">It is obvious that reconstruction is not possible if we have aliasing of the type described in Fig. <xref rid="Fig7" ref-type="fig">3.7</xref> on p. 8.<xref ref-type="fn" rid="Fn18">18</xref> So, we assume that the sampling rate is larger than twice the highest frequency of the decomposition of the input signal into sine waves (sampling criterion; see Eq. (<xref rid="Equ8" ref-type="disp-formula">3.8</xref>)).<index-term id="ITerm256"><term>aliasing</term></index-term> Does meeting this criterion allow us to reconstruct the original signal? Let us have a closer look!<index-term id="ITerm257"><term>sampling criterion</term></index-term></p><p id="Par327">Feeding DACs with a discrete sequence of digital values will result in a sequence of analog values being generated. Values of the input signal in between the sampling instances are not generated by DACs. The simple zero-order hold functionality (if present) would generate only step functions. This seems to indicate that reconstruction of <italic>e</italic>(<italic>t</italic>) would require an infinitely large sampling rate, such that all intermediate values can be generated.</p><p id="Par328">However, there could be some kind of smart interpolation computing values in between the sampling instances from the values at sampling instances. And indeed, sampling theory [<xref ref-type="bibr" rid="CR440">440</xref>] tells us that a corresponding time-continuous signal <italic>z</italic>(<italic>t</italic>) can be constructed from the sequence <italic>y</italic>(<italic>t</italic>) of analog values.</p><p id="Par329">Let {<italic>t</italic><sub><italic>s</italic></sub>}, <italic>s</italic> = …, −1, 0, 1, 2, … be the time points at which we sample our input signal. Let us assume a constant sampling rate of <inline-formula id="IEq10"><alternatives><mml:math id="IEq10_Math"><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$f_s=\frac {1}{T_s}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq10.gif"/></alternatives></inline-formula> (∀<italic>s</italic> : <italic>T</italic><sub><italic>s</italic></sub> = <italic>t</italic><sub><italic>s</italic>+1</sub> − <italic>t</italic><sub><italic>s</italic></sub>). Then, sampling theory tells us that we can approximate <italic>e</italic>(<italic>t</italic>) from <italic>y</italic>(<italic>t</italic>) as follows: <index-term id="ITerm258"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ29"><label>3.29</label><alternatives><mml:math id="Equ29_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:munderover accentunder="false"><mml:mrow><mml:mo mathsize="big">∑</mml:mo></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mrow><mml:mi>∞</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ29_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} z(t) &amp;\displaystyle =&amp;\displaystyle \sum_{s=- \infty}^{\infty} \frac{y(t_s) sin \frac{\pi}{T_s} (t-t_s)}{\frac{\pi}{T_s} (t-t_s)} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ29.gif"/></alternatives></disp-formula></p><p id="Par330">This equation is known as the <bold>Shannon-Whittaker interpolation</bold>. <italic>y</italic>(<italic>t</italic><sub><italic>s</italic></sub>) is the contribution of signal <italic>y</italic> at sampling instance <italic>t</italic><sub><italic>s</italic></sub>. This means, all 2<sup>64</sup> Boolean functions of 6 inputs respectively all 2<sup>32</sup> Boolean functions of 5 inputs can be implemented. The decrease follows a weighting factor, also known as the <italic>sinc</italic> function <index-term id="ITerm259"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ30"><label>3.30</label><alternatives><mml:math id="Equ30_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mfrac><mml:mrow><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ30_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} sinc(t-t_s) &amp;\displaystyle =&amp;\displaystyle \frac{sin (\frac{\pi}{T_s} (t-t_s))}{\frac{\pi}{T_s} (t-t_s)} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ30.gif"/></alternatives></disp-formula></p><p id="Par331">which decreases non-monotonically as a function of |<italic>t</italic> − <italic>t</italic><sub><italic>s</italic></sub>|. This weighting factor is used to compute values in between the sampling instances. Figure <xref rid="Fig53" ref-type="fig">3.53</xref> shows the weighting factor for the case <italic>T</italic><sub><italic>s</italic></sub> = 1.<fig id="Fig53"><label>Fig. 3.53</label><caption xml:lang="en"><p>Visualization of Eq. (<xref rid="Equ30" ref-type="disp-formula">3.30</xref>) used for interpolation</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig53_HTML.png" id="MO55"/></fig></p><p id="Par332">Using the <italic>sinc</italic> function, we can compute the terms of the sum in Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>). Figures <xref rid="Fig54" ref-type="fig">3.54</xref> and <xref rid="Fig55" ref-type="fig">3.55</xref> show the resulting terms if <italic>e</italic>(<italic>t</italic>) = <italic>e</italic><sub>3</sub>(<italic>t</italic>) and processing performs the identify function (<italic>x</italic>(<italic>t</italic>) = <italic>w</italic>(<italic>t</italic>)).<fig id="Fig54"><label>Fig. 3.54</label><caption xml:lang="en"><p><italic>y′</italic>(<italic>t</italic>) (red) and the first three terms of Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig54_HTML.png" id="MO56"/></fig><fig id="Fig55"><label>Fig. 3.55</label><caption xml:lang="en"><p><italic>y′</italic>(<italic>t</italic>) (red) and the last three non-zero terms of Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig55_HTML.png" id="MO57"/></fig></p><p id="Par333">At each of the sampling instances <italic>t</italic><sub><italic>s</italic></sub> (integer times in our case), <italic>z</italic>(<italic>t</italic><sub><italic>s</italic></sub>) is computed just from the corresponding value <italic>y</italic>(<italic>t</italic><sub><italic>s</italic></sub>), since the <italic>sinc</italic> function is zero in this case for all other sampled values. In between the sampling instances, all of the adjacent discrete values contribute to the resulting value of <italic>z</italic>(<italic>t</italic>). Figure <xref rid="Fig56" ref-type="fig">3.56</xref> shows the resulting <italic>z</italic>(<italic>t</italic>) if <italic>e</italic>(<italic>t</italic>) = <italic>e</italic><sub>3</sub>(<italic>t</italic>) and processing performs the identify function (<italic>x</italic>(<italic>t</italic>) = <italic>w</italic>(<italic>t</italic>)).<fig id="Fig56"><label>Fig. 3.56</label><caption xml:lang="en"><p><italic>e</italic><sub>3</sub>(<italic>t</italic>) (blue), <italic>y′</italic>(<italic>t</italic>) (red), <italic>z</italic>(<italic>t</italic>) (magenta)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig56_HTML.png" id="MO58"/></fig></p><p id="Par334">The figure includes signals <italic>e</italic><sub>3</sub>(<italic>t</italic>) (blue), <italic>y′</italic>(<italic>t</italic>) (red), and <italic>z</italic>(<italic>t</italic>) (magenta). <italic>z</italic>(<italic>t</italic>) is computed by summing up the contributions of all sampling instances shown in the diagrams in Figs. <xref rid="Fig54" ref-type="fig">3.54</xref> and <xref rid="Fig55" ref-type="fig">3.55</xref>. <italic>e</italic><sub>3</sub>(<italic>t</italic>) and <italic>z</italic>(<italic>t</italic>) are very similar.</p><p id="Par335">How close could we get to the original input signal by implementing Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>)? Sampling theory tells us (see, e.g., [<xref ref-type="bibr" rid="CR440">440</xref>]) that <bold>Eq. (</bold><xref rid="Equ29" ref-type="disp-formula"><bold>3.29</bold></xref><bold>) computes an exact approximation</bold> if the sampling criterion (Eq. (<xref rid="Equ8" ref-type="disp-formula">3.8</xref>)) is met. Therefore, let us see how we can implement Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>).</p><p id="Par336">How do we compute Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>) in an electronic system? We cannot compute this equation in the discrete time domain using a digital signal processor for this, since this computation has to generate a time-continuous signal. Computing such a complex equation with analog circuits seems to be difficult at first sight.</p><p id="Par337">Fortunately, the required computation is a so-called folding operation between signal <italic>y</italic>(<italic>t</italic>) and the <italic>sinc</italic> function. According to the classical theory of Fourier transforms, a folding operation in the time domain is equivalent to a multiplication with frequency-dependent filter function in the frequency domain. This filter function is the Fourier transform of the corresponding function in the time domain. Therefore, Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>) can be computed with some appropriate filter. Figure <xref rid="Fig57" ref-type="fig">3.57</xref> shows the corresponding placement of the filter.<fig id="Fig57"><label>Fig. 3.57</label><caption xml:lang="en"><p>Converting signal <italic>e</italic>(<italic>t</italic>) from the analog time/value domain to the digital domain and back</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig57_HTML.png" id="MO59"/></fig></p><p id="Par338">Which frequency-dependent filter function is the Fourier transform of the <italic>sinc</italic> function? Computing the Fourier transform of the <italic>sinc</italic> function yields a low-pass filter function [<xref ref-type="bibr" rid="CR440">440</xref>]. So, “all” we must do to compute Eq. (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>) is to pass signal <italic>y</italic>(<italic>t</italic>) through a low-pass filter, filtering frequencies as shown for the ideal filter in Fig. <xref rid="Fig58" ref-type="fig">3.58</xref>. The representation of function <italic>y</italic>(<italic>t</italic>) as a sum of sine waves would require very high-frequency components, making such a filtering non-redundant, even though we have already assumed an anti-aliasing filter to be present at the input. <index-term id="ITerm260"><term>brick-wall filter</term></index-term><fig id="Fig58"><label>Fig. 3.58</label><caption xml:lang="en"><p>Low-pass filter: ideal (<italic>blue, dashed</italic>) and realistic (<italic>red, solid)</italic></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig58_HTML.png" id="MO60"/></fig></p><p id="Par339">Unfortunately, ideal low-pass filters do not exist. We must live with compromises and design filters approximating the low-pass filters. Actually, we must live with several imperfections preventing a precise reconstruction of the input signals: <list list-type="bullet"><list-item><p id="Par340">Ideal low-pass filters cannot be designed. Therefore, we must use approximations of such filters. Designing good compromises is an art (performed extensively, e.g., for audio equipment).</p></list-item><list-item><p id="Par341">Similarly, we cannot completely remove input frequencies beyond the Nyquist frequency.</p></list-item><list-item><p id="Par342">The impact of value quantization is visible in Fig. <xref rid="Fig56" ref-type="fig">3.56</xref>. Due to value quantization, <italic>e</italic><sub>3</sub>(<italic>t</italic>) is sometimes different from <italic>z</italic>(<italic>t</italic>). Quantization noise, as introduced by ADCs, cannot be removed during output generation. Signal <italic>w</italic>(<italic>t</italic>) from the output of the ADC will remain distorted by the quantization noise. However, this effect does not affect the signal <italic>h</italic>(<italic>t</italic>) from the output of sample-and-hold circuits.</p></list-item><list-item><p id="Par343">Equation (<xref rid="Equ29" ref-type="disp-formula">3.29</xref>) is based on an infinite sum, involving also values at future instances in time. In practice, we can delay signals by some finite amount to know a finite number of “future” samples. Infinite delays are impossible. In Fig. <xref rid="Fig56" ref-type="fig">3.56</xref>, we did not consider contributions of sampling instances outside the diagram.</p></list-item></list></p><p id="Par344">The functionality provided by low-pass filters demonstrates the power of analog circuits: there would be no way of implementing the behavior of analog filters in the digital domain, due to the inherent restriction to discretized time and values.</p><p id="Par345">Many authors have contributed to sampling theory. Therefore, many names can be associated with the sampling theorem. Contributors include Shannon, Whittaker, Kotelnikov, Nyquist, Küpfmüller, and others. Therefore, the fact that the original signal can be reconstructed should simply be called the sampling theorem, since there is no way of attaching all names of relevant contributors to the theorem.</p></sec><sec id="Sec39"><title>Pulse-Width Modulation</title><p id="Par346"><index-term id="ITerm261"><term>pulse width modulation (PWM)</term></index-term></p><p id="Par347">In practice, the presented generation of analog signals has a number of disadvantages: <list list-type="bullet"><list-item><p id="Par348">DACs using an array of resistors are difficult to build. The precision of the resistors must be excellent. The deviation of the resistor handling the most significant bit from its nominal value must be less than the overall resolution of the converter. For example, this means that, for a 14 bit converter, the deviation of the real resistance from its nominal value must be in the order of 0.01%. This precision is difficult to achieve in practice, in particular over the full temperature range. If this precision is not achieved, the converter is not linear, possibly not even monotone.</p></list-item><list-item><p id="Par349">In order to generate a sufficient power<index-term id="ITerm262"><term>power</term></index-term> for motors, lamps, loudspeakers, etc., analog outputs would need to be amplified in a power amplifier. Analog power amplifiers, such as so-called class A power amplifiers, are very power-inefficient, since they contain an always conducting path between the two rails of the power supply. This path results in a constant power consumption, irrespective of the actual output signal. For very small output signals, the ratio between the actually used power and the consumed power is therefore very small. As a result, the efficiency of audio power amplifiers for low-volume audio would be terribly bad.</p></list-item><list-item><p id="Par350">It is not easy to integrate analog circuitry on digital micro-controller chips. Adding external analog active components increases costs substantially.</p></list-item></list></p><p id="Par351">Therefore, pulse-width modulation (PWM) <index-term id="ITerm263"><term>pulse width modulation (PWM)</term></index-term> is very popular. With PWM, we are using a digital output and generate a digital signal whose duty cycle corresponds to the value to be converted. Figure <xref rid="Fig59" ref-type="fig">3.59</xref> shows digital signals with duty cycles of 25% and 75%. Such signals can be represented by Fourier series like in Eq. (<xref rid="Equ1" ref-type="disp-formula">3.1</xref>). For applications of PWM, we try to eliminate effects of higher-frequency components.<fig id="Fig59"><label>Fig. 3.59</label><caption xml:lang="en"><p>Duty cycles</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig59_HTML.png" id="MO61"/></fig></p><p id="Par352">PWM signals can be generated by comparing a counter against a value stored in a programmable register (see Fig. <xref rid="Fig60" ref-type="fig">3.60</xref>). A high voltage is output whenever the value in the counter exceeds the value in the register. Otherwise, a voltage close to zero is generated. The clock signal of the counter must be programmable to select the basic frequency of the PWM signals. In our schematic, we have assumed that the PWM frequency is identical for all PWM outputs. Registers must be loaded with the values to be converted, typically at the sampling rate of the analog signals.<fig id="Fig60"><label>Fig. 3.60</label><caption xml:lang="en"><p>Hardware for PWM output</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig60_HTML.png" id="MO62"/></fig></p><p id="Par353">The effort required for filtering higher-frequency components depends upon the application. For driving a motor, the averaging takes place in the motor, due to the mass of the moving parts in the motor and possibly also due to the self-inductance of the motor. Hence, no external components are needed (see Fig. <xref rid="Fig60" ref-type="fig">3.60</xref>). For lamps, the averaging takes place in the human eye, as long as the frequencies are not too low. It may also be okay to drive simple buzzers directly. In other cases, filtering out higher-frequency components may be needed. For example, electromagnetic radiation caused by higher-frequency components may be unacceptable, or audio applications may be demanding filtered high-frequency signals. In Fig. <xref rid="Fig60" ref-type="fig">3.60</xref>, two capacitors and one inductor have been used to filter out high-frequency components for the loudspeakers. In our example, we are showing four PWM outputs. Having several PWM outputs is a common situation. For example, Atmel 32 bit AVR micro-controllers in the AT32UC3A Series have seven PWM outputs [<xref ref-type="bibr" rid="CR27">27</xref>]. In practice, there are many options for the detailed behavior of PWM hardware.</p><p id="Par354">The choice of the basic frequency (the reciprocal of the period) of the PWM signal and the filter is a matter of compromises. The basic frequency has to be higher than the highest-frequency component of the analog signal to be converted. Higher frequencies simplify the design of the filter if any is present. Selecting a too high frequency results in more electromagnetic radiation and in unnecessary energy consumption, since switching will consume energy. Compromises typically use a basic PWM frequency that is larger than the highest frequency of the analog signal by a factor between 2 and 10.</p></sec><sec id="Sec40"><title>Actuators</title><p id="Par355"><index-term id="ITerm264"><term>actuator</term></index-term></p><p id="Par356">There is a huge amount of actuators [<xref ref-type="bibr" rid="CR151">151</xref>]. Actuators range from large ones that are able to move tons of weight to tiny ones with dimensions in the μm area, like the one shown in Fig. <xref rid="Fig61" ref-type="fig">3.61</xref>.<fig id="Fig61"><label>Fig. 3.61</label><caption xml:lang="en"><p>Detail of a rotary stepper micromotor: top: stationary part; lower left: rotary part. The micromotor uses three-phase electrostatic power [<xref ref-type="bibr" rid="CR478">478</xref>]. Ⓒ Sarajlic et al. (2010)</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/106758_4_En_3_Fig61_HTML.jpg" id="MO63"/></fig></p><p id="Par357">Figure <xref rid="Fig61" ref-type="fig">3.61</xref> shows a tiny motor manufactured with microsystem technology. The dimensions are in the μm range. The rotating center is controlled by electrostatic forces.</p><p id="Par358">As an example, we mention only a special kind of actuators which will become more important in the future: microsystem technology enables the fabrication of tiny actuators, which can be put into the human body, for example. Using such tiny actuators, the amount of drugs fed into the body can be adapted to the actual need. This allows a much better medication than needle-based injections.</p><p id="Par359">Actuators are important for the Internet of Things. <index-term id="ITerm265"><term>Internet of Things (IoT)</term></index-term> It is impossible to provide a complete overview over actuators.</p></sec></sec><sec id="Sec41"><title>Electrical Energy</title><p id="Par360">General constraints and objectives for the design of embedded and cyber-physical systems (see pp. 8–16 and Table <ext-link xlink:href="10.1007/978-3-030-60910-8_1#Tab2" ext-link-type="doi">1.2</ext-link>) have to be obeyed for hardware design. Among the different objectives, we will focus on energy efficiency.<index-term id="ITerm266"><term>energy</term></index-term> Reasons for caring about the energy efficiency were listed in Table <ext-link xlink:href="10.1007/978-3-030-60910-8_1#Tab1" ext-link-type="doi">1.1</ext-link> on p. 13.</p><sec id="Sec42"><title>Energy Sources</title><p id="Par361">For plugged devices (i.e., for those connected to the power grid), energy is easily available. For all others, energy must be made available via other techniques. In particular, this applies to sensor networks used in IoT systems where energy can be a very scarce resource. <index-term id="ITerm267"><term>Internet of Things (IoT)</term></index-term><index-term id="ITerm268"><term>sensor network</term></index-term> Batteries store energy in the form of chemical energy. Their main limitation is that they must be carried to the location where the energy is required. If we would like to avoid this limitation, we have to use <bold>energy harvesting</bold>, also called <bold>energy scavenging</bold>. <index-term id="ITerm269"><term>energy harvesting</term></index-term><index-term id="ITerm270"><term>energy scavenging</term></index-term> A large amount of techniques for energy harvesting is available [<xref ref-type="bibr" rid="CR570">570</xref>, <xref ref-type="bibr" rid="CR577">577</xref>], but the amount of energy is typically much more limited: <list list-type="bullet"><list-item><p id="Par362"><bold>Photovoltaics</bold> allows the conversion of light into electrical energy. The conversion is usually based on the photovoltaic effect of semiconductors. <index-term id="ITerm271"><term>photovoltaics</term></index-term> Panels of photovoltaic material are in widespread use. Examples can be seen in Fig. <xref rid="Fig62" ref-type="fig">3.62</xref>.<fig id="Fig62"><label>Fig. 3.62</label><caption xml:lang="en"><p>Photovoltaic material: <bold>left</bold>, panel; <bold>right</bold>, solar-powered watch</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig62_HTML.png" id="MO64"/></fig></p></list-item><list-item><p id="Par363">The <bold>piezoelectric effect</bold> can be used to convert mechanical strain into electrical energy. <index-term id="ITerm272"><term>piezoelectric effect</term></index-term> Piezoelectric lighters exploit this effect.</p></list-item><list-item><p id="Par364"><bold>Thermoelectric generators</bold> (TEGs) allow turning temperature gradients into electrical energy. They can be used even on the human body.</p></list-item><list-item><p id="Par365"><bold>Kinetic energy</bold> can be turned into electrical energy. This is exploited, for example, for some watches. Also, wind energy falls into this category.</p></list-item><list-item><p id="Par366"><bold>Ambient electromagnetic radiation</bold> can be turned into electrical energy as well.</p></list-item><list-item><p id="Par367">There are many other physical effects allowing us to convert other forms of energy into electrical energy.</p></list-item></list></p></sec><sec id="Sec43"><title>Energy Storage</title><p id="Par368">For many applications of embedded systems, power sources are not guaranteed to provide power whenever it is needed. However, we may be able to store electrical energy. Methods for storing electrical energy include the following: <list list-type="order"><list-item><p id="Par369"><bold>Non-rechargeable batteries</bold> can be used only once and will not be considered.</p></list-item><list-item><p id="Par370"><bold>Capacitors</bold> are a very convenient means of storing electrical energy. <index-term id="ITerm273"><term>capacitor</term></index-term> Their advantages include a potentially fast charging process, very high output currents, close to 100% efficiency, low leakage currents (for high-quality capacitors), and a large number of charge/discharge cycles. The limited amount of energy that can be stored is their main disadvantage.</p></list-item><list-item><p id="Par371"><bold>Rechargeable batteries</bold> allow storing and using electrical energy, very much like capacitors. <index-term id="ITerm274"><term>battery</term></index-term> Storing electrical energy is based on certain chemical processes, and using this energy is based on reversing these chemical processes.</p></list-item></list></p><p id="Par372">Due to their importance for embedded systems, we will discuss rechargeable batteries. If we want to include sources of electrical energy in our system model, we will need models of rechargeable batteries. Various models can be used. They differ in the amount of details that are included, and there is not a single model that fits all needs [<xref ref-type="bibr" rid="CR467">467</xref>]. The following models are popular: <list list-type="bullet"><list-item><p id="Par373"><bold>Chemical and physical models</bold>: They describe the chemical and/or physical operation of the battery in detail. Such models may include partial differential equations, including many parameters. These models are beneficial for battery manufacturers but typically too complex for designers of embedded systems (who will typically not know the parameters).</p></list-item><list-item><p id="Par374"><bold>Simple empirical models</bold>: Such models are based on simple equations for which some parameter fitting has been performed. Peukert’s law [<xref ref-type="bibr" rid="CR451">451</xref>] is a frequently cited empirical model. <index-term id="ITerm275"><term>Peukert’s law</term></index-term> According to this law, the lifetime of a battery is <index-term id="ITerm276"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equ31"><label>3.31</label><alternatives><mml:math id="Equ31_Math"><mml:mtable columnalign="right center left" displaystyle="true"><mml:mtr><mml:mtd class="eqnarray-1"><mml:mtext>lifetime</mml:mtext></mml:mtd><mml:mtd class="eqnarray-2"><mml:mo>=</mml:mo></mml:mtd><mml:mtd class="eqnarray-3"><mml:mi>C</mml:mi><mml:mo>∕</mml:mo><mml:msup><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msup></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math><tex-math id="Equ31_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} \begin{array}{rcl} \mbox{lifetime}&amp;\displaystyle =&amp;\displaystyle C/I^{\alpha} {} \end{array} \end{aligned} $$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equ31.gif"/></alternatives></disp-formula></p><p id="Par375">where <italic>α</italic> &gt; 1 is the result of some empirical fitting process. Peukert’s law reflects the fact that higher currents will typically lead to an effective decrease of the battery capacity. Other details of battery behavior are not included in this model.</p></list-item><list-item><p id="Par376"><bold>Abstract models</bold>: These provide more details than the very simple empirical models, but do not refer to chemical processes. We would like to present two such models: <list list-type="bullet"><list-item><p id="Par377">The model proposed by Chen and Ricón [<xref ref-type="bibr" rid="CR94">94</xref>]. The model is an electrical model, as shown in Fig. <xref rid="Fig63" ref-type="fig">3.63</xref>. According to this model, a charging current <italic>I</italic><sub><italic>Batt</italic></sub> controls a current source in the left part of the schematic. The current generated by the current source is equal to the charging current entering on the right. This current will charge the capacitor <italic>C</italic><sub><italic>Capacity</italic></sub>. The amount of charge on the capacitor is called <bold>state of charge</bold> (SoC). The state of charge is reflected by the voltage <italic>V</italic><sub><italic>SOC</italic></sub> on the capacitor, since the charge on the capacitor can be computed as <italic>Q</italic> = <italic>C</italic><sub><italic>Capacity</italic></sub> ∗ <italic>V</italic><sub><italic>SOC</italic></sub>. <index-term id="ITerm277"><term>state of charge (SoC)</term></index-term> Resistor <italic>R</italic><sub><italic>Self</italic>−<italic>Discharge</italic></sub> models the self-discharge (leakage) of this capacitor which happens even when no current is drawn at the terminal pins of the battery.<fig id="Fig63"><label>Fig. 3.63</label><caption xml:lang="en"><p>Battery model according to Chen et al. (simplified)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig63_HTML.png" id="MO65"/></fig></p><p id="Par378">Let us consider the voltage which is available at the battery terminals when the current through these terminals is zero. The voltage at the battery terminals will typically non-linearly depend on <italic>V</italic><sub><italic>SOC</italic></sub>. This dependency can be modeled by a non-linear function <italic>V</italic><sub><italic>OC</italic></sub>(<italic>V</italic><sub><italic>SOC</italic></sub>), representing the <bold>open terminal output voltage</bold> of the battery. This voltage decreases when the battery provides some current. For a constant discharging current, <inline-formula id="IEq11"><alternatives><mml:math id="IEq11_Math"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$R_{Series}+R_{Transient\_S}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq11.gif"/></alternatives></inline-formula> models the corresponding voltage drop. For short current spikes, the decrease is determined by the value of <italic>R</italic><sub><italic>Series</italic></sub> only, since <italic>C</italic><sub><italic>T</italic></sub> will act as a buffer. When the current consumption increases, <italic>time constant</italic><inline-formula id="IEq12"><alternatives><mml:math id="IEq12_Math"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>S</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$R_{Transient\_S} * C_T$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq12.gif"/></alternatives></inline-formula> determines the speed for the transition from only <italic>R</italic><sub><italic>Series</italic></sub> causing the voltage drop to <inline-formula id="IEq13"><alternatives><mml:math id="IEq13_Math"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>S</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mtext>_</mml:mtext><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$R_{Series}+R_{Transient\_S}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq13.gif"/></alternatives></inline-formula> causing the voltage drop. The original proposal by Chen et al. includes a second resistor/capacitor pair in order to model transient output voltage behavior more precisely. Overall, this model captures the impact of high output currents on the voltage, the non-linear dependency of the output voltage, and self-discharge reasonably well. Simpler versions of this model exist, i.e., ones that do not model all three effects.</p></list-item><list-item><p id="Par379">Actual batteries exhibit the so-called charge recovery effect: whenever the discharge process of batteries is paused for some time interval, the battery recovers, i.e., more charge becomes available, and the voltage is typically also increased. This effect is not considered in Chen’s model. However, it is the focus of the so-called kinetic battery model (KiBaM) of Manwell et al. [<xref ref-type="bibr" rid="CR364">364</xref>]. <index-term id="ITerm278"><term>kinetic battery model</term></index-term> The name reflects the analogy upon which this model is based. The model assumes two different bins of charge, as shown in Fig. <xref rid="Fig64" ref-type="fig">3.64</xref>. The right bin contains the charge <italic>y</italic><sub>1</sub> which is immediately available. The left bin contains charge <italic>y</italic><sub>2</sub> which exists in the battery but which needs to flow into the right bin to become available. An interval of heavy usage of the battery may almost empty the right bin. It will then take some time for charge to become available again. The speed of the recovery process is determined by parameter <italic>k</italic>, the width of the pipe connecting the two bins. The details of the model (like the amount of charge flowing) reflect the physical situation of the bins. This model describes the charge recovery process with some reasonable precision but fails to describe transients and self-discharge as captured in Chen’s model. The kinetic model has an impact on how embedded systems should be used. For example, it has been demonstrated that it is beneficial to plan for intervals, during which wireless transmission is turned off [<xref ref-type="bibr" rid="CR144">144</xref>].<fig id="Fig64"><label>Fig. 3.64</label><caption xml:lang="en"><p>Kinetic battery model</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig64_HTML.png" id="MO66"/></fig></p></list-item></list></p><p id="Par380">Overall, the two models demonstrate nicely that models must be selected to reflect the effects that should be taken into account.</p></list-item><list-item><p id="Par381">There may be <bold>mixed models</bold> which are partially based on abstract models and partially on chemical and physical models.</p></list-item></list></p></sec><sec id="Sec44"><title>Energy Efficiency of Hardware Components</title><p id="Par382"><index-term id="ITerm279"><term>energy efficiency</term></index-term><index-term id="ITerm280"><term>energy awareness</term></index-term></p><p id="Par383">We will continue our discussion of energy efficiency by comparing the energy efficiency for the different technologies which we have at our disposal. Hardware components discussed in this chapter are quite different as far as their energy efficiency is concerned. A comparison between these technologies and changes over time (corresponding to a certain fabrication technology) can be seen in Fig. <xref rid="Fig65" ref-type="fig">3.65</xref>.<xref ref-type="fn" rid="Fn19">19</xref><index-term id="ITerm281"><term>energy efficiency</term></index-term> The figure reflects the conflict between efficiency and flexibility of currently available hardware technologies.<fig id="Fig65"><label>Fig. 3.65</label><caption xml:lang="en"><p>Hardware efficiency (ⒸDe Man and Philips)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/106758_4_En_3_Fig65_HTML.png" id="MO67"/></fig></p><p id="Par385">The diagram shows the energy efficiency GOP/J in terms of number of operations per unit of energy of various target technologies as a function of time and the target technology. In this context, operations could be 32 bit additions. Obviously, the number of operations per joule is increasing as technology advances to smaller and smaller feature sizes of integrated circuits. However, for any given technology, the number of operations per joule is largest for hardwired application-specific integrated circuits (ASICs).<index-term id="ITerm282"><term>application-specific integrated circuit (ASIC)</term></index-term> For reconfigurable logic usually coming in the form of field programmable gate arrays (FPGAs;<index-term id="ITerm283"><term>field programmable gate array (FPGA)</term></index-term> see p. 39), this value is about one order of magnitude less. For programmable processors, it is even lower. However, processors offer the largest amount of flexibility, resulting from the flexibility of software. There is also some flexibility for reconfigurable logic, but it is limited to the size of applications that can be mapped to such logic. For hardwired designs, there is no flexibility. The trade-off between flexibility and efficiency also applies to processors: for processors optimized for an application domain, such as processors optimized for digital signal processing (DSP), power-efficiency values approach those of reconfigurable logic. For general standard microprocessors, the values for this figure of merit are the worst. This can be seen from Fig. <xref rid="Fig65" ref-type="fig">3.65</xref>, comprising values for microprocessors such as ×86-like processors (see “MPU” entries), RISC processors, and the cell processor designed by IBM, Toshiba, and Sony. <index-term id="ITerm284"><term>processor</term><index-term><term>cell ˜</term></index-term></index-term></p><p id="Par386">Figure <xref rid="Fig65" ref-type="fig">3.65</xref> does not identify exactly the applications which are compared, and it does not allow us to study the type of application mapping that has been performed.</p><p id="Par387">More detailed and more recent comparisons have been made, enabling us to study the assumptions and the approach of these comparisons in a more comprehensive manner. A survey of comparisons involving GPUs has been published by Mittal et al. [<xref ref-type="bibr" rid="CR398">398</xref>]. The survey includes a list of 28 publications for which GPUs have been found to be more energy-efficient than CPUs and 2 publications for which the reverse was true. Also, the survey comprises a list of 26 publications for which FPGAs have been found to be more energy-efficient than GPUs and 1 for which the reverse was true. For example, Hamada et al. [<xref ref-type="bibr" rid="CR200">200</xref>] found for a gravitational <italic>n</italic>-body simulation that the number of operations per watt was by a factor of 15 higher for FPGAs than for GPUs. For a comparison against CPUs, the factor was 34. The exact factors certainly depend on the application, but as a rule of thumb, we can state the following: If we aim at top power- and energy-efficient designs, we should use ASICs. <index-term id="ITerm285"><term>application-specific integrated circuit (ASIC)</term></index-term> If we cannot afford ASICs, we should go for FPGAs. If FPGAs are also not an option, we should select GPUs. Also, we have already seen that heterogeneous processors are in general more energy-efficient than homogeneous processors. More detailed information can be computed for particular application areas.</p><sec id="Sec45"><title>The Case of Mobile Phones</title><p id="Par388">Among the different applications of embedded systems (see pp. 4–8), we are now looking at telecommunication and smart phones. For smart phones, computational requirements are increasing at a rapid rate, especially for multimedia applications. De Man and Philips estimated that advanced multimedia applications need about 10–100 billion operations per second. Figure <xref rid="Fig65" ref-type="fig">3.65</xref> demonstrates that advanced hardware technologies provided us more or less with this number of operations per joule (= Ws) in 2007. This means that the most power<index-term id="ITerm286"><term>power</term></index-term>-efficient platform technologies hardly provided the efficiency which was needed. Standard processors (entries for MPU and RISC) were hopelessly inefficient. It also meant that all sources of efficiency improvements needed to be exploited. More recently, the power efficiency has been improved. However, all such improvements are typically compensated by trends to provide a higher quality, e.g., by an increase of the resolution of still and moving images as well as a higher bandwidth for communication.</p><p id="Par389">A detailed analysis of the power consumption has been published by Berkel [<xref ref-type="bibr" rid="CR553">553</xref>] and by Carroll et al. [<xref ref-type="bibr" rid="CR84">84</xref>]. A more recent analysis including LTE mobile phones has been published by Dusza et al. [<xref ref-type="bibr" rid="CR144">144</xref>]. A power consumption of up to around 4 watts has been observed. The display itself caused a consumption of up to around 1 watt, depending on the display brightness.</p><p id="Par390">Improving battery technology would allow us to consume power over longer periods, but the thermal limitation prevents us from going significantly beyond the current consumption in the near future. Due to thermal issues, it has become standard to design mobile phones with temperature sensors and to throttle devices in case of overheating. Of course, a larger power consumption would be feasible for larger devices. Nevertheless, environmental concerns also result in the need to keep the power consumption low.</p><p id="Par391">Technology forecasts have been published as so-called International Technology Roadmap for Semiconductors. In the ITRS edition of 2013 [<xref ref-type="bibr" rid="CR261">261</xref>], it is explicitly stated that mobile phones are driving technological development: “<italic>System integration has shifted from a computational, PC-centric approach to a highly diversified mobile communication approach. The heterogeneous integration of multiple technologies in a limited space (e.g., GPS, phone, tablet, mobile phones, etc.) has truly revolutionized the semiconductor industry by shifting the main goal of any design from a performance driven approach to a reduced power driven approach. In few words, in the past performance was the one and only goal; today minimization of power consumption drives IC design</italic>.”</p></sec><sec id="Sec46"><title>Sensor Networks</title><p id="Par392">Sensor networks used for the Internet of Things are another special case. <index-term id="ITerm287"><term>sensor network</term></index-term><index-term id="ITerm288"><term>Internet of Things (IoT)</term></index-term> For sensor networks, there may be even much less energy available than for mobile phones. Hence, energy efficiency is of utmost importance, comprising of course energy-efficient communication [<xref ref-type="bibr" rid="CR543">543</xref>].</p></sec></sec></sec><sec id="Sec47"><title>Secure Hardware</title><p id="Par393"><index-term id="ITerm289"><term>secure hardware</term></index-term><index-term id="ITerm290"><term>secure hardware</term></index-term></p><p id="Par394">The general requirements for embedded systems can often include security (see p. 9). In particular, security is important for the Internet of Things. <index-term id="ITerm291"><term>Internet of Things (IoT)</term></index-term> If security is a major concern, special secure hardware may need to be developed. Security may need to be guaranteed for communication and for storage [<xref ref-type="bibr" rid="CR309">309</xref>]. Security has to be provided despite possible attacks and countermeasures must be designed. <bold>Attacks</bold> can be partitioned into the following [<xref ref-type="bibr" rid="CR300">300</xref>]: <list list-type="bullet"><list-item><p id="Par395"><bold>Software attacks</bold> are based on the execution of software. The deployment of software Trojans is an example of such an attack. Also, software defects can be exploited. Buffer overflows are a frequent cause of security hazards. Side-channel attacks try to exploit additional sources of information complementing the specified interfaces. Side-channel attacks based on software execution are difficult, but not infeasible. For example, it may be possible to exploit execution time information.<xref ref-type="fn" rid="Fn20">20</xref> Security-relevant algorithms should be designed such that their execution time does not depend on data values. This requirement also affects the implementation of computer arithmetic: instructions should not have data-dependent execution times.</p></list-item><list-item><p id="Par397">Attacks which require physical access and which can be classified into the following: <list list-type="bullet"><list-item><p id="Par398"><bold>Physical attacks</bold> try to open a side channel by physically tampering with the system. For example, silicon chips can be opened and analyzed. The first step in this procedure is de-packaging (removing the plastic covering the silicon). Next, micro-probing or optical analysis can be used. Such attacks are difficult, but they reveal many details of the chip.</p></list-item><list-item><p id="Par399"><bold>Power analysis</bold> is another class of attacks. Power analysis techniques include simple power analysis (SPA) and differential power analysis (DPA). In some cases, SPA may be sufficient to compute encryption keys. In other cases, advanced statistical methods may be needed to directly compute keys from small statistical fluctuations of measured currents.</p></list-item><list-item><p id="Par400"><bold>Analysis of electromagnetic radiation</bold> is another class of side-channel attacks.</p></list-item></list></p></list-item></list></p><p id="Par401">Different classes of people might try these attacks, and different classes of people may have an interest in blocking these attacks. The attacker may actually be the user of an embedded device trying to obtain unauthorized network access or unauthorized access to protected media such as music.</p><p id="Par402">We can distinguish between the following <bold>countermeasures</bold>: <list list-type="bullet"><list-item><p id="Par403">A security-aware software development process is required as a shield against software attacks.</p></list-item><list-item><p id="Par404">Tamper-resistant devices include special mechanisms for physical protection (shielding, or sensors to detect tampering with the modules).</p></list-item><list-item><p id="Par405">Devices can be designed such that processed data patterns have very little impact on the power consumption. This requires special devices which are typically not used in complex chips.</p></list-item><list-item><p id="Par406">Logical security, typically provided by cryptographic methods: encryption can be based on either symmetric or asymmetric ciphers. <index-term id="ITerm294"><term>encryption</term></index-term><list list-type="bullet"><list-item><p id="Par407">For symmetric ciphers, sender and receiver are using the same secret key to encrypt and decrypt messages. DES, 3DES, and AES are examples of symmetric ciphers.</p></list-item><list-item><p id="Par408">For asymmetric ciphers, messages are encrypted with a public key and decrypted with a private key. RSA and Diffie-Hellman are examples of asymmetric ciphers.</p></list-item><list-item><p id="Par409">Also, hash codes can be added to messages, allowing the detection of message modifications. MD5 and SHA are examples of hashing algorithms. <index-term id="ITerm295"><term>hashing</term></index-term></p></list-item></list></p><p id="Par410">Due to the performance gap, some processors may support encryption and decryption with dedicated instructions. Also, specialized solutions such as ARM’s TrustZone computing exist. “<italic>At the heart of the TrustZone approach is the concept of secure and non-secure worlds that are hardware separated, with non-secure software blocked from accessing secure resources directly. Within the processor, software either resides in the secure world or the non-secure world; a switch between these two worlds is accomplished via software referred to as the secure monitor (Cortex-A) or by the core logic (Cortex-M). This concept of secure (trusted) and non-secure (non-trusted) worlds extends beyond the processor to encompass memory, software, bus transactions, interrupts, and peripherals within an SoC</italic>” (see <ext-link xlink:href="https://www.arm.com/products/security-on-arm/trustzone" ext-link-type="uri">https://www.arm.com/products/security-on-arm/trustzone</ext-link>).</p><p id="Par411">The Kalray MPPA2<sup>®</sup> -256 multi-core processor chip contains as many as 128 specialized crypto co-processors connected to a matrix of 288 “regular” cores (see <ext-link xlink:href="http://www.kalrayinc.com/kalray/products/" ext-link-type="uri">http://www.kalrayinc.com/kalray/products/</ext-link>). Cores are 64 bit VLIW processors. <index-term id="ITerm296"><term>very long instruction word (VLIW)</term></index-term></p></list-item></list></p><p id="Par412">The following <bold>challenges</bold> exist for the design of countermeasures [<xref ref-type="bibr" rid="CR300">300</xref>]: <list list-type="order"><list-item><p id="Par413"><bold>Performance gap:</bold> Due to the limited performance of embedded systems, advanced encryption techniques may be too slow, in particular if high data rates have to be processed.</p></list-item><list-item><p id="Par414"><bold>Battery gap:</bold> Advanced encryption techniques require a significant amount of energy. This energy may be unavailable in a portable system. Smart cards are a special case of hardware that must run using a very small amount of energy.<index-term id="ITerm297"><term>energy</term></index-term></p></list-item><list-item><p id="Par415"><bold>Lack of flexibility:</bold> Frequently, many different security protocols are required within one system, and these protocols may have to be updated from time to time. This hinders using special hardware accelerators for encryption.</p></list-item><list-item><p id="Par416"><bold>Tamper resistance:</bold> Mechanisms against malicious attacks need to be built in. Their design is far from trivial. For example, it may be difficult if not impossible to guarantee that the current consumption is independent of the cryptographic keys that are processed.</p></list-item><list-item><p id="Par417"><bold>Assurance gap:</bold> The verification of security requires extra efforts during the design.</p></list-item><list-item><p id="Par418"><bold>Cost:</bold> Higher security levels increase the cost of the system.</p></list-item></list></p><p id="Par419">Ravi et al. have analyzed these challenges in detail for a Secure Sockets Layer (SSL) protocol [<xref ref-type="bibr" rid="CR300">300</xref>]. <index-term id="ITerm298"><term>Secure Socket Layer (SSL)</term></index-term></p><p id="Par420">More information on secure hardware is available, for example, in a book by Gebotys [<xref ref-type="bibr" rid="CR180">180</xref>] and in proceedings of a workshop series dedicated to this topic (see [<xref ref-type="bibr" rid="CR183">183</xref>] for the most recent edition).</p></sec><sec id="Sec48"><title>Problems</title><sec><p id="Par421">We suggest solving the following problems either at home or during a flipped classroom session:<index-term id="ITerm299"><term>flipped classroom</term></index-term></p></sec><sec id="FPar24"><title>3.1</title><p id="Par422">It is suggested that locally available small robots are used to demonstrate hardware in the loop, corresponding to Fig. <xref rid="Fig2" ref-type="fig">3.2</xref>. The robots should include sensors and actuators. Robots should run a program implementing a control loop. For example, an optical sensor could be used to let a robot follow a black line on the ground. The details of this assignment depend on the availability of robots.</p></sec><sec id="FPar25"><title>3.2</title><p id="Par423">Define the term “signal”!</p></sec><sec id="FPar26"><title>3.3</title><p id="Par424">Which circuit do we need for the transition from continuous time to discrete time?</p></sec><sec id="FPar27"><title>3.4</title><p id="Par425">What does the sampling theorem tell us?</p></sec><sec id="FPar28"><title>3.5</title><p id="Par426">Assume that we have an input signal <italic>x</italic> consisting of the sum of sine waves of 1.75 kHz and 2 kHz. We are sampling <italic>x</italic> at a rate of 3 kHz. Will we be able to reconstruct the original signal after discretization of time? Please explain your result!</p></sec><sec id="FPar29"><title>3.6</title><p id="Par427">Discretization of values is based on ADCs. Develop the schematic of a flash-based ADC for positive and negative input voltages! The output should be encoded as 3 bit two’s complement numbers, allowing to distinguish between eight different voltage intervals.</p></sec><sec id="FPar30"><title>3.7</title><p id="Par428">Suppose that we are working with a successive approximation-based 4 bit ADC. The input voltage range extends from <italic>V</italic><sub><italic>min</italic></sub> =1 V (="0000") to <italic>V</italic><sub><italic>max</italic></sub> =4.75 V (="1111"). Which steps are used to convert voltages of 2.25 V, 3.75 V, and 1.8 V? Draw a diagram similar to Fig. <xref rid="Fig12" ref-type="fig">3.12</xref> which depicts the successive approximation to these voltages!</p></sec><sec id="FPar31"><title>3.8</title><p id="Par429">Compare the complexity of flash-based and successive approximation-based ADC. Assume that you would like to distinguish between <italic>n</italic> different voltage intervals. Enter the complexity into Table <xref rid="Tab2" ref-type="table">3.2</xref>, using the <inline-formula id="IEq14"><alternatives><mml:math id="IEq14_Math"><mml:mi mathvariant="script">O</mml:mi></mml:math><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$${\mathcal {O}}$$
\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_IEq14.gif"/></alternatives></inline-formula>-notation. <table-wrap id="Tab2"><label>Table 3.2</label><caption xml:lang="en"><p>Complexity of ADCs</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"><p>Flash-based converter</p></th><th align="left"><p>Successive approximation converter</p></th></tr></thead><tbody><tr><td align="left"><p>Time complexity</p></td><td align="left"/><td align="left"/></tr><tr><td align="left"><p>Space complexity</p></td><td align="left"/><td align="left"/></tr></tbody></table></table-wrap></p></sec><sec id="FPar32"><title>3.9</title><p id="Par430">Suppose a sine wave is used as an input signal to the converter designed in Problem <xref rid="FPar29" ref-type="">3.6</xref>. Depict the quantization noise signal for this case!</p></sec><sec id="FPar33"><title>3.10</title><p id="Par431">Create a list of features of DSP processors!</p></sec><sec id="FPar34"><title>3.11</title><p id="Par432">Which components do FPGAs comprise? Which of these are used to implement Boolean functions? How are FPGAs configured? Are FPGAs energy-efficient? Which kind of applications are FPGAs good for?</p></sec><sec id="FPar35"><title>3.12</title><p id="Par433">What is the key idea of VLIW processors?</p></sec><sec id="FPar36"><title>3.13</title><p id="Par434">What is a “single-ISA heterogeneous multi-core architecture”? Which advantages do you see for such an architecture?</p></sec><sec id="FPar37"><title>3.14</title><p id="Par435">Explain the terms “GPU” and “MPSoC”!</p></sec><sec id="FPar38"><title>3.15</title><p id="Par436">Some FPGAs support an implementation of all Boolean functions of six variables. How many such functions exist? We ignore that some functions differ only by a renaming of variables.</p></sec><sec id="FPar39"><title>3.16</title><p id="Par437">In the context of memories, we are sometimes saying “small is beautiful.” What could be the reason for this?</p></sec><sec id="FPar40"><title>3.17</title><p id="Par438">Some levels of the memory hierarchy may be hidden from the application programmer. Why should such a programmer nevertheless care about the architecture of such levels?</p></sec><sec id="FPar41"><title>3.18</title><p id="Par439">What is a “scratchpad memory” (SPM)? How can we ensure that some memory object is stored in the SPM?</p></sec><sec id="FPar42"><title>3.19</title><p id="Par440">Develop the following FlexRay<sup>™</sup> cluster: The cluster consists<index-term id="ITerm300"><term>FlexRay<sup>™</sup></term></index-term> of the five nodes A, B, C, D, and E. All nodes should be connected via two channels. The cluster uses a bus topology. The nodes A, B, and C are executing a safety critical task, and therefore their bus requests should be guaranteed at the time of 20 macroticks. The following is expected from you: <list list-type="bullet"><list-item><p id="Par441">Download the levi FlexRay simulator [<xref ref-type="bibr" rid="CR495">495</xref>]. Unpack the ZIP file and install!</p></list-item><list-item><p id="Par442">Start the training module by executing the file leviFRP.jar.</p></list-item><list-item><p id="Par443">Design the described FlexRay cluster within the training module.</p></list-item><list-item><p id="Par444">Configure the communication cycle such that the nodes A, B, and C have a guaranteed bus access within a maximal delay of 20 macroticks. The nodes D and E should use only the dynamic segment.</p></list-item><list-item><p id="Par445">Configure the node bus requests. The node A sends a message every cycle. The nodes B and C send a message every second cycle. The node D sends a message of the length of 2 minislots every cycle, and the node E sends every second cycle a message of the length of 2 minislots.</p></list-item><list-item><p id="Par446">Start the visualization and check if the bus requests of the nodes A, B, and C are guaranteed.</p></list-item><list-item><p id="Par447">Swap the positions of nodes D and E in the dynamic segment. What is the resulting behavior?</p></list-item></list></p></sec><sec id="FPar43"><title>3.20</title><p id="Par448">Develop the schematic of a 3 bit DAC! The conversion should be done for a 3 bit vector <italic>x</italic> encoding positive numbers. Prove that the output voltage is proportional to the value represented by the input vector <italic>x</italic>. How would you modify the circuit if <italic>x</italic> represented two’s complement numbers?</p></sec><sec id="FPar44"><title>3.21</title><p id="Par449">The circuit shown in Fig. B.4 in Appendix B is an amplifier, amplifying input voltage <italic>V</italic><sub>1</sub>: <index-term id="ITerm301"><term>digital-to-analog converter (DAC)</term></index-term><disp-formula id="Equd"><alternatives><mml:math id="Equd_Math"><mml:mtable columnalign="right left"><mml:mtr><mml:mtd class="align-1"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math><tex-math id="Equd_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}
$$\displaystyle \begin{aligned} V_{out} = g_{closed} * V_1 \end{aligned}$$
\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="106758_4_En_3_Chapter_Equd.gif"/></alternatives></disp-formula> Compute the gain <italic>g</italic><sub><italic>closed</italic></sub> for the circuit of Fig. B.4 as a function of <italic>R</italic> and <italic>R</italic><sub>1</sub>!</p></sec><sec id="FPar45"><title>3.22</title><p id="Par450">How do different hardware technologies differ with respect to their energy efficiency?</p></sec><sec id="FPar46"><title>3.23</title><p id="Par451">The computational efficiency is sometimes also measured in terms of billions of operations per second per watt. How is this different from the figure of merit used in Fig. <xref rid="Fig65" ref-type="fig">3.65</xref>?</p></sec><sec id="FPar47"><title>3.24</title><p id="Par452">Why is it so important to optimize embedded systems? Compare different technologies for processing information in an embedded system with respect to their efficiency!</p></sec><sec id="FPar48"><title>3.25</title><p id="Par453">Suppose that your mobile phone uses a lithium battery rated at 720 mAh. The nominal voltage of the battery is 3.7 V. Assuming a constant power<index-term id="ITerm302"><term>power</term></index-term> consumption of 1 W, how long would it take to empty the battery? All secondary effects such as decreasing voltages should be ignored in this calculation.</p></sec><sec id="FPar49"><title>3.26</title><p id="Par454">Which challenges do you see for the security of embedded systems?</p></sec><sec id="FPar50"><title>3.27</title><p id="Par455">What is a “side-channel attack”? Please provide examples of side-channel attacks!</p></sec></sec></body><back><ref-list id="Bib1" specific-use="web-only"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Aamodt, T., Chow, P.: Embedded ISA support for enhanced floating-point to fixed-point ANSI C compilation. In: Proceedings of the International Conference on Compilers, Architectures, and Synthesis for Embedded Systems (CASES), pp. 128–137 (2000)</mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Absint: aiT worst-case execution time analyzers (2020). <ext-link xlink:href="http://www.absint.de/ait" ext-link-type="uri">http://www.absint.de/ait</ext-link></mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Analog Devices Inc. Eng.: ADSP-2100 Family User’s Manual. Out of print (1995)</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Analog Devices Inc. Eng.: Data Conversion Handbook (Analog Devices). Newnes, London (2004)</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">ARM Ltd.: AMBA specifications (2020). <ext-link xlink:href="http://www.arm.com/products/system-ip/amba-specifications.php" ext-link-type="uri">http://www.arm.com/products/system-ip/amba-specifications.php</ext-link></mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">ARM Ltd.: big.LITTLE Technology (2020). <ext-link xlink:href="http://www.arm.com/products/processors/technologies/biglittleprocessing.php" ext-link-type="uri">http://www.arm.com/products/processors/technologies/biglittleprocessing.php</ext-link></mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">ARM Ltd.: Mali-T860 &amp; Mali-T880 (2020). <ext-link xlink:href="http://www.arm.com/products/multimedia/mali-gpu/high-performance/mali-t860-t880.php" ext-link-type="uri">http://www.arm.com/products/multimedia/mali-gpu/high-performance/mali-t860-t880.php</ext-link></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Atmel: 32-Bit AVR Microcontroller (2012). <ext-link xlink:href="http://www.atmel.com/Images/doc32058.pdf" ext-link-type="uri">http://www.atmel.com/Images/doc32058.pdf</ext-link></mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Banakar, R., Steinke, S., Lee, B.S., Balakrishnan, M., Marwedel, P.: Scratchpad memory: A design alternative for cache on-chip memory in embedded systems. In: Proceedings of the International Symposium on Hardware-Software Codesign (CODES), pp. 73–78 (2002). <ext-link xlink:href="http://doi.acm.org/10.1145/774789.774805" ext-link-type="uri">http://doi.acm.org/10.1145/774789.774805</ext-link></mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benini</surname><given-names>L</given-names></name><name><surname>Bogliolo</surname><given-names>A</given-names></name><name><surname>De Micheli</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">A survey of design techniques for system-level dynamic power management</article-title><source>IEEE Trans. VLSI Syst.</source><year>2000</year><volume>8</volume><issue>3</issue><fpage>299</fpage><lpage>316</lpage><pub-id pub-id-type="doi">10.1109/92.845896</pub-id></mixed-citation></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beszedes</surname><given-names>A</given-names></name></person-group><article-title xml:lang="en">Survey of code size reduction methods</article-title><source>ACM Comput. Surv.</source><year>2003</year><volume>35</volume><fpage>223</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1145/937503.937504</pub-id></mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Bonny, T., Henkel, J.: Huffman-based code compression techniques for embedded processors. ACM Trans. Des. Autom. Electron. Syst. <bold>15</bold>(4), 31:1–31:37 (2010). <ext-link xlink:href="https://doi.org/10.1145/1835420.1835424" ext-link-type="uri">https://doi.org/10.1145/1835420.1835424</ext-link>. <ext-link xlink:href="http://doi.acm.org/10.1145/1835420.1835424" ext-link-type="uri">http://doi.acm.org/10.1145/1835420.1835424</ext-link></mixed-citation></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Burd, T., Brodersen, R.: Design issues for dynamic voltage scaling. In: International Symposium on Low Power Electronics and Design (ISLPED), pp. 9–14 (2000)</mixed-citation></ref><ref id="CR77"><label>77.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burd</surname><given-names>T</given-names></name><name><surname>Brodersen</surname><given-names>RW</given-names></name></person-group><source>Energy efficient microprocessor design</source><year>2003</year><publisher-loc>Dordrecht</publisher-loc><publisher-name>Kluwer Academic Publishers</publisher-name><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0990.68566</pub-id></mixed-citation></ref><ref id="CR78"><label>78.</label><mixed-citation publication-type="other">Burks, A., Goldstine, H., von Neumann, J.: Preliminary discussion of the logical design of an electronic computing element. Report to U.S. Army Ordnance Department (1946). Reprinted at <ext-link xlink:href="https://www.cs.princeton.edu/courses/archive/fall10/cos375/Burks.pdf" ext-link-type="uri">https://www.cs.princeton.edu/courses/archive/fall10/cos375/Burks.pdf</ext-link></mixed-citation></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Carroll, A., Heiser, G.: An analysis of power consumption in a smartphone. In: Proceedings of the 2010 USENIX Conference on USENIX Annual Technical Conference, USENIXATC’10, pp. 21–21. USENIX Association, Berkeley (2010). <ext-link xlink:href="http://dl.acm.org/citation.cfm?id=1855840.1855861" ext-link-type="uri">http://dl.acm.org/citation.cfm?id=1855840.1855861</ext-link></mixed-citation></ref><ref id="CR90"><label>90.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrakasan</surname><given-names>AP</given-names></name><name><surname>Sheng</surname><given-names>S</given-names></name><name><surname>Brodersen</surname><given-names>RW</given-names></name></person-group><article-title xml:lang="en">Low-power CMOS digital design</article-title><source>IEEE J. Solid-State Circuits</source><year>1992</year><volume>27</volume><issue>4</issue><fpage>119</fpage><lpage>123</lpage><pub-id pub-id-type="doi">10.1109/4.126534</pub-id></mixed-citation></ref><ref id="CR93"><label>93.</label><mixed-citation publication-type="other">Chao, C., Saeta, B.: HotChips 2019 Tutorial – Cloud TPU: Codesigning Architecture and Infrastructure (2019). <ext-link xlink:href="https://www.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf" ext-link-type="uri">https://www.hotchips.org/hc31/HC31_T3_Cloud_TPU_Codesign.pdf</ext-link></mixed-citation></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Rincón-Mora</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">Accurate electrical battery model capable of predicting runtime and i-v performance</article-title><source>IEEE Trans. Energy Convers.</source><year>2006</year><volume>21</volume><fpage>504</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1109/TEC.2006.874229</pub-id></mixed-citation></ref><ref id="CR100"><label>100.</label><mixed-citation publication-type="other">Chiu, Y.: Folding and Interpolating ADC. University of Texas at Dallas, EECT 7327 (2014). <ext-link xlink:href="http://www.utdallas.edu/%7Eyxc101000/courses/7327/slides/intp%20folding%20adc.pptx" ext-link-type="uri">http://www.utdallas.edu/~yxc101000/courses/7327/slides/intp%20folding%20adc.pptx</ext-link></mixed-citation></ref><ref id="CR118"><label>118.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dasgupta</surname><given-names>S</given-names></name></person-group><article-title xml:lang="en">The organization of microprogram stores</article-title><source>ACM Comput. Surv.</source><year>1979</year><volume>11</volume><fpage>39</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1145/356757.356761</pub-id></mixed-citation></ref><ref id="CR139"><label>139.</label><mixed-citation publication-type="other">Drepper, U.: What every programmer should know about memory (2007). <ext-link xlink:href="http://www.akkadia.org/drepper/cpumemory.pdf" ext-link-type="uri">http://www.akkadia.org/drepper/cpumemory.pdf</ext-link></mixed-citation></ref><ref id="CR144"><label>144.</label><mixed-citation publication-type="other">Dusza, B., Marwedel, P., Spinczyk, O., Wietfeld, C.: A context-aware battery lifetime model for carrier aggregation enabled LTE-A systems. In: IEEE Consumer Communications and Networking Conference (CCNC) (2014)</mixed-citation></ref><ref id="CR151"><label>151.</label><mixed-citation publication-type="other">Elsevier B.V.: Sensors and actuators A: physical. An Int. J. (2020)</mixed-citation></ref><ref id="CR152"><label>152.</label><mixed-citation publication-type="other">Elsevier B.V.: Sensors and actuators B: chemical. An Int. J. (2020)</mixed-citation></ref><ref id="CR165"><label>165.</label><mixed-citation publication-type="other">Flautner, K.: Heterogeneity to the rescue (2011). <ext-link xlink:href="https://www.bscmsrc.eu/sites/default/files/media/arm-heterogenous-mp-november-2011.pdf" ext-link-type="uri">https://www.bscmsrc.eu/sites/default/files/media/arm-heterogenous-mp-november-2011.pdf</ext-link></mixed-citation></ref><ref id="CR170"><label>170.</label><mixed-citation publication-type="other">Freescale Semiconductor/NXP: ColdFire<sup>®</sup> Family Programmer’s Reference Manual (2005). <ext-link xlink:href="http://www.nxp.com/assets/documents/data/en/reference-manuals/CFPRM.pdf" ext-link-type="uri">http://www.nxp.com/assets/documents/data/en/reference-manuals/CFPRM.pdf</ext-link></mixed-citation></ref><ref id="CR180"><label>180.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gebotys</surname><given-names>C</given-names></name></person-group><source>Security in Embedded Devices</source><year>2010</year><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4419-1530-6</pub-id></mixed-citation></ref><ref id="CR183"><label>183.</label><mixed-citation publication-type="other">Gierlichs, B., Poschmann, A.: International Workshop on Cryptographic Hardware and Embedded Systems (CHES) (2016). <ext-link xlink:href="http://www.chesworkshop.org/" ext-link-type="uri">http://www.chesworkshop.org/</ext-link></mixed-citation></ref><ref id="CR186"><label>186.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldberg</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">What every computer scientist should know about floating point arithmetic</article-title><source>ACM Comput. Surv.</source><year>1991</year><volume>23</volume><fpage>5</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1145/103162.103163</pub-id></mixed-citation></ref><ref id="CR188"><label>188.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><source>Deep Learning</source><year>2016</year><publisher-loc>Cambridge</publisher-loc><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1373.68009</pub-id></mixed-citation></ref><ref id="CR200"><label>200.</label><mixed-citation publication-type="other">Hamada, T., Benkrid, K., Nitadori, K., Taiji, M.: A comparative study on ASIC, FPGAs, GPUs and general purpose processors in the O(<italic>N</italic><sup>2</sup>) gravitational N-body simulation. In: Proceedings of the 2009 NASA/ESA Conference on Adaptive Hardware and Systems, AHS ’09, pp. 447–452. IEEE Computer Society, Washington (2009). <ext-link xlink:href="https://doi.org/10.1109/AHS.2009.55" ext-link-type="uri">https://doi.org/10.1109/AHS.2009.55</ext-link>. <ext-link xlink:href="http://dx.doi.org/10.1109/AHS.2009.55" ext-link-type="uri">http://dx.doi.org/10.1109/AHS.2009.55</ext-link></mixed-citation></ref><ref id="CR204"><label>204.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hastie</surname><given-names>T</given-names></name><name><surname>Tibshirani</surname><given-names>R</given-names></name><name><surname>Friedman</surname><given-names>R</given-names></name></person-group><source>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</source><year>2009</year><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-0-387-84858-7</pub-id></mixed-citation></ref><ref id="CR205"><label>205.</label><mixed-citation publication-type="other">Hattori, T.: MPSoC approaches for low-power embedded SoC’s (2007). <ext-link xlink:href="http://www.mpsoc-forum.org/previous/2007/slides/Hattori.pdf" ext-link-type="uri">http://www.mpsoc-forum.org/previous/2007/slides/Hattori.pdf</ext-link>. <ext-link xlink:href="http://www.mpsoc-forum.org/2007/slides/Hattori.pdf" ext-link-type="uri">http://www.mpsoc-forum.org/2007/slides/Hattori.pdf</ext-link></mixed-citation></ref><ref id="CR211"><label>211.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hennessy</surname><given-names>JL</given-names></name><name><surname>Patterson</surname><given-names>DA</given-names></name></person-group><source>Computer Architecture: A Quantitative Approach</source><year>2011</year><edition>5</edition><publisher-loc>Burlington</publisher-loc><publisher-name>Morgan Kaufmann</publisher-name><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">0752.68014</pub-id></mixed-citation></ref><ref id="CR226"><label>226.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>VD</given-names></name><name><surname>Puglia</surname><given-names>A</given-names></name><name><surname>Puglia</surname><given-names>M</given-names></name></person-group><source>RFID: A Guide to Radio Frequency Identification</source><year>2007</year><publisher-loc>Hoboken</publisher-loc><publisher-name>Wiley</publisher-name><pub-id pub-id-type="doi">10.1002/0470112255</pub-id></mixed-citation></ref><ref id="CR246"><label>246.</label><mixed-citation publication-type="other">Intel: Enhanced Intel<sup>®</sup> SpeedStep<sup>®</sup> Technology for the Intel<sup>®</sup> Pentium<sup>®</sup> M Processor - White paper (2004). <ext-link xlink:href="ftp://download.intel.com/design/network/papers/30117401.pdf" ext-link-type="uri">ftp://download.intel.com/design/network/papers/30117401.pdf</ext-link></mixed-citation></ref><ref id="CR247"><label>247.</label><mixed-citation publication-type="other">Intel: Motion estimation with Intel<sup>®</sup> streaming SIMD extensions 4 (Intel<sup>®</sup> SSE4) (2008). <ext-link xlink:href="http://software.intel.com/en-us/articles/motion-estimation-with-intel-streaming-simd-extensions-4-intel-sse4" ext-link-type="uri">http://software.intel.com/en-us/articles/motion-estimation-with-intel-streaming-simd-extensions-4-intel-sse4</ext-link></mixed-citation></ref><ref id="CR248"><label>248.</label><mixed-citation publication-type="other">Intel: Intel<sup>®</sup> AVX (2010). <ext-link xlink:href="http://software.intel.com/en-us/avx" ext-link-type="uri">http://software.intel.com/en-us/avx</ext-link></mixed-citation></ref><ref id="CR249"><label>249.</label><mixed-citation publication-type="other">Intel: Intel Itanium processor family (2016). <ext-link xlink:href="http://www.intel.com/itcenter/products/itanium" ext-link-type="uri">http://www.intel.com/itcenter/products/itanium</ext-link></mixed-citation></ref><ref id="CR253"><label>253.</label><mixed-citation publication-type="other">ISO: Road vehicles - FlexRay communications system – Part 1: General information and use case definition (2013). <ext-link xlink:href="http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=59804" ext-link-type="uri">http://www.iso.org/iso/home/store/catalogue_tc/catalogue_detail.htm?csnumber=59804</ext-link></mixed-citation></ref><ref id="CR261"><label>261.</label><mixed-citation publication-type="other">ITRS Organization: International Technology Roadmap for Semiconductors – 2013 Edition – Executive Summary (2013). <ext-link xlink:href="http://www.itrs2.net/2013-itrs.html" ext-link-type="uri">http://www.itrs2.net/2013-itrs.html</ext-link></mixed-citation></ref><ref id="CR262"><label>262.</label><mixed-citation publication-type="other">Iyer, A., Marculescu, D.: Power and performance evaluation of globally asynchronous locally synchronous processors. In: International Symposium on Computer Architecture (ISCA), pp. 158–168 (2002)</mixed-citation></ref><ref id="CR277"><label>277.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jouppi</surname><given-names>N</given-names></name><name><surname>Young</surname><given-names>C</given-names></name><name><surname>Patil</surname><given-names>N</given-names></name><name><surname>Patterson</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">A domain-specific architecture for deep neural networks</article-title><source>Commun. ACM</source><year>2018</year><volume>61</volume><fpage>50</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1145/3154484</pub-id><comment>https://doi.org/10.1145/3154484</comment></mixed-citation></ref><ref id="CR291"><label>291.</label><mixed-citation publication-type="other">Khorramabadi, H.: ADC Converters - Pipelined ADCs. UC Berkeley, EECS 247, Lecture 22 (2005). <ext-link xlink:href="http://www-inst.eecs.berkeley.edu/%7Eee247/fa05/lectures/L22_f05.pdf" ext-link-type="uri">http://www-inst.eecs.berkeley.edu/~ee247/fa05/lectures/L22_f05.pdf</ext-link></mixed-citation></ref><ref id="CR292"><label>292.</label><mixed-citation publication-type="other">Khorramabadi, H.: Oversampled ADCs. UC Berkeley, EECS 247, Lecture 23 (2009). <ext-link xlink:href="http://www-inst.eecs.berkeley.edu/%7Eee247/fa05/lectures/L22_f05.pdf" ext-link-type="uri">http://www-inst.eecs.berkeley.edu/~ee247/fa05/lectures/L22_f05.pdf</ext-link></mixed-citation></ref><ref id="CR295"><label>295.</label><mixed-citation publication-type="other">Klaiber, A.: The technology behind Crusoe<sup>™</sup> processors (2000). <ext-link xlink:href="http://web.archive.org/web/20010602205826/www.transmeta.com/crusoe/download/pdf/crusoetechwp.pdf" ext-link-type="uri">http://web.archive.org/web/20010602205826/www.transmeta.com/crusoe/download/pdf/crusoetechwp.pdf</ext-link></mixed-citation></ref><ref id="CR300"><label>300.</label><mixed-citation publication-type="other">Kocher, P., Lee, R., McGraw, G., Raghunathan, A.: Security as a new dimension in embedded system design. In: Proceedings of the 41st Annual Design Automation Conference, DAC ’04, pp. 753–760. ACM, New York (2004). Moderator-Ravi, Srivaths. <ext-link xlink:href="https://doi.org/10.1145/996566.996771" ext-link-type="uri">https://doi.org/10.1145/996566.996771</ext-link>. <ext-link xlink:href="http://doi.acm.org/10.1145/996566.996771" ext-link-type="uri">http://doi.acm.org/10.1145/996566.996771</ext-link></mixed-citation></ref><ref id="CR302"><label>302.</label><mixed-citation publication-type="other">Koopman, P.J., Upender, B.P.: Time division multiple access without a bus master. United Technologies Research Center, UTRC Technical Report RR-9500470 (1995). <ext-link xlink:href="http://www.ece.cmu.edu/%7Ekoopman/jtdma/jtdma.html" ext-link-type="uri">http://www.ece.cmu.edu/~koopman/jtdma/jtdma.html</ext-link></mixed-citation></ref><ref id="CR304"><label>304.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kopetz</surname><given-names>H</given-names></name><name><surname>Grunsteidl</surname><given-names>G</given-names></name></person-group><article-title xml:lang="en">TTP — a protocol for fault-tolerant real-time systems</article-title><source>IEEE Comput.</source><year>1994</year><volume>27</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1109/2.248873</pub-id></mixed-citation></ref><ref id="CR309"><label>309.</label><mixed-citation publication-type="other">Krhovjak, J., Matyas, V.: Secure hardware - pv018 (2006). <ext-link xlink:href="http://www.fi.muni.cz/%7Exkrhovj/lectures/2006_PV018_Secure_Hardware_slides.pdf" ext-link-type="uri">http://www.fi.muni.cz/~xkrhovj/lectures/2006_PV018_Secure_Hardware_slides.pdf</ext-link></mixed-citation></ref><ref id="CR316"><label>316.</label><mixed-citation publication-type="other">Kumar, R., Farkas, K.I., Jouppi, N.P., Ranganathan, P., Tullsen, D.M.: Single-ISA heterogeneous multi-core architectures: The potential for processor power reduction. In: Proceedings of the 36th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO 36, pp. 81–. IEEE Computer Society, Washington (2003). <ext-link xlink:href="http://dl.acm.org/citation.cfm?id=956417.956569" ext-link-type="uri">http://dl.acm.org/citation.cfm?id=956417.956569</ext-link></mixed-citation></ref><ref id="CR321"><label>321.</label><mixed-citation publication-type="other">Lang, F.: A/D-converter for high-speed communication (in German). Ph.D. Thesis, Fakultät Informatik, Elektrotechnik und Informationstechnik der Universität Stuttgart (2014)</mixed-citation></ref><ref id="CR324"><label>324.</label><mixed-citation publication-type="other">Latendresse, M.: The code compression bibliography (2004). <ext-link xlink:href="http://www.iro.umontreal.ca/%7Elatendre/compactBib" ext-link-type="uri">http://www.iro.umontreal.ca/~latendre/compactBib</ext-link></mixed-citation></ref><ref id="CR342"><label>342.</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Li</surname><given-names>ZR</given-names></name></person-group><source>Organic Light-Emitting Materials and Devices</source><year>2015</year><publisher-loc>Boca Raton</publisher-loc><publisher-name>CRC Press</publisher-name></mixed-citation></ref><ref id="CR346"><label>346.</label><mixed-citation publication-type="other">LIN Consortium: LIN Specification Package - Revision 2.2A (2010). <ext-link xlink:href="http://www.cs-group.de/fileadmin/media/Documents/LIN_Specification_Package_2.2A.pdf" ext-link-type="uri">http://www.cs-group.de/fileadmin/media/Documents/LIN_Specification_Package_2.2A.pdf</ext-link></mixed-citation></ref><ref id="CR358"><label>358.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Machanik</surname><given-names>P</given-names></name></person-group><source>Approaches to addressing the memory wall</source><year>2002</year><publisher-loc>Technical Report</publisher-loc><publisher-name>November, University Brisbane</publisher-name></mixed-citation></ref><ref id="CR359"><label>359.</label><mixed-citation publication-type="book"><person-group person-group-type="editor"><name><surname>Macii</surname><given-names>E</given-names></name></person-group><source>Ultra Low-Power Electronics and Design</source><year>2004</year><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name></mixed-citation></ref><ref id="CR360"><label>360.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Macii</surname><given-names>A</given-names></name><name><surname>Benini</surname><given-names>L</given-names></name><name><surname>Poncino</surname><given-names>M</given-names></name></person-group><source>Memory Design Techniques for Low Energy Embedded Systems</source><year>2002</year><publisher-loc>Dordrecht</publisher-loc><publisher-name>Kluwer Academic Publishers</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4757-5808-5</pub-id></mixed-citation></ref><ref id="CR363"><label>363.</label><mixed-citation publication-type="other">Man, H.D.: From the heaven of software to the hell of nanoscale physics: an industry in transition. Keynote, HiPEAC ACACES Summer School, L’Aquila (2007)</mixed-citation></ref><ref id="CR364"><label>364.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Manwell</surname><given-names>JF</given-names></name><name><surname>McGowan</surname><given-names>JG</given-names></name></person-group><article-title xml:lang="en">Lead acid battery storage model for hybrid energy systems</article-title><source>Sol. Energy</source><year>1993</year><volume>50</volume><fpage>399</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/0038-092X(93)90060-2</pub-id></mixed-citation></ref><ref id="CR388"><label>388.</label><mixed-citation publication-type="other">Meena, J.S., Sze, S.M., Chand, U., Tseng, T.Y.: Overview of emerging nonvolatile memory technologies. Nanoscale Res. Lett. <bold>9</bold>(1), 526 (2014). <ext-link xlink:href="https://doi.org/10.1186/1556-276X-9-526" ext-link-type="uri">https://doi.org/10.1186/1556-276X-9-526</ext-link>. <ext-link xlink:href="http://dx.doi.org/10.1186/1556-276X-9-526" ext-link-type="uri">http://dx.doi.org/10.1186/1556-276X-9-526</ext-link></mixed-citation></ref><ref id="CR398"><label>398.</label><mixed-citation publication-type="other">Mittal, S., Vetter, J.S.: A survey of methods for analyzing and improving GPU energy efficiency. ACM Comput. Surv. <bold>47</bold>(2), 19:1–19:23 (2014). <ext-link xlink:href="https://doi.org/10.1145/2636342" ext-link-type="uri">https://doi.org/10.1145/2636342</ext-link>. <ext-link xlink:href="http://doi.acm.org/10.1145/2636342" ext-link-type="uri">http://doi.acm.org/10.1145/2636342</ext-link></mixed-citation></ref><ref id="CR402"><label>402.</label><mixed-citation publication-type="other">MOST Cooperation: MOST Worldwide (2010). <ext-link xlink:href="http://www.mostcooperation.com/" ext-link-type="uri">http://www.mostcooperation.com/</ext-link></mixed-citation></ref><ref id="CR404"><label>404.</label><mixed-citation publication-type="other">Moynihan, T.: CMOS is winning the camera sensor battle, and here’s why (2011). <ext-link xlink:href="http://www.techhive.com/article/246931/cmos_is_winning_the_camera_sensor_battle_and_heres_why.html?page=0" ext-link-type="uri">http://www.techhive.com/article/246931/cmos_is_winning_the_camera_sensor_battle_and_heres_why.html?page=0</ext-link></mixed-citation></ref><ref id="CR436"><label>436.</label><mixed-citation publication-type="other">Oliveira, D.L., Faria, L.A., Delsoto, H.A., Garcia, K.: An architecture for globally-synchronous locally-asynchronous systems on FPGAs. In: IEEE XXIII International Congress on Electronics, Electrical Engineering and Computing (INTERCON), pp. 1–6 (2016)</mixed-citation></ref><ref id="CR437"><label>437.</label><mixed-citation publication-type="other">O’Neill, A.: Analog to digital types. IEEE TV (for members only) (2006). <ext-link xlink:href="https://ieeetv.ieee.org/conference-highlights/analog-to-digital-types?" ext-link-type="uri">https://ieeetv.ieee.org/conference-highlights/analog-to-digital-types?</ext-link></mixed-citation></ref><ref id="CR440"><label>440.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Oppenheim</surname><given-names>AV</given-names></name><name><surname>Schafer</surname><given-names>R</given-names></name><name><surname>Buck</surname><given-names>JR</given-names></name></person-group><source>Digital Signal Processing</source><year>2009</year><publisher-loc>New York</publisher-loc><publisher-name>Pearson Higher Education</publisher-name></mixed-citation></ref><ref id="CR448"><label>448.</label><mixed-citation publication-type="other">Patterson, D.: Domain-Specific Architectures for Deep Neural Networks (2019). <ext-link xlink:href="https://inst.eecs.berkeley.edu//%7Ecs152/sp19/lectures/L20-DSA.pdf" ext-link-type="uri">https://inst.eecs.berkeley.edu//~cs152/sp19/lectures/L20-DSA.pdf</ext-link></mixed-citation></ref><ref id="CR451"><label>451.</label><mixed-citation publication-type="other">Peukert, W.: Über die Abhängigkeit der Kapacität von der Entladestromstärcke bei Bleiakkumulatoren. Elektrotechnische Zeitschrift, vol. 20 (1897)</mixed-citation></ref><ref id="CR453"><label>453.</label><mixed-citation publication-type="other">Piatkowski, N.: Exponential families on resource-constrained systems. Ph.D. Thesis, TU Dortmund University, Dortmund (2018). URL <ext-link xlink:href="https://eldorado.tu-dortmund.de/handle/2003/36877" ext-link-type="uri">https://eldorado.tu-dortmund.de/handle/2003/36877</ext-link></mixed-citation></ref><ref id="CR467"><label>467.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>R</given-names></name><name><surname>Vrudhula</surname><given-names>S</given-names></name><name><surname>Rakhmatov</surname><given-names>D</given-names></name></person-group><article-title xml:lang="en">Battery modeling for energy-aware system design</article-title><source>IEEE Comput.</source><year>2003</year><volume>36</volume><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1109/MC.2003.1250886</pub-id></mixed-citation></ref><ref id="CR471"><label>471.</label><mixed-citation publication-type="other">Rixner, S., Dally, W.J., Khailany, B.J., Mattson, P.J., Kapasi, U.J.: Register organization for media processing. In: 6th High-Performance Computer Architecture (HPCA-6), pp. 375–386 (2000)</mixed-citation></ref><ref id="CR478"><label>478.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarajlic</surname><given-names>E</given-names></name><name><surname>Yamahata</surname><given-names>C</given-names></name><name><surname>Cordero</surname><given-names>M</given-names></name><name><surname>Fujita</surname><given-names>H</given-names></name></person-group><article-title xml:lang="en">Three-phase electrostatic rotary stepper micromotor with a flexural pivot bearing</article-title><source>J. Microelectromech. Syst.</source><year>2010</year><volume>19</volume><issue>2</issue><fpage>338</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1109/JMEMS.2010.2040139</pub-id><comment>https://doi.org/10.1109/JMEMS.2010.2040139</comment></mixed-citation></ref><ref id="CR481"><label>481.</label><mixed-citation publication-type="other">Science X: Imec reports breakthrough power efficiency and performance of coarse-grain processor (2005). <ext-link xlink:href="https://phys.org/news/2005-11-imec-breakthrough-power-efficiency-coarse-grain.html" ext-link-type="uri">https://phys.org/news/2005-11-imec-breakthrough-power-efficiency-coarse-grain.html</ext-link></mixed-citation></ref><ref id="CR484"><label>484.</label><mixed-citation publication-type="other">SGI: OpenGL software development kit (2016). <ext-link xlink:href="https://www.opengl.org/sdk/libs/" ext-link-type="uri">https://www.opengl.org/sdk/libs/</ext-link></mixed-citation></ref><ref id="CR487"><label>487.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Siciliano</surname><given-names>B</given-names></name><name><surname>Oussama</surname><given-names>O</given-names></name></person-group><source>Handbook of Robotics</source><year>2016</year><publisher-loc>Berlin</publisher-loc><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-32552-1</pub-id></mixed-citation></ref><ref id="CR495"><label>495.</label><mixed-citation publication-type="other">Sirocic, B., Marwedel, P.: Levi Flexray<sup>®</sup> simulation software (2007). <ext-link xlink:href="https://ls12-www.cs.tu-dortmund.de/daes/media/documents/teaching/downloads/levi/download/leviFRP.zip" ext-link-type="uri">https://ls12-www.cs.tu-dortmund.de/daes/media/documents/teaching/downloads/levi/download/leviFRP.zip</ext-link></mixed-citation></ref><ref id="CR503"><label>503.</label><mixed-citation publication-type="other">Society for Display Technology: Home page (2003). <ext-link xlink:href="http://www.sid.org" ext-link-type="uri">http://www.sid.org</ext-link></mixed-citation></ref><ref id="CR514"><label>514.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stritter</surname><given-names>E</given-names></name><name><surname>Gunter</surname><given-names>T</given-names></name></person-group><article-title xml:lang="en">Microprocessor architecture for a changing world: The Motorola 68000</article-title><source>IEEE Comput.</source><year>1979</year><volume>12</volume><fpage>43</fpage><lpage>52</lpage><pub-id pub-id-type="doi">10.1109/MC.1979.1658617</pub-id></mixed-citation></ref><ref id="CR530"><label>530.</label><mixed-citation publication-type="other">Texas Instruments Inc.: 66AK2x Multicore DSP + ARM Processors (2016). <ext-link xlink:href="http://www.ti.com/lsds/ti/processors/dsp/c6000_dsp-arm/66ak2x/overview.page" ext-link-type="uri">http://www.ti.com/lsds/ti/processors/dsp/c6000_dsp-arm/66ak2x/overview.page</ext-link></mixed-citation></ref><ref id="CR532"><label>532.</label><mixed-citation publication-type="other">The Dobelle Institute: Home page (2003). <ext-link xlink:href="http://www.dobelle.com" ext-link-type="uri">http://www.dobelle.com</ext-link> (no longer accessible)</mixed-citation></ref><ref id="CR540"><label>540.</label><mixed-citation publication-type="other">Tian, T., Shih, C.P.: Software techniques for shared-cache multi-core systems (2012). <ext-link xlink:href="https://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems" ext-link-type="uri">https://software.intel.com/en-us/articles/software-techniques-for-shared-cache-multi-core-systems</ext-link></mixed-citation></ref><ref id="CR543"><label>543.</label><mixed-citation publication-type="other">Tiwari, A., Ballal, P., Lewis, F.L.: Energy-efficient wireless sensor network design and implementation for condition-based maintenance. ACM Trans. Sen. Netw. <bold>3</bold>(1), 1-es (2007). <ext-link xlink:href="https://doi.org/10.1145/1210669.1210670" ext-link-type="uri">https://doi.org/10.1145/1210669.1210670</ext-link>. <ext-link xlink:href="http://doi.acm.org/10.1145/1210669.1210670" ext-link-type="uri">http://doi.acm.org/10.1145/1210669.1210670</ext-link></mixed-citation></ref><ref id="CR551"><label>551.</label><mixed-citation publication-type="other">Vahid, F.: Procedure exlining. In: Proceedings of the International Symposium on System Synthesis (ISSS), pp. 84–89 (1995)</mixed-citation></ref><ref id="CR553"><label>553.</label><mixed-citation publication-type="other">van Berkel, C.H.K.: Multi-core for mobile phones. In: Proceedings of the Conference on Design, Automation and Test in Europe, DATE ’09, pp. 1260–1265. European Design and Automation Association, 3001 Leuven, Belgium (2009). <ext-link xlink:href="http://dl.acm.org/citation.cfm?id=1874620.1874924" ext-link-type="uri">http://dl.acm.org/citation.cfm?id=1874620.1874924</ext-link></mixed-citation></ref><ref id="CR558"><label>558.</label><mixed-citation publication-type="other">Vogels, M., Gielen, G.: Architectural selection of A/D converters. In: Proceedings of the Design Automation Conference (DAC), pp. 974–977 (2003). <ext-link xlink:href="https://doi.org/10.1145/775832.776076" ext-link-type="uri">https://doi.org/10.1145/775832.776076</ext-link>. <ext-link xlink:href="http://doi.acm.org/10.1145/775832.776076" ext-link-type="uri">http://doi.acm.org/10.1145/775832.776076</ext-link></mixed-citation></ref><ref id="CR560"><label>560.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wainwright</surname><given-names>MJ</given-names></name><name><surname>Jordan</surname><given-names>MI</given-names></name></person-group><article-title xml:lang="en">Graphical models, exponential families, and variational inference</article-title><source>Found. Trends Mach. Learn.</source><year>2008</year><volume>1</volume><issue>1–2</issue><fpage>1</fpage><lpage>305</lpage><pub-id pub-id-type="doi">10.1561/2200000001</pub-id><comment>http://dx.doi.org/10.1561/2200000001</comment></mixed-citation></ref><ref id="CR570"><label>570.</label><mixed-citation publication-type="other">Weddell, A.S., Magno, M., Merrett, G.V., Brunelli, D., Al-Hashimi, B.M., Benini, L.: A survey of multi-source energy harvesting systems. In: Proceedings of the Conference on Design, Automation and Test in Europe, DATE ’13, pp. 905–908. EDA Consortium, San Jose (2013). <ext-link xlink:href="http://dl.acm.org/citation.cfm?id=2485288.2485505" ext-link-type="uri">http://dl.acm.org/citation.cfm?id=2485288.2485505</ext-link></mixed-citation></ref><ref id="CR575"><label>575.</label><mixed-citation publication-type="other">Wikipedia: Aircraft principal axes (2020). <ext-link xlink:href="https://en.wikipedia.org/wiki/Aircraft_principal_axes" ext-link-type="uri">https://en.wikipedia.org/wiki/Aircraft_principal_axes</ext-link></mixed-citation></ref><ref id="CR577"><label>577.</label><mixed-citation publication-type="other">Wikipedia: Energy harvesting (2020). <ext-link xlink:href="https://en.wikipedia.org/wiki/Energy_harvesting" ext-link-type="uri">https://en.wikipedia.org/wiki/Energy_harvesting</ext-link></mixed-citation></ref><ref id="CR589"><label>589.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilton</surname><given-names>S</given-names></name><name><surname>Jouppi</surname><given-names>N</given-names></name></person-group><article-title xml:lang="en">CACTI: An enhanced access and cycle time model</article-title><source>Int. J. Solid State Circ.</source><year>1996</year><volume>31</volume><issue>5</issue><fpage>677</fpage><lpage>688</lpage><pub-id pub-id-type="doi">10.1109/4.509850</pub-id></mixed-citation></ref><ref id="CR593"><label>593.</label><mixed-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>W</given-names></name></person-group><source>Computers as Components</source><year>2001</year><publisher-loc>Burlington</publisher-loc><publisher-name>Morgan Kaufmann</publisher-name></mixed-citation></ref><ref id="CR598"><label>598.</label><mixed-citation publication-type="other">Xilinx: MicroBlaze processor reference guide (2008). <ext-link xlink:href="http://www.xilinx.com/support/documentation/sw_manuals/mb_ref_guide.pdf" ext-link-type="uri">http://www.xilinx.com/support/documentation/sw_manuals/mb_ref_guide.pdf</ext-link></mixed-citation></ref><ref id="CR599"><label>599.</label><mixed-citation publication-type="other">Xilinx Inc.: UltraScale Architecture Configurable Logic Block (2015). <ext-link xlink:href="http://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf" ext-link-type="uri">http://www.xilinx.com/support/documentation/user_guides/ug574-ultrascale-clb.pdf</ext-link></mixed-citation></ref><ref id="CR601"><label>601.</label><mixed-citation publication-type="other">Xilinx Inc.: UltraScale+ FPGAs - Product Tables and Product Selection Guide (2016). <ext-link xlink:href="http://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf" ext-link-type="uri">http://www.xilinx.com/support/documentation/selection-guides/ultrascale-plus-fpga-product-selection-guide.pdf</ext-link></mixed-citation></ref><ref id="CR602"><label>602.</label><mixed-citation publication-type="other">Xilinx Inc.: UltraScale Architecture and Product Overview (2016). <ext-link xlink:href="http://www.xilinx.com/support/documentation/data_sheets/ds890-ultrascale-overview.pdf" ext-link-type="uri">http://www.xilinx.com/support/documentation/data_sheets/ds890-ultrascale-overview.pdf</ext-link></mixed-citation></ref></ref-list></ref-list><fn-group><fn id="Fn1"><label>1</label><p id="Par32">This presentation is based on the assumption that a comprehensive coverage of Fourier approximations<index-term id="ITerm26"><term>Fourier approximation</term></index-term> cannot be included in our course. Therefore, only the impact of these approximations is demonstrated by examples. Knowing the theory behind these examples would be beneficial (see, e.g., <ext-link xlink:href="http://www.dspguide.com" ext-link-type="uri">http://www.dspguide.com</ext-link>).</p></fn><fn id="Fn2"><label>2</label><p id="Par49">This would require knowing the signal to be filtered for an infinite amount of time.</p></fn><fn id="Fn3"><label>3</label><p id="Par55">In practice, the case of equal voltages is not relevant, as the actual behavior for very small differences between the voltages at the two inputs depends on many factors (like temperatures, manufacturing processes, etc.) anyway.</p></fn><fn id="Fn4"><label>4</label><p id="Par60">Such encoders are also useful for finding the most significant ’1’ in the mantissa of floating-point numbers.</p></fn><fn id="Fn5"><label>5</label><p id="Par68">Fortunately, the conversion<index-term id="ITerm48"><term>digital-to-analog converter (DAC)</term></index-term> from digital-to-analog values (D/A conversion) can be implemented very efficiently and can be very fast (see p. 54).</p></fn><fn id="Fn6"><label>6</label><p id="Par74">This can be done with a capacitor in the feedback loop of an operational amplifier (see p. 397).<index-term id="ITerm53"><term>feedback</term></index-term></p></fn><fn id="Fn7"><label>7</label><p id="Par91">In 2017, <ext-link xlink:href="http://anysilicon.com/semiconductor-wafer-mask-costs/" ext-link-type="uri">http://anysilicon.com/semiconductor-wafer-mask-costs/</ext-link> mentioned an average cost of about $ 1.5M for a 28 nm technology.</p></fn><fn id="Fn8"><label>8</label><p id="Par100">In practice, the increase may actually come with a larger exponential.</p></fn><fn id="Fn9"><label>9</label><p id="Par114">The availability of large flash memories and 3D integration make memory size constraints less tight.</p></fn><fn id="Fn10"><label>10</label><p id="Par118">We continue denoting multiplexers, arithmetic units, and memories by shape symbols, due to their widespread use in technical documentation. For memories, we adopt shape symbols<index-term id="ITerm99"><term>shape symbol for memories</term></index-term> including an explicit address decoder (included in the shape symbols for the ROMs on the right). These decoders identify the address input.</p></fn><fn id="Fn11"><label>11</label><p id="Par129">Instructions using predicated execution have an effect only if a certain condition encoded in the instruction evaluates to true. This condition typically involves values stored in condition code registers, resulting from previous instructions. For example, instructions might have an effect only if a previous &lt;= -expression was true. Predication can be used to implement <bold>if</bold> statements efficiently: the condition is stored in one of the condition registers, and <bold>if</bold>-statement bodies are implemented as predicated instructions which depend on this condition. For ARM processors, the condition is encoded in the first 4 bits of the instruction format. As a special case, an “always” condition can be encoded, like in Fig. <xref rid="Fig19" ref-type="fig">3.19</xref>. The more recently introduced 64 bit instruction set places less emphasis on predicated execution.</p></fn><fn id="Fn12"><label>12</label><p id="Par131">Using VHDL notation (see p. 70), concatenation is denoted by an &amp; sign, and constants are enclosed in quotes in Fig. <xref rid="Fig19" ref-type="fig">3.19</xref>.</p></fn><fn id="Fn13"><label>13</label><p id="Par138">In our notation, <italic>a</italic><sub>0</sub> is the weight of the oldest input value. If we would define <italic>a</italic><sub>0</sub> as the weight of the youngest value of <italic>w</italic>, the first term would take the more commonly used form <italic>w</italic><sub><italic>s</italic>−<italic>k</italic></sub>. Our notation simplifies understanding the program code shown below.</p></fn><fn id="Fn14"><label>14</label><p id="Par158">See Appendix C on p. 401 for an introduction to paging.</p></fn><fn id="Fn15"><label>15</label><p id="Par216">Rotation of this figure would improve its readability but would contradict the official designation of this layout style.</p></fn><fn id="Fn16"><label>16</label><p id="Par231">In fact, it is frequently difficult to select the right parameters.</p></fn><fn id="Fn17"><label>17</label><p id="Par320">In practice, due to rise and fall times being &gt;  0, transitions from one step to the next will not be ideal, but take some time.</p></fn><fn id="Fn18"><label>18</label><p id="Par326">Reconstruction may be possible if additional information about the signal is available, e.g., if we restrict ourselves to certain signal types.</p></fn><fn id="Fn19"><label>19</label><p id="Par384">The figure approximates information provided by H. De Man [<xref ref-type="bibr" rid="CR363">363</xref>] and is based on information provided by Philips.</p></fn><fn id="Fn20"><label>20</label><p id="Par396">Side-channel attacks based on timing information have been published under the names Spectre and Meltdown. They apply to modern processors using speculative execution; <index-term id="ITerm292"><term>Spectre</term></index-term><index-term id="ITerm293"><term>Meltdown</term></index-term> see <ext-link xlink:href="https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)" ext-link-type="uri">https://en.wikipedia.org/wiki/Spectre_(security_vulnerability)</ext-link>.</p></fn></fn-group></back></book-part></book-part-wrapper></records><facets><facet name="subject"><facet-value count="1">Circuits and Systems</facet-value><facet-value count="1">Cyber-physical systems, IoT</facet-value><facet-value count="1">Engineering</facet-value><facet-value count="1">Processor Architectures</facet-value><facet-value count="1">Professional Computing</facet-value></facet><facet name="keyword"/><facet name="pub"><facet-value count="1">Embedded System Design</facet-value></facet><facet name="year"><facet-value count="1">2021</facet-value></facet><facet name="country"><facet-value count="1">Germany</facet-value></facet><facet name="type"><facet-value count="1">Book</facet-value></facet></facets></response>
