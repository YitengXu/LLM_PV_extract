<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="/resources/spdi-openaccess-jats.xsl"?>
<!DOCTYPE response [
	
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<!ENTITY % book-part-wrapper SYSTEM "http://jats.nlm.nih.gov/extensions/bits/2.0/BITS-book2.dtd">
	]><response><apiMessage>This XML was provided by Springer Nature</apiMessage><query>doi:10.1186/s40537-023-00804-6</query><apiKey>87ba7cb21f89ce78154df796840621f4</apiKey><result><total>1</total><start>1</start><pageLength>2</pageLength><recordsDisplayed>1</recordsDisplayed></result><records><article dtd-version="1.2" article-type="research-article" xml:lang="en" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="publisher-id">40537</journal-id><journal-title-group><journal-title>Journal of Big Data</journal-title><abbrev-journal-title abbrev-type="publisher">J Big Data</abbrev-journal-title></journal-title-group><issn pub-type="epub">2196-1115</issn><publisher><publisher-name>Springer International Publishing</publisher-name><publisher-loc>Cham</publisher-loc></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">s40537-023-00804-6</article-id><article-id pub-id-type="manuscript">804</article-id><article-id pub-id-type="doi">10.1186/s40537-023-00804-6</article-id><article-categories><subj-group subj-group-type="heading"><subject>Research</subject></subj-group></article-categories><title-group><article-title xml:lang="en">Optimizing classification efficiency with machine learning techniques for pattern matching</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="Au1"><name><surname>Hamed</surname><given-names>Belal A.</given-names></name><address><email>belal.ahmed@mu.edu.eg</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="corresp" rid="IDs40537023008046_cor1">a</xref></contrib><contrib contrib-type="author" id="Au2"><name><surname>Ibrahim</surname><given-names>Osman Ali Sadek</given-names></name><address><email>osman.ibrahim@mu.edu.eg</email></address><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" id="Au3"><contrib-id contrib-id-type="orcid">https://orcid.org/0000-0003-1785-1058</contrib-id><name><surname>Abd El-Hafeez</surname><given-names>Tarek</given-names></name><address><email>tarek@mu.edu.eg</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="GRID">grid.411806.a</institution-id><institution-id institution-id-type="ISNI">0000 0000 8999 4945</institution-id><institution content-type="org-division">Department of Computer Science, Faculty of Science</institution><institution content-type="org-name">Minia University</institution></institution-wrap><addr-line content-type="city">EL-Minia</addr-line><country country="EG">Egypt</country></aff><aff id="Aff2"><label>2</label><institution-wrap><institution content-type="org-division">Computer Science Unit</institution><institution content-type="org-name">Deraya University</institution></institution-wrap><addr-line content-type="city">EL-Minia</addr-line><country country="EG">Egypt</country></aff></contrib-group><author-notes><corresp id="IDs40537023008046_cor1"><label>a</label><email>belal.ahmed@mu.edu.eg</email></corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>25</day><month>7</month><year>2023</year></pub-date><pub-date date-type="collection" publication-format="electronic"><month>12</month><year>2023</year></pub-date><volume>10</volume><issue seq="124">1</issue><elocation-id>124</elocation-id><history><date date-type="registration"><day>16</day><month>7</month><year>2023</year></date><date date-type="received"><day>30</day><month>3</month><year>2023</year></date><date date-type="accepted"><day>16</day><month>7</month><year>2023</year></date><date date-type="online"><day>25</day><month>7</month><year>2023</year></date></history><permissions><copyright-statement content-type="compact">© The Author(s) 2023</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>The Author(s)</copyright-holder><license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>.</license-p></license></permissions><abstract xml:lang="en" id="Abs1"><title>Abstract</title><p id="Par1">The study proposes a novel model for DNA sequence classification that combines machine learning methods and a pattern-matching algorithm. This model aims to effectively categorize DNA sequences based on their features and enhance the accuracy and efficiency of DNA sequence classification. The performance of the proposed model is evaluated using various machine learning algorithms, and the results indicate that the SVM linear classifier achieves the highest accuracy and F1 score among the tested algorithms. This finding suggests that the proposed model can provide better overall performance than other algorithms in DNA sequence classification. In addition, the proposed model is compared to two suggested algorithms, namely FLPM and PAPM, and the results show that the proposed model outperforms these algorithms in terms of accuracy and efficiency. The study further explores the impact of pattern length on the accuracy and time complexity of each algorithm. The results show that as the pattern length increases, the execution time of each algorithm varies. For a pattern length of 5, SVM Linear and EFLPM have the lowest execution time of 0.0035 s. However, at a pattern length of 25, SVM Linear has the lowest execution time of 0.0012 s. The experimental results of the proposed model show that SVM Linear has the highest accuracy and F1 score among the tested algorithms. SVM Linear achieved an accuracy of 0.963 and an F1 score of 0.97, indicating that it can provide the best overall performance in DNA sequence classification. Naive Bayes also performs well with an accuracy of 0.838 and an F1 score of 0.94. The proposed model offers a valuable contribution to the field of DNA sequence analysis by providing a novel approach to pre-processing and feature extraction. The model’s potential applications include drug discovery, personalized medicine, and disease diagnosis. The study’s findings highlight the importance of considering the impact of pattern length on the accuracy and time complexity of DNA sequence classification algorithms.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Bioinformatics</kwd><kwd>Feature extraction</kwd><kwd>Pattern matching</kwd><kwd>Machine learning</kwd><kwd>DNA sequences</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution>Minia University</institution></institution-wrap></funding-source></award-group></funding-group><custom-meta-group><custom-meta><meta-name>publisher-imprint-name</meta-name><meta-value>Springer</meta-value></custom-meta><custom-meta><meta-name>volume-issue-count</meta-name><meta-value>1</meta-value></custom-meta><custom-meta><meta-name>issue-article-count</meta-name><meta-value>124</meta-value></custom-meta><custom-meta><meta-name>issue-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>issue-pricelist-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-holder</meta-name><meta-value>Springer Nature Switzerland AG</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-contains-esm</meta-name><meta-value>No</meta-value></custom-meta><custom-meta><meta-name>article-numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-year</meta-name><meta-value>2023</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-month</meta-name><meta-value>7</meta-value></custom-meta><custom-meta><meta-name>article-registration-date-day</meta-name><meta-value>16</meta-value></custom-meta><custom-meta><meta-name>article-toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>toc-levels</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>volume-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>journal-product</meta-name><meta-value>ArchiveJournal</meta-value></custom-meta><custom-meta><meta-name>numbering-style</meta-name><meta-value>Unnumbered</meta-value></custom-meta><custom-meta><meta-name>article-grants-type</meta-name><meta-value>OpenChoice</meta-value></custom-meta><custom-meta><meta-name>metadata-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>abstract-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodypdf-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bodyhtml-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>bibliography-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>esm-grant</meta-name><meta-value>OpenAccess</meta-value></custom-meta><custom-meta><meta-name>online-first</meta-name><meta-value>false</meta-value></custom-meta><custom-meta><meta-name>pdf-file-reference</meta-name><meta-value>BodyRef/PDF/40537_2023_Article_804.pdf</meta-value></custom-meta><custom-meta><meta-name>output-medium</meta-name><meta-value>All</meta-value></custom-meta><custom-meta><meta-name>pdf-type</meta-name><meta-value>Typeset</meta-value></custom-meta><custom-meta><meta-name>target-type</meta-name><meta-value>OnlinePDF</meta-value></custom-meta><custom-meta><meta-name>issue-type</meta-name><meta-value>Regular</meta-value></custom-meta><custom-meta><meta-name>article-type</meta-name><meta-value>OriginalPaper</meta-value></custom-meta><custom-meta><meta-name>journal-subject-primary</meta-name><meta-value>Computer Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Database Management</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Information Storage and Retrieval</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Data Mining and Knowledge Discovery</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Computational Science and Engineering</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Mathematical Applications in Computer Science</meta-value></custom-meta><custom-meta><meta-name>journal-subject-secondary</meta-name><meta-value>Communications Engineering, Networks</meta-value></custom-meta><custom-meta><meta-name>journal-subject-collection</meta-name><meta-value>Computer Science</meta-value></custom-meta><custom-meta><meta-name>open-access</meta-name><meta-value>true</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">DNA is a kind of molecule that contains the genetic information needed by an organism to develop, survive, and reproduce. In addition, the sequencing of DNA is a technique used to identify the exact nucleotide sequences in a DNA molecule. The base sequence of DNA transmits the knowledge that a cell needs to assemble RNA and protein components. Additionally, DNA methylation is a genetic alteration important for controlling how the genome functions. It is important for both tumor suppression and carcinogenesis.</p><p id="Par3">The suggested method can therefore be used with any genome or DNA sequence, it has been discovered. The suggested methods may be used with additional data kinds, such as larger datasets. The suggested approach may be used as a conduit element to provide a feeling of raw data that focuses on tiny sets of dimensions and reduces entropy [<xref ref-type="bibr" rid="CR1">1</xref>]. Additionally, the categorization of biological sequences was one of several tasks in which the convolutional neural network (CNN) performed well. The available executions are frequently best for a certain task. Reusing it is challenging. According to this work, the suggested system can recover structural motifs and known sequences and conduct sequence classification with the highest accuracy compared to standard approaches [<xref ref-type="bibr" rid="CR2">2</xref>]. Additionally, this work concentrated on effectively categorizing the DNA sequence using machine learning methods. To verify its efficacy, the suggested system is also examined in terms of a few performance measures. The primary contributions of this work are effective feature extraction and pre-processing for locating pertinent DNA data.</p><p id="Par4">Therefore, researching data about DNA methylation may be useful to identify cancer biomarkers. Being able to analyze huge datasets efficiently is important given the abundance of publicly available data on matching methylation of DNA and the genome’s large number of methylation regions. As a result, our work has successfully computed a variety of alternative categorization models.</p><p id="Par5">To classify DNA sequences and accurately extract matched sequences using a pattern-matching technique. To assess the performance of the suggested model in terms of DNA sequence occurrence, execution time, F1-score, accuracy, precision, recall, and other metrics.</p><sec id="Sec2"><title>Related work</title><p id="Par7">DNA patterns have evolved into a significant setting for Sequence Data Analysis (SDA) [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR4">4</xref>] that helps predict a Sequence Function. (SF). It also investigates how the DNA patterns have evolved together. The model utilized machine learning (ML) techniques and was trained on an actual G4 [<xref ref-type="bibr" rid="CR5">5</xref>] generation dataset. This approach demonstrates the use of feature engineering techniques to extract relevant information from DNA sequences and classifiers based on ML algorithms to predict the formation of specific DNA patterns.</p><p id="Par8">Touati, R., et al. [<xref ref-type="bibr" rid="CR6">6</xref>] focused on the classification of helitron families using a combination of machine learning algorithms and feature extraction from DNA sequences. By extracting specific characteristics from DNA sequences, a fresh set of features related to helitrons was obtained. This study showcases the application of feature engineering techniques to capture important properties of DNA sequences and the utilization of machine learning classifiers to automatically classify DNA sequences into different helitron families.</p><p id="Par9">Norlin, S. in [<xref ref-type="bibr" rid="CR7">7</xref>] explored the categorization of DNA sequences using Nearest Neighbor categorization (NNC) based on Variable Length Markov Chains (VLMC). VLMC information was stored using a Vantage Point Tree (VPT) for efficient retrieval. This demonstrates the use of feature engineering techniques involving VLMC for characterizing DNA sequences and NNC as a classifier to categorize the sequences based on their similarity.</p><p id="Par10">Ryu, C., T. Lecroq, and K. Park in [<xref ref-type="bibr" rid="CR8">8</xref>] introduced a MAS (Maximal Average Shift), which finds a PSO (Pattern Scan Order) and lengthens the average shift by increasing the length. Two additional expansions were examined in this study—MAS. Through the scan results of the previous frame, it first increases the Scan Speed (SS) of MAS. Using q-grams, the second expansion increases the MAS running duration. As a result, these algorithms demonstrated superior performance compared to traditional algorithms. Further information regarding this project can be found in various details [<xref ref-type="bibr" rid="CR9">9</xref>–<xref ref-type="bibr" rid="CR14">14</xref>].</p></sec></sec><sec id="Sec3"><title>ML methodology for PM from DNA sequences</title><p id="Par11">We opted to build databases on DNA to examine the machine learning algorithms discussed in the following paragraphs. The rationale behind this decision was to sample some of the genes that we had worked on in our previous research endeavors [<xref ref-type="bibr" rid="CR15">15</xref>], Our objective was to integrate automated learning algorithms and pattern-matching algorithms that are based on specific DNA sequences, in order to create a biological data collection that could be utilized in a classification process. We conducted experiments on a dataset that included DNA sequences, where we compared the effectiveness of searching for a specific pattern with other classification models, such as <bold>Random Forest</bold> [<xref ref-type="bibr" rid="CR3">3</xref>, <xref ref-type="bibr" rid="CR16">16</xref>], <bold>KNN</bold>[<xref ref-type="bibr" rid="CR16">16</xref>–<xref ref-type="bibr" rid="CR20">20</xref>], <bold>Naïve Bayes</bold> [<xref ref-type="bibr" rid="CR21">21</xref>–<xref ref-type="bibr" rid="CR24">24</xref>], <bold>Decision tree</bold> [<xref ref-type="bibr" rid="CR23">23</xref>, <xref ref-type="bibr" rid="CR25">25</xref>–<xref ref-type="bibr" rid="CR30">30</xref>], and <bold>Support Vector Machine</bold>[<xref ref-type="bibr" rid="CR18">18</xref>, <xref ref-type="bibr" rid="CR31">31</xref>–<xref ref-type="bibr" rid="CR36">36</xref>] with <bold>Linear</bold>[<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR38">38</xref>], <bold>RBF</bold>[<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR39">39</xref>], and <bold>sigmoid</bold>[<xref ref-type="bibr" rid="CR21">21</xref>, <xref ref-type="bibr" rid="CR40">40</xref>] classifiers, the results of these classifiers models are calculated by F1 score, recall, precision rate, execution time, and with the accuracy which calculates the most effective pattern-matching classifier. The comparison of DNA sequences is a crucial task in various fields of research, including molecular biology and genetics. To facilitate this task, our study utilizes a machine learning (ML) approach that combines pattern-matching algorithms with ML techniques. This approach enables efficient pattern-matching and comparison of DNA sequences, thereby aiding in the identification of specific query patterns.</p><p id="Par12">Our methodology consists of several different phases, each of which plays a critical role in the overall process. The first phase involves the pre-processing of the DNA sequence data, which includes cleaning and filtering the data to remove any noise or irrelevant information. The pre-processed data is then subjected to feature extraction, which involves identifying and extracting relevant features from the data. In the next phase, the extracted features are used to create a model that can classify the DNA sequences based on their similarity to the query patterns. This step involves the use of various ML algorithms, including supervised and unsupervised learning techniques, to develop an accurate and efficient classification model.</p><p id="Par13">Once the classification model is developed, the pattern-matching algorithm is applied to the DNA sequences to identify any matches with the query patterns. This step involves the efficient comparison of DNA sequences based on their similarity to the query patterns, thereby enabling the identification of specific patterns of interest.</p><p id="Par14">The overall framework is illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, which provides an overview of the different phases involved in our ML approach for pattern-matching in DNA sequences. By combining both pattern-matching algorithms and ML techniques, our approach enables the efficient search of DNA sequences for specific query patterns, thereby facilitating the identification of critical information for various applications in molecular biology and genetics.</p><p id="Par15"><fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>Framework of the proposed work</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig1_HTML.png"/></fig></p><sec id="Sec4"><title>Pre-processing step</title><sec id="Sec5"><title>Gathering the dataset</title><p id="Par17">In this study, we acquired a set of biological DNA sequences from “The National Centre for Biotechnology Information (NCBI)” (<ext-link xlink:href="https://www.ncbi.nlm.nih.gov" ext-link-type="uri">https://www.ncbi.nlm.nih.gov</ext-link>) [<xref ref-type="bibr" rid="CR41">41</xref>]. The DNA data is stored in the (FASTA) format and is comprised of genomic sequences. Upon analyzing the dataset, we noticed that there was an unbalanced dataset problem, which needed to be addressed during the pre-processing stage. Additionally, the genomic sequence of the DNA dataset is categorical, which presents unique challenges for analysis. The DNA Sequences in FASTA files are illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>, and they feature sequence sizes of over 12 million characters of ATCG. This large dataset requires careful handling during the pre-processing stage to ensure that the data is clean, relevant, and appropriately prepared for analysis.</p><p id="Par18"><fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>Sample of FASTA Dataset</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig2_HTML.png"/></fig></p></sec><sec id="Sec6"><title>Transforming the data</title><p id="Par20">After consolidating all of the DNA sequence data, the next step involved transforming the data to make it suitable for machine learning model training. There are various methods for data transformation, depending on the type of data and business requirements. In our study, we converted the DNA Sequences in the FASTA file to a CSV file. We collected data for a group of biological DNA sequences by categorizing each group and assigning them a label (e.g., 1). We also added some sequences that were not related to DNA and assigned them to a separate category (e.g., 0). We created this dataset based on the use of FASTA DNA sequence files for specific genes. We converted the FASTA files for a particular gene to a CSV file and selected some of the sequences from it, labeling them to indicate that they belonged to this gene (e.g., 1). We also added another sequence that was not associated with this gene and labeled it accordingly (e.g., 0). Table <xref rid="Tab1" ref-type="table">1</xref> displays a sample of the conversion process from DNA Sequences from the FASTA file to a CSV file.</p><p id="Par21"><table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Sample of the converted DNA Sequences from the FASTA file to a CSV file</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>DNA Sequence</p></th><th align="left"><p>label</p></th></tr></thead><tbody><tr><td align="left"><p>aatcacgtacatcaccttgtaagaatttatctgcaatagtccttcggtattgtacattgttccaagcatag</p></td><td char="." align="char"><p>1</p></td></tr><tr><td align="left"><p>gtaaactaacgatatcaagtttgcctttctagcccatgacctacagtcagaagtgtaagccatatcactg</p></td><td char="." align="char"><p>1</p></td></tr><tr><td align="left"><p>tcggcatgttcaaactttgtcaaaccacaaaataaacacagtccttgaaatcgaatacgtagtttacatt</p></td><td char="." align="char"><p>1</p></td></tr><tr><td align="left"><p>ctcgcaagttgtggtcggccttgccacatttataacaagtagataagcgtacggggcatgctttcccagt</p></td><td char="." align="char"><p>1</p></td></tr><tr><td align="left"><p>atgagcacgaatttctgtgtctgggttaccaagagtgcaacttagacattcatctttatacactcgaaag</p></td><td char="." align="char"><p>1</p></td></tr><tr><td align="left"><p>tgctttggaaggaagatctggccatataaatttactgcatgctcttactggtcagtttgctacaagcttt</p></td><td char="." align="char"><p>0</p></td></tr><tr><td align="left"><p>gtgcggaggtatggcattttaatgttgagcaacgttcagtcgttcgtcgttggcaagttcaagatggtgt</p></td><td char="." align="char"><p>0</p></td></tr></tbody></table></table-wrap></p></sec><sec id="Sec7"><title>Cleaning and labeling the dataset</title><p id="Par23">After completing the conversion process from DNA Sequences in the FASTA file to a CSV file, we ensured that the resulting dataset was valid and that each sample had a corresponding class label (1 or 0). Figure <xref rid="Fig3" ref-type="fig">3</xref> provides a sample of the cleaned CSV dataset, demonstrating the successful completion of the data pre-processing stage.</p><p id="Par24"><fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>Sample of CSV Dataset</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Figa_HTML.png"/></fig></p><p id="Par28"><fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Conversion Datasets to numeric variables</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig4_HTML.png"/></fig></p></sec></sec><sec id="Sec8"><title>Feature extraction</title><p id="Par26">The extraction of important features is a critical phase in our analysis as irrelevant features can negatively impact the efficiency of the ML classifier. By selecting features appropriately, we can enhance classification accuracy and reduce the training time for the model. Feature extraction involves breaking down vast amounts of raw data into smaller groupings for processing. Due to the vast number of variables in these massive datasets, processing them requires significant computer resources.</p><p id="Par27">However, we cannot run machine learning algorithms on data in ‘Sequence’ (text) formats. Therefore, we must preprocess the data to transform it into a usable format for our algorithms. In this case, we convert the DNA sequence data into numerical data. There are several methods for calculating the numerical value for each feature. To accomplish this, we utilize the <bold>GET_DUMMIES</bold> function from the <bold>Pandas Library</bold>[<xref ref-type="bibr" rid="CR42">42</xref>–<xref ref-type="bibr" rid="CR44">44</xref>]. This function converts DNA Sequences to a numeric variable that encodes categorical information. Dummy variables have two possible values: 0 or 1. Once we have transformed the data into numerical format, we can use it in ML models for classification. The conversion of datasets to numeric variables is illustrated in</p></sec><sec id="Sec9"><title>Train/test splitting</title><p id="Par30">To ensure an unbiased evaluation of the machine learning model, it is essential to use data that was not previously used in the training process. Therefore, we need to split the collected dataset into separate training and testing datasets. In this step, after converting the data into a usable format for our ML model, we divided the dataset into a testing set and a training set. The training set was used to build and train our models using various classification algorithms, while the test set was used to evaluate the trained models on unseen data. The train_test_split function was utilized to split the data into a 25% testing set and a 75% training set. Additionally, stratify splitting was applied to ensure that the same split percentage was applied for each class in our data. The process of splitting the dataset into training and testing sets is depicted in Fig. <xref rid="Fig5" ref-type="fig">5</xref>.</p><p id="Par31"><fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Splitting Datasets</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Figb_HTML.png"/></fig></p></sec><sec id="Sec10"><title>Defining algorithms</title><p id="Par33">After completing the data pre-processing and splitting the dataset into training and testing sets, we can begin deploying various classification algorithms. To ensure a comprehensive evaluation of the models, we will compare the performance of seven different algorithms. The training data comprises 75% of the dataset, and these classification methods are used to train the models. The remaining 25% of the data is reserved for testing, and various metrics are used to evaluate the performance of the machine learning algorithms in conducting DNA sequence pattern-matching and retrieving matched sequences. To create the algorithms, we need to import each algorithm we intend to use and various performance measures from the SKLEARN library [<xref ref-type="bibr" rid="CR44">44</xref>], such as accuracy_score and classification_report, for each ML algorithm. The machine learning methods employed include (K-Nearest Neighbors) KNN, (Decision Tree) DT, (Random Forest) RF, (Naive Bayes) NB, Support Vector Machines (SVM) SIGMOID, SVM LINEAR, and SVM RBF, as depicted in Fig. <xref rid="Fig6" ref-type="fig">6</xref>.</p><p id="Par34"><fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p>Classification Algorithms Used</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig6_HTML.png"/></fig></p><p id="Par50"><fig id="Fig7"><label>Fig. 7</label><caption xml:lang="en"><p>The Confusion Matrix</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig7_HTML.png"/></fig></p></sec><sec id="Sec11"><title>Fitting models</title><p id="Par36">Model fitting is the process of assessing how well a classification machine learning model generalizes to a dataset that is similar to the one on which it was trained. A well-fitted model produces more accurate results, whereas an overfitted model closely fits the training data and may not perform well on new data. An underfitted model, on the other hand, does not fit the training data closely enough and may not capture the underlying patterns and relationships in the data.</p></sec><sec id="Sec12"><title>Tuning parameters</title><sec><p id="Par38">Modifying parameters before executing a training operation can help regulate the behavior of our ML algorithm. These parameter settings can significantly impact model training in terms of training duration, model accuracy, and model convergence. In this stage, we experiment with varying the algorithm’s parameters to find the best classification model solutions. For instance, when using the KNN algorithm, we set K = 3 to achieve the highest accuracy in identifying the DNA dataset. In the Decision Tree algorithm, we use max depth = 5 while classifying the dataset, which provides an optimal solution. For the RF model, we found that setting max depth = 5 and n estimators = 10 yielded the best classification results. We also employ several kernels to ensure the accuracy of SVM Classification, including SVM RBF, SVM SIGMOID, and SVM LINEAR, while using the SVM method. By experimenting with different parameter settings, we can optimize the behavior of our ML algorithm and achieve better classification performance.</p></sec><sec><p id="Par39">Table <xref rid="Tab2" ref-type="table">2</xref> provides a comparison of different classification algorithms based on their time complexity, advantages, and disadvantages. The table lists the algorithms in rows, with columns indicating the time complexity, advantages, and disadvantages of each algorithm. The time complexity column provides an estimate of the time required by each algorithm to complete the classification task. The advantages column lists the strengths and benefits of each algorithm, such as high accuracy, robustness, and interpretability. The disadvantages column highlights the limitations and weaknesses of each algorithm, such as high computational complexity, overfitting, and poor performance on imbalanced datasets.</p></sec><sec><p id="Par46"><table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Classification algorithms Comparisons</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Algorithms</p></th><th align="left"><p>Time Complexity</p></th><th align="left"><p>Advantages</p></th><th align="left"><p>Disadvantages</p></th></tr></thead><tbody><tr><td align="left"><p><bold>KNN</bold></p></td><td align="left"><p>O (n * d)</p><p><underline>Where</underline>:</p><p>n: the number of instances,</p><p>d: dimensions</p></td><td align="left"><p>1. There is no training period- KNN.</p><p>2. Simple Implementation</p></td><td align="left"><p>1. It does not perform well with huge datasets.</p><p>2. Does not function properly with several dimensions.</p><p>3. Sensitive to missing and noisy data</p><p>4. Scaling of Features</p></td></tr><tr><td align="left"><p><bold>SVM</bold></p></td><td align="left"><p>O(s*d)</p><p><underline>Where</underline>:</p><p>s: number of SV,</p><p>d: data dimensionality</p></td><td align="left"><p>1. In higher dimensions, it performs effectively.</p><p>2. When classes can be separated, the best algorithm is used.</p><p>3. Outliers have less influence.</p><p>4. SVM is well-suited for binary classification in extreme cases.</p></td><td align="left"><p>1. Slower with bigger datasets</p><p>2. Overlapped classes perform poorly.</p><p>3. It is critical to choose proper hyperparameters.</p><p>4. Choosing the right kernel function might be difficult.</p></td></tr><tr><td align="left"><p><bold>Decision Tree</bold></p></td><td align="left"><p>O(k)</p><p><underline>Where</underline>:</p><p>k: depth of tree</p></td><td align="left"><p>1. No data normalization or scaling is required.</p><p>2. Missing value handling</p><p>3. Feature selection that is automatic</p></td><td align="left"><p>1. Susceptible to overfitting.</p><p>2. Data sensitivity. When data changes little, the consequences might alter dramatically.</p><p>3. It takes more time to train decision trees.</p></td></tr><tr><td align="left"><p><bold>Random Forest</bold></p></td><td align="left"><p>O(k*m)</p><p><underline>Where</underline>:</p><p>k: depth of tree, m: decision trees</p></td><td align="left"><p>1. Error reduction</p><p>2. Excellent performance on unbalanced datasets</p><p>3. Dealing with massive amounts of data</p><p>4. Effective handling of missing data</p><p>5. Outliers have little influence</p></td><td align="left"><p>1. Features must have some predictive power, or they will not operate.</p><p>2. The tree predictions must be uncorrelated.</p></td></tr><tr><td align="left"><p><bold>Naive Bayes</bold></p></td><td align="left"><p>O(n*d)</p></td><td align="left"><p>1. Scalable when dealing with large datasets.</p><p>2. Insensitive to unimportant characteristics.</p><p>3. Effective multi-class prediction</p><p>4. High dimensional performance with good performance</p></td><td align="left"><p>1. The independence of characteristics is not valid.</p><p>2. Training data should accurately represent the population.</p></td></tr></tbody></table></table-wrap></p></sec><sec><p id="Par40">Some of the algorithms listed in Table <xref rid="Tab2" ref-type="table">2</xref> include Naive Bayes, Random Forest, K-Nearest Neighbors, Decision Trees, and Support Vector Machines (SVMs). For example, Naive Bayes is known for its simplicity and efficiency, making it a popular choice for text classification tasks. However, its main disadvantage is its assumption of independence between features, which can result in poor performance when dealing with highly correlated features.</p></sec><sec><title>Random Forest,</title><p id="Par41">on the other hand, is known for its high accuracy and robustness to noise and outliers. However, its training time can be significant, and it may suffer from overfitting when dealing with highly complex or imbalanced datasets.</p></sec><sec><title>K-Nearest Neighbors</title><p id="Par42">is a simple and effective algorithm that can be used for both classification and regression tasks. However, its main disadvantage is its high computational complexity, which can make it impractical for large datasets.</p></sec><sec><title>Decision Trees</title><p id="Par43">are easy to understand and interpret, making them a popular choice for applications where interpretability is important. However, they can suffer from overfitting and poor performance on imbalanced datasets.</p></sec><sec><title>Finally, SVMs</title><p id="Par44">are known for their high accuracy and ability to handle complex datasets. However, their training time can be significant, and they may require careful selection and tuning of hyperparameters to achieve optimal performance.</p></sec><sec><p id="Par45">Overall, Table <xref rid="Tab2" ref-type="table">2</xref> provides a useful summary of the advantages and disadvantages of different classification algorithms and can help guide researchers in selecting the most appropriate algorithm for their specific task and dataset.</p></sec></sec><sec id="Sec13"><title>Evaluation</title><p id="Par48">In this phase, several classification measurements were applied to report the performance of each model. These measurements are accuracy, recall, precision, and F1-score. All those measurements cannot be calculated without a confusion matrix [<xref ref-type="bibr" rid="CR45">45</xref>, <xref ref-type="bibr" rid="CR46">46</xref>].</p><p id="Par49">The Confusion Matrix is a performance statistic for a machine learning classification task where the output might be two or more classes. It is a table with four alternative combinations of projected and actual values, as shown below.</p><sec id="Sec01"><title>Accuracy</title><p id="Par51">is a measure of how close a measurement is to the truth, represented as the percentage of correctly classified instances. It is calculated using the equation:</p><p id="Par52"><disp-formula id="Equ1"><alternatives><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Accuracy}}\,{\rm{ = }}\,\left( {{\rm{TP}}\,{\rm{ + }}\,{\rm{TN}}} \right)\,{\rm{/}}\,\left( {{\rm{TP}}\,{\rm{ + }}\,{\rm{TN}}\,{\rm{ + }}\,{\rm{FP}}\,{\rm{ + }}\,{\rm{FN}}} \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ1.gif"/></alternatives></disp-formula></p><p id="Par53">where TN refers to the correct number of classifications of negative instances, TP refers to the correct number of classifications of positive instances, FP refers to the incorrect number of classifications of negative instances, and FN refers to the incorrect number of classifications of positive instances.</p></sec><sec id="Sec02"><title>Precision</title><p id="Par54">refers to the metric that measures how many of the predicted outputs were predicted correctly, calculated using the equation:</p><p id="Par55"><disp-formula id="Equ2"><alternatives><tex-math id="Equ2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Precision}}\,{\rm{ = }}\,{\rm{True}}\,{\rm{positives}}\,{\rm{/}}\,\left( {{\rm{True}}\,{\rm{positives}}\,{\rm{ + }}\,{\rm{False}}\,{\rm{positives}}} \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ2.gif"/></alternatives></disp-formula></p></sec><sec id="Sec03"><title>Recall</title><p id="Par56">is the percentage of correct positive predictions that have been made, based on all the correct positives in the dataset. It is calculated using the equation:</p><p id="Par57"><disp-formula id="Equ3"><alternatives><tex-math id="Equ3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{Recall}}\,{\rm{ = }}\,{\rm{True}}\,{\rm{positives}}\,{\rm{/}}\,\left( {{\rm{True}}\,{\rm{positives}}\,{\rm{ + }}\,{\rm{False}}\,{\rm{negatives}}} \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ3.gif"/></alternatives></disp-formula></p></sec><sec id="Sec04"><title>F1-Score</title><p id="Par58">is defined as the harmonic mean of both recall and precision of a model, scaled appropriately, calculated using the equation:</p><p id="Par59"><disp-formula id="Equ4"><alternatives><tex-math id="Equ4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{F1}}\,{\rm{Score}}\,{\rm{ = }}\,{\rm{2}}\,{\rm{*}}\,\left( {{\rm{Precision}}\,{\rm{*}}\,{\rm{Recall}}} \right)\,{\rm{/}}\,\left( {{\rm{Precision}}\,{\rm{ + }}\,{\rm{Recall}}} \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ4.gif"/></alternatives></disp-formula></p></sec><sec id="Sec05"><title>The ROC AUC</title><p id="Par60">can also be defined in terms of precision and recall, which are two other common metrics used in binary classification. Precision measures the fraction of positive predictions that are correct, while recall measures the fraction of positive examples that are correctly identified by the model. The relationship between precision, recall, and the true positive rate (TPR) and false positive rate (FPR) is as follows:</p><p id="Par61"><disp-formula id="Equ5"><alternatives><tex-math id="Equ5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{array}{l}{\rm{TPR}}\,{\rm{ = }}\,{\rm{recall}}\,{\rm{ = }}\,{\rm{true}}\,{\rm{positives}}\,{\rm{/}}\,\left( {{\rm{true}}\,{\rm{positives}}\,{\rm{ + }}\,{\rm{false}}\,{\rm{negatives}}} \right)\\{\rm{FPR}}\,{\rm{ = }}\,{\rm{false}}\,{\rm{positives}}\,{\rm{/}}\,\left( {{\rm{false}}\,{\rm{positives}}\,{\rm{ + }}\,{\rm{true}}\,{\rm{negatives}}} \right)\end{array}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par63">The ROC curve can be constructed by varying the classification threshold of the model and plotting the TPR against the FPR. The ROC AUC is then calculated by integrating the ROC curve as follows:</p><p id="Par64"><disp-formula id="Equ7"><alternatives><tex-math id="Equ7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\rm{ROC}}\,{\rm{AUC}}\,{\rm{ = }}\,{\rm{integral}}\left( {{\rm{recall}}\left( {{\rm{FPR}}} \right)} \right)\,{\rm{dFPR}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="40537_2023_804_Article_Equ7.gif"/></alternatives></disp-formula></p><p id="Par65">where recall(FPR) is the recall as a function of the false positive rate, and the integral is taken over the range of FPR. The ROC AUC can be interpreted as the probability that a randomly chosen positive example is ranked higher than a randomly chosen negative example by the model’s predicted scores, when the threshold for positive classification is varied. A perfect classifier has an ROC AUC of 1, while a random classifier has an ROC AUC of 0.5.</p></sec><sec id="Sec06"><title>Computation time</title><p id="Par66">is the learning time of the model in the DNA classification model using different Machine Learning Algorithms, as well as the time spent in the model testing process. The time function is used to record the time it takes to train the data during the classification process for all the methods used in the model. The computation time for each algorithm is shown in Table <xref rid="Tab2" ref-type="table">2</xref>.</p></sec></sec></sec><sec id="Sec14"><title>The experimental results</title><p id="Par68">Table <xref rid="Tab3" ref-type="table">3</xref> presents the results of the DNA sequence classification algorithms, along with their time complexity. The table includes seven algorithms: K-Nearest Neighbors (KNN), Decision Tree, Random Forest, Naive Bayes, Support Vector Machine with Radial Basis Function kernel (SVM RBF), Support Vector Machine with Sigmoid kernel (SVM Sigmoid), and Support Vector Machine with Linear kernel (SVM Linear).</p><p id="Par70"><table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Results of the DNA sequence Classification Algorithms and Time Complexity</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"/><th align="left"><p>Algorithms</p></th><th align="left"><p>Accuracy</p></th><th align="left"><p>Precision</p></th><th align="left"><p>ROC_AUC</p></th><th align="left"><p>Recall</p></th><th align="left"><p>F1 score</p></th><th align="left"><p>Execution time</p></th></tr></thead><tbody><tr><td align="left"><p>1</p></td><td align="left"><p>KNN</p></td><td char="." align="char"><p>0.778</p></td><td char="." align="char"><p>0.62</p></td><td char="." align="char"><p>0.701</p></td><td char="." align="char"><p>0.65</p></td><td char="." align="char"><p>0.79</p></td><td char="." align="char"><p>14.448</p></td></tr><tr><td align="left"><p>2</p></td><td align="left"><p>Decision Tree</p></td><td char="." align="char"><p>0.815</p></td><td char="." align="char"><p>0.92</p></td><td char="." align="char"><p>0.891</p></td><td char="." align="char"><p>0.71</p></td><td char="." align="char"><p>0.8</p></td><td char="." align="char"><p>13.271</p></td></tr><tr><td align="left"><p>3</p></td><td align="left"><p>Random Forest</p></td><td char="." align="char"><p>0.609</p></td><td char="." align="char"><p>0.67</p></td><td char="." align="char"><p>0.623</p></td><td char="." align="char"><p>0.47</p></td><td char="." align="char"><p>0.55</p></td><td char="." align="char"><p>12.983</p></td></tr><tr><td align="left"><p>4</p></td><td align="left"><p>Naive Bayes</p></td><td char="." align="char"><p>0.838</p></td><td char="." align="char"><p>0.83</p></td><td char="." align="char"><p>0.855</p></td><td char="." align="char"><p>0.88</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>12.606</p></td></tr><tr><td align="left"><p>5</p></td><td align="left"><p>SVM RBF</p></td><td char="." align="char"><p>0.925</p></td><td char="." align="char"><p>0.83</p></td><td char="." align="char"><p>0.937</p></td><td char="." align="char"><p>0.88</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>13.173</p></td></tr><tr><td align="left"><p>6</p></td><td align="left"><p>SVM Sigmoid</p></td><td char="." align="char"><p>0.925</p></td><td char="." align="char"><p>0.83</p></td><td char="." align="char"><p>0.952</p></td><td char="." align="char"><p>0.88</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>14.189</p></td></tr><tr><td align="left"><p>7</p></td><td align="left"><p>SVM Linear</p></td><td char="." align="char"><p>0.963</p></td><td char="." align="char"><p>0.91</p></td><td char="." align="char"><p>0.963</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>0.97</p></td><td char="." align="char"><p>10.059</p></td></tr></tbody></table></table-wrap></p><p id="Par69">The performance of the algorithms is evaluated using several metrics, including accuracy, precision, ROC_AUC, recall, and F1 score. The execution time of each algorithm is also reported.</p><p id="Par72">Table <xref rid="Tab3" ref-type="table">3</xref> shows that SVM Linear has the highest accuracy (0.963) and F1 score (0.97), indicating that it has the best overall performance among the algorithms. Naive Bayes also performs well with an accuracy of 0.838 and an F1 score of 0.94. KNN has the lowest accuracy (0.778) and F1 score (0.79) among the algorithms, while Random Forest performs poorly with an accuracy of 0.609 and an F1 score of 0.55. In terms of time complexity, SVM Linear has the lowest execution time (10.059 seconds), followed by Decision Tree (13.271 seconds) and SVM RBF (13.173 seconds). KNN has the highest execution time (14.448 seconds), while SVM Sigmoid has the second-highest execution time (14.189 seconds).</p><p id="Par73">Overall, the results suggest that SVM Linear and Naive Bayes are the top-performing algorithms for DNA sequence classification, while KNN and Random Forest are less effective. The time complexity results indicate that SVM Linear, Decision Tree, and SVM RBF are the most efficient algorithms in terms of execution time.</p><p id="Par74">Figure <xref rid="Fig8" ref-type="fig">8</xref> displays the metrics of each algorithm on the same DNA sequence, while Fig. <xref rid="Fig9" ref-type="fig">9</xref> shows the execution time for each technique in the DNA sequence classification process.</p><p id="Par75"><fig id="Fig8"><label>Fig. 8</label><caption xml:lang="en"><p>Metrics of each algorithm on the same DNA sequence</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig8_HTML.png"/></fig></p><p id="Par76"><fig id="Fig9"><label>Fig. 9</label><caption xml:lang="en"><p>DNA sequence execution time for each technique</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Fig9_HTML.png"/></fig></p><p id="Par77">Table <xref rid="Tab4" ref-type="table">4</xref> provides a comparison of the machine learning techniques discussed above with traditional techniques, including our two proposed methods (EFLPM and EPAPM) [<xref ref-type="bibr" rid="CR47">47</xref>] based on their execution time for different pattern lengths in DNA sequences.</p><p id="Par79"><table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>A comparison of machine learning techniques with traditional techniques based on their execution time for different pattern lengths in DNA sequences</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"><p>No</p></th><th align="left" rowspan="2"><p>Algorithms</p></th><th align="left" colspan="5"><p>Pattern Length</p></th></tr><tr><th align="left"><p>5</p></th><th align="left"><p>10</p></th><th align="left"><p>15</p></th><th align="left"><p>20</p></th><th align="left"><p>25</p></th></tr></thead><tbody><tr><td align="left"><p>1</p></td><td align="left"><p>FLPM</p></td><td char="." align="char"><p>0.0045</p></td><td char="." align="char"><p>0.0025</p></td><td char="." align="char"><p>0.0025</p></td><td char="." align="char"><p>0.0026</p></td><td char="." align="char"><p>0.0018</p></td></tr><tr><td align="left"><p>2</p></td><td align="left"><p>PAPM</p></td><td char="." align="char"><p>0.0054</p></td><td char="." align="char"><p>0.0018</p></td><td char="." align="char"><p>0.0022</p></td><td char="." align="char"><p>0.0025</p></td><td char="." align="char"><p>0.0024</p></td></tr><tr><td align="left"><p>3</p></td><td align="left"><p>EFLPM</p></td><td char="." align="char"><p>0.0035</p></td><td char="." align="char"><p>0.0018</p></td><td char="." align="char"><p>0.0019</p></td><td char="." align="char"><p>0.0020</p></td><td char="." align="char"><p>0.0012</p></td></tr><tr><td align="left"><p>4</p></td><td align="left"><p>EPAPM</p></td><td char="." align="char"><p>0.0044</p></td><td char="." align="char"><p>0.0011</p></td><td char="." align="char"><p>0.0017</p></td><td char="." align="char"><p>0.0013</p></td><td char="." align="char"><p>0.0015</p></td></tr><tr><td align="left"><p>5</p></td><td align="left"><p>SVM Linear</p></td><td char="." align="char"><p>0.0035</p></td><td char="." align="char"><p>0.0020</p></td><td char="." align="char"><p>0.0019</p></td><td char="." align="char"><p>0.0015</p></td><td char="." align="char"><p>0.0012</p></td></tr></tbody></table></table-wrap></p><p id="Par78">Table <xref rid="Tab4" ref-type="table">4</xref> summarizes the results of experiments conducted on different pattern matching and classification algorithms for different pattern lengths in DNA sequences. The table shows the pattern lengths in columns and the algorithms in rows, with corresponding values indicating execution time achieved by each algorithm for a particular pattern length.</p><p id="Par80">The experiments involved the use of FLPM, PAPM, EFLPM, EPAPM, and SVM Linear algorithms for DNA sequence classification. At a pattern length of 5, FLPM achieved timeof 0.0045, PAPM achieved time of 0.0054, EFLPM achieved time of 0.0035, EPAPM achieved time of 0.0044, and SVM Linear achieved time of 0.0035.</p><p id="Par81">As the pattern length increased, the accuracy of each algorithm varied. For instance, at a pattern length of 10, both EFLPM and EPAPM achieved the highest accuracy of 0.0018, while PAPM achieved time of 0.0018. SVM Linear performed the best at a pattern length of 25, achieving time of 0.0012.</p><p id="Par82">In addition, the table indicates that our proposed methods, EFLPM and EPAPM, outperformed traditional techniques such as FLPM and PAPM in terms of classification accuracy for certain pattern lengths. Furthermore, SVM Linear consistently performed well across all pattern lengths.</p><p id="Par83">Overall, the results suggest that the accuracy of each algorithm is dependent on the pattern length, and machine learning techniques, specifically SVM Linear and our proposed methods, are better suited for DNA sequence classification than traditional techniques like FLPM and PAPM.The visualization of the summary of experimental results is shown in Fig. <xref rid="Fig10" ref-type="fig">10</xref>.</p><p id="Par84"><fig id="Fig10"><label>Fig. 10</label><caption xml:lang="en"><p>Summary of the experiments results</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/40537_2023_804_Figd_HTML.png"/></fig></p><p id="Par85">As shown in Fig. <xref rid="Fig10" ref-type="fig">10</xref>, SVM Linear outperforms other methods in terms of execution time, indicating that our model is efficient in classifying DNA sequences and matching patterns. Moreover, we compared the proposed and existing systems based on their runtime, and the results demonstrate that both our proposed methods (EFLPM and EPAPM) and SVM Linear with a linear kernel have similar execution times, minimizing time complexity. Therefore, these methods are effective and practical solutions for DNA sequence classification tasks that require fast execution times.</p></sec><sec id="Sec15" sec-type="discussion"><title>Discussion</title><p id="Par86">The use of machine learning algorithms in pattern matching has gained significant attention in recent years due to their ability to accurately classify and identify patterns in large datasets. In this <xref rid="Sec15" ref-type="sec">discussion</xref> section, we will explore the advantages and limitations of using machine learning algorithms in pattern matching and their potential applications.</p><p id="Par87">One of the main advantages of using machine learning algorithms in pattern matching is their ability to process large amounts of data quickly and accurately. Machine learning algorithms can identify complex patterns and relationships in data sets that may not be immediately apparent to human analysts. Furthermore, these algorithms can learn and adapt to new patterns as they are discovered, making them a powerful tool for identifying and classifying new patterns and trends.</p><p id="Par88">Another advantage of using machine learning algorithms in pattern matching is their ability to automate the process, reducing the need for human intervention. This can significantly reduce the time and resources required to analyze large datasets, enabling researchers to focus on other aspects of their research.</p><p id="Par89">However, there are also limitations to using machine learning algorithms in pattern matching. One of the main challenges is the need for large amounts of high-quality data to train the algorithms effectively. In many cases, obtaining high-quality data can be difficult, particularly when working with complex data sets such as DNA sequences.</p><p id="Par90">Another limitation is the potential for overfitting, where the algorithm becomes too specialized in recognizing specific patterns in the training data and performs poorly when presented with new data. To address this challenge, researchers must carefully select and preprocess the data used to train the algorithms and use appropriate techniques such as cross-validation to evaluate their performance.</p><p id="Par91">Despite these limitations, machine learning algorithms have many potential applications in pattern matching, including DNA sequence classification, image recognition, and natural language processing. For example, in DNA sequence classification, machine learning algorithms can be used to identify specific patterns associated with various diseases, enabling researchers to develop more targeted treatments.</p><p id="Par92">Overall, the use of machine learning algorithms in pattern matching has the potential to revolutionize many fields and disciplines, enabling researchers to identify and analyze patterns in large datasets quickly and accurately. However, it is important to carefully evaluate the strengths and limitations of these algorithms to ensure they are used effectively and appropriately.</p></sec><sec id="Sec16"><title>Limitations</title><p id="Par93">The limitations for Pattern Matching classification can be summarized as follows:</p><p id="Par01"><list list-type="bullet"><list-item><p id="Par94"><bold>Algorithm comparison</bold>: The study compares the proposed model with only two other algorithms, FLPM and PAPM. While the results show that the proposed model outperforms these algorithms, it would be valuable to compare the Deep Learning models with a wider range of algorithms to further validate its effectiveness.</p></list-item><list-item><p id="Par95"><bold>Pattern length evaluation</bold>: The study examines the impact of pattern length on the accuracy and time complexity of each algorithm, but only for a limited range of pattern lengths. It would be valuable to investigate the performance of the algorithms for longer or more complex patterns.</p></list-item><list-item><p id="Par96"><bold>Feature extraction</bold>: More complex feature extraction methods could potentially improve the model’s performance.</p></list-item><list-item><p id="Par97"><bold>Scope of applications</bold>: The study focuses primarily on DNA sequence classification for drug discovery, personalized medicine, and disease diagnosis. While these are important applications, the model’s potential for other applications or fields is not explored in depth.</p></list-item><list-item><p id="Par98"><bold>Imbalanced dataset</bold>: The dataset used in the study may be imbalanced, meaning that there are more examples of one class than the other. This could affect the model’s performance and lead to biased results.</p></list-item><list-item><p id="Par99"><bold>hyperparameter tuning</bold>: The study uses a limited range of hyperparameters for each algorithm, which may not be optimal for all datasets or applications.</p></list-item></list></p></sec><sec id="Sec17"><title>Conclusion and future work</title><sec><p id="Par100">The proposed model for DNA sequence classification offers a valuable contribution to the field and has significant potential for practical applications. Further research and development in this area could lead to improved accuracy and efficiency in DNA sequence classification, with important implications for drug discovery, personalized medicine, and disease diagnosis.</p></sec><sec><p id="Par101">This paper focuses on using a pattern-matching method to retrieve matched DNA sequences. The study covers the following steps:</p></sec><sec><p id="Par001"><list list-type="bullet"><list-item><p id="Par102">Building a DNA Sequences dataset from DNA FASTA files and converting it to a CSV file.</p></list-item><list-item><p id="Par103">Importing data from the CSV file.</p></list-item><list-item><p id="Par104">Converting text inputs to numerical data.</p></list-item><list-item><p id="Par105">Building and training classification algorithms.</p></list-item><list-item><p id="Par106">Comparing and contrasting classification algorithms based on execution time, recall, precision, F1-score, ROC_AUC, occurrences, and accuracy.</p></list-item></list></p></sec><sec><p id="Par107">The performance of the proposed model is evaluated using various machine learning algorithms, and the results indicate that the SVM linear classifier achieves the highest accuracy and F1 score among the tested algorithms. This finding suggests that the proposed model can provide better overall performance than other algorithms in DNA sequence classification. In addition, the proposed model is compared to two suggested algorithms, namely FLPM and PAPM, and the results show that the proposed model outperforms these algorithms in terms of accuracy and efficiency. The study further explores the impact of pattern length on the accuracy and time complexity of each algorithm. The results show that as the pattern length increases, the execution time of each algorithm varies. For a pattern length of 5, SVM Linear and EFLPM have the lowest execution time of 0.0035 s. However, at a pattern length of 25, SVM Linear has the lowest execution time of 0.0012 s. The experimental results of the proposed model show that SVM Linear has the highest accuracy and F1 score among the tested algorithms. SVM Linear achieved an accuracy of 0.963 and an F1 score of 0.97, indicating that it can provide the best overall performance in DNA sequence classification. Naive Bayes also performs well with an accuracy of 0.838 and an F1 score of 0.94.</p></sec><sec><title>Future work</title><p id="Par108">The proposed model for DNA sequence classification is a promising development that can enhance the accuracy and efficiency of DNA sequence classification. However, there are several future directions that could be pursued to further improve the model’s performance and expand its potential applications.</p></sec><sec><p id="Par109">One possible future direction is to investigate the performance of the proposed model on larger datasets. The current study used a relatively small dataset, and it would be interesting to see how the model performs on larger-scale datasets with more diverse sequences. This would help to validate the model’s effectiveness in real-world scenarios and enhance its potential applications.</p></sec><sec><p id="Par110">Another possible future direction is to explore the use of deep learning techniques for DNA sequence classification. Deep learning models, such as convolutional neural networks (CNNs,MLP and LSTM) and recurrent neural networks (RNNs), have shown promising results in various domains, including natural language processing and computer vision. It would be interesting to see how these techniques could be adapted to DNA sequence classification and whether they could provide improved performance compared to the proposed model.</p></sec><sec><p id="Par111">Furthermore, it would be valuable to investigate the model’s performance on different types of DNA sequences, such as those from different organisms or with different functional roles. This would help to identify any potential limitations of the model and areas where it could be further improved.</p></sec></sec></body><back><ack><title>Acknowledgements</title><p>Authors sincerely acknowledge Computer Science Department in Faculty of Science, Minia University for the facilities and support.</p></ack><sec sec-type="author-contribution"><title>Authors’ contributions</title><p>This work was carried out in collaboration among all authors. Author BAH designed the study, performed the statistical analysis and wrote the protocol. Authors OASI and TAEH managed the analyses of the study, managed the literature searches and wrote the first draft of the manuscript. All authors read and approved the final manuscript.</p></sec><sec><title>Funding</title><p>Open access funding provided by The Science, Technology &amp; Innovation Funding Authority (STDF) in cooperation with The Egyptian Knowledge Bank (EKB).</p></sec><sec sec-type="data-availability"><title>Data availability</title><p>The data that support the findings of this study are available in <ext-link xlink:href="https://github.com/belalahmedhamed/Bioinformatics-Dataset" ext-link-type="uri">https://github.com/belalahmedhamed/Bioinformatics-Dataset</ext-link>. These data were derived from the following resources available in the public domain of “The National Center for Biotechnology Information advances science and health by providing access to biomedical and genomic information” <ext-link xlink:href="https://www.ncbi.nlm.nih.gov/guide/dna-rna/" ext-link-type="uri">https://www.ncbi.nlm.nih.gov/guide/dna-rna/</ext-link>.</p></sec><sec sec-type="ethics-statement"><title>Declarations</title><sec id="FPar1" sec-type="COI-statement"><title>Competing interests</title><p id="Par112">The authors declare that there is no conflict of interest. The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></sec><sec id="FPar2"><title>Ethical statement</title><p id="Par113">All procedures performed in studies involving human participants were in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards.</p></sec><sec id="FPar3"><title>Consent statement</title><p id="Par03">Informed consent was obtained from all individual participants included in the study.</p></sec></sec><ref-list id="Bib1"><title>References</title><ref-list><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Marczyk VR, Recamonde-Mendoza M, Maia AL, Goemann IMJT. <italic>Classification of Thyroid Tumors Based on DNA Methylation Patterns</italic> 2023(ja).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>PJFiG</given-names></name></person-group><source>Pan-cancer DNA methylation analysis and tumor origin identification of carcinoma of unknown primary site based on multi-omics</source><year>2022</year><volume>12</volume><fpage>798748</fpage></mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Lin</surname><given-names>P</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Xing</surname><given-names>S</given-names></name><name><surname>Du</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Yang</surname><given-names>J</given-names></name><name><surname>Huan</surname><given-names>T</given-names></name><name><surname>Long</surname><given-names>C</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>M</given-names></name></person-group><source>HExpPredict: In Vivo Exposure Prediction of Human Blood Exposome Using a Random Forest Model and Its Application in Chemical Risk Prioritization</source><year>2023</year><volume>131</volume><issue>3</issue><fpage>037009</fpage></mixed-citation></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Suyama Y, Hirota SK, Matsuo A, Tsunamoto Y, Mitsuyuki C, Shimura A, Okano K. Complementary combination of multiplex high-throughput DNA sequencing for molecular phylogeny. Wiley Online Library; 2022.</mixed-citation></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">Zhong H-S, Dong M-J, F.J.I.S.C.L S, Gao. <italic>G4Bank: A database of experimentally identified DNA G-quadruplex sequences</italic> 2023: p. 1–9.</mixed-citation></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Touati</surname><given-names>R</given-names></name><name><surname>Messaoudi</surname><given-names>I</given-names></name><name><surname>Oueslati</surname><given-names>AE</given-names></name><name><surname>Lachiri</surname><given-names>Z</given-names></name><name><surname>Kharrat</surname><given-names>M</given-names></name></person-group><article-title xml:lang="en">New Intraclass Helitrons classification using DNA-Image sequences and machine learning approaches</article-title><source>IRBM</source><year>2021</year><volume>42</volume><issue>3</issue><fpage>154</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.irbm.2019.12.004</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><mixed-citation publication-type="other">Norlin S. “DNA Seq Classif Using Variable Length Markov Models” 2020.</mixed-citation></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryu</surname><given-names>C</given-names></name><name><surname>Lecroq</surname><given-names>T</given-names></name><name><surname>Park</surname><given-names>K</given-names></name></person-group><article-title xml:lang="en">Fast string matching for DNA sequences</article-title><source>Theor Comput Sci</source><year>2020</year><volume>812</volume><fpage>137</fpage><lpage>48</lpage><pub-id pub-id-type="other" assigning-authority="American Mathematical Society">4066781</pub-id><pub-id pub-id-type="doi">10.1016/j.tcs.2019.09.031</pub-id><pub-id pub-id-type="other" assigning-authority="Zentralblatt MATH">1435.68403</pub-id></mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Ren</surname><given-names>H</given-names></name><name><surname>Lin</surname><given-names>X</given-names></name><name><surname>X.J.I.T.o.C</surname><given-names>C</given-names></name><name><surname>Shen</surname></name></person-group><source>DNA similarity search with access control over encrypted cloud data</source><year>2020</year><volume>10</volume><issue>2</issue><fpage>1233</fpage><lpage>52</lpage></mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Han</surname><given-names>Y</given-names></name></person-group><article-title xml:lang="en">L.J.F.i.B. Zhang, and Biotechnology</article-title><source>Rev application Mach Learn algorithms Seq data Min DNA</source><year>2020</year><volume>8</volume><fpage>1032</fpage></mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Ravikumar M, Prashanth MJC, Cognition. and M.L.A.P.o. ICCCMLA, <italic>Analysis of DNA sequence pattern matching: a brief survey</italic> 2021: p. 221–229.</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millán Arias</surname><given-names>P</given-names></name><name><surname>Alipour</surname><given-names>F</given-names></name><name><surname>Hill</surname><given-names>KA</given-names></name><name><surname>Kari</surname><given-names>LJPo</given-names></name></person-group><source>DeLUCS: Deep learning for unsupervised clustering of DNA sequences</source><year>2022</year><volume>17</volume><issue>1</issue><fpage>e0261531</fpage></mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Rossi F, Paiardini AJCB. <italic>A machine learning perspective on DNA and RNA G-quadruplexes</italic>. 2022. 17(4): p. 305–9.</mixed-citation></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>X</given-names></name><name><surname>Zhu</surname><given-names>T</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Cao</surname><given-names>M</given-names></name><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>L</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Fan</surname><given-names>C</given-names></name><name><surname>Pei</surname><given-names>HJNMI</given-names></name></person-group><source>Mol convolutional neural networks DNA Regul circuits</source><year>2022</year><volume>4</volume><issue>7</issue><fpage>625</fpage><lpage>35</lpage></mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Ibrahim OAS, Hamed BA, El-Hafeez TAbd. <italic>A new fast technique for pattern matching in biological sequences</italic> 2022: p. 1–22.</mixed-citation></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Jukic S, Saracevic M, Subasi A, Kevric JJM. <italic>Comparison of ensemble machine learning methods for automated classification of focal and non-focal epileptic EEG signals</italic>. 2020. 8(9): p. 1481.</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassan</surname><given-names>SU</given-names></name><name><surname>Ahamed</surname><given-names>J</given-names></name><name><surname>Ahmad</surname><given-names>KJSO</given-names></name><name><surname>Computers</surname></name></person-group><source>Analytics of machine learning-based algorithms for text classification</source><year>2022</year><volume>3</volume><fpage>238</fpage><lpage>48</lpage></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Kurani A, Doshi P, Vakharia A, J.A.o.D M. <italic>A comprehensive comparative study of artificial neural network (ANN) and support vector machines (SVM) on stock forecasting</italic>. 2023. 10(1): p. 183–208.</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mondal</surname><given-names>HS</given-names></name><name><surname>Ahmed</surname><given-names>KA</given-names></name><name><surname>Birbilis</surname><given-names>N</given-names></name><name><surname>Hossain</surname><given-names>MZJSR</given-names></name></person-group><source>Mach Learn detecting DNA attachment SPR Biosens</source><year>2023</year><volume>13</volume><issue>1</issue><fpage>3742</fpage></mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alshayeji</surname><given-names>MH</given-names></name><name><surname>S.C.J.E.S.w.A</surname></name><name><surname>Sindhu</surname></name></person-group><source>Viral genome prediction from raw human DNA sequence samples by combining natural language processing and machine learning techniques</source><year>2023</year><volume>218</volume><fpage>119641</fpage></mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Sarkar S, Mridha K, Ghosh A, Shaw RN. <italic>Machine Learning in Bioinformatics: New Technique for DNA Sequencing Classification</italic>, in <italic>Advanced Computing and Intelligent Technologies: Proceedings of ICACIT 2022</italic>. 2022, Springer. p. 335–355.</mixed-citation></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Karr AF, Bowen Z. and A.A.J.a.p.a. Porter, <italic>Structure of Classifier Boundaries: Case Study for a Naive Bayes Classifier</italic> 2022.</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Habib MA, Manik MMH, Khulna B. <italic>Classification of DNA Sequence Using Machine Learning Techniques</italic>. 2022, EasyChair.</mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Khatun ME, Rabeya T. <italic>A Machine Learning Approach for Sentiment Analysis of Book Reviews in Bangla Language</italic>. in 2022 <italic>6th International Conference on Trends in Electronics and Informatics (ICOEI)</italic>. 2022. IEEE.</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kushwah</surname><given-names>JS</given-names></name><name><surname>Kumar</surname><given-names>A</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Soni</surname><given-names>R</given-names></name><name><surname>Gawande</surname><given-names>A</given-names></name><name><surname>Gupta</surname><given-names>SJMTP</given-names></name></person-group><source>Comp study regressor classifier Decis tree using Mod tools</source><year>2022</year><volume>56</volume><fpage>3571</fpage><lpage>6</lpage></mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rivera-Lopez</surname><given-names>R</given-names></name><name><surname>Canul-Reich</surname><given-names>J</given-names></name><name><surname>Mezura-Montes</surname><given-names>E</given-names></name><name><surname>Cruz-Chávez</surname><given-names>MAJS</given-names></name><name><surname>Computation</surname><given-names>E</given-names></name></person-group><source>Induction of decision trees as classification models through metaheuristics</source><year>2022</year><volume>69</volume><fpage>101006</fpage></mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Costa VG, C.E.J.A.I R, Pedreira. <italic>Recent advances in decision trees: An updated survey</italic> 2023. 56(5): p. 4765–4800.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>CS</given-names></name><name><surname>Cheang</surname><given-names>PYS</given-names></name><name><surname>J.A.i.D</surname><given-names>M</given-names></name></person-group><source>Predictive analytics in business analytics: decision tree</source><year>2022</year><volume>26</volume><issue>1</issue><fpage>1</fpage><lpage>29</lpage></mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Bansal M, Goyal A, A.J.D.A.J., Choudhary. <italic>A comparative analysis of K-nearest neighbor, genetic, support vector machine, decision tree, and long short term memory algorithms in machine learning</italic>. 2022. 3: p. 100071.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorabeh</surname><given-names>SN</given-names></name><name><surname>Samany</surname><given-names>NN</given-names></name><name><surname>Minaei</surname><given-names>F</given-names></name><name><surname>Firozjaei</surname><given-names>HK</given-names></name><name><surname>Homaee</surname><given-names>M</given-names></name><name><surname>Boloorani</surname><given-names>ADJRE</given-names></name></person-group><source>Decis model based Decis tree Part swarm Optim algorithms identify optimal locations solar power plants Constr Iran</source><year>2022</year><volume>187</volume><fpage>56</fpage><lpage>67</lpage></mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Ravikumar M, Prashanth M, Guru D. Matching pattern in DNA sequences using machine learning Approach based on K-Mer function, Modern approaches in machine Learning &amp; Cognitive Science: a Walkthrough. 2022, Springer. 159–71.</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Koul N, Manvi SS, Gardiner B. <italic>Method for Classification of Cancers with Partial Least Squares Regression as Feature Selector with Kernel SVM</italic>. in <italic>2022 International Conference for Advancement in Technology (ICONAT)</italic>. 2022. IEEE.</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Manoharan A, Begam K, Aparow VR, J.J.o.E D. <italic>Artificial neural networks, gradient boosting and support Vector Machines for electric vehicle battery state estimation: a review</italic>. 2022. 55: p. 105384.</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Zhang H, Zou Q, Ju Y, Song C, Chen DJCB. <italic>Distance-based support vector machine to predict DNA N6-methyladenine modification</italic>. 2022. 17(5): p. 473–82.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Roy A, Chakraborty SJRE, Safety S. <italic>Support vector machine in structural reliability analysis: A review</italic> 2023: p. 109126.</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Jäger J, Krems RVJNC. <italic>Universal expressiveness of variational quantum classifiers and quantum kernels for support vector machines</italic>. 2023. 14(1): p. 576.</mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Dragomir MP, Calina TG, Perez E, Schallenberg S, Chen M, Albrecht T, Koch I, Wolkenstein P, Goeppert B, Roessler SJE. DNA methylation-based classifier differentiates intrahepatic pancreato-biliary tumours 2023. 93.</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Chadha A, Dara R, Pearl DL, Sharif S, Poljak ZJPVM. <italic>Predictive analysis for pathogenicity classification of H5Nx avian influenza strains using machine learning techniques</italic> 2023. 216: p. 105924.</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Mangkunegara IS, Purwono P. <italic>Analysis of DNA Sequence Classification Using SVM Model with Hyperparameter Tuning Grid Search CV</italic>. in 2022 <italic>IEEE International Conference on Cybernetics and Computational Intelligence (CyberneticsCom)</italic>. 2022. IEEE.</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Andrade-Girón D, Carreño-Cisneros E, Mejía-Dominguez C, Velásquez-Gamarra J, Marín-Rodriguez W, Villarreal-Torres H. R.J.E.E.T.o.P.H. Meleán-Romero, and Technology, <italic>support vector machine with optimized parameters for the classification of patients with COVID-19</italic>. 2023. 9: p. e8–e8.</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">NCBI., <italic>National Center for Biotechnology Information</italic> 2020.</mixed-citation></ref><ref id="CR42"><label>42.</label><mixed-citation publication-type="other">Borjigin C. <italic>Data analysis with Python</italic>, in <italic>Python Data Science</italic>. Springer; 2023. pp. 295–342.</mixed-citation></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Rajamani SK, Iyer RS. <italic>Machine Learning-Based Mobile Applications Using Python and Scikit-Learn</italic>, in <italic>Designing and Developing Innovative Mobile Applications</italic>. 2023, IGI Global. p. 282–306.</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Lavanya A, Gaurav L, Sindhuja S, Seam H, Joydeep M, Uppalapati V, Ali W. Assessing the performance of Python Data visualization libraries: a review. and V.S. SD; 2023.</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valero-Carreras</surname><given-names>D</given-names></name><name><surname>Alcaraz</surname><given-names>J</given-names></name><name><surname>Landete</surname><given-names>MJC</given-names></name><name><surname>Research</surname><given-names>O</given-names></name></person-group><source>Comparing two SVM models through different metrics based on the confusion matrix</source><year>2023</year><volume>152</volume><fpage>106131</fpage></mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Li J, Sun H, Li JJML. <italic>Beyond confusion matrix: learning from multiple annotators with awareness of instance features</italic>. 2023. 112(3): p. 1053–75.</mixed-citation></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>OAS</given-names></name><name><surname>Hamed</surname><given-names>BA</given-names></name><name><surname>El-Hafeez</surname><given-names>TAbd</given-names></name></person-group><source>A new fast technique for pattern matching in biological sequences</source><year>2023</year><volume>79</volume><issue>1</issue><fpage>367</fpage><lpage>88</lpage></mixed-citation></ref></ref-list></ref-list><notes notes-type="Misc"><title>Publisher’s Note</title><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></notes></back></article></records><facets><facet name="subject"><facet-value count="1">Communications Engineering, Networks</facet-value><facet-value count="1">Computational Science and Engineering</facet-value><facet-value count="1">Computer Science</facet-value><facet-value count="1">Data Mining and Knowledge Discovery</facet-value><facet-value count="1">Database Management</facet-value><facet-value count="1">Information Storage and Retrieval</facet-value><facet-value count="1">Mathematical Applications in Computer Science</facet-value></facet><facet name="keyword"><facet-value count="1">Bioinformatics</facet-value><facet-value count="1">DNA sequences</facet-value><facet-value count="1">Feature extraction</facet-value><facet-value count="1">Machine learning</facet-value><facet-value count="1">Pattern matching</facet-value></facet><facet name="pub"><facet-value count="1">Journal of Big Data</facet-value></facet><facet name="year"><facet-value count="1">2023</facet-value></facet><facet name="country"><facet-value count="1">Egypt</facet-value></facet><facet name="type"><facet-value count="1">Journal</facet-value></facet></facets></response>
